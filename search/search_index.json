{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"Welcome! Here you'll find everything you need to know about Anovos ! \ud83e\udded What is Anovos? Anovos is an open source project, built by data scientist for the data science community, that brings automation to the feature engineering process. By rethinking ingestion and transformation, and including deeper analytics, drift identification, and stability analysis, Anovos improves productivity and helps data scientists build more resilient, higher performing models. \ud83d\ude80 Getting Started To get a first impression of Anovos ' capabilities, check out our interactive Getting Started guide . \u2753 Need help? Need a little help? We're here! This documentation provides a thorough introduction to Anovos and includes a comprehensive API documentation . If you have any questions on how to use Anovos or would like to suggest future enhancements, here's how to reach out . \ud83d\udee0 Contributing We're happy to welcome you as an Anovos contributor! Check out our Contributors' page for more information. \ud83d\udd0b What's Powering Anovos? Anovos is built on a curated collection of powerful open source libraries, including: Apache Spark datapane findspark numpy loguru Pandas plotly popmon pyarrow py4j pyaml s3fs seaborn sentence-transformers scipy scikit-learn statsmodels varclushi","title":"Home"},{"location":"index.html#welcome","text":"Here you'll find everything you need to know about Anovos !","title":"Welcome!"},{"location":"index.html#what-is-anovos","text":"Anovos is an open source project, built by data scientist for the data science community, that brings automation to the feature engineering process. By rethinking ingestion and transformation, and including deeper analytics, drift identification, and stability analysis, Anovos improves productivity and helps data scientists build more resilient, higher performing models.","title":"\ud83e\udded What is Anovos?"},{"location":"index.html#getting-started","text":"To get a first impression of Anovos ' capabilities, check out our interactive Getting Started guide .","title":"\ud83d\ude80 Getting Started"},{"location":"index.html#need-help","text":"Need a little help? We're here! This documentation provides a thorough introduction to Anovos and includes a comprehensive API documentation . If you have any questions on how to use Anovos or would like to suggest future enhancements, here's how to reach out .","title":"\u2753 Need help?"},{"location":"index.html#contributing","text":"We're happy to welcome you as an Anovos contributor! Check out our Contributors' page for more information.","title":"\ud83d\udee0 Contributing"},{"location":"index.html#whats-powering-anovos","text":"Anovos is built on a curated collection of powerful open source libraries, including: Apache Spark datapane findspark numpy loguru Pandas plotly popmon pyarrow py4j pyaml s3fs seaborn sentence-transformers scipy scikit-learn statsmodels varclushi","title":"\ud83d\udd0b What's Powering Anovos?"},{"location":"about.html","text":"About Anovos \ud83e\uddd0 Why Anovos? Data science teams spend up to 80% of their time on feature engineering and still end up building models with poor resilience. Anovos seeks to address both these issues (lack of modeler productivity and insufficient model resilience) by enabling data scientists to understand the data they work with and to engineer reproducible and robust features. In turn, these serve as the foundation for the training of resilient models that perform reliably and consistently when deployed. Unlike current feature engineering workflows, which are ad hoc, error-prone, and modeler-driven, Anovos seeks to inject process-driven efficiency into feature creation based on innovations in understanding the stability of data and how data items impact the features they anchor. With Anovos , ML models become more consistent, more accurate, and deliver results faster. At the same time, the process of building models becomes more projectable, saving time and decreasing cost. \ud83d\udc65 Who's behind Anovos? Anovos is built by a team of highly talented and experienced data scientists at Mobilewalla with years of experience in applying ML techniques to some of the most extensive consumer data sets available. Frequently, the team found a need to create novel tools to simplify and speed up the feature engineering process to increase efficiency. Through open sourcing these tools, we share our learnings with the community.","title":"About"},{"location":"about.html#about-anovos","text":"","title":"About Anovos"},{"location":"about.html#why-anovos","text":"Data science teams spend up to 80% of their time on feature engineering and still end up building models with poor resilience. Anovos seeks to address both these issues (lack of modeler productivity and insufficient model resilience) by enabling data scientists to understand the data they work with and to engineer reproducible and robust features. In turn, these serve as the foundation for the training of resilient models that perform reliably and consistently when deployed. Unlike current feature engineering workflows, which are ad hoc, error-prone, and modeler-driven, Anovos seeks to inject process-driven efficiency into feature creation based on innovations in understanding the stability of data and how data items impact the features they anchor. With Anovos , ML models become more consistent, more accurate, and deliver results faster. At the same time, the process of building models becomes more projectable, saving time and decreasing cost.","title":"\ud83e\uddd0 Why Anovos?"},{"location":"about.html#whos-behind-anovos","text":"Anovos is built by a team of highly talented and experienced data scientists at Mobilewalla with years of experience in applying ML techniques to some of the most extensive consumer data sets available. Frequently, the team found a need to create novel tools to simplify and speed up the feature engineering process to increase efficiency. Through open sourcing these tools, we share our learnings with the community.","title":"\ud83d\udc65 Who's behind Anovos?"},{"location":"getting-started.html","text":"Getting Started with Anovos \ud83d\ude80 Anovos provides data scientists and ML engineers with powerful and versatile tools for feature engineering. To get you started quickly, we have prepared an interactive Getting Started Guide . All you need is Docker (see here for instructions how to install it on your machine). The easiest way to try out Anovos and explore its capabilities is through the provided examples that you can run via Docker without the need to install anything on your local machine. # Launch an anovos-examples Docker container sudo docker run -p 8888 :8888 anovos/anovos-examples-3.2.0:latest To reach the Jupyter environment, open the link to http://127.0.0.1:8888/?token... generated by the Jupyter NotebookApp. If you're not familiar with Anovos or feature engineering, the Getting Started with Anovos guide is a good place to begin your journey. You can find it in the /guides folder within the Jupyter environment. For more detailed instructions on how to install Docker and how to troubleshoot potential issues, see the examples README .","title":"Getting Started \ud83d\ude80"},{"location":"getting-started.html#getting-started-with-anovos","text":"Anovos provides data scientists and ML engineers with powerful and versatile tools for feature engineering. To get you started quickly, we have prepared an interactive Getting Started Guide . All you need is Docker (see here for instructions how to install it on your machine). The easiest way to try out Anovos and explore its capabilities is through the provided examples that you can run via Docker without the need to install anything on your local machine. # Launch an anovos-examples Docker container sudo docker run -p 8888 :8888 anovos/anovos-examples-3.2.0:latest To reach the Jupyter environment, open the link to http://127.0.0.1:8888/?token... generated by the Jupyter NotebookApp. If you're not familiar with Anovos or feature engineering, the Getting Started with Anovos guide is a good place to begin your journey. You can find it in the /guides folder within the Jupyter environment. For more detailed instructions on how to install Docker and how to troubleshoot potential issues, see the examples README .","title":"Getting Started with Anovos \ud83d\ude80"},{"location":"license.html","text":"License Copyright 2021-2022 Anovos Developers Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at: http://www.apache.org/licenses/LICENSE-2.0. Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. Apache License Version 2.0, January 2004 http://www.apache.org/licenses/ TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION 1. Definitions. \"License\" shall mean the terms and conditions for use, reproduction, and distribution as defined by Sections 1 through 9 of this document. \"Licensor\" shall mean the copyright owner or entity authorized by the copyright owner that is granting the License. \"Legal Entity\" shall mean the union of the acting entity and all other entities that control, are controlled by, or are under common control with that entity. For the purposes of this definition, \"control\" means (i) the power, direct or indirect, to cause the direction or management of such entity, whether by contract or otherwise, or (ii) ownership of fifty percent (50%) or more of the outstanding shares, or (iii) beneficial ownership of such entity. \"You\" (or \"Your\") shall mean an individual or Legal Entity exercising permissions granted by this License. \"Source\" form shall mean the preferred form for making modifications, including but not limited to software source code, documentation source, and configuration files. \"Object\" form shall mean any form resulting from mechanical transformation or translation of a Source form, including but not limited to compiled object code, generated documentation, and conversions to other media types. \"Work\" shall mean the work of authorship, whether in Source or Object form, made available under the License, as indicated by a copyright notice that is included in or attached to the work (an example is provided in the Appendix below). \"Derivative Works\" shall mean any work, whether in Source or Object form, that is based on (or derived from) the Work and for which the editorial revisions, annotations, elaborations, or other modifications represent, as a whole, an original work of authorship. For the purposes of this License, Derivative Works shall not include works that remain separable from, or merely link (or bind by name) to the interfaces of, the Work and Derivative Works thereof. \"Contribution\" shall mean any work of authorship, including the original version of the Work and any modifications or additions to that Work or Derivative Works thereof, that is intentionally submitted to Licensor for inclusion in the Work by the copyright owner or by an individual or Legal Entity authorized to submit on behalf of the copyright owner. For the purposes of this definition, \"submitted\" means any form of electronic, verbal, or written communication sent to the Licensor or its representatives, including but not limited to communication on electronic mailing lists, source code control systems, and issue tracking systems that are managed by, or on behalf of, the Licensor for the purpose of discussing and improving the Work, but excluding communication that is conspicuously marked or otherwise designated in writing by the copyright owner as \"Not a Contribution.\" \"Contributor\" shall mean Licensor and any individual or Legal Entity on behalf of whom a Contribution has been received by Licensor and subsequently incorporated within the Work. 2. Grant of Copyright License. Subject to the terms and conditions of this License, each Contributor hereby grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable copyright license to reproduce, prepare Derivative Works of, publicly display, publicly perform, sublicense, and distribute the Work and such Derivative Works in Source or Object form. 3. Grant of Patent License. Subject to the terms and conditions of this License, each Contributor hereby grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable (except as stated in this section) patent license to make, have made, use, offer to sell, sell, import, and otherwise transfer the Work, where such license applies only to those patent claims licensable by such Contributor that are necessarily infringed by their Contribution(s) alone or by combination of their Contribution(s) with the Work to which such Contribution(s) was submitted. If You institute patent litigation against any entity (including a cross-claim or counterclaim in a lawsuit) alleging that the Work or a Contribution incorporated within the Work constitutes direct or contributory patent infringement, then any patent licenses granted to You under this License for that Work shall terminate as of the date such litigation is filed. 4. Redistribution. You may reproduce and distribute copies of the Work or Derivative Works thereof in any medium, with or without modifications, and in Source or Object form, provided that You meet the following conditions: (a) You must give any other recipients of the Work or Derivative Works a copy of this License; and (b) You must cause any modified files to carry prominent notices stating that You changed the files; and (c) You must retain, in the Source form of any Derivative Works that You distribute, all copyright, patent, trademark, and attribution notices from the Source form of the Work, excluding those notices that do not pertain to any part of the Derivative Works; and (d) If the Work includes a \"NOTICE\" text file as part of its distribution, then any Derivative Works that You distribute must include a readable copy of the attribution notices contained within such NOTICE file, excluding those notices that do not pertain to any part of the Derivative Works, in at least one of the following places: within a NOTICE text file distributed as part of the Derivative Works; within the Source form or documentation, if provided along with the Derivative Works; or, within a display generated by the Derivative Works, if and wherever such third-party notices normally appear. The contents of the NOTICE file are for informational purposes only and do not modify the License. You may add Your own attribution notices within Derivative Works that You distribute, alongside or as an addendum to the NOTICE text from the Work, provided that such additional attribution notices cannot be construed as modifying the License. You may add Your own copyright statement to Your modifications and may provide additional or different license terms and conditions for use, reproduction, or distribution of Your modifications, or for any such Derivative Works as a whole, provided Your use, reproduction, and distribution of the Work otherwise complies with the conditions stated in this License. 5. Submission of Contributions. Unless You explicitly state otherwise, any Contribution intentionally submitted for inclusion in the Work by You to the Licensor shall be under the terms and conditions of this License, without any additional terms or conditions. Notwithstanding the above, nothing herein shall supersede or modify the terms of any separate license agreement you may have executed with Licensor regarding such Contributions. 6. Trademarks. This License does not grant permission to use the trade names, trademarks, service marks, or product names of the Licensor, except as required for reasonable and customary use in describing the origin of the Work and reproducing the content of the NOTICE file. 7. Disclaimer of Warranty. Unless required by applicable law or agreed to in writing, Licensor provides the Work (and each Contributor provides its Contributions) on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied, including, without limitation, any warranties or conditions of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A PARTICULAR PURPOSE. You are solely responsible for determining the appropriateness of using or redistributing the Work and assume any risks associated with Your exercise of permissions under this License. 8. Limitation of Liability. In no event and under no legal theory, whether in tort (including negligence), contract, or otherwise, unless required by applicable law (such as deliberate and grossly negligent acts) or agreed to in writing, shall any Contributor be liable to You for damages, including any direct, indirect, special, incidental, or consequential damages of any character arising as a result of this License or out of the use or inability to use the Work (including but not limited to damages for loss of goodwill, work stoppage, computer failure or malfunction, or any and all other commercial damages or losses), even if such Contributor has been advised of the possibility of such damages. 9. Accepting Warranty or Additional Liability. While redistributing the Work or Derivative Works thereof, You may choose to offer, and charge a fee for, acceptance of support, warranty, indemnity, or other liability obligations and/or rights consistent with this License. However, in accepting such obligations, You may act only on Your own behalf and on Your sole responsibility, not on behalf of any other Contributor, and only if You agree to indemnify, defend, and hold each Contributor harmless for any liability incurred by, or claims asserted against, such Contributor by reason of your accepting any such warranty or additional liability. END OF TERMS AND CONDITIONS Licenses of third-party dependencies The Anovos project is licensed under the terms of the Apache 2.0 License . However, Anovos builds on several third-party open source libraries that are made available under the following licenses: Java dependencies Apache Spark Apache License Version 2.0, see https://github.com/apache/spark/blob/master/LICENSE Histogrammar Apache License Version 2.0, see https://github.com/histogrammar/histogrammar-scala/blob/master/LICENSE Histogrammar-SparkSQL Apache License Version 2.0, see https://github.com/histogrammar/histogrammar-scala/blob/master/LICENSE Python packages boto3 Apache License Version 2.0, see https://github.com/boto/boto3/blob/develop/LICENSE cython Apache License Version 2.0, see https://github.com/cython/cython/blob/master/LICENSE.txt datapane Apache License Version 2.0, see https://github.com/datapane/datapane/blob/master/LICENSE findspark BSD 3-Clause License, see https://github.com/minrk/findspark/blob/main/LICENSE.md fsspec BSD 3-Clause License, see https://github.com/fsspec/filesystem_spec/blob/master/LICENSE loguru MIT License, see https://github.com/Delgan/loguru/blob/master/LICENSE matplotlib License agreement for matplotlib versions 1.3.0 and later, see https://github.com/matplotlib/matplotlib/blob/main/LICENSE/LICENSE numpy BSD 3-Clause License, see https://github.com/numpy/numpy/blob/main/LICENSE.txt pandas BSD 3-Clause License, see https://github.com/pandas-dev/pandas/blob/master/LICENSE plotly MIT License, see https://github.com/plotly/plotly.py/blob/master/LICENSE.txt popmon MIT License, see https://github.com/ing-bank/popmon/blob/master/LICENSE py4j BSD 3-Clause License, see https://github.com/py4j/py4j/blob/master/LICENSE.txt pyaml MIT License, see https://github.com/yaml/pyyaml/blob/master/LICENSE pyarrow Apache License Version 2.0, see https://github.com/apache/arrow/blob/master/LICENSE.txt pybind11 BSD 3-Clause License, see https://github.com/pybind/pybind11/blob/master/LICENSE s3fs BSD 3-Clause License, see https://github.com/fsspec/s3fs/blob/main/LICENSE.txt s3path Apache License Version 2.0, see https://github.com/liormizr/s3path/blob/master/LICENSE scikit-learn BSD 3-Clause License, see https://github.com/scikit-learn/scikit-learn/blob/main/COPYING scipy BSD 3-Clause License, see https://github.com/scipy/scipy/blob/master/LICENSE.txt sentence-transformers Apache License Version 2.0, see https://github.com/UKPLab/sentence-transformers/blob/master/LICENSE soupsieve MIT License, see https://github.com/facelessuser/soupsieve/blob/main/LICENSE.md sympy BSD 3-Clause License, see https://github.com/sympy/sympy/blob/master/LICENSE Note that some submodules are published under different Licenses. varclushi GNU General Public License 3.0, see https://github.com/jingtt/varclushi/blob/master/LICENSE","title":"License"},{"location":"license.html#license","text":"Copyright 2021-2022 Anovos Developers Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at: http://www.apache.org/licenses/LICENSE-2.0. Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. Apache License Version 2.0, January 2004 http://www.apache.org/licenses/ TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION 1. Definitions. \"License\" shall mean the terms and conditions for use, reproduction, and distribution as defined by Sections 1 through 9 of this document. \"Licensor\" shall mean the copyright owner or entity authorized by the copyright owner that is granting the License. \"Legal Entity\" shall mean the union of the acting entity and all other entities that control, are controlled by, or are under common control with that entity. For the purposes of this definition, \"control\" means (i) the power, direct or indirect, to cause the direction or management of such entity, whether by contract or otherwise, or (ii) ownership of fifty percent (50%) or more of the outstanding shares, or (iii) beneficial ownership of such entity. \"You\" (or \"Your\") shall mean an individual or Legal Entity exercising permissions granted by this License. \"Source\" form shall mean the preferred form for making modifications, including but not limited to software source code, documentation source, and configuration files. \"Object\" form shall mean any form resulting from mechanical transformation or translation of a Source form, including but not limited to compiled object code, generated documentation, and conversions to other media types. \"Work\" shall mean the work of authorship, whether in Source or Object form, made available under the License, as indicated by a copyright notice that is included in or attached to the work (an example is provided in the Appendix below). \"Derivative Works\" shall mean any work, whether in Source or Object form, that is based on (or derived from) the Work and for which the editorial revisions, annotations, elaborations, or other modifications represent, as a whole, an original work of authorship. For the purposes of this License, Derivative Works shall not include works that remain separable from, or merely link (or bind by name) to the interfaces of, the Work and Derivative Works thereof. \"Contribution\" shall mean any work of authorship, including the original version of the Work and any modifications or additions to that Work or Derivative Works thereof, that is intentionally submitted to Licensor for inclusion in the Work by the copyright owner or by an individual or Legal Entity authorized to submit on behalf of the copyright owner. For the purposes of this definition, \"submitted\" means any form of electronic, verbal, or written communication sent to the Licensor or its representatives, including but not limited to communication on electronic mailing lists, source code control systems, and issue tracking systems that are managed by, or on behalf of, the Licensor for the purpose of discussing and improving the Work, but excluding communication that is conspicuously marked or otherwise designated in writing by the copyright owner as \"Not a Contribution.\" \"Contributor\" shall mean Licensor and any individual or Legal Entity on behalf of whom a Contribution has been received by Licensor and subsequently incorporated within the Work. 2. Grant of Copyright License. Subject to the terms and conditions of this License, each Contributor hereby grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable copyright license to reproduce, prepare Derivative Works of, publicly display, publicly perform, sublicense, and distribute the Work and such Derivative Works in Source or Object form. 3. Grant of Patent License. Subject to the terms and conditions of this License, each Contributor hereby grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable (except as stated in this section) patent license to make, have made, use, offer to sell, sell, import, and otherwise transfer the Work, where such license applies only to those patent claims licensable by such Contributor that are necessarily infringed by their Contribution(s) alone or by combination of their Contribution(s) with the Work to which such Contribution(s) was submitted. If You institute patent litigation against any entity (including a cross-claim or counterclaim in a lawsuit) alleging that the Work or a Contribution incorporated within the Work constitutes direct or contributory patent infringement, then any patent licenses granted to You under this License for that Work shall terminate as of the date such litigation is filed. 4. Redistribution. You may reproduce and distribute copies of the Work or Derivative Works thereof in any medium, with or without modifications, and in Source or Object form, provided that You meet the following conditions: (a) You must give any other recipients of the Work or Derivative Works a copy of this License; and (b) You must cause any modified files to carry prominent notices stating that You changed the files; and (c) You must retain, in the Source form of any Derivative Works that You distribute, all copyright, patent, trademark, and attribution notices from the Source form of the Work, excluding those notices that do not pertain to any part of the Derivative Works; and (d) If the Work includes a \"NOTICE\" text file as part of its distribution, then any Derivative Works that You distribute must include a readable copy of the attribution notices contained within such NOTICE file, excluding those notices that do not pertain to any part of the Derivative Works, in at least one of the following places: within a NOTICE text file distributed as part of the Derivative Works; within the Source form or documentation, if provided along with the Derivative Works; or, within a display generated by the Derivative Works, if and wherever such third-party notices normally appear. The contents of the NOTICE file are for informational purposes only and do not modify the License. You may add Your own attribution notices within Derivative Works that You distribute, alongside or as an addendum to the NOTICE text from the Work, provided that such additional attribution notices cannot be construed as modifying the License. You may add Your own copyright statement to Your modifications and may provide additional or different license terms and conditions for use, reproduction, or distribution of Your modifications, or for any such Derivative Works as a whole, provided Your use, reproduction, and distribution of the Work otherwise complies with the conditions stated in this License. 5. Submission of Contributions. Unless You explicitly state otherwise, any Contribution intentionally submitted for inclusion in the Work by You to the Licensor shall be under the terms and conditions of this License, without any additional terms or conditions. Notwithstanding the above, nothing herein shall supersede or modify the terms of any separate license agreement you may have executed with Licensor regarding such Contributions. 6. Trademarks. This License does not grant permission to use the trade names, trademarks, service marks, or product names of the Licensor, except as required for reasonable and customary use in describing the origin of the Work and reproducing the content of the NOTICE file. 7. Disclaimer of Warranty. Unless required by applicable law or agreed to in writing, Licensor provides the Work (and each Contributor provides its Contributions) on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied, including, without limitation, any warranties or conditions of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A PARTICULAR PURPOSE. You are solely responsible for determining the appropriateness of using or redistributing the Work and assume any risks associated with Your exercise of permissions under this License. 8. Limitation of Liability. In no event and under no legal theory, whether in tort (including negligence), contract, or otherwise, unless required by applicable law (such as deliberate and grossly negligent acts) or agreed to in writing, shall any Contributor be liable to You for damages, including any direct, indirect, special, incidental, or consequential damages of any character arising as a result of this License or out of the use or inability to use the Work (including but not limited to damages for loss of goodwill, work stoppage, computer failure or malfunction, or any and all other commercial damages or losses), even if such Contributor has been advised of the possibility of such damages. 9. Accepting Warranty or Additional Liability. While redistributing the Work or Derivative Works thereof, You may choose to offer, and charge a fee for, acceptance of support, warranty, indemnity, or other liability obligations and/or rights consistent with this License. However, in accepting such obligations, You may act only on Your own behalf and on Your sole responsibility, not on behalf of any other Contributor, and only if You agree to indemnify, defend, and hold each Contributor harmless for any liability incurred by, or claims asserted against, such Contributor by reason of your accepting any such warranty or additional liability. END OF TERMS AND CONDITIONS","title":"License"},{"location":"license.html#licenses-of-third-party-dependencies","text":"The Anovos project is licensed under the terms of the Apache 2.0 License . However, Anovos builds on several third-party open source libraries that are made available under the following licenses:","title":"Licenses of third-party dependencies"},{"location":"license.html#java-dependencies","text":"","title":"Java dependencies"},{"location":"license.html#apache-spark","text":"Apache License Version 2.0, see https://github.com/apache/spark/blob/master/LICENSE","title":"Apache Spark"},{"location":"license.html#histogrammar","text":"Apache License Version 2.0, see https://github.com/histogrammar/histogrammar-scala/blob/master/LICENSE","title":"Histogrammar"},{"location":"license.html#histogrammar-sparksql","text":"Apache License Version 2.0, see https://github.com/histogrammar/histogrammar-scala/blob/master/LICENSE","title":"Histogrammar-SparkSQL"},{"location":"license.html#python-packages","text":"","title":"Python packages"},{"location":"license.html#boto3","text":"Apache License Version 2.0, see https://github.com/boto/boto3/blob/develop/LICENSE","title":"boto3"},{"location":"license.html#cython","text":"Apache License Version 2.0, see https://github.com/cython/cython/blob/master/LICENSE.txt","title":"cython"},{"location":"license.html#datapane","text":"Apache License Version 2.0, see https://github.com/datapane/datapane/blob/master/LICENSE","title":"datapane"},{"location":"license.html#findspark","text":"BSD 3-Clause License, see https://github.com/minrk/findspark/blob/main/LICENSE.md","title":"findspark"},{"location":"license.html#fsspec","text":"BSD 3-Clause License, see https://github.com/fsspec/filesystem_spec/blob/master/LICENSE","title":"fsspec"},{"location":"license.html#loguru","text":"MIT License, see https://github.com/Delgan/loguru/blob/master/LICENSE","title":"loguru"},{"location":"license.html#matplotlib","text":"License agreement for matplotlib versions 1.3.0 and later, see https://github.com/matplotlib/matplotlib/blob/main/LICENSE/LICENSE","title":"matplotlib"},{"location":"license.html#numpy","text":"BSD 3-Clause License, see https://github.com/numpy/numpy/blob/main/LICENSE.txt","title":"numpy"},{"location":"license.html#pandas","text":"BSD 3-Clause License, see https://github.com/pandas-dev/pandas/blob/master/LICENSE","title":"pandas"},{"location":"license.html#plotly","text":"MIT License, see https://github.com/plotly/plotly.py/blob/master/LICENSE.txt","title":"plotly"},{"location":"license.html#popmon","text":"MIT License, see https://github.com/ing-bank/popmon/blob/master/LICENSE","title":"popmon"},{"location":"license.html#py4j","text":"BSD 3-Clause License, see https://github.com/py4j/py4j/blob/master/LICENSE.txt","title":"py4j"},{"location":"license.html#pyaml","text":"MIT License, see https://github.com/yaml/pyyaml/blob/master/LICENSE","title":"pyaml"},{"location":"license.html#pyarrow","text":"Apache License Version 2.0, see https://github.com/apache/arrow/blob/master/LICENSE.txt","title":"pyarrow"},{"location":"license.html#pybind11","text":"BSD 3-Clause License, see https://github.com/pybind/pybind11/blob/master/LICENSE","title":"pybind11"},{"location":"license.html#s3fs","text":"BSD 3-Clause License, see https://github.com/fsspec/s3fs/blob/main/LICENSE.txt","title":"s3fs"},{"location":"license.html#s3path","text":"Apache License Version 2.0, see https://github.com/liormizr/s3path/blob/master/LICENSE","title":"s3path"},{"location":"license.html#scikit-learn","text":"BSD 3-Clause License, see https://github.com/scikit-learn/scikit-learn/blob/main/COPYING","title":"scikit-learn"},{"location":"license.html#scipy","text":"BSD 3-Clause License, see https://github.com/scipy/scipy/blob/master/LICENSE.txt","title":"scipy"},{"location":"license.html#sentence-transformers","text":"Apache License Version 2.0, see https://github.com/UKPLab/sentence-transformers/blob/master/LICENSE","title":"sentence-transformers"},{"location":"license.html#soupsieve","text":"MIT License, see https://github.com/facelessuser/soupsieve/blob/main/LICENSE.md","title":"soupsieve"},{"location":"license.html#sympy","text":"BSD 3-Clause License, see https://github.com/sympy/sympy/blob/master/LICENSE Note that some submodules are published under different Licenses.","title":"sympy"},{"location":"license.html#varclushi","text":"GNU General Public License 3.0, see https://github.com/jingtt/varclushi/blob/master/LICENSE","title":"varclushi"},{"location":"api/_index.html","text":"Overview Expand source code from .version import __version__ Sub-modules anovos.data_analyzer anovos.data_ingest anovos.data_report anovos.data_transformer anovos.drift anovos.feature_recommender anovos.shared anovos.version","title":"Overview"},{"location":"api/_index.html#overview","text":"Expand source code from .version import __version__","title":"Overview"},{"location":"api/_index.html#sub-modules","text":"anovos.data_analyzer anovos.data_ingest anovos.data_report anovos.data_transformer anovos.drift anovos.feature_recommender anovos.shared anovos.version","title":"Sub-modules"},{"location":"api/data_analyzer/_index.html","text":"Overview Sub-modules anovos.data_analyzer.association_evaluator This submodule focuses on understanding the interaction between different attributes and/or the relationship between an attribute & the binary target \u2026 anovos.data_analyzer.quality_checker This submodule focuses on assessing the data quality at both row-level and column-level and also provides an appropriate treatment option to fix \u2026 anovos.data_analyzer.stats_generator This module generates all the descriptive statistics related to the ingested data. Descriptive statistics are split into different metric types, and \u2026 anovos.data_analyzer.ts_analyzer This module generates the intermediate output specific to the inspection of Time series analysis \u2026","title":"Overview"},{"location":"api/data_analyzer/_index.html#overview","text":"","title":"Overview"},{"location":"api/data_analyzer/_index.html#sub-modules","text":"anovos.data_analyzer.association_evaluator This submodule focuses on understanding the interaction between different attributes and/or the relationship between an attribute & the binary target \u2026 anovos.data_analyzer.quality_checker This submodule focuses on assessing the data quality at both row-level and column-level and also provides an appropriate treatment option to fix \u2026 anovos.data_analyzer.stats_generator This module generates all the descriptive statistics related to the ingested data. Descriptive statistics are split into different metric types, and \u2026 anovos.data_analyzer.ts_analyzer This module generates the intermediate output specific to the inspection of Time series analysis \u2026","title":"Sub-modules"},{"location":"api/data_analyzer/association_evaluator.html","text":"association_evaluator This submodule focuses on understanding the interaction between different attributes and/or the relationship between an attribute & the binary target variable. Association between attributes is measured by: - correlation_matrix - variable_clustering Association between an attribute and binary target is measured by: - IV_calculation - IG_calculation Expand source code # coding=utf-8 \"\"\" This submodule focuses on understanding the interaction between different attributes and/or the relationship between an attribute & the binary target variable. Association between attributes is measured by: - correlation_matrix - variable_clustering Association between an attribute and binary target is measured by: - IV_calculation - IG_calculation \"\"\" import itertools import math import pyspark from phik.phik import spark_phik_matrix_from_hist2d_dict from popmon.analysis.hist_numpy import get_2dgrid from pyspark.sql import Window from pyspark.sql import functions as F from varclushi import VarClusHi from anovos.data_analyzer.stats_generator import uniqueCount_computation from anovos.data_ingest.data_ingest import read_dataset from anovos.data_transformer.transformers import ( attribute_binning , monotonic_binning , cat_to_num_unsupervised , imputation_MMM , ) from anovos.shared.utils import attributeType_segregation def correlation_matrix ( spark , idf , list_of_cols = \"all\" , drop_cols = [], stats_unique = {}, print_impact = False ): \"\"\" This function calculates correlation coefficient statistical, which measures the strength of the relationship between the relative movements of two attributes. Pearson\u2019s correlation coefficient is a standard approach of measuring correlation between two variables. However, it has some drawbacks: a) It works only with continuous variables, b) It only accounts for a linear relationship between variables, and c) It is sensitive to outliers. To avoid these issues, we are computing Phik (\ud835\udf19k), which is a new and practical correlation coefficient that works consistently between categorical, ordinal and interval variables, captures non-linear dependency and reverts to the Pearson correlation coefficient in case of a bivariate normal input distribution. The correlation coefficient is calculated for every pair of attributes and its value lies between 0 and 1, where 0 means there is no correlation between the two attributes and 1 means strong correlation. However, this methodology have drawbacks of its own as it is found to be more computational expensive especially when number of columns in the input dataset is on higher side (number of pairs to analyse increases exponentially with number of columns). Further, there is no indication of the direction of the relationship. More detail can be referred from the [ source paper] [1]. [1]: https://arxiv.org/abs/1811.11440/ \"source paper\" This function returns a correlation matrix dataframe of schema \u2013 attribute, <attribute_names>. Correlation between attribute X and Y can be found at intersection of a) row with value X in \u2018attribute\u2019 column and b) column \u2018Y\u2019 (or row with value Y in \u2018attribute\u2019 column and column \u2018X\u2019). Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of columns to analyse e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) stats_unique Takes arguments for read_dataset (data_ingest module) function in a dictionary format to read pre-saved statistics on unique value count i.e. if measures_of_cardinality or uniqueCount_computation (data_analyzer.stats_generator module) has been computed & saved before. (Default value = {}) print_impact True, False This argument is to print out the statistics.(Default value = False) Returns ------- DataFrame [attribute,*attribute_names] \"\"\" if list_of_cols == \"all\" : num_cols , cat_cols , other_cols = attributeType_segregation ( idf ) list_of_cols = num_cols + cat_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] if stats_unique == {}: remove_cols = ( uniqueCount_computation ( spark , idf , list_of_cols ) . where ( F . col ( \"unique_values\" ) < 2 ) . select ( \"attribute\" ) . rdd . flatMap ( lambda x : x ) . collect () ) else : remove_cols = ( read_dataset ( spark , ** stats_unique ) . where ( F . col ( \"unique_values\" ) < 2 ) . select ( \"attribute\" ) . rdd . flatMap ( lambda x : x ) . collect () ) list_of_cols = list ( set ([ e for e in list_of_cols if e not in ( drop_cols + remove_cols )]) ) if any ( x not in idf . columns for x in list_of_cols ) | ( len ( list_of_cols ) == 0 ): raise TypeError ( \"Invalid input for Column(s)\" ) combis = [ list ( c ) for c in itertools . combinations_with_replacement ( list_of_cols , 2 )] hists = idf . select ( list_of_cols ) . pm_make_histograms ( combis ) grids = { k : get_2dgrid ( h ) for k , h in hists . items ()} odf_pd = spark_phik_matrix_from_hist2d_dict ( spark . sparkContext , grids ) odf_pd [ \"attribute\" ] = odf_pd . index list_of_cols . sort () odf = ( spark . createDataFrame ( odf_pd ) . select ([ \"attribute\" ] + list_of_cols ) . orderBy ( \"attribute\" ) ) if print_impact : odf . show ( odf . count ()) return odf def variable_clustering ( spark , idf , list_of_cols = \"all\" , drop_cols = [], sample_size = 100000 , stats_unique = {}, stats_mode = {}, print_impact = False , ): \"\"\" Variable Clustering groups attributes that are as correlated as possible among themselves within a cluster and as uncorrelated as possible with attribute in other clusters. The function is leveraging [VarClusHi] [2] library to do variable clustering; however, this library is not implemented in a scalable manner due to which the analysis is done on a sample dataset. Further, it is found to be a bit computational expensive especially when number of columns in the input dataset is on higher side (number of pairs to analyse increases exponentially with number of columns). [2]: https://github.com/jingtt/varclushi \"VarCluShi\" It returns a Spark Dataframe with schema \u2013 Cluster, Attribute, RS_Ratio. Attributes similar to each other are grouped together with the same cluster id. The attribute with the lowest (1 \u2014 RS_Ratio) can be chosen as a representative of the cluster while discarding the other attributes from that cluster. This can also help in achieving the dimension reduction, if required. Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of columns to analyse e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) sample_size Maximum sample size (in terms of number of rows) taken for the computation. Sample dataset is extracted using random sampling. (Default value = 100000) stats_unique Takes arguments for read_dataset (data_ingest module) function in a dictionary format to read pre-saved statistics on unique value count i.e. if measures_of_cardinality or uniqueCount_computation (data_analyzer.stats_generator module) has been computed & saved before. This is used to remove single value columns from the analysis purpose. (Default value = {}) stats_mode Takes arguments for read_dataset (data_ingest module) function in a dictionary format to read pre-saved statistics on most frequently seen values i.e. if measures_of_centralTendency or mode_computation (data_analyzer.stats_generator module) has been computed & saved before. This is used for MMM imputation as Variable Clustering doesn\u2019t work with missing values. (Default value = {}) print_impact True, False This argument is to print out the statistics.(Default value = False) Returns ------- DataFrame [Cluster, Attribute, RS_Ratio] \"\"\" if list_of_cols == \"all\" : num_cols , cat_cols , other_cols = attributeType_segregation ( idf ) list_of_cols = num_cols + cat_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in drop_cols ])) if any ( x not in idf . columns for x in list_of_cols ) | ( len ( list_of_cols ) == 0 ): raise TypeError ( \"Invalid input for Column(s)\" ) idf_sample = idf . sample ( False , min ( 1.0 , float ( sample_size ) / idf . count ()), 0 ) idf_sample . persist ( pyspark . StorageLevel . MEMORY_AND_DISK ) . count () if stats_unique == {}: remove_cols = ( uniqueCount_computation ( spark , idf_sample , list_of_cols ) . where ( F . col ( \"unique_values\" ) < 2 ) . select ( \"attribute\" ) . rdd . flatMap ( lambda x : x ) . collect () ) else : remove_cols = ( read_dataset ( spark , ** stats_unique ) . where ( F . col ( \"unique_values\" ) < 2 ) . select ( \"attribute\" ) . rdd . flatMap ( lambda x : x ) . collect () ) list_of_cols = [ e for e in list_of_cols if e not in remove_cols ] idf_sample = idf_sample . select ( list_of_cols ) num_cols , cat_cols , other_cols = attributeType_segregation ( idf_sample ) for i in idf_sample . dtypes : if i [ 1 ] . startswith ( \"decimal\" ): idf_sample = idf_sample . withColumn ( i [ 0 ], F . col ( i [ 0 ]) . cast ( \"double\" )) idf_encoded = cat_to_num_unsupervised ( spark , idf_sample , list_of_cols = cat_cols , method_type = 1 ) idf_imputed = imputation_MMM ( spark , idf_encoded , stats_mode = stats_mode ) idf_imputed . persist ( pyspark . StorageLevel . MEMORY_AND_DISK ) . count () idf_sample . unpersist () idf_pd = idf_imputed . toPandas () vc = VarClusHi ( idf_pd , maxeigval2 = 1 , maxclus = None ) vc . varclus () odf_pd = vc . rsquare odf = spark . createDataFrame ( odf_pd ) . select ( \"Cluster\" , F . col ( \"Variable\" ) . alias ( \"Attribute\" ), F . round ( F . col ( \"RS_Ratio\" ), 4 ) . alias ( \"RS_Ratio\" ), ) if print_impact : odf . show ( odf . count ()) return odf def IV_calculation ( spark , idf , list_of_cols = \"all\" , drop_cols = [], label_col = \"label\" , event_label = 1 , encoding_configs = { \"bin_method\" : \"equal_frequency\" , \"bin_size\" : 10 , \"monotonicity_check\" : 0 , }, print_impact = False , ): \"\"\" Information Value (IV) is simple and powerful technique to conduct attribute relevance analysis. It measures how well an attribute is able to distinguish between a binary target variable i.e. label 0 from label 1, and hence helps in ranking attributes on the basis of their importance. In the heart of IV methodology are groups (bins) of observations. For categorical attributes, usually each category is a bin while numerical attributes need to be split into categories. IV = \u2211 (% of non-events - % of events) * WOE <br>where: <br>WOE = In(% of non-events \u2797 % of events) <br>% of event = % label 1 in a bin <br>% of non-event = % label 0 in a bin General rule of thumb while creating the bins are that a) each bin should have at least 5% of the observations, b) the WOE should be monotonic, i.e. either growing or decreasing with the bins, and c) missing values should be binned separately. An article from listendata.com can be referred for good understanding of IV & WOE concepts. Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of columns to analyse e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) label_col Label/Target column (Default value = \"label\") event_label Value of (positive) event (i.e label 1) (Default value = 1) encoding_configs Takes input in dictionary format. {} i.e. empty dict means no encoding is required. In case numerical columns are present and encoding is required, following keys shall be provided - \"bin_size\" (Default value = 10) i.e. no. of bins for converting the numerical columns to categorical, \"bin_method\" i.e. method of binning - \"equal_frequency\" or \"equal_range\" (Default value = \"equal_frequency\") and \"monotonicity_check\" 1 for monotonic binning else 0. monotonicity_check of 1 will dynamically calculate the bin_size ensuring monotonic nature but can be expensive operation (Default value = 0). print_impact True, False This argument is to print out the statistics.(Default value = False) Returns ------- DataFrame [attribute, iv] \"\"\" if label_col not in idf . columns : raise TypeError ( \"Invalid input for Label Column\" ) if list_of_cols == \"all\" : num_cols , cat_cols , other_cols = attributeType_segregation ( idf ) list_of_cols = num_cols + cat_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in ( drop_cols + [ label_col ])]) ) if any ( x not in idf . columns for x in list_of_cols ) | ( len ( list_of_cols ) == 0 ): raise TypeError ( \"Invalid input for Column(s)\" ) if idf . where ( F . col ( label_col ) == event_label ) . count () == 0 : raise TypeError ( \"Invalid input for Event Label Value\" ) num_cols , cat_cols , other_cols = attributeType_segregation ( idf . select ( list_of_cols )) if ( len ( num_cols ) > 0 ) & bool ( encoding_configs ): bin_size = encoding_configs [ \"bin_size\" ] bin_method = encoding_configs [ \"bin_method\" ] monotonicity_check = encoding_configs [ \"monotonicity_check\" ] if monotonicity_check == 1 : idf_encoded = monotonic_binning ( spark , idf , num_cols , [], label_col , event_label , bin_method , bin_size ) else : idf_encoded = attribute_binning ( spark , idf , num_cols , [], bin_method , bin_size ) idf_encoded . persist ( pyspark . StorageLevel . MEMORY_AND_DISK ) . count () else : idf_encoded = idf output = [] for col in list_of_cols : df_iv = ( idf_encoded . groupBy ( col , label_col ) . count () . withColumn ( label_col , F . when ( F . col ( label_col ) == event_label , 1 ) . otherwise ( 0 ) ) . groupBy ( col ) . pivot ( label_col ) . sum ( \"count\" ) . fillna ( 0.5 ) . withColumn ( \"event_pct\" , F . col ( \"1\" ) / F . sum ( \"1\" ) . over ( Window . partitionBy ())) . withColumn ( \"nonevent_pct\" , F . col ( \"0\" ) / F . sum ( \"0\" ) . over ( Window . partitionBy ()) ) . withColumn ( \"iv\" , ( F . col ( \"nonevent_pct\" ) - F . col ( \"event_pct\" )) * F . log ( F . col ( \"nonevent_pct\" ) / F . col ( \"event_pct\" )), ) ) iv_value = df_iv . select ( F . sum ( \"iv\" )) . collect ()[ 0 ][ 0 ] output . append ([ col , iv_value ]) odf = ( spark . createDataFrame ( output , [ \"attribute\" , \"iv\" ]) . withColumn ( \"iv\" , F . round ( F . col ( \"iv\" ), 4 )) . orderBy ( F . desc ( \"iv\" )) ) if print_impact : odf . show ( odf . count ()) return odf def IG_calculation ( spark , idf , list_of_cols = \"all\" , drop_cols = [], label_col = \"label\" , event_label = 1 , encoding_configs = { \"bin_method\" : \"equal_frequency\" , \"bin_size\" : 10 , \"monotonicity_check\" : 0 , }, print_impact = False , ): \"\"\" Information Gain (IG) is another powerful technique for feature selection analysis. Information gain is calculated by comparing the entropy of the dataset before and after a transformation (introduction of attribute in this particular case). Similar to IV calculation, each category is a bin for categorical attributes, while numerical attributes need to be split into categories. IG = Total Entropy \u2013 Entropy Total Entropy= -%event*log\u2061(%event)-(1-%event)*log\u2061(1-%event) Entropy = \u2211(-%\u3016event\u3017_i*log\u2061(%\u3016event\u3017_i )-(1-%\u3016event\u3017_i )*log\u2061(1-%\u3016event\u3017_i) Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of columns to analyse e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) label_col Label/Target column (Default value = \"label\") event_label Value of (positive) event (i.e label 1) (Default value = 1) encoding_configs Takes input in dictionary format. {} i.e. empty dict means no encoding is required. In case numerical columns are present and encoding is required, following keys shall be provided - \"bin_size\" (Default value = 10) i.e. no. of bins for converting the numerical columns to categorical, \"bin_method\" i.e. method of binning - \"equal_frequency\" or \"equal_range\" (Default value = \"equal_frequency\") and \"monotonicity_check\" 1 for monotonic binning else 0. monotonicity_check of 1 will dynamically calculate the bin_size ensuring monotonic nature but can be expensive operation (Default value = 0). print_impact True, False This argument is to print out the statistics.(Default value = False) Returns ------- DataFrame [attribute, id] \"\"\" if label_col not in idf . columns : raise TypeError ( \"Invalid input for Label Column\" ) if list_of_cols == \"all\" : num_cols , cat_cols , other_cols = attributeType_segregation ( idf ) list_of_cols = num_cols + cat_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in ( drop_cols + [ label_col ])]) ) if any ( x not in idf . columns for x in list_of_cols ) | ( len ( list_of_cols ) == 0 ): raise TypeError ( \"Invalid input for Column(s)\" ) if idf . where ( F . col ( label_col ) == event_label ) . count () == 0 : raise TypeError ( \"Invalid input for Event Label Value\" ) num_cols , cat_cols , other_cols = attributeType_segregation ( idf . select ( list_of_cols )) if ( len ( num_cols ) > 0 ) & bool ( encoding_configs ): bin_size = encoding_configs [ \"bin_size\" ] bin_method = encoding_configs [ \"bin_method\" ] monotonicity_check = encoding_configs [ \"monotonicity_check\" ] if monotonicity_check == 1 : idf_encoded = monotonic_binning ( spark , idf , num_cols , [], label_col , event_label , bin_method , bin_size ) else : idf_encoded = attribute_binning ( spark , idf , num_cols , [], bin_method , bin_size ) idf_encoded . persist ( pyspark . StorageLevel . MEMORY_AND_DISK ) . count () else : idf_encoded = idf output = [] total_event = idf . where ( F . col ( label_col ) == event_label ) . count () / idf . count () total_entropy = - ( total_event * math . log2 ( total_event ) + (( 1 - total_event ) * math . log2 (( 1 - total_event ))) ) for col in list_of_cols : idf_entropy = ( idf_encoded . withColumn ( label_col , F . when ( F . col ( label_col ) == event_label , 1 ) . otherwise ( 0 ) ) . groupBy ( col ) . agg ( F . sum ( F . col ( label_col )) . alias ( \"event_count\" ), F . count ( F . col ( label_col )) . alias ( \"total_count\" ), ) . dropna () . withColumn ( \"event_pct\" , F . col ( \"event_count\" ) / F . col ( \"total_count\" )) . withColumn ( \"segment_pct\" , F . col ( \"total_count\" ) / F . sum ( \"total_count\" ) . over ( Window . partitionBy ()), ) . withColumn ( \"entropy\" , - F . col ( \"segment_pct\" ) * ( ( F . col ( \"event_pct\" ) * F . log2 ( F . col ( \"event_pct\" ))) + (( 1 - F . col ( \"event_pct\" )) * F . log2 (( 1 - F . col ( \"event_pct\" )))) ), ) ) entropy = ( idf_entropy . groupBy () . sum ( \"entropy\" ) . rdd . flatMap ( lambda x : x ) . collect ()[ 0 ] ) ig_value = total_entropy - entropy if entropy else None output . append ([ col , ig_value ]) odf = ( spark . createDataFrame ( output , [ \"attribute\" , \"ig\" ]) . withColumn ( \"ig\" , F . round ( F . col ( \"ig\" ), 4 )) . orderBy ( F . desc ( \"ig\" )) ) if print_impact : odf . show ( odf . count ()) return odf Functions def IG_calculation ( spark, idf, list_of_cols='all', drop_cols=[], label_col='label', event_label=1, encoding_configs={'bin_method': 'equal_frequency', 'bin_size': 10, 'monotonicity_check': 0}, print_impact=False) Information Gain (IG) is another powerful technique for feature selection analysis. Information gain is calculated by comparing the entropy of the dataset before and after a transformation (introduction of attribute in this particular case). Similar to IV calculation, each category is a bin for categorical attributes, while numerical attributes need to be split into categories. IG = Total Entropy \u2013 Entropy Total Entropy= -%event log\u2061(%event)-(1-%event) log\u2061(1-%event) Entropy = \u2211(-%\u3016event\u3017_i log\u2061(%\u3016event\u3017_i )-(1-%\u3016event\u3017_i ) log\u2061(1-%\u3016event\u3017_i) Parameters spark Spark Session idf Input Dataframe list_of_cols List of columns to analyse e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) label_col Label/Target column (Default value = \"label\") event_label Value of (positive) event (i.e label 1) (Default value = 1) encoding_configs Takes input in dictionary format. {} i.e. empty dict means no encoding is required. In case numerical columns are present and encoding is required, following keys shall be provided - \"bin_size\" (Default value = 10) i.e. no. of bins for converting the numerical columns to categorical, \"bin_method\" i.e. method of binning - \"equal_frequency\" or \"equal_range\" (Default value = \"equal_frequency\") and \"monotonicity_check\" 1 for monotonic binning else 0. monotonicity_check of 1 will dynamically calculate the bin_size ensuring monotonic nature but can be expensive operation (Default value = 0). print_impact True, False This argument is to print out the statistics.(Default value = False) Returns DataFrame [attribute, id] Expand source code def IG_calculation ( spark , idf , list_of_cols = \"all\" , drop_cols = [], label_col = \"label\" , event_label = 1 , encoding_configs = { \"bin_method\" : \"equal_frequency\" , \"bin_size\" : 10 , \"monotonicity_check\" : 0 , }, print_impact = False , ): \"\"\" Information Gain (IG) is another powerful technique for feature selection analysis. Information gain is calculated by comparing the entropy of the dataset before and after a transformation (introduction of attribute in this particular case). Similar to IV calculation, each category is a bin for categorical attributes, while numerical attributes need to be split into categories. IG = Total Entropy \u2013 Entropy Total Entropy= -%event*log\u2061(%event)-(1-%event)*log\u2061(1-%event) Entropy = \u2211(-%\u3016event\u3017_i*log\u2061(%\u3016event\u3017_i )-(1-%\u3016event\u3017_i )*log\u2061(1-%\u3016event\u3017_i) Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of columns to analyse e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) label_col Label/Target column (Default value = \"label\") event_label Value of (positive) event (i.e label 1) (Default value = 1) encoding_configs Takes input in dictionary format. {} i.e. empty dict means no encoding is required. In case numerical columns are present and encoding is required, following keys shall be provided - \"bin_size\" (Default value = 10) i.e. no. of bins for converting the numerical columns to categorical, \"bin_method\" i.e. method of binning - \"equal_frequency\" or \"equal_range\" (Default value = \"equal_frequency\") and \"monotonicity_check\" 1 for monotonic binning else 0. monotonicity_check of 1 will dynamically calculate the bin_size ensuring monotonic nature but can be expensive operation (Default value = 0). print_impact True, False This argument is to print out the statistics.(Default value = False) Returns ------- DataFrame [attribute, id] \"\"\" if label_col not in idf . columns : raise TypeError ( \"Invalid input for Label Column\" ) if list_of_cols == \"all\" : num_cols , cat_cols , other_cols = attributeType_segregation ( idf ) list_of_cols = num_cols + cat_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in ( drop_cols + [ label_col ])]) ) if any ( x not in idf . columns for x in list_of_cols ) | ( len ( list_of_cols ) == 0 ): raise TypeError ( \"Invalid input for Column(s)\" ) if idf . where ( F . col ( label_col ) == event_label ) . count () == 0 : raise TypeError ( \"Invalid input for Event Label Value\" ) num_cols , cat_cols , other_cols = attributeType_segregation ( idf . select ( list_of_cols )) if ( len ( num_cols ) > 0 ) & bool ( encoding_configs ): bin_size = encoding_configs [ \"bin_size\" ] bin_method = encoding_configs [ \"bin_method\" ] monotonicity_check = encoding_configs [ \"monotonicity_check\" ] if monotonicity_check == 1 : idf_encoded = monotonic_binning ( spark , idf , num_cols , [], label_col , event_label , bin_method , bin_size ) else : idf_encoded = attribute_binning ( spark , idf , num_cols , [], bin_method , bin_size ) idf_encoded . persist ( pyspark . StorageLevel . MEMORY_AND_DISK ) . count () else : idf_encoded = idf output = [] total_event = idf . where ( F . col ( label_col ) == event_label ) . count () / idf . count () total_entropy = - ( total_event * math . log2 ( total_event ) + (( 1 - total_event ) * math . log2 (( 1 - total_event ))) ) for col in list_of_cols : idf_entropy = ( idf_encoded . withColumn ( label_col , F . when ( F . col ( label_col ) == event_label , 1 ) . otherwise ( 0 ) ) . groupBy ( col ) . agg ( F . sum ( F . col ( label_col )) . alias ( \"event_count\" ), F . count ( F . col ( label_col )) . alias ( \"total_count\" ), ) . dropna () . withColumn ( \"event_pct\" , F . col ( \"event_count\" ) / F . col ( \"total_count\" )) . withColumn ( \"segment_pct\" , F . col ( \"total_count\" ) / F . sum ( \"total_count\" ) . over ( Window . partitionBy ()), ) . withColumn ( \"entropy\" , - F . col ( \"segment_pct\" ) * ( ( F . col ( \"event_pct\" ) * F . log2 ( F . col ( \"event_pct\" ))) + (( 1 - F . col ( \"event_pct\" )) * F . log2 (( 1 - F . col ( \"event_pct\" )))) ), ) ) entropy = ( idf_entropy . groupBy () . sum ( \"entropy\" ) . rdd . flatMap ( lambda x : x ) . collect ()[ 0 ] ) ig_value = total_entropy - entropy if entropy else None output . append ([ col , ig_value ]) odf = ( spark . createDataFrame ( output , [ \"attribute\" , \"ig\" ]) . withColumn ( \"ig\" , F . round ( F . col ( \"ig\" ), 4 )) . orderBy ( F . desc ( \"ig\" )) ) if print_impact : odf . show ( odf . count ()) return odf def IV_calculation ( spark, idf, list_of_cols='all', drop_cols=[], label_col='label', event_label=1, encoding_configs={'bin_method': 'equal_frequency', 'bin_size': 10, 'monotonicity_check': 0}, print_impact=False) Information Value (IV) is simple and powerful technique to conduct attribute relevance analysis. It measures how well an attribute is able to distinguish between a binary target variable i.e. label 0 from label 1, and hence helps in ranking attributes on the basis of their importance. In the heart of IV methodology are groups (bins) of observations. For categorical attributes, usually each category is a bin while numerical attributes need to be split into categories. IV = \u2211 (% of non-events - % of events) * WOE where: WOE = In(% of non-events \u2797 % of events) % of event = % label 1 in a bin % of non-event = % label 0 in a bin General rule of thumb while creating the bins are that a) each bin should have at least 5% of the observations, b) the WOE should be monotonic, i.e. either growing or decreasing with the bins, and c) missing values should be binned separately. An article from listendata.com can be referred for good understanding of IV & WOE concepts. Parameters spark Spark Session idf Input Dataframe list_of_cols List of columns to analyse e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) label_col Label/Target column (Default value = \"label\") event_label Value of (positive) event (i.e label 1) (Default value = 1) encoding_configs Takes input in dictionary format. {} i.e. empty dict means no encoding is required. In case numerical columns are present and encoding is required, following keys shall be provided - \"bin_size\" (Default value = 10) i.e. no. of bins for converting the numerical columns to categorical, \"bin_method\" i.e. method of binning - \"equal_frequency\" or \"equal_range\" (Default value = \"equal_frequency\") and \"monotonicity_check\" 1 for monotonic binning else 0. monotonicity_check of 1 will dynamically calculate the bin_size ensuring monotonic nature but can be expensive operation (Default value = 0). print_impact True, False This argument is to print out the statistics.(Default value = False) Returns DataFrame [attribute, iv] Expand source code def IV_calculation ( spark , idf , list_of_cols = \"all\" , drop_cols = [], label_col = \"label\" , event_label = 1 , encoding_configs = { \"bin_method\" : \"equal_frequency\" , \"bin_size\" : 10 , \"monotonicity_check\" : 0 , }, print_impact = False , ): \"\"\" Information Value (IV) is simple and powerful technique to conduct attribute relevance analysis. It measures how well an attribute is able to distinguish between a binary target variable i.e. label 0 from label 1, and hence helps in ranking attributes on the basis of their importance. In the heart of IV methodology are groups (bins) of observations. For categorical attributes, usually each category is a bin while numerical attributes need to be split into categories. IV = \u2211 (% of non-events - % of events) * WOE <br>where: <br>WOE = In(% of non-events \u2797 % of events) <br>% of event = % label 1 in a bin <br>% of non-event = % label 0 in a bin General rule of thumb while creating the bins are that a) each bin should have at least 5% of the observations, b) the WOE should be monotonic, i.e. either growing or decreasing with the bins, and c) missing values should be binned separately. An article from listendata.com can be referred for good understanding of IV & WOE concepts. Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of columns to analyse e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) label_col Label/Target column (Default value = \"label\") event_label Value of (positive) event (i.e label 1) (Default value = 1) encoding_configs Takes input in dictionary format. {} i.e. empty dict means no encoding is required. In case numerical columns are present and encoding is required, following keys shall be provided - \"bin_size\" (Default value = 10) i.e. no. of bins for converting the numerical columns to categorical, \"bin_method\" i.e. method of binning - \"equal_frequency\" or \"equal_range\" (Default value = \"equal_frequency\") and \"monotonicity_check\" 1 for monotonic binning else 0. monotonicity_check of 1 will dynamically calculate the bin_size ensuring monotonic nature but can be expensive operation (Default value = 0). print_impact True, False This argument is to print out the statistics.(Default value = False) Returns ------- DataFrame [attribute, iv] \"\"\" if label_col not in idf . columns : raise TypeError ( \"Invalid input for Label Column\" ) if list_of_cols == \"all\" : num_cols , cat_cols , other_cols = attributeType_segregation ( idf ) list_of_cols = num_cols + cat_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in ( drop_cols + [ label_col ])]) ) if any ( x not in idf . columns for x in list_of_cols ) | ( len ( list_of_cols ) == 0 ): raise TypeError ( \"Invalid input for Column(s)\" ) if idf . where ( F . col ( label_col ) == event_label ) . count () == 0 : raise TypeError ( \"Invalid input for Event Label Value\" ) num_cols , cat_cols , other_cols = attributeType_segregation ( idf . select ( list_of_cols )) if ( len ( num_cols ) > 0 ) & bool ( encoding_configs ): bin_size = encoding_configs [ \"bin_size\" ] bin_method = encoding_configs [ \"bin_method\" ] monotonicity_check = encoding_configs [ \"monotonicity_check\" ] if monotonicity_check == 1 : idf_encoded = monotonic_binning ( spark , idf , num_cols , [], label_col , event_label , bin_method , bin_size ) else : idf_encoded = attribute_binning ( spark , idf , num_cols , [], bin_method , bin_size ) idf_encoded . persist ( pyspark . StorageLevel . MEMORY_AND_DISK ) . count () else : idf_encoded = idf output = [] for col in list_of_cols : df_iv = ( idf_encoded . groupBy ( col , label_col ) . count () . withColumn ( label_col , F . when ( F . col ( label_col ) == event_label , 1 ) . otherwise ( 0 ) ) . groupBy ( col ) . pivot ( label_col ) . sum ( \"count\" ) . fillna ( 0.5 ) . withColumn ( \"event_pct\" , F . col ( \"1\" ) / F . sum ( \"1\" ) . over ( Window . partitionBy ())) . withColumn ( \"nonevent_pct\" , F . col ( \"0\" ) / F . sum ( \"0\" ) . over ( Window . partitionBy ()) ) . withColumn ( \"iv\" , ( F . col ( \"nonevent_pct\" ) - F . col ( \"event_pct\" )) * F . log ( F . col ( \"nonevent_pct\" ) / F . col ( \"event_pct\" )), ) ) iv_value = df_iv . select ( F . sum ( \"iv\" )) . collect ()[ 0 ][ 0 ] output . append ([ col , iv_value ]) odf = ( spark . createDataFrame ( output , [ \"attribute\" , \"iv\" ]) . withColumn ( \"iv\" , F . round ( F . col ( \"iv\" ), 4 )) . orderBy ( F . desc ( \"iv\" )) ) if print_impact : odf . show ( odf . count ()) return odf def correlation_matrix ( spark, idf, list_of_cols='all', drop_cols=[], stats_unique={}, print_impact=False) This function calculates correlation coefficient statistical, which measures the strength of the relationship between the relative movements of two attributes. Pearson\u2019s correlation coefficient is a standard approach of measuring correlation between two variables. However, it has some drawbacks: a) It works only with continuous variables, b) It only accounts for a linear relationship between variables, and c) It is sensitive to outliers. To avoid these issues, we are computing Phik (\ud835\udf19k), which is a new and practical correlation coefficient that works consistently between categorical, ordinal and interval variables, captures non-linear dependency and reverts to the Pearson correlation coefficient in case of a bivariate normal input distribution. The correlation coefficient is calculated for every pair of attributes and its value lies between 0 and 1, where 0 means there is no correlation between the two attributes and 1 means strong correlation. However, this methodology have drawbacks of its own as it is found to be more computational expensive especially when number of columns in the input dataset is on higher side (number of pairs to analyse increases exponentially with number of columns). Further, there is no indication of the direction of the relationship. More detail can be referred from the source paper . This function returns a correlation matrix dataframe of schema \u2013 attribute, . Correlation between attribute X and Y can be found at intersection of a) row with value X in \u2018attribute\u2019 column and b) column \u2018Y\u2019 (or row with value Y in \u2018attribute\u2019 column and column \u2018X\u2019). Parameters spark Spark Session idf Input Dataframe list_of_cols List of columns to analyse e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) stats_unique Takes arguments for read_dataset (data_ingest module) function in a dictionary format to read pre-saved statistics on unique value count i.e. if measures_of_cardinality or uniqueCount_computation (data_analyzer.stats_generator module) has been computed & saved before. (Default value = {}) print_impact True, False This argument is to print out the statistics.(Default value = False) Returns DataFrame [attribute,*attribute_names] Expand source code def correlation_matrix ( spark , idf , list_of_cols = \"all\" , drop_cols = [], stats_unique = {}, print_impact = False ): \"\"\" This function calculates correlation coefficient statistical, which measures the strength of the relationship between the relative movements of two attributes. Pearson\u2019s correlation coefficient is a standard approach of measuring correlation between two variables. However, it has some drawbacks: a) It works only with continuous variables, b) It only accounts for a linear relationship between variables, and c) It is sensitive to outliers. To avoid these issues, we are computing Phik (\ud835\udf19k), which is a new and practical correlation coefficient that works consistently between categorical, ordinal and interval variables, captures non-linear dependency and reverts to the Pearson correlation coefficient in case of a bivariate normal input distribution. The correlation coefficient is calculated for every pair of attributes and its value lies between 0 and 1, where 0 means there is no correlation between the two attributes and 1 means strong correlation. However, this methodology have drawbacks of its own as it is found to be more computational expensive especially when number of columns in the input dataset is on higher side (number of pairs to analyse increases exponentially with number of columns). Further, there is no indication of the direction of the relationship. More detail can be referred from the [ source paper] [1]. [1]: https://arxiv.org/abs/1811.11440/ \"source paper\" This function returns a correlation matrix dataframe of schema \u2013 attribute, <attribute_names>. Correlation between attribute X and Y can be found at intersection of a) row with value X in \u2018attribute\u2019 column and b) column \u2018Y\u2019 (or row with value Y in \u2018attribute\u2019 column and column \u2018X\u2019). Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of columns to analyse e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) stats_unique Takes arguments for read_dataset (data_ingest module) function in a dictionary format to read pre-saved statistics on unique value count i.e. if measures_of_cardinality or uniqueCount_computation (data_analyzer.stats_generator module) has been computed & saved before. (Default value = {}) print_impact True, False This argument is to print out the statistics.(Default value = False) Returns ------- DataFrame [attribute,*attribute_names] \"\"\" if list_of_cols == \"all\" : num_cols , cat_cols , other_cols = attributeType_segregation ( idf ) list_of_cols = num_cols + cat_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] if stats_unique == {}: remove_cols = ( uniqueCount_computation ( spark , idf , list_of_cols ) . where ( F . col ( \"unique_values\" ) < 2 ) . select ( \"attribute\" ) . rdd . flatMap ( lambda x : x ) . collect () ) else : remove_cols = ( read_dataset ( spark , ** stats_unique ) . where ( F . col ( \"unique_values\" ) < 2 ) . select ( \"attribute\" ) . rdd . flatMap ( lambda x : x ) . collect () ) list_of_cols = list ( set ([ e for e in list_of_cols if e not in ( drop_cols + remove_cols )]) ) if any ( x not in idf . columns for x in list_of_cols ) | ( len ( list_of_cols ) == 0 ): raise TypeError ( \"Invalid input for Column(s)\" ) combis = [ list ( c ) for c in itertools . combinations_with_replacement ( list_of_cols , 2 )] hists = idf . select ( list_of_cols ) . pm_make_histograms ( combis ) grids = { k : get_2dgrid ( h ) for k , h in hists . items ()} odf_pd = spark_phik_matrix_from_hist2d_dict ( spark . sparkContext , grids ) odf_pd [ \"attribute\" ] = odf_pd . index list_of_cols . sort () odf = ( spark . createDataFrame ( odf_pd ) . select ([ \"attribute\" ] + list_of_cols ) . orderBy ( \"attribute\" ) ) if print_impact : odf . show ( odf . count ()) return odf def variable_clustering ( spark, idf, list_of_cols='all', drop_cols=[], sample_size=100000, stats_unique={}, stats_mode={}, print_impact=False) Variable Clustering groups attributes that are as correlated as possible among themselves within a cluster and as uncorrelated as possible with attribute in other clusters. The function is leveraging VarClusHi library to do variable clustering; however, this library is not implemented in a scalable manner due to which the analysis is done on a sample dataset. Further, it is found to be a bit computational expensive especially when number of columns in the input dataset is on higher side (number of pairs to analyse increases exponentially with number of columns). It returns a Spark Dataframe with schema \u2013 Cluster, Attribute, RS_Ratio. Attributes similar to each other are grouped together with the same cluster id. The attribute with the lowest (1 \u2014 RS_Ratio) can be chosen as a representative of the cluster while discarding the other attributes from that cluster. This can also help in achieving the dimension reduction, if required. Parameters spark Spark Session idf Input Dataframe list_of_cols List of columns to analyse e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) sample_size Maximum sample size (in terms of number of rows) taken for the computation. Sample dataset is extracted using random sampling. (Default value = 100000) stats_unique Takes arguments for read_dataset (data_ingest module) function in a dictionary format to read pre-saved statistics on unique value count i.e. if measures_of_cardinality or uniqueCount_computation (data_analyzer.stats_generator module) has been computed & saved before. This is used to remove single value columns from the analysis purpose. (Default value = {}) stats_mode Takes arguments for read_dataset (data_ingest module) function in a dictionary format to read pre-saved statistics on most frequently seen values i.e. if measures_of_centralTendency or mode_computation (data_analyzer.stats_generator module) has been computed & saved before. This is used for MMM imputation as Variable Clustering doesn\u2019t work with missing values. (Default value = {}) print_impact True, False This argument is to print out the statistics.(Default value = False) Returns DataFrame [Cluster, Attribute, RS_Ratio] Expand source code def variable_clustering ( spark , idf , list_of_cols = \"all\" , drop_cols = [], sample_size = 100000 , stats_unique = {}, stats_mode = {}, print_impact = False , ): \"\"\" Variable Clustering groups attributes that are as correlated as possible among themselves within a cluster and as uncorrelated as possible with attribute in other clusters. The function is leveraging [VarClusHi] [2] library to do variable clustering; however, this library is not implemented in a scalable manner due to which the analysis is done on a sample dataset. Further, it is found to be a bit computational expensive especially when number of columns in the input dataset is on higher side (number of pairs to analyse increases exponentially with number of columns). [2]: https://github.com/jingtt/varclushi \"VarCluShi\" It returns a Spark Dataframe with schema \u2013 Cluster, Attribute, RS_Ratio. Attributes similar to each other are grouped together with the same cluster id. The attribute with the lowest (1 \u2014 RS_Ratio) can be chosen as a representative of the cluster while discarding the other attributes from that cluster. This can also help in achieving the dimension reduction, if required. Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of columns to analyse e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) sample_size Maximum sample size (in terms of number of rows) taken for the computation. Sample dataset is extracted using random sampling. (Default value = 100000) stats_unique Takes arguments for read_dataset (data_ingest module) function in a dictionary format to read pre-saved statistics on unique value count i.e. if measures_of_cardinality or uniqueCount_computation (data_analyzer.stats_generator module) has been computed & saved before. This is used to remove single value columns from the analysis purpose. (Default value = {}) stats_mode Takes arguments for read_dataset (data_ingest module) function in a dictionary format to read pre-saved statistics on most frequently seen values i.e. if measures_of_centralTendency or mode_computation (data_analyzer.stats_generator module) has been computed & saved before. This is used for MMM imputation as Variable Clustering doesn\u2019t work with missing values. (Default value = {}) print_impact True, False This argument is to print out the statistics.(Default value = False) Returns ------- DataFrame [Cluster, Attribute, RS_Ratio] \"\"\" if list_of_cols == \"all\" : num_cols , cat_cols , other_cols = attributeType_segregation ( idf ) list_of_cols = num_cols + cat_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in drop_cols ])) if any ( x not in idf . columns for x in list_of_cols ) | ( len ( list_of_cols ) == 0 ): raise TypeError ( \"Invalid input for Column(s)\" ) idf_sample = idf . sample ( False , min ( 1.0 , float ( sample_size ) / idf . count ()), 0 ) idf_sample . persist ( pyspark . StorageLevel . MEMORY_AND_DISK ) . count () if stats_unique == {}: remove_cols = ( uniqueCount_computation ( spark , idf_sample , list_of_cols ) . where ( F . col ( \"unique_values\" ) < 2 ) . select ( \"attribute\" ) . rdd . flatMap ( lambda x : x ) . collect () ) else : remove_cols = ( read_dataset ( spark , ** stats_unique ) . where ( F . col ( \"unique_values\" ) < 2 ) . select ( \"attribute\" ) . rdd . flatMap ( lambda x : x ) . collect () ) list_of_cols = [ e for e in list_of_cols if e not in remove_cols ] idf_sample = idf_sample . select ( list_of_cols ) num_cols , cat_cols , other_cols = attributeType_segregation ( idf_sample ) for i in idf_sample . dtypes : if i [ 1 ] . startswith ( \"decimal\" ): idf_sample = idf_sample . withColumn ( i [ 0 ], F . col ( i [ 0 ]) . cast ( \"double\" )) idf_encoded = cat_to_num_unsupervised ( spark , idf_sample , list_of_cols = cat_cols , method_type = 1 ) idf_imputed = imputation_MMM ( spark , idf_encoded , stats_mode = stats_mode ) idf_imputed . persist ( pyspark . StorageLevel . MEMORY_AND_DISK ) . count () idf_sample . unpersist () idf_pd = idf_imputed . toPandas () vc = VarClusHi ( idf_pd , maxeigval2 = 1 , maxclus = None ) vc . varclus () odf_pd = vc . rsquare odf = spark . createDataFrame ( odf_pd ) . select ( \"Cluster\" , F . col ( \"Variable\" ) . alias ( \"Attribute\" ), F . round ( F . col ( \"RS_Ratio\" ), 4 ) . alias ( \"RS_Ratio\" ), ) if print_impact : odf . show ( odf . count ()) return odf","title":"<code>association_evaluator</code>"},{"location":"api/data_analyzer/association_evaluator.html#association_evaluator","text":"This submodule focuses on understanding the interaction between different attributes and/or the relationship between an attribute & the binary target variable. Association between attributes is measured by: - correlation_matrix - variable_clustering Association between an attribute and binary target is measured by: - IV_calculation - IG_calculation Expand source code # coding=utf-8 \"\"\" This submodule focuses on understanding the interaction between different attributes and/or the relationship between an attribute & the binary target variable. Association between attributes is measured by: - correlation_matrix - variable_clustering Association between an attribute and binary target is measured by: - IV_calculation - IG_calculation \"\"\" import itertools import math import pyspark from phik.phik import spark_phik_matrix_from_hist2d_dict from popmon.analysis.hist_numpy import get_2dgrid from pyspark.sql import Window from pyspark.sql import functions as F from varclushi import VarClusHi from anovos.data_analyzer.stats_generator import uniqueCount_computation from anovos.data_ingest.data_ingest import read_dataset from anovos.data_transformer.transformers import ( attribute_binning , monotonic_binning , cat_to_num_unsupervised , imputation_MMM , ) from anovos.shared.utils import attributeType_segregation def correlation_matrix ( spark , idf , list_of_cols = \"all\" , drop_cols = [], stats_unique = {}, print_impact = False ): \"\"\" This function calculates correlation coefficient statistical, which measures the strength of the relationship between the relative movements of two attributes. Pearson\u2019s correlation coefficient is a standard approach of measuring correlation between two variables. However, it has some drawbacks: a) It works only with continuous variables, b) It only accounts for a linear relationship between variables, and c) It is sensitive to outliers. To avoid these issues, we are computing Phik (\ud835\udf19k), which is a new and practical correlation coefficient that works consistently between categorical, ordinal and interval variables, captures non-linear dependency and reverts to the Pearson correlation coefficient in case of a bivariate normal input distribution. The correlation coefficient is calculated for every pair of attributes and its value lies between 0 and 1, where 0 means there is no correlation between the two attributes and 1 means strong correlation. However, this methodology have drawbacks of its own as it is found to be more computational expensive especially when number of columns in the input dataset is on higher side (number of pairs to analyse increases exponentially with number of columns). Further, there is no indication of the direction of the relationship. More detail can be referred from the [ source paper] [1]. [1]: https://arxiv.org/abs/1811.11440/ \"source paper\" This function returns a correlation matrix dataframe of schema \u2013 attribute, <attribute_names>. Correlation between attribute X and Y can be found at intersection of a) row with value X in \u2018attribute\u2019 column and b) column \u2018Y\u2019 (or row with value Y in \u2018attribute\u2019 column and column \u2018X\u2019). Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of columns to analyse e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) stats_unique Takes arguments for read_dataset (data_ingest module) function in a dictionary format to read pre-saved statistics on unique value count i.e. if measures_of_cardinality or uniqueCount_computation (data_analyzer.stats_generator module) has been computed & saved before. (Default value = {}) print_impact True, False This argument is to print out the statistics.(Default value = False) Returns ------- DataFrame [attribute,*attribute_names] \"\"\" if list_of_cols == \"all\" : num_cols , cat_cols , other_cols = attributeType_segregation ( idf ) list_of_cols = num_cols + cat_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] if stats_unique == {}: remove_cols = ( uniqueCount_computation ( spark , idf , list_of_cols ) . where ( F . col ( \"unique_values\" ) < 2 ) . select ( \"attribute\" ) . rdd . flatMap ( lambda x : x ) . collect () ) else : remove_cols = ( read_dataset ( spark , ** stats_unique ) . where ( F . col ( \"unique_values\" ) < 2 ) . select ( \"attribute\" ) . rdd . flatMap ( lambda x : x ) . collect () ) list_of_cols = list ( set ([ e for e in list_of_cols if e not in ( drop_cols + remove_cols )]) ) if any ( x not in idf . columns for x in list_of_cols ) | ( len ( list_of_cols ) == 0 ): raise TypeError ( \"Invalid input for Column(s)\" ) combis = [ list ( c ) for c in itertools . combinations_with_replacement ( list_of_cols , 2 )] hists = idf . select ( list_of_cols ) . pm_make_histograms ( combis ) grids = { k : get_2dgrid ( h ) for k , h in hists . items ()} odf_pd = spark_phik_matrix_from_hist2d_dict ( spark . sparkContext , grids ) odf_pd [ \"attribute\" ] = odf_pd . index list_of_cols . sort () odf = ( spark . createDataFrame ( odf_pd ) . select ([ \"attribute\" ] + list_of_cols ) . orderBy ( \"attribute\" ) ) if print_impact : odf . show ( odf . count ()) return odf def variable_clustering ( spark , idf , list_of_cols = \"all\" , drop_cols = [], sample_size = 100000 , stats_unique = {}, stats_mode = {}, print_impact = False , ): \"\"\" Variable Clustering groups attributes that are as correlated as possible among themselves within a cluster and as uncorrelated as possible with attribute in other clusters. The function is leveraging [VarClusHi] [2] library to do variable clustering; however, this library is not implemented in a scalable manner due to which the analysis is done on a sample dataset. Further, it is found to be a bit computational expensive especially when number of columns in the input dataset is on higher side (number of pairs to analyse increases exponentially with number of columns). [2]: https://github.com/jingtt/varclushi \"VarCluShi\" It returns a Spark Dataframe with schema \u2013 Cluster, Attribute, RS_Ratio. Attributes similar to each other are grouped together with the same cluster id. The attribute with the lowest (1 \u2014 RS_Ratio) can be chosen as a representative of the cluster while discarding the other attributes from that cluster. This can also help in achieving the dimension reduction, if required. Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of columns to analyse e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) sample_size Maximum sample size (in terms of number of rows) taken for the computation. Sample dataset is extracted using random sampling. (Default value = 100000) stats_unique Takes arguments for read_dataset (data_ingest module) function in a dictionary format to read pre-saved statistics on unique value count i.e. if measures_of_cardinality or uniqueCount_computation (data_analyzer.stats_generator module) has been computed & saved before. This is used to remove single value columns from the analysis purpose. (Default value = {}) stats_mode Takes arguments for read_dataset (data_ingest module) function in a dictionary format to read pre-saved statistics on most frequently seen values i.e. if measures_of_centralTendency or mode_computation (data_analyzer.stats_generator module) has been computed & saved before. This is used for MMM imputation as Variable Clustering doesn\u2019t work with missing values. (Default value = {}) print_impact True, False This argument is to print out the statistics.(Default value = False) Returns ------- DataFrame [Cluster, Attribute, RS_Ratio] \"\"\" if list_of_cols == \"all\" : num_cols , cat_cols , other_cols = attributeType_segregation ( idf ) list_of_cols = num_cols + cat_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in drop_cols ])) if any ( x not in idf . columns for x in list_of_cols ) | ( len ( list_of_cols ) == 0 ): raise TypeError ( \"Invalid input for Column(s)\" ) idf_sample = idf . sample ( False , min ( 1.0 , float ( sample_size ) / idf . count ()), 0 ) idf_sample . persist ( pyspark . StorageLevel . MEMORY_AND_DISK ) . count () if stats_unique == {}: remove_cols = ( uniqueCount_computation ( spark , idf_sample , list_of_cols ) . where ( F . col ( \"unique_values\" ) < 2 ) . select ( \"attribute\" ) . rdd . flatMap ( lambda x : x ) . collect () ) else : remove_cols = ( read_dataset ( spark , ** stats_unique ) . where ( F . col ( \"unique_values\" ) < 2 ) . select ( \"attribute\" ) . rdd . flatMap ( lambda x : x ) . collect () ) list_of_cols = [ e for e in list_of_cols if e not in remove_cols ] idf_sample = idf_sample . select ( list_of_cols ) num_cols , cat_cols , other_cols = attributeType_segregation ( idf_sample ) for i in idf_sample . dtypes : if i [ 1 ] . startswith ( \"decimal\" ): idf_sample = idf_sample . withColumn ( i [ 0 ], F . col ( i [ 0 ]) . cast ( \"double\" )) idf_encoded = cat_to_num_unsupervised ( spark , idf_sample , list_of_cols = cat_cols , method_type = 1 ) idf_imputed = imputation_MMM ( spark , idf_encoded , stats_mode = stats_mode ) idf_imputed . persist ( pyspark . StorageLevel . MEMORY_AND_DISK ) . count () idf_sample . unpersist () idf_pd = idf_imputed . toPandas () vc = VarClusHi ( idf_pd , maxeigval2 = 1 , maxclus = None ) vc . varclus () odf_pd = vc . rsquare odf = spark . createDataFrame ( odf_pd ) . select ( \"Cluster\" , F . col ( \"Variable\" ) . alias ( \"Attribute\" ), F . round ( F . col ( \"RS_Ratio\" ), 4 ) . alias ( \"RS_Ratio\" ), ) if print_impact : odf . show ( odf . count ()) return odf def IV_calculation ( spark , idf , list_of_cols = \"all\" , drop_cols = [], label_col = \"label\" , event_label = 1 , encoding_configs = { \"bin_method\" : \"equal_frequency\" , \"bin_size\" : 10 , \"monotonicity_check\" : 0 , }, print_impact = False , ): \"\"\" Information Value (IV) is simple and powerful technique to conduct attribute relevance analysis. It measures how well an attribute is able to distinguish between a binary target variable i.e. label 0 from label 1, and hence helps in ranking attributes on the basis of their importance. In the heart of IV methodology are groups (bins) of observations. For categorical attributes, usually each category is a bin while numerical attributes need to be split into categories. IV = \u2211 (% of non-events - % of events) * WOE <br>where: <br>WOE = In(% of non-events \u2797 % of events) <br>% of event = % label 1 in a bin <br>% of non-event = % label 0 in a bin General rule of thumb while creating the bins are that a) each bin should have at least 5% of the observations, b) the WOE should be monotonic, i.e. either growing or decreasing with the bins, and c) missing values should be binned separately. An article from listendata.com can be referred for good understanding of IV & WOE concepts. Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of columns to analyse e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) label_col Label/Target column (Default value = \"label\") event_label Value of (positive) event (i.e label 1) (Default value = 1) encoding_configs Takes input in dictionary format. {} i.e. empty dict means no encoding is required. In case numerical columns are present and encoding is required, following keys shall be provided - \"bin_size\" (Default value = 10) i.e. no. of bins for converting the numerical columns to categorical, \"bin_method\" i.e. method of binning - \"equal_frequency\" or \"equal_range\" (Default value = \"equal_frequency\") and \"monotonicity_check\" 1 for monotonic binning else 0. monotonicity_check of 1 will dynamically calculate the bin_size ensuring monotonic nature but can be expensive operation (Default value = 0). print_impact True, False This argument is to print out the statistics.(Default value = False) Returns ------- DataFrame [attribute, iv] \"\"\" if label_col not in idf . columns : raise TypeError ( \"Invalid input for Label Column\" ) if list_of_cols == \"all\" : num_cols , cat_cols , other_cols = attributeType_segregation ( idf ) list_of_cols = num_cols + cat_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in ( drop_cols + [ label_col ])]) ) if any ( x not in idf . columns for x in list_of_cols ) | ( len ( list_of_cols ) == 0 ): raise TypeError ( \"Invalid input for Column(s)\" ) if idf . where ( F . col ( label_col ) == event_label ) . count () == 0 : raise TypeError ( \"Invalid input for Event Label Value\" ) num_cols , cat_cols , other_cols = attributeType_segregation ( idf . select ( list_of_cols )) if ( len ( num_cols ) > 0 ) & bool ( encoding_configs ): bin_size = encoding_configs [ \"bin_size\" ] bin_method = encoding_configs [ \"bin_method\" ] monotonicity_check = encoding_configs [ \"monotonicity_check\" ] if monotonicity_check == 1 : idf_encoded = monotonic_binning ( spark , idf , num_cols , [], label_col , event_label , bin_method , bin_size ) else : idf_encoded = attribute_binning ( spark , idf , num_cols , [], bin_method , bin_size ) idf_encoded . persist ( pyspark . StorageLevel . MEMORY_AND_DISK ) . count () else : idf_encoded = idf output = [] for col in list_of_cols : df_iv = ( idf_encoded . groupBy ( col , label_col ) . count () . withColumn ( label_col , F . when ( F . col ( label_col ) == event_label , 1 ) . otherwise ( 0 ) ) . groupBy ( col ) . pivot ( label_col ) . sum ( \"count\" ) . fillna ( 0.5 ) . withColumn ( \"event_pct\" , F . col ( \"1\" ) / F . sum ( \"1\" ) . over ( Window . partitionBy ())) . withColumn ( \"nonevent_pct\" , F . col ( \"0\" ) / F . sum ( \"0\" ) . over ( Window . partitionBy ()) ) . withColumn ( \"iv\" , ( F . col ( \"nonevent_pct\" ) - F . col ( \"event_pct\" )) * F . log ( F . col ( \"nonevent_pct\" ) / F . col ( \"event_pct\" )), ) ) iv_value = df_iv . select ( F . sum ( \"iv\" )) . collect ()[ 0 ][ 0 ] output . append ([ col , iv_value ]) odf = ( spark . createDataFrame ( output , [ \"attribute\" , \"iv\" ]) . withColumn ( \"iv\" , F . round ( F . col ( \"iv\" ), 4 )) . orderBy ( F . desc ( \"iv\" )) ) if print_impact : odf . show ( odf . count ()) return odf def IG_calculation ( spark , idf , list_of_cols = \"all\" , drop_cols = [], label_col = \"label\" , event_label = 1 , encoding_configs = { \"bin_method\" : \"equal_frequency\" , \"bin_size\" : 10 , \"monotonicity_check\" : 0 , }, print_impact = False , ): \"\"\" Information Gain (IG) is another powerful technique for feature selection analysis. Information gain is calculated by comparing the entropy of the dataset before and after a transformation (introduction of attribute in this particular case). Similar to IV calculation, each category is a bin for categorical attributes, while numerical attributes need to be split into categories. IG = Total Entropy \u2013 Entropy Total Entropy= -%event*log\u2061(%event)-(1-%event)*log\u2061(1-%event) Entropy = \u2211(-%\u3016event\u3017_i*log\u2061(%\u3016event\u3017_i )-(1-%\u3016event\u3017_i )*log\u2061(1-%\u3016event\u3017_i) Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of columns to analyse e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) label_col Label/Target column (Default value = \"label\") event_label Value of (positive) event (i.e label 1) (Default value = 1) encoding_configs Takes input in dictionary format. {} i.e. empty dict means no encoding is required. In case numerical columns are present and encoding is required, following keys shall be provided - \"bin_size\" (Default value = 10) i.e. no. of bins for converting the numerical columns to categorical, \"bin_method\" i.e. method of binning - \"equal_frequency\" or \"equal_range\" (Default value = \"equal_frequency\") and \"monotonicity_check\" 1 for monotonic binning else 0. monotonicity_check of 1 will dynamically calculate the bin_size ensuring monotonic nature but can be expensive operation (Default value = 0). print_impact True, False This argument is to print out the statistics.(Default value = False) Returns ------- DataFrame [attribute, id] \"\"\" if label_col not in idf . columns : raise TypeError ( \"Invalid input for Label Column\" ) if list_of_cols == \"all\" : num_cols , cat_cols , other_cols = attributeType_segregation ( idf ) list_of_cols = num_cols + cat_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in ( drop_cols + [ label_col ])]) ) if any ( x not in idf . columns for x in list_of_cols ) | ( len ( list_of_cols ) == 0 ): raise TypeError ( \"Invalid input for Column(s)\" ) if idf . where ( F . col ( label_col ) == event_label ) . count () == 0 : raise TypeError ( \"Invalid input for Event Label Value\" ) num_cols , cat_cols , other_cols = attributeType_segregation ( idf . select ( list_of_cols )) if ( len ( num_cols ) > 0 ) & bool ( encoding_configs ): bin_size = encoding_configs [ \"bin_size\" ] bin_method = encoding_configs [ \"bin_method\" ] monotonicity_check = encoding_configs [ \"monotonicity_check\" ] if monotonicity_check == 1 : idf_encoded = monotonic_binning ( spark , idf , num_cols , [], label_col , event_label , bin_method , bin_size ) else : idf_encoded = attribute_binning ( spark , idf , num_cols , [], bin_method , bin_size ) idf_encoded . persist ( pyspark . StorageLevel . MEMORY_AND_DISK ) . count () else : idf_encoded = idf output = [] total_event = idf . where ( F . col ( label_col ) == event_label ) . count () / idf . count () total_entropy = - ( total_event * math . log2 ( total_event ) + (( 1 - total_event ) * math . log2 (( 1 - total_event ))) ) for col in list_of_cols : idf_entropy = ( idf_encoded . withColumn ( label_col , F . when ( F . col ( label_col ) == event_label , 1 ) . otherwise ( 0 ) ) . groupBy ( col ) . agg ( F . sum ( F . col ( label_col )) . alias ( \"event_count\" ), F . count ( F . col ( label_col )) . alias ( \"total_count\" ), ) . dropna () . withColumn ( \"event_pct\" , F . col ( \"event_count\" ) / F . col ( \"total_count\" )) . withColumn ( \"segment_pct\" , F . col ( \"total_count\" ) / F . sum ( \"total_count\" ) . over ( Window . partitionBy ()), ) . withColumn ( \"entropy\" , - F . col ( \"segment_pct\" ) * ( ( F . col ( \"event_pct\" ) * F . log2 ( F . col ( \"event_pct\" ))) + (( 1 - F . col ( \"event_pct\" )) * F . log2 (( 1 - F . col ( \"event_pct\" )))) ), ) ) entropy = ( idf_entropy . groupBy () . sum ( \"entropy\" ) . rdd . flatMap ( lambda x : x ) . collect ()[ 0 ] ) ig_value = total_entropy - entropy if entropy else None output . append ([ col , ig_value ]) odf = ( spark . createDataFrame ( output , [ \"attribute\" , \"ig\" ]) . withColumn ( \"ig\" , F . round ( F . col ( \"ig\" ), 4 )) . orderBy ( F . desc ( \"ig\" )) ) if print_impact : odf . show ( odf . count ()) return odf","title":"association_evaluator"},{"location":"api/data_analyzer/association_evaluator.html#functions","text":"def IG_calculation ( spark, idf, list_of_cols='all', drop_cols=[], label_col='label', event_label=1, encoding_configs={'bin_method': 'equal_frequency', 'bin_size': 10, 'monotonicity_check': 0}, print_impact=False) Information Gain (IG) is another powerful technique for feature selection analysis. Information gain is calculated by comparing the entropy of the dataset before and after a transformation (introduction of attribute in this particular case). Similar to IV calculation, each category is a bin for categorical attributes, while numerical attributes need to be split into categories. IG = Total Entropy \u2013 Entropy Total Entropy= -%event log\u2061(%event)-(1-%event) log\u2061(1-%event) Entropy = \u2211(-%\u3016event\u3017_i log\u2061(%\u3016event\u3017_i )-(1-%\u3016event\u3017_i ) log\u2061(1-%\u3016event\u3017_i)","title":"Functions"},{"location":"api/data_analyzer/quality_checker.html","text":"quality_checker This submodule focuses on assessing the data quality at both row-level and column-level and also provides an appropriate treatment option to fix quality issues. At the row level, the following checks are done: duplicate_detection nullRows_detection At the column level, the following checks are done: nullColumns_detection outlier_detection IDness_detection biasedness_detection invalidEntries_detection Expand source code # coding=utf-8 \"\"\" This submodule focuses on assessing the data quality at both row-level and column-level and also provides an appropriate treatment option to fix quality issues. At the row level, the following checks are done: - duplicate_detection - nullRows_detection At the column level, the following checks are done: - nullColumns_detection - outlier_detection - IDness_detection - biasedness_detection - invalidEntries_detection \"\"\" import re import warnings from pyspark.sql import functions as F from pyspark.sql import types as T from anovos.data_analyzer.stats_generator import ( uniqueCount_computation , missingCount_computation , mode_computation , measures_of_cardinality , ) from anovos.data_ingest.data_ingest import read_dataset from anovos.data_transformer.transformers import ( imputation_MMM , imputation_sklearn , imputation_matrixFactorization , auto_imputation , ) from anovos.shared.utils import ( attributeType_segregation , transpose_dataframe , get_dtype , ) def duplicate_detection ( spark , idf , list_of_cols = \"all\" , drop_cols = [], treatment = False , print_impact = False ): \"\"\" As the name implies, this function detects duplication in the input dataset. This means, for a pair of duplicate rows, the values in each column coincide. Duplication check is confined to the list of columns passed in the arguments. As part of treatment, duplicated rows are removed. This function returns two dataframes in tuple format; the 1st dataframe is the input dataset after deduplication (if treated else the original dataset). The 2nd dataframe is of schema \u2013 metric, value and contains the total number of rows, number of unique rows, number of duplicate rows and percentage of duplicate rows in total. Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of columns to analyse e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) treatment Boolean argument \u2013 True or False. If True, duplicate rows are removed from the input dataframe. (Default value = False) print_impact True, False This argument is to print out the statistics.(Default value = False) Returns ------- odf : DataFrame de-duplicated dataframe if treated, else original input dataframe. odf_print : DataFrame schema [metric, value] and contains metrics - number of rows, number of unique rows, number of duplicate rows and percentage of duplicate rows in total. \"\"\" if list_of_cols == \"all\" : num_cols , cat_cols , other_cols = attributeType_segregation ( idf ) list_of_cols = num_cols + cat_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in drop_cols ])) if any ( x not in idf . columns for x in list_of_cols ) | ( len ( list_of_cols ) == 0 ): raise TypeError ( \"Invalid input for Column(s)\" ) if str ( treatment ) . lower () == \"true\" : treatment = True elif str ( treatment ) . lower () == \"false\" : treatment = False else : raise TypeError ( \"Non-Boolean input for treatment\" ) odf_tmp = idf . drop_duplicates ( subset = list_of_cols ) odf = odf_tmp if treatment else idf odf_print = spark . createDataFrame ( [ [ \"rows_count\" , float ( idf . count ())], [ \"unique_rows_count\" , float ( odf_tmp . count ())], [ \"duplicate_rows\" , float ( idf . count () - odf_tmp . count ())], [ \"duplicate_pct\" , round (( idf . count () - odf_tmp . count ()) / idf . count (), 4 )], ], schema = [ \"metric\" , \"value\" ], ) if print_impact : print ( \"No. of Rows: \" + str ( idf . count ())) print ( \"No. of UNIQUE Rows: \" + str ( odf_tmp . count ())) print ( \"No. of Duplicate Rows: \" + str ( idf . count () - odf_tmp . count ())) print ( \"Percentage of Duplicate Rows: \" + str ( round (( idf . count () - odf_tmp . count ()) / idf . count (), 4 )) ) return odf , odf_print def nullRows_detection ( spark , idf , list_of_cols = \"all\" , drop_cols = [], treatment = False , treatment_threshold = 0.8 , print_impact = False , ): \"\"\" This function inspects the row quality and computes the number of columns that are missing for a row. This metric is further aggregated to check how many columns are missing for how many rows (or % rows). Intuition is if too many columns are missing for a row, removing it from the modeling may give better results than relying on its imputed values. Therefore as part of the treatment, rows with missing columns above the specified threshold are removed. This function returns two dataframes in tuple format; the 1st dataframe is the input dataset after filtering rows with a high number of missing columns (if treated else the original dataframe). The 2nd dataframe is of schema \u2013 null_cols_count, row_count, row_pct, flagged/treated. | null_cols_count | row_count | row_pct | flagged | |-----------------|-----------|---------|---------| | 5 | 11 | 3.0E-4 | 0 | | 7 | 1306 | 0.0401 | 1 | Interpretation: 1306 rows (4.01% of total rows) have 7 missing columns and flagged for are removal because null_cols_count is above the threshold. If treatment is True, then flagged column is renamed as treated to show rows which has been removed. Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of columns to analyse e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) treatment Boolean argument \u2013 True or False. If True, rows with high no. of null columns (defined by treatment_threshold argument) are removed from the input dataframe. (Default value = False) treatment_threshold Defines % of columns allowed to be Null per row and takes value between 0 to 1. If % of null columns is above the threshold for a row, it is removed from the dataframe. There is no row removal if the threshold is 1.0. And if the threshold is 0, all rows with null value are removed. (Default value = 0.8) print_impact True, False This argument is to print out the statistics.(Default value = False) Returns ------- odf : DataFrame Dataframe after row removal if treated, else original input dataframe. odf_print : DataFrame schema [null_cols_count, row_count, row_pct, flagged/treated]. null_cols_count is defined as no. of missing columns in a row. row_count is no. of rows with null_cols_count missing columns. row_pct is row_count divided by number of rows. flagged/treated is 1 if null_cols_count is more than (threshold X Number of Columns), else 0. \"\"\" if list_of_cols == \"all\" : num_cols , cat_cols , other_cols = attributeType_segregation ( idf ) list_of_cols = num_cols + cat_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in drop_cols ])) if any ( x not in idf . columns for x in list_of_cols ) | ( len ( list_of_cols ) == 0 ): raise TypeError ( \"Invalid input for Column(s)\" ) if str ( treatment ) . lower () == \"true\" : treatment = True elif str ( treatment ) . lower () == \"false\" : treatment = False else : raise TypeError ( \"Non-Boolean input for treatment\" ) treatment_threshold = float ( treatment_threshold ) if ( treatment_threshold < 0 ) | ( treatment_threshold > 1 ): raise TypeError ( \"Invalid input for Treatment Threshold Value\" ) def null_count ( * cols ): return cols . count ( None ) f_null_count = F . udf ( null_count , T . LongType ()) odf_tmp = idf . withColumn ( \"null_cols_count\" , f_null_count ( * list_of_cols )) . withColumn ( \"flagged\" , F . when ( F . col ( \"null_cols_count\" ) > ( len ( list_of_cols ) * treatment_threshold ), 1 ) . otherwise ( 0 ), ) if treatment_threshold == 1 : odf_tmp = odf_tmp . withColumn ( \"flagged\" , F . when ( F . col ( \"null_cols_count\" ) == len ( list_of_cols ), 1 ) . otherwise ( 0 ), ) odf_print = ( odf_tmp . groupBy ( \"null_cols_count\" , \"flagged\" ) . agg ( F . count ( F . lit ( 1 )) . alias ( \"row_count\" )) . withColumn ( \"row_pct\" , F . round ( F . col ( \"row_count\" ) / float ( idf . count ()), 4 )) . select ( \"null_cols_count\" , \"row_count\" , \"row_pct\" , \"flagged\" ) . orderBy ( \"null_cols_count\" ) ) if treatment : odf = odf_tmp . where ( F . col ( \"flagged\" ) == 0 ) . drop ( * [ \"null_cols_count\" , \"flagged\" ]) odf_print = odf_print . withColumnRenamed ( \"flagged\" , \"treated\" ) else : odf = idf if print_impact : odf_print . show ( odf . count ()) return odf , odf_print def nullColumns_detection ( spark , idf , list_of_cols = \"missing\" , drop_cols = [], treatment = False , treatment_method = \"row_removal\" , treatment_configs = {}, stats_missing = {}, stats_unique = {}, stats_mode = {}, print_impact = False , ): \"\"\" This function inspects the column quality and computes the number of rows that are missing for a column. This function also leverages statistics computed as part of the State Generator module. Statistics are not computed twice if already available. As part of treatments, it currently supports the following methods \u2013 Mean Median Mode (MMM), row_removal, column_removal, KNN, regression, Matrix Factorization (MF), auto imputation (auto). - MMM replaces null value with the measure of central tendency (mode for categorical features and mean/median for numerical features). - row_removal removes all rows with any missing value (output of this treatment is same as nullRows_detection with treatment_threshold of 0). - column_removal remove a column if %rows with a missing value is above treatment_threshold. - KNN/regression create an imputation model for every to-be-imputed column based on the rest of columns in the list_of_cols columns. KNN leverages sklearn.impute.KNNImputer and regression sklearn.impute.IterativeImputer. Since sklearn algorithms are not scalable, we create imputation model on sample dataset and apply that model on the whole dataset in distributed manner using pyspark pandas udf. - Matrix Factorization leverages pyspark.ml.recommendation.ALS algorithm. - auto imputation compares all imputation methods and select the best imputation method based on the least RMSE. This function returns two dataframes in tuple format \u2013 1st dataframe is input dataset after imputation (if treated else the original dataset) and 2nd dataframe is of schema \u2013 attribute, missing_count, missing_pct. Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of columns to inspect e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all (non-array) columns for analysis. This is super useful instead of specifying all column names manually. \"missing\" (default) can be passed to include only those columns with missing values. One of the usecases where \"all\" may be preferable over \"missing\" is when the user wants to save the imputation model for the future use e.g. a column may not have missing value in the training dataset but missing values may possibly appear in the prediction dataset. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) treatment Boolean argument \u2013 True or False. If True, missing values are treated as per treatment_method argument. (Default value = False) treatment_method \"MMM\", \"row_removal\", \"column_removal\", \"KNN\", \"regression\", \"MF\", \"auto\". (Default value = \"row_removal\") treatment_configs Takes input in dictionary format. For column_removal treatment, key \u2018treatment_threshold\u2019 is provided with a value between 0 to 1 (remove column if % of rows with missing value is above this threshold) For row_removal, this argument can be skipped. For MMM, arguments corresponding to imputation_MMM function (transformer module) are provided, where each key is an argument from imputation_MMM function. For KNN, arguments corresponding to imputation_sklearn function (transformer module) are provided, where each key is an argument from imputation_sklearn function. method_type should be \"KNN\" For regression, arguments corresponding to imputation_sklearn function (transformer module) are provided, where each key is an argument from imputation_sklearn function. method_type should be \"regression\" For MF, arguments corresponding to imputation_matrixFactorization function (transformer module) are provided, where each key is an argument from imputation_matrixFactorization function. For auto, arguments corresponding to auto_imputation function (transformer module) are provided, where each key is an argument from auto_imputation function. (Default value = {}) stats_missing Takes arguments for read_dataset (data_ingest module) function in a dictionary format to read pre-saved statistics on missing count/pct i.e. if measures_of_counts or missingCount_computation (data_analyzer.stats_generator module) has been computed & saved before. (Default value = {}) stats_unique Takes arguments for read_dataset (data_ingest module) function in a dictionary format to read pre-saved statistics on unique value count i.e. if measures_of_cardinality or uniqueCount_computation (data_analyzer.stats_generator module) has been computed & saved before. (Default value = {}) stats_mode Takes arguments for read_dataset (data_ingest module) function in a dictionary format to read pre-saved statistics on most frequently seen values i.e. if measures_of_centralTendency or mode_computation (data_analyzer.stats_generator module) has been computed & saved before. (Default value = {}) print_impact True, False This argument is to print out the statistics or the impact of imputation (if applicable).(Default value = False) Returns ------- odf : DataFrame Imputed dataframe if treated, else original input dataframe. odf_print : DataFrame schema [attribute, missing_count, missing_pct]. missing_count is number of rows with null values for an attribute, and missing_pct is missing_count divided by number of rows. \"\"\" if stats_missing == {}: odf_print = missingCount_computation ( spark , idf ) else : odf_print = read_dataset ( spark , ** stats_missing ) . select ( \"attribute\" , \"missing_count\" , \"missing_pct\" ) missing_cols = ( odf_print . where ( F . col ( \"missing_count\" ) > 0 ) . select ( \"attribute\" ) . rdd . flatMap ( lambda x : x ) . collect () ) if list_of_cols == \"all\" : num_cols , cat_cols , other_cols = attributeType_segregation ( idf ) list_of_cols = num_cols + cat_cols if list_of_cols == \"missing\" : list_of_cols = missing_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in drop_cols ])) if len ( list_of_cols ) == 0 : warnings . warn ( \"No Null Detection - No column(s) to analyze\" ) odf = idf schema = T . StructType ( [ T . StructField ( \"attribute\" , T . StringType (), True ), T . StructField ( \"missing_count\" , T . StringType (), True ), T . StructField ( \"missing_pct\" , T . StringType (), True ), ] ) odf_print = spark . sparkContext . emptyRDD () . toDF ( schema ) return odf , odf_print if any ( x not in idf . columns for x in list_of_cols ): raise TypeError ( \"Invalid input for Column(s)\" ) if str ( treatment ) . lower () == \"true\" : treatment = True elif str ( treatment ) . lower () == \"false\" : treatment = False else : raise TypeError ( \"Non-Boolean input for treatment\" ) if treatment_method not in ( \"MMM\" , \"row_removal\" , \"column_removal\" , \"KNN\" , \"regression\" , \"MF\" , \"auto\" , ): raise TypeError ( \"Invalid input for method_type\" ) treatment_threshold = treatment_configs . pop ( \"treatment_threshold\" , None ) if treatment_threshold : treatment_threshold = float ( treatment_threshold ) else : if treatment_method == \"column_removal\" : raise TypeError ( \"Invalid input for column removal threshold\" ) odf_print = odf_print . where ( F . col ( \"attribute\" ) . isin ( list_of_cols )) if treatment : if treatment_threshold : threshold_cols = ( odf_print . where ( F . col ( \"attribute\" ) . isin ( list_of_cols )) . where ( F . col ( \"missing_pct\" ) > treatment_threshold ) . select ( \"attribute\" ) . rdd . flatMap ( lambda x : x ) . collect () ) if treatment_method == \"column_removal\" : odf = idf . drop ( * threshold_cols ) if print_impact : odf_print . show ( len ( list_of_cols )) print ( \"Removed Columns: \" , threshold_cols ) if treatment_method == \"row_removal\" : remove_cols = ( odf_print . where ( F . col ( \"attribute\" ) . isin ( list_of_cols )) . where ( F . col ( \"missing_pct\" ) == 1.0 ) . select ( \"attribute\" ) . rdd . flatMap ( lambda x : x ) . collect () ) list_of_cols = [ e for e in list_of_cols if e not in remove_cols ] if treatment_threshold : list_of_cols = [ e for e in threshold_cols if e not in remove_cols ] odf = idf . dropna ( subset = list_of_cols ) if print_impact : odf_print . show ( len ( list_of_cols )) print ( \"Before Count: \" + str ( idf . count ())) print ( \"After Count: \" + str ( odf . count ())) if treatment_method == \"MMM\" : if stats_unique == {}: remove_cols = ( uniqueCount_computation ( spark , idf , list_of_cols ) . where ( F . col ( \"unique_values\" ) < 2 ) . select ( \"attribute\" ) . rdd . flatMap ( lambda x : x ) . collect () ) else : remove_cols = ( read_dataset ( spark , ** stats_unique ) . where ( F . col ( \"unique_values\" ) < 2 ) . select ( \"attribute\" ) . rdd . flatMap ( lambda x : x ) . collect () ) list_of_cols = [ e for e in list_of_cols if e not in remove_cols ] if treatment_threshold : list_of_cols = [ e for e in threshold_cols if e not in remove_cols ] odf = imputation_MMM ( spark , idf , list_of_cols , ** treatment_configs , stats_missing = stats_missing , stats_mode = stats_mode , print_impact = print_impact ) if treatment_method in ( \"KNN\" , \"regression\" , \"MF\" , \"auto\" ): if treatment_threshold : list_of_cols = threshold_cols list_of_cols = [ e for e in list_of_cols if e in num_cols ] func_mapping = { \"KNN\" : imputation_sklearn , \"regression\" : imputation_sklearn , \"MF\" : imputation_matrixFactorization , \"auto\" : auto_imputation , } func = func_mapping [ treatment_method ] odf = func ( spark , idf , list_of_cols , ** treatment_configs , stats_missing = stats_missing , print_impact = print_impact ) else : odf = idf if print_impact : odf_print . show ( len ( list_of_cols )) return odf , odf_print def outlier_detection ( spark , idf , list_of_cols = \"all\" , drop_cols = [], detection_side = \"upper\" , detection_configs = { \"pctile_lower\" : 0.05 , \"pctile_upper\" : 0.95 , \"stdev_lower\" : 3.0 , \"stdev_upper\" : 3.0 , \"IQR_lower\" : 1.5 , \"IQR_upper\" : 1.5 , \"min_validation\" : 2 , }, treatment = False , treatment_method = \"value_replacement\" , pre_existing_model = False , model_path = \"NA\" , output_mode = \"replace\" , stats_unique = {}, print_impact = False , ): \"\"\" In Machine Learning, outlier detection identifies values that deviate drastically from the rest of the attribute values. An outlier may be caused simply by chance, measurement error, or inherent heavy-tailed distribution. This function identifies extreme values in both directions (or any direction provided by the user via detection_side argument). Outlier is identified by 3 different methodologies and tagged an outlier only if it is validated by at least 2 methods (can be changed by the user via min_validation under detection_configs argument). - Percentile Method: In this methodology, a value higher than a certain (default 95th) percentile value is considered as an outlier. Similarly, a value lower than a certain (default 5th) percentile value is considered as an outlier. - Standard Deviation Method: In this methodology, if a value is a certain number of standard deviations (default 3) away from the mean, it is identified as an outlier. - Interquartile Range (IQR) Method: A value below Q1 \u2013 1.5 IQR or above Q3 + 1.5 IQR are identified as outliers, where Q1 is in first quantile/25th percentile, Q3 is in third quantile/75th percentile, and IQR is the difference between third quantile & first quantile. This function also leverages statistics which were computed as the part of the State Generator module so that statistics are not computed twice if already available. As part of treatments available, outlier values can be replaced by null so that it can be imputed by a reliable imputation methodology (null_replacement). It can also be replaced by maximum or minimum permissible by above methodologies (value_replacement). Lastly, rows can be removed if it is identified with any outlier (row_removal). This function returns two dataframes in tuple format \u2013 1st dataframe is input dataset after treating outlier (the original dataset if no treatment) and 2nd dataframe is of schema \u2013 attribute, lower_outliers, upper_outliers. If outliers are checked only for upper end, then lower_outliers column will be shown all zero. Similarly if checked only for lower end, then upper_outliers will be zero for all attributes. Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of numerical columns to analyse e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all numerical columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) detection_side \"upper\", \"lower\", \"both\". \"lower\" detects outliers in the lower spectrum of the column range, whereas \"upper\" detects in the upper spectrum. \"Both\" detects in both upper and lower end of the spectrum. (Default value = \"upper\") detection_configs Takes input in dictionary format with keys representing upper & lower parameter for three outlier detection methodologies. a) Percentile Method: lower and upper percentile threshold can be set via \"pctile_lower\" & \"pctile_upper\" (default 0.05 & 0.95) Any value above \"pctile_upper\" is considered as an outlier. Similarly, a value lower than \"pctile_lower\" is considered as an outlier. b) Standard Deviation Method: In this methodology, if a value which is below (mean - \"stdev_lower\" * standard deviation) or above (mean + \"stdev_upper\" * standard deviation), then it is identified as an outlier (default 3.0 & 3.0). c) Interquartile Range (IQR) Method: A value which is below (Q1 \u2013 \"IQR_lower\" * IQR) or above (Q3 + \"IQR_lower\" * IQR) is identified as outliers, where Q1 is first quartile/25th percentile, Q3 is third quartile/75th percentile and IQR is difference between third quartile & first quartile (default 1.5 & 1.5). If an attribute value is less (more) than its derived lower (upper) bound value, it is considered as outlier by a methodology. A attribute value is considered as outlier if it is declared as outlier by atleast 'min_validation' methodologies (default 2). treatment Boolean argument \u2013 True or False. If True, outliers are treated as per treatment_method argument. (Default value = False) treatment_method \"null_replacement\", \"row_removal\", \"value_replacement\". In \"null_replacement\", outlier values are replaced by null so that it can be imputed by a reliable imputation methodology. In \"value_replacement\", outlier values are replaced by maximum or minimum permissible value by above methodologies. Lastly in \"row_removal\", rows are removed if it is found with any outlier. (Default value = \"value_replacement\") pre_existing_model Boolean argument \u2013 True or False. True if the model with upper/lower permissible values for each attribute exists already to be used, False otherwise. (Default value = False) model_path If pre_existing_model is True, this argument is path for the pre-saved model. If pre_existing_model is False, this field can be used for saving the model. Default \"NA\" means there is neither pre-existing model nor there is a need to save one. output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with treated column. \u201cappend\u201d option append treated column to the input dataset with a postfix \"_outliered\" e.g. column X is appended as X_outliered. (Default value = \"replace\") stats_unique Takes arguments for read_dataset (data_ingest module) function in a dictionary format to read pre-saved statistics on unique value count i.e. if measures_of_cardinality or uniqueCount_computation (data_analyzer.stats_generator module) has been computed & saved before. (Default value = {}) print_impact True, False This argument is to print out the statistics and the impact of treatment (if applicable).(Default value = False) Returns ------- odf : DataFrame Imputed dataframe if treated, else original input dataframe. odf_print : DataFrame schema [attribute, lower_outliers, upper_outliers]. lower_outliers is no. of outliers found in the lower spectrum of the attribute range, and upper_outliers is outlier count in the upper spectrum. \"\"\" num_cols = attributeType_segregation ( idf )[ 0 ] if len ( num_cols ) == 0 : warnings . warn ( \"No Outlier Check - No numerical column(s) to analyse\" ) odf = idf schema = T . StructType ( [ T . StructField ( \"attribute\" , T . StringType (), True ), T . StructField ( \"lower_outliers\" , T . StringType (), True ), T . StructField ( \"upper_outliers\" , T . StringType (), True ), ] ) odf_print = spark . sparkContext . emptyRDD () . toDF ( schema ) return odf , odf_print if list_of_cols == \"all\" : list_of_cols = num_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] if stats_unique == {}: remove_cols = ( uniqueCount_computation ( spark , idf , list_of_cols ) . where ( F . col ( \"unique_values\" ) < 2 ) . select ( \"attribute\" ) . rdd . flatMap ( lambda x : x ) . collect () ) else : remove_cols = ( read_dataset ( spark , ** stats_unique ) . where ( F . col ( \"unique_values\" ) < 2 ) . select ( \"attribute\" ) . rdd . flatMap ( lambda x : x ) . collect () ) list_of_cols = list ( set ([ e for e in list_of_cols if e not in ( drop_cols + remove_cols )]) ) if any ( x not in num_cols for x in list_of_cols ): raise TypeError ( \"Invalid input for Column(s)\" ) if detection_side not in ( \"upper\" , \"lower\" , \"both\" ): raise TypeError ( \"Invalid input for detection_side\" ) if treatment_method not in ( \"null_replacement\" , \"row_removal\" , \"value_replacement\" ): raise TypeError ( \"Invalid input for treatment_method\" ) if output_mode not in ( \"replace\" , \"append\" ): raise TypeError ( \"Invalid input for output_mode\" ) if str ( treatment ) . lower () == \"true\" : treatment = True elif str ( treatment ) . lower () == \"false\" : treatment = False else : raise TypeError ( \"Non-Boolean input for treatment\" ) if str ( pre_existing_model ) . lower () == \"true\" : pre_existing_model = True elif str ( pre_existing_model ) . lower () == \"false\" : pre_existing_model = False else : raise TypeError ( \"Non-Boolean input for pre_existing_model\" ) for arg in [ \"pctile_lower\" , \"pctile_upper\" ]: if arg in detection_configs : if ( detection_configs [ arg ] < 0 ) | ( detection_configs [ arg ] > 1 ): raise TypeError ( \"Invalid input for \" + arg ) recast_cols = [] recast_type = [] for i in list_of_cols : if get_dtype ( idf , i ) . startswith ( \"decimal\" ): idf = idf . withColumn ( i , F . col ( i ) . cast ( T . DoubleType ())) recast_cols . append ( i ) recast_type . append ( get_dtype ( idf , i )) if pre_existing_model : df_model = spark . read . parquet ( model_path + \"/outlier_numcols\" ) params = [] for i in list_of_cols : mapped_value = ( df_model . where ( F . col ( \"attribute\" ) == i ) . select ( \"parameters\" ) . rdd . flatMap ( lambda x : x ) . collect ()[ 0 ] ) params . append ( mapped_value ) pctile_params = idf . approxQuantile ( list_of_cols , [ detection_configs . get ( \"pctile_lower\" , 0.05 ), detection_configs . get ( \"pctile_upper\" , 0.95 ), ], 0.01 , ) skewed_cols = [] for i , p in zip ( list_of_cols , pctile_params ): if p [ 0 ] == p [ 1 ]: skewed_cols . append ( i ) else : detection_configs [ \"pctile_lower\" ] = detection_configs [ \"pctile_lower\" ] or 0.0 detection_configs [ \"pctile_upper\" ] = detection_configs [ \"pctile_upper\" ] or 1.0 pctile_params = idf . approxQuantile ( list_of_cols , [ detection_configs [ \"pctile_lower\" ], detection_configs [ \"pctile_upper\" ]], 0.01 , ) skewed_cols = [] for i , p in zip ( list_of_cols , pctile_params ): if p [ 0 ] == p [ 1 ]: skewed_cols . append ( i ) detection_configs [ \"stdev_lower\" ] = ( detection_configs [ \"stdev_lower\" ] or detection_configs [ \"stdev_upper\" ] ) detection_configs [ \"stdev_upper\" ] = ( detection_configs [ \"stdev_upper\" ] or detection_configs [ \"stdev_lower\" ] ) stdev_params = [] for i in list_of_cols : mean , stdev = idf . select ( F . mean ( i ), F . stddev ( i )) . first () stdev_params . append ( [ mean - detection_configs [ \"stdev_lower\" ] * stdev , mean + detection_configs [ \"stdev_upper\" ] * stdev , ] ) detection_configs [ \"IQR_lower\" ] = ( detection_configs [ \"IQR_lower\" ] or detection_configs [ \"IQR_upper\" ] ) detection_configs [ \"IQR_upper\" ] = ( detection_configs [ \"IQR_upper\" ] or detection_configs [ \"IQR_lower\" ] ) quantiles = idf . approxQuantile ( list_of_cols , [ 0.25 , 0.75 ], 0.01 ) IQR_params = [ [ e [ 0 ] - detection_configs [ \"IQR_lower\" ] * ( e [ 1 ] - e [ 0 ]), e [ 1 ] + detection_configs [ \"IQR_upper\" ] * ( e [ 1 ] - e [ 0 ]), ] for e in quantiles ] n = detection_configs [ \"min_validation\" ] params = [ [ sorted ([ x [ 0 ], y [ 0 ], z [ 0 ]], reverse = True )[ n - 1 ], sorted ([ x [ 1 ], y [ 1 ], z [ 1 ]])[ n - 1 ], ] for x , y , z in list ( zip ( pctile_params , stdev_params , IQR_params )) ] # Saving model File if required if model_path != \"NA\" : df_model = spark . createDataFrame ( zip ( list_of_cols , params ), schema = [ \"attribute\" , \"parameters\" ] ) df_model . coalesce ( 1 ) . write . parquet ( model_path + \"/outlier_numcols\" , mode = \"overwrite\" ) for i , j in zip ( recast_cols , recast_type ): idf = idf . withColumn ( i , F . col ( i ) . cast ( j )) def composite_outlier ( * v ): output = [] for idx , e in enumerate ( v ): if e is None : output . append ( None ) continue if detection_side in ( \"upper\" , \"both\" ): if e > params [ idx ][ 1 ]: output . append ( 1 ) continue if detection_side in ( \"lower\" , \"both\" ): if e < params [ idx ][ 0 ]: output . append ( - 1 ) continue output . append ( 0 ) return output f_composite_outlier = F . udf ( composite_outlier , T . ArrayType ( T . IntegerType ())) odf = idf . withColumn ( \"outliered\" , f_composite_outlier ( * list_of_cols )) odf . persist () output_print = [] for index , i in enumerate ( list_of_cols ): odf = odf . withColumn ( i + \"_outliered\" , F . col ( \"outliered\" )[ index ]) output_print . append ( [ i , odf . where ( F . col ( i + \"_outliered\" ) == - 1 ) . count (), odf . where ( F . col ( i + \"_outliered\" ) == 1 ) . count (), ] ) if treatment & ( treatment_method in ( \"value_replacement\" , \"null_replacement\" )): if skewed_cols : warnings . warn ( \"Columns dropped from outlier treatment due to highly skewed distribution: \" + ( \",\" ) . join ( skewed_cols ) ) if i not in skewed_cols : replace_vals = { \"value_replacement\" : [ params [ index ][ 0 ], params [ index ][ 1 ]], \"null_replacement\" : [ None , None ], } odf = odf . withColumn ( i + \"_outliered\" , F . when ( F . col ( i + \"_outliered\" ) == 1 , replace_vals [ treatment_method ][ 1 ] ) . otherwise ( F . when ( F . col ( i + \"_outliered\" ) == - 1 , replace_vals [ treatment_method ][ 0 ], ) . otherwise ( F . col ( i )) ), ) if output_mode == \"replace\" : odf = odf . drop ( i ) . withColumnRenamed ( i + \"_outliered\" , i ) else : odf = odf . drop ( i + \"_outliered\" ) odf = odf . drop ( \"outliered\" ) if treatment & ( treatment_method == \"row_removal\" ): if skewed_cols : warnings . warn ( \"Columns dropped from outlier treatment due to highly skewed distribution: \" + ( \",\" ) . join ( skewed_cols ) ) for index , i in enumerate ( list_of_cols ): if i not in skewed_cols : odf = odf . where ( ( F . col ( i + \"_outliered\" ) == 0 ) | ( F . col ( i + \"_outliered\" ) . isNull ()) ) . drop ( i + \"_outliered\" ) else : odf = odf . drop ( i + \"_outliered\" ) if not treatment : odf = idf odf_print = spark . createDataFrame ( output_print , schema = [ \"attribute\" , \"lower_outliers\" , \"upper_outliers\" ] ) if print_impact : odf_print . show ( len ( list_of_cols )) return odf , odf_print def IDness_detection ( spark , idf , list_of_cols = \"all\" , drop_cols = [], treatment = False , treatment_threshold = 0.8 , stats_unique = {}, print_impact = False , ): \"\"\" IDness of an attribute is defined as the ratio of number of unique values seen in an attribute by number of non-null rows. It varies between 0 to 100% where IDness of 100% means there are as many unique values as number of rows (primary key in the input dataset). IDness is computed only for discrete features. This function leverages the statistics from Measures of Cardinality function and flag the columns if IDness is above a certain threshold. Such columns can be deleted from the modelling analysis if directed for a treatment. This function returns two dataframes in tuple format \u2013 1st dataframe is input dataset after removing high IDness columns (if treated else the original dataset) and 2nd dataframe is of schema \u2013 attribute, unique_values, IDness. Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of Discrete (Categorical + Integer) columns to analyse e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all discrete columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) treatment Boolean argument \u2013 True or False. If True, columns with high IDness (defined by treatment_threshold argument) are removed from the input dataframe. (Default value = False) treatment_threshold Defines acceptable level of IDness (calculated as no. of unique values divided by no. of non-null values) for a column and takes value between 0 to 1. Default threshold of 0.8 can be interpreted as remove column if its unique values count is more than 80% of total rows (after excluding null values). stats_unique Takes arguments for read_dataset (data_ingest module) function in a dictionary format to read pre-saved statistics on unique value count i.e. if measures_of_cardinality or uniqueCount_computation (data_analyzer.stats_generator module) has been computed & saved before. (Default value = {}) print_impact True, False This argument is to print out the statistics and the impact of treatment (if applicable).(Default value = False) Returns ------- odf : DataFrame Dataframe after column removal if treated, else original input dataframe. odf_print : DataFrame schema [attribute, unique_values, IDness, flagged/treated]. unique_values is no. of distinct values in a column, IDness is unique_values divided by no. of non-null values. A column is flagged 1 if IDness is above the threshold, else 0. \"\"\" if list_of_cols == \"all\" : num_cols , cat_cols , other_cols = attributeType_segregation ( idf ) list_of_cols = num_cols + cat_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in drop_cols ])) for i in idf . select ( list_of_cols ) . dtypes : if i [ 1 ] not in ( \"string\" , \"int\" , \"bigint\" , \"long\" ): list_of_cols . remove ( i [ 0 ]) if any ( x not in idf . columns for x in list_of_cols ): raise TypeError ( \"Invalid input for Column(s)\" ) if len ( list_of_cols ) == 0 : warnings . warn ( \"No IDness Check - No discrete column(s) to analyze\" ) odf = idf schema = T . StructType ( [ T . StructField ( \"attribute\" , T . StringType (), True ), T . StructField ( \"unique_values\" , T . StringType (), True ), T . StructField ( \"IDness\" , T . StringType (), True ), T . StructField ( \"flagged\" , T . StringType (), True ), ] ) odf_print = spark . sparkContext . emptyRDD () . toDF ( schema ) return odf , odf_print treatment_threshold = float ( treatment_threshold ) if ( treatment_threshold < 0 ) | ( treatment_threshold > 1 ): raise TypeError ( \"Invalid input for Treatment Threshold Value\" ) if str ( treatment ) . lower () == \"true\" : treatment = True elif str ( treatment ) . lower () == \"false\" : treatment = False else : raise TypeError ( \"Non-Boolean input for treatment\" ) if stats_unique == {}: odf_print = measures_of_cardinality ( spark , idf , list_of_cols ) else : odf_print = read_dataset ( spark , ** stats_unique ) . where ( F . col ( \"attribute\" ) . isin ( list_of_cols ) ) odf_print = odf_print . withColumn ( \"flagged\" , F . when ( F . col ( \"IDness\" ) >= treatment_threshold , 1 ) . otherwise ( 0 ) ) if treatment : remove_cols = ( odf_print . where ( F . col ( \"flagged\" ) == 1 ) . select ( \"attribute\" ) . rdd . flatMap ( lambda x : x ) . collect () ) odf = idf . drop ( * remove_cols ) odf_print = odf_print . withColumnRenamed ( \"flagged\" , \"treated\" ) else : odf = idf if print_impact : odf_print . show ( len ( list_of_cols )) if treatment : print ( \"Removed Columns: \" , remove_cols ) return odf , odf_print def biasedness_detection ( spark , idf , list_of_cols = \"all\" , drop_cols = [], treatment = False , treatment_threshold = 0.8 , stats_mode = {}, print_impact = False , ): \"\"\" This function flags column if they are biased or skewed towards one specific value and leverages mode_pct computation from Measures of Central Tendency i.e. number of rows with mode value (most frequently seen value) divided by number of non-null values. It varies between 0 to 100% where biasedness of 100% means there is only a single value (other than null). The function flags a column if its biasedness is above a certain threshold. Such columns can be deleted from the modelling analysis, if required. This function returns two dataframes in tuple format \u2013 1st dataframe is input dataset after removing high biased columns (the original dataset if no treatment) and 2nd dataframe is of schema \u2013 attribute, mode, mode_pct. Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of Discrete (Categorical + Integer) columns to analyse e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all discrete columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) treatment Boolean argument \u2013 True or False. If True, columns with high biasedness (defined by treatment_threshold argument) are removed from the input dataframe. (Default value = False) treatment_threshold Defines acceptable level of biasedness (frequency of most-frequently seen value)for a column and takes value between 0 to 1. Default threshold of 0.8 can be interpreted as remove column if the number of rows with most-frequently seen value is more than 80% of total rows (after excluding null values). stats_mode Takes arguments for read_dataset (data_ingest module) function in a dictionary format to read pre-saved statistics on most frequently seen values i.e. if measures_of_centralTendency or mode_computation (data_analyzer.stats_generator module) has been computed & saved before. (Default value = {}) print_impact True, False This argument is to print out the statistics and the impact of treatment (if applicable).(Default value = False) Returns ------- odf : DataFrame Dataframe after column removal if treated, else original input dataframe. odf_print : DataFrame schema [attribute, mode, mode_rows, mode_pct, flagged/treated]. mode is the most frequently seen value, mode_rows is number of rows with mode value, and mode_pct is number of rows with mode value divided by non-null values. A column is flagged 1 if mode_pct is above the threshold else 0. \"\"\" if list_of_cols == \"all\" : num_cols , cat_cols , other_cols = attributeType_segregation ( idf ) list_of_cols = num_cols + cat_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in drop_cols ])) for i in idf . select ( list_of_cols ) . dtypes : if i [ 1 ] not in ( \"string\" , \"int\" , \"bigint\" , \"long\" ): list_of_cols . remove ( i [ 0 ]) if any ( x not in idf . columns for x in list_of_cols ): raise TypeError ( \"Invalid input for Column(s)\" ) if len ( list_of_cols ) == 0 : warnings . warn ( \"No biasedness Check - No discrete column(s) to analyze\" ) odf = idf schema = T . StructType ( [ T . StructField ( \"attribute\" , T . StringType (), True ), T . StructField ( \"mode\" , T . StringType (), True ), T . StructField ( \"mode_rows\" , T . StringType (), True ), T . StructField ( \"mode_pct\" , T . StringType (), True ), T . StructField ( \"flagged\" , T . StringType (), True ), ] ) odf_print = spark . sparkContext . emptyRDD () . toDF ( schema ) return odf , odf_print if ( treatment_threshold < 0 ) | ( treatment_threshold > 1 ): raise TypeError ( \"Invalid input for Treatment Threshold Value\" ) if str ( treatment ) . lower () == \"true\" : treatment = True elif str ( treatment ) . lower () == \"false\" : treatment = False else : raise TypeError ( \"Non-Boolean input for treatment\" ) if stats_mode == {}: odf_print = ( transpose_dataframe ( idf . select ( list_of_cols ) . summary ( \"count\" ), \"summary\" ) . withColumnRenamed ( \"key\" , \"attribute\" ) . join ( mode_computation ( spark , idf , list_of_cols ), \"attribute\" , \"full_outer\" ) . withColumn ( \"mode_pct\" , F . round ( F . col ( \"mode_rows\" ) / F . col ( \"count\" ) . cast ( T . DoubleType ()), 4 ), ) . select ( \"attribute\" , \"mode\" , \"mode_rows\" , \"mode_pct\" ) ) else : odf_print = ( read_dataset ( spark , ** stats_mode ) . select ( \"attribute\" , \"mode\" , \"mode_rows\" , \"mode_pct\" ) . where ( F . col ( \"attribute\" ) . isin ( list_of_cols )) ) odf_print = odf_print . withColumn ( \"flagged\" , F . when ( ( F . col ( \"mode_pct\" ) >= treatment_threshold ) | ( F . col ( \"mode_pct\" ) . isNull ()), 1 ) . otherwise ( 0 ), ) if treatment : remove_cols = ( odf_print . where ( ( F . col ( \"mode_pct\" ) >= treatment_threshold ) | ( F . col ( \"mode_pct\" ) . isNull ()) ) . select ( \"attribute\" ) . rdd . flatMap ( lambda x : x ) . collect () ) odf = idf . drop ( * remove_cols ) odf_print = odf_print . withColumnRenamed ( \"flagged\" , \"treated\" ) else : odf = idf if print_impact : odf_print . show ( len ( list_of_cols )) if treatment : print ( \"Removed Columns: \" , remove_cols ) return odf , odf_print def invalidEntries_detection ( spark , idf , list_of_cols = \"all\" , drop_cols = [], detection_type = \"auto\" , invalid_entries = [], valid_entries = [], partial_match = False , treatment = False , treatment_method = \"null_replacement\" , treatment_configs = {}, stats_missing = {}, stats_unique = {}, stats_mode = {}, output_mode = \"replace\" , print_impact = False , ): \"\"\" This function checks for certain suspicious patterns in attributes\u2019 values. Patterns that are considered for this quality check: - Missing Values: The function checks for all column values which directly or indirectly indicate the missing value in an attribute such as 'nan', 'null', 'na', 'inf', 'n/a', 'not defined' etc. The function also check for special characters. - Repetitive Characters: Certain attributes\u2019 values with repetitive characters may be default value or system error, rather than being a legit value etc xx, zzzzz, 99999 etc. Such values are flagged for the user to take an appropriate action. There may be certain false positive which are legit values. - Consecutive Characters: Similar to repetitive characters, consecutive characters (at least 3 characters long) such as abc, 1234 etc may not be legit values, and hence flagged. There may be certain false positive which are legit values. This function returns two dataframes in tuple format \u2013 1st dataframe is input dataset after treating the invalid values (or the original dataset if no treatment) and 2nd dataframe is of schema \u2013 attribute, invalid_entries, invalid_count, invalid_pct. All potential invalid values (separated by delimiter pipe \u201c|\u201d) are shown under invalid_entries column. Total number of rows impacted by these entries for each attribute is shown under invalid_count. invalid_pct is invalid_count divided by number of rows. Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of Discrete (Categorical + Integer) columns to analyse e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all discrete columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) detection_type \"auto\",\"manual\",\"both\" (Default value = \"auto\") invalid_entries List of values or regex patterns to be classified as invalid. Valid only for \"auto\" or \"both\" detection type. (Default value = []) valid_entries List of values or regex patterns such that a value will be classified as invalid if it does not match any value or regex pattern in it. Valid only for \"auto\" or \"both\" detection type. (Default value = []) partial_match Boolean argument \u2013 True or False. If True, values with substring same as invalid_entries is declared invalid. (Default value = False) treatment Boolean argument \u2013 True or False. If True, outliers are treated as per treatment_method argument. (Default value = False) treatment_method \"MMM\", \"null_replacement\", \"column_removal\" (more methods to be added soon). MMM (Mean Median Mode) replaces invalid value by the measure of central tendency (mode for categorical features and mean or median for numerical features). null_replacement removes all values with any invalid values as null. column_removal remove a column if % of rows with invalid value is above a threshold (defined by key \"treatment_threshold\" under treatment_configs argument). (Default value = \"null_replacement\") treatment_configs Takes input in dictionary format. For column_removal treatment, key \u2018treatment_threshold\u2019 is provided with a value between 0 to 1. For value replacement, by MMM, arguments corresponding to imputation_MMM function (transformer module) are provided, where each key is an argument from imputation_MMM function. For null_replacement, this argument can be skipped. (Default value = {}) stats_missing Takes arguments for read_dataset (data_ingest module) function in a dictionary format to read pre-saved statistics on missing count/pct i.e. if measures_of_counts or missingCount_computation (data_analyzer.stats_generator module) has been computed & saved before. (Default value = {}) stats_unique Takes arguments for read_dataset (data_ingest module) function in a dictionary format to read pre-saved statistics on unique value count i.e. if measures_of_cardinality or uniqueCount_computation (data_analyzer.stats_generator module) has been computed & saved before. (Default value = {}) stats_mode Takes arguments for read_dataset (data_ingest module) function in a dictionary format to read pre-saved statistics on most frequently seen values i.e. if measures_of_centralTendency or mode_computation (data_analyzer.stats_generator module) has been computed & saved before. (Default value = {}) output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with treated column. \u201cappend\u201d option append treated column to the input dataset with a postfix \"_invalid\" e.g. column X is appended as X_invalid. (Default value = \"replace\") print_impact True, False This argument is to print out the statistics.(Default value = False) Returns ------- odf : DataFrame Dataframe after treatment if applicable, else original input dataframe. odf_print : DataFrame schema [attribute, invalid_entries, invalid_count, invalid_pct]. invalid_entries are all potential invalid values (separated by delimiter pipe \u201c|\u201d), invalid_count is no. of rows which are impacted by invalid entries, and invalid_pct is invalid_count divided by no of rows. \"\"\" if list_of_cols == \"all\" : list_of_cols = [] for i in idf . dtypes : if i [ 1 ] in ( \"string\" , \"int\" , \"bigint\" , \"long\" ): list_of_cols . append ( i [ 0 ]) if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in drop_cols ])) if any ( x not in idf . columns for x in list_of_cols ): raise TypeError ( \"Invalid input for Column(s)\" ) if len ( list_of_cols ) == 0 : warnings . warn ( \"No Invalid Entries Check - No discrete column(s) to analyze\" ) odf = idf schema = T . StructType ( [ T . StructField ( \"attribute\" , T . StringType (), True ), T . StructField ( \"invalid_entries\" , T . StringType (), True ), T . StructField ( \"invalid_count\" , T . StringType (), True ), T . StructField ( \"invalid_pct\" , T . StringType (), True ), ] ) odf_print = spark . sparkContext . emptyRDD () . toDF ( schema ) return odf , odf_print if output_mode not in ( \"replace\" , \"append\" ): raise TypeError ( \"Invalid input for output_mode\" ) if str ( treatment ) . lower () == \"true\" : treatment = True elif str ( treatment ) . lower () == \"false\" : treatment = False else : raise TypeError ( \"Non-Boolean input for treatment\" ) if treatment_method not in ( \"MMM\" , \"null_replacement\" , \"column_removal\" ): raise TypeError ( \"Invalid input for method_type\" ) treatment_threshold = treatment_configs . pop ( \"treatment_threshold\" , None ) if treatment_threshold : treatment_threshold = float ( treatment_threshold ) else : if treatment_method == \"column_removal\" : raise TypeError ( \"Invalid input for column removal threshold\" ) null_vocab = [ \"\" , \" \" , \"nan\" , \"null\" , \"na\" , \"inf\" , \"n/a\" , \"not defined\" , \"none\" , \"undefined\" , \"blank\" , \"unknown\" , ] special_chars_vocab = [ \"&\" , \"$\" , \";\" , \":\" , \".\" , \",\" , \"*\" , \"#\" , \"@\" , \"_\" , \"?\" , \"%\" , \"!\" , \"^\" , \"(\" , \")\" , \"-\" , \"/\" , \"'\" , ] def detect ( * v ): output = [] for idx , e in enumerate ( v ): if e is None : output . append ( None ) continue if detection_type in ( \"auto\" , \"both\" ): e = str ( e ) . lower () . strip () # Null & Special Chars Search if e in ( null_vocab + special_chars_vocab ): output . append ( 1 ) continue # Consecutive Identical Chars Search regex = \" \\\\ b([a-zA-Z0-9]) \\\\ 1 \\\\ 1+ \\\\ b\" p = re . compile ( regex ) if re . search ( p , e ): output . append ( 1 ) continue # Ordered Chars Search l = len ( e ) check = 0 if l >= 3 : for i in range ( 1 , l ): if ord ( e [ i ]) - ord ( e [ i - 1 ]) != 1 : check = 1 break if check == 0 : output . append ( 1 ) continue check = 0 if detection_type in ( \"manual\" , \"both\" ): e = str ( e ) . lower () . strip () for regex in invalid_entries : p = re . compile ( regex ) if partial_match : if re . search ( p , e ): check = 1 output . append ( 1 ) break else : if p . fullmatch ( e ): check = 1 output . append ( 1 ) break match_valid_entries = [] for regex in valid_entries : p = re . compile ( regex ) if partial_match : if re . search ( p , e ): match_valid_entries . append ( 1 ) else : match_valid_entries . append ( 0 ) else : if p . fullmatch ( e ): match_valid_entries . append ( 1 ) else : match_valid_entries . append ( 0 ) if ( len ( match_valid_entries ) > 0 ) & ( sum ( match_valid_entries ) == 0 ): check = 1 output . append ( 1 ) if check == 0 : output . append ( 0 ) return output f_detect = F . udf ( detect , T . ArrayType ( T . LongType ())) odf = idf . withColumn ( \"invalid\" , f_detect ( * list_of_cols )) odf . persist () output_print = [] for index , i in enumerate ( list_of_cols ): tmp = odf . withColumn ( i + \"_invalid\" , F . col ( \"invalid\" )[ index ]) invalid = ( tmp . where ( F . col ( i + \"_invalid\" ) == 1 ) . select ( i ) . distinct () . rdd . flatMap ( lambda x : x ) . collect () ) invalid = [ str ( x ) for x in invalid ] invalid_count = tmp . where ( F . col ( i + \"_invalid\" ) == 1 ) . count () output_print . append ( [ i , \"|\" . join ( invalid ), invalid_count , round ( invalid_count / idf . count (), 4 )] ) odf_print = spark . createDataFrame ( output_print , schema = [ \"attribute\" , \"invalid_entries\" , \"invalid_count\" , \"invalid_pct\" ], ) if treatment : if treatment_threshold : threshold_cols = ( odf_print . where ( F . col ( \"attribute\" ) . isin ( list_of_cols )) . where ( F . col ( \"invalid_pct\" ) > treatment_threshold ) . select ( \"attribute\" ) . rdd . flatMap ( lambda x : x ) . collect () ) if treatment_method in ( \"null_replacement\" , \"MMM\" ): for index , i in enumerate ( list_of_cols ): if treatment_threshold : if i not in threshold_cols : odf = odf . drop ( i + \"_invalid\" ) continue odf = odf . withColumn ( i + \"_invalid\" , F . when ( F . col ( \"invalid\" )[ index ] == 1 , None ) . otherwise ( F . col ( i )), ) if output_mode == \"replace\" : odf = odf . drop ( i ) . withColumnRenamed ( i + \"_invalid\" , i ) else : if ( odf_print . where ( F . col ( \"attribute\" ) == i ) . select ( \"invalid_pct\" ) . collect ()[ 0 ][ 0 ] == 0.0 ): odf = odf . drop ( i + \"_invalid\" ) odf = odf . drop ( \"invalid\" ) if treatment_method == \"column_removal\" : odf = idf . drop ( * threshold_cols ) if print_impact : print ( \"Removed Columns: \" , threshold_cols ) if treatment_method == \"MMM\" : if stats_unique == {} or output_mode == \"append\" : remove_cols = ( uniqueCount_computation ( spark , odf , list_of_cols ) . where ( F . col ( \"unique_values\" ) < 2 ) . select ( \"attribute\" ) . rdd . flatMap ( lambda x : x ) . collect () ) else : remove_cols = ( read_dataset ( spark , ** stats_unique ) . where ( F . col ( \"unique_values\" ) < 2 ) . select ( \"attribute\" ) . rdd . flatMap ( lambda x : x ) . collect () ) list_of_cols = [ e for e in list_of_cols if e not in remove_cols ] if treatment_threshold : list_of_cols = [ e for e in threshold_cols if e not in remove_cols ] if output_mode == \"append\" : if len ( list_of_cols ) > 0 : list_of_cols = [ e + \"_invalid\" for e in list_of_cols ] odf = imputation_MMM ( spark , odf , list_of_cols , ** treatment_configs , stats_missing = stats_missing , stats_mode = stats_mode , print_impact = print_impact ) else : odf = idf if print_impact : odf_print . show ( len ( list_of_cols )) return odf , odf_print Functions def IDness_detection ( spark, idf, list_of_cols='all', drop_cols=[], treatment=False, treatment_threshold=0.8, stats_unique={}, print_impact=False) IDness of an attribute is defined as the ratio of number of unique values seen in an attribute by number of non-null rows. It varies between 0 to 100% where IDness of 100% means there are as many unique values as number of rows (primary key in the input dataset). IDness is computed only for discrete features. This function leverages the statistics from Measures of Cardinality function and flag the columns if IDness is above a certain threshold. Such columns can be deleted from the modelling analysis if directed for a treatment. This function returns two dataframes in tuple format \u2013 1st dataframe is input dataset after removing high IDness columns (if treated else the original dataset) and 2nd dataframe is of schema \u2013 attribute, unique_values, IDness. Parameters spark Spark Session idf Input Dataframe list_of_cols List of Discrete (Categorical + Integer) columns to analyse e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all discrete columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) treatment Boolean argument \u2013 True or False. If True, columns with high IDness (defined by treatment_threshold argument) are removed from the input dataframe. (Default value = False) treatment_threshold Defines acceptable level of IDness (calculated as no. of unique values divided by no. of non-null values) for a column and takes value between 0 to 1. Default threshold of 0.8 can be interpreted as remove column if its unique values count is more than 80% of total rows (after excluding null values). stats_unique Takes arguments for read_dataset (data_ingest module) function in a dictionary format to read pre-saved statistics on unique value count i.e. if measures_of_cardinality or uniqueCount_computation (data_analyzer.stats_generator module) has been computed & saved before. (Default value = {}) print_impact True, False This argument is to print out the statistics and the impact of treatment (if applicable).(Default value = False) Returns odf :\u2002 DataFrame Dataframe after column removal if treated, else original input dataframe. odf_print :\u2002 DataFrame schema [attribute, unique_values, IDness, flagged/treated]. unique_values is no. of distinct values in a column, IDness is unique_values divided by no. of non-null values. A column is flagged 1 if IDness is above the threshold, else 0. Expand source code def IDness_detection ( spark , idf , list_of_cols = \"all\" , drop_cols = [], treatment = False , treatment_threshold = 0.8 , stats_unique = {}, print_impact = False , ): \"\"\" IDness of an attribute is defined as the ratio of number of unique values seen in an attribute by number of non-null rows. It varies between 0 to 100% where IDness of 100% means there are as many unique values as number of rows (primary key in the input dataset). IDness is computed only for discrete features. This function leverages the statistics from Measures of Cardinality function and flag the columns if IDness is above a certain threshold. Such columns can be deleted from the modelling analysis if directed for a treatment. This function returns two dataframes in tuple format \u2013 1st dataframe is input dataset after removing high IDness columns (if treated else the original dataset) and 2nd dataframe is of schema \u2013 attribute, unique_values, IDness. Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of Discrete (Categorical + Integer) columns to analyse e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all discrete columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) treatment Boolean argument \u2013 True or False. If True, columns with high IDness (defined by treatment_threshold argument) are removed from the input dataframe. (Default value = False) treatment_threshold Defines acceptable level of IDness (calculated as no. of unique values divided by no. of non-null values) for a column and takes value between 0 to 1. Default threshold of 0.8 can be interpreted as remove column if its unique values count is more than 80% of total rows (after excluding null values). stats_unique Takes arguments for read_dataset (data_ingest module) function in a dictionary format to read pre-saved statistics on unique value count i.e. if measures_of_cardinality or uniqueCount_computation (data_analyzer.stats_generator module) has been computed & saved before. (Default value = {}) print_impact True, False This argument is to print out the statistics and the impact of treatment (if applicable).(Default value = False) Returns ------- odf : DataFrame Dataframe after column removal if treated, else original input dataframe. odf_print : DataFrame schema [attribute, unique_values, IDness, flagged/treated]. unique_values is no. of distinct values in a column, IDness is unique_values divided by no. of non-null values. A column is flagged 1 if IDness is above the threshold, else 0. \"\"\" if list_of_cols == \"all\" : num_cols , cat_cols , other_cols = attributeType_segregation ( idf ) list_of_cols = num_cols + cat_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in drop_cols ])) for i in idf . select ( list_of_cols ) . dtypes : if i [ 1 ] not in ( \"string\" , \"int\" , \"bigint\" , \"long\" ): list_of_cols . remove ( i [ 0 ]) if any ( x not in idf . columns for x in list_of_cols ): raise TypeError ( \"Invalid input for Column(s)\" ) if len ( list_of_cols ) == 0 : warnings . warn ( \"No IDness Check - No discrete column(s) to analyze\" ) odf = idf schema = T . StructType ( [ T . StructField ( \"attribute\" , T . StringType (), True ), T . StructField ( \"unique_values\" , T . StringType (), True ), T . StructField ( \"IDness\" , T . StringType (), True ), T . StructField ( \"flagged\" , T . StringType (), True ), ] ) odf_print = spark . sparkContext . emptyRDD () . toDF ( schema ) return odf , odf_print treatment_threshold = float ( treatment_threshold ) if ( treatment_threshold < 0 ) | ( treatment_threshold > 1 ): raise TypeError ( \"Invalid input for Treatment Threshold Value\" ) if str ( treatment ) . lower () == \"true\" : treatment = True elif str ( treatment ) . lower () == \"false\" : treatment = False else : raise TypeError ( \"Non-Boolean input for treatment\" ) if stats_unique == {}: odf_print = measures_of_cardinality ( spark , idf , list_of_cols ) else : odf_print = read_dataset ( spark , ** stats_unique ) . where ( F . col ( \"attribute\" ) . isin ( list_of_cols ) ) odf_print = odf_print . withColumn ( \"flagged\" , F . when ( F . col ( \"IDness\" ) >= treatment_threshold , 1 ) . otherwise ( 0 ) ) if treatment : remove_cols = ( odf_print . where ( F . col ( \"flagged\" ) == 1 ) . select ( \"attribute\" ) . rdd . flatMap ( lambda x : x ) . collect () ) odf = idf . drop ( * remove_cols ) odf_print = odf_print . withColumnRenamed ( \"flagged\" , \"treated\" ) else : odf = idf if print_impact : odf_print . show ( len ( list_of_cols )) if treatment : print ( \"Removed Columns: \" , remove_cols ) return odf , odf_print def biasedness_detection ( spark, idf, list_of_cols='all', drop_cols=[], treatment=False, treatment_threshold=0.8, stats_mode={}, print_impact=False) This function flags column if they are biased or skewed towards one specific value and leverages mode_pct computation from Measures of Central Tendency i.e. number of rows with mode value (most frequently seen value) divided by number of non-null values. It varies between 0 to 100% where biasedness of 100% means there is only a single value (other than null). The function flags a column if its biasedness is above a certain threshold. Such columns can be deleted from the modelling analysis, if required. This function returns two dataframes in tuple format \u2013 1st dataframe is input dataset after removing high biased columns (the original dataset if no treatment) and 2nd dataframe is of schema \u2013 attribute, mode, mode_pct. Parameters spark Spark Session idf Input Dataframe list_of_cols List of Discrete (Categorical + Integer) columns to analyse e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all discrete columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) treatment Boolean argument \u2013 True or False. If True, columns with high biasedness (defined by treatment_threshold argument) are removed from the input dataframe. (Default value = False) treatment_threshold Defines acceptable level of biasedness (frequency of most-frequently seen value)for a column and takes value between 0 to 1. Default threshold of 0.8 can be interpreted as remove column if the number of rows with most-frequently seen value is more than 80% of total rows (after excluding null values). stats_mode Takes arguments for read_dataset (data_ingest module) function in a dictionary format to read pre-saved statistics on most frequently seen values i.e. if measures_of_centralTendency or mode_computation (data_analyzer.stats_generator module) has been computed & saved before. (Default value = {}) print_impact True, False This argument is to print out the statistics and the impact of treatment (if applicable).(Default value = False) Returns odf :\u2002 DataFrame Dataframe after column removal if treated, else original input dataframe. odf_print :\u2002 DataFrame schema [attribute, mode, mode_rows, mode_pct, flagged/treated]. mode is the most frequently seen value, mode_rows is number of rows with mode value, and mode_pct is number of rows with mode value divided by non-null values. A column is flagged 1 if mode_pct is above the threshold else 0. Expand source code def biasedness_detection ( spark , idf , list_of_cols = \"all\" , drop_cols = [], treatment = False , treatment_threshold = 0.8 , stats_mode = {}, print_impact = False , ): \"\"\" This function flags column if they are biased or skewed towards one specific value and leverages mode_pct computation from Measures of Central Tendency i.e. number of rows with mode value (most frequently seen value) divided by number of non-null values. It varies between 0 to 100% where biasedness of 100% means there is only a single value (other than null). The function flags a column if its biasedness is above a certain threshold. Such columns can be deleted from the modelling analysis, if required. This function returns two dataframes in tuple format \u2013 1st dataframe is input dataset after removing high biased columns (the original dataset if no treatment) and 2nd dataframe is of schema \u2013 attribute, mode, mode_pct. Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of Discrete (Categorical + Integer) columns to analyse e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all discrete columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) treatment Boolean argument \u2013 True or False. If True, columns with high biasedness (defined by treatment_threshold argument) are removed from the input dataframe. (Default value = False) treatment_threshold Defines acceptable level of biasedness (frequency of most-frequently seen value)for a column and takes value between 0 to 1. Default threshold of 0.8 can be interpreted as remove column if the number of rows with most-frequently seen value is more than 80% of total rows (after excluding null values). stats_mode Takes arguments for read_dataset (data_ingest module) function in a dictionary format to read pre-saved statistics on most frequently seen values i.e. if measures_of_centralTendency or mode_computation (data_analyzer.stats_generator module) has been computed & saved before. (Default value = {}) print_impact True, False This argument is to print out the statistics and the impact of treatment (if applicable).(Default value = False) Returns ------- odf : DataFrame Dataframe after column removal if treated, else original input dataframe. odf_print : DataFrame schema [attribute, mode, mode_rows, mode_pct, flagged/treated]. mode is the most frequently seen value, mode_rows is number of rows with mode value, and mode_pct is number of rows with mode value divided by non-null values. A column is flagged 1 if mode_pct is above the threshold else 0. \"\"\" if list_of_cols == \"all\" : num_cols , cat_cols , other_cols = attributeType_segregation ( idf ) list_of_cols = num_cols + cat_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in drop_cols ])) for i in idf . select ( list_of_cols ) . dtypes : if i [ 1 ] not in ( \"string\" , \"int\" , \"bigint\" , \"long\" ): list_of_cols . remove ( i [ 0 ]) if any ( x not in idf . columns for x in list_of_cols ): raise TypeError ( \"Invalid input for Column(s)\" ) if len ( list_of_cols ) == 0 : warnings . warn ( \"No biasedness Check - No discrete column(s) to analyze\" ) odf = idf schema = T . StructType ( [ T . StructField ( \"attribute\" , T . StringType (), True ), T . StructField ( \"mode\" , T . StringType (), True ), T . StructField ( \"mode_rows\" , T . StringType (), True ), T . StructField ( \"mode_pct\" , T . StringType (), True ), T . StructField ( \"flagged\" , T . StringType (), True ), ] ) odf_print = spark . sparkContext . emptyRDD () . toDF ( schema ) return odf , odf_print if ( treatment_threshold < 0 ) | ( treatment_threshold > 1 ): raise TypeError ( \"Invalid input for Treatment Threshold Value\" ) if str ( treatment ) . lower () == \"true\" : treatment = True elif str ( treatment ) . lower () == \"false\" : treatment = False else : raise TypeError ( \"Non-Boolean input for treatment\" ) if stats_mode == {}: odf_print = ( transpose_dataframe ( idf . select ( list_of_cols ) . summary ( \"count\" ), \"summary\" ) . withColumnRenamed ( \"key\" , \"attribute\" ) . join ( mode_computation ( spark , idf , list_of_cols ), \"attribute\" , \"full_outer\" ) . withColumn ( \"mode_pct\" , F . round ( F . col ( \"mode_rows\" ) / F . col ( \"count\" ) . cast ( T . DoubleType ()), 4 ), ) . select ( \"attribute\" , \"mode\" , \"mode_rows\" , \"mode_pct\" ) ) else : odf_print = ( read_dataset ( spark , ** stats_mode ) . select ( \"attribute\" , \"mode\" , \"mode_rows\" , \"mode_pct\" ) . where ( F . col ( \"attribute\" ) . isin ( list_of_cols )) ) odf_print = odf_print . withColumn ( \"flagged\" , F . when ( ( F . col ( \"mode_pct\" ) >= treatment_threshold ) | ( F . col ( \"mode_pct\" ) . isNull ()), 1 ) . otherwise ( 0 ), ) if treatment : remove_cols = ( odf_print . where ( ( F . col ( \"mode_pct\" ) >= treatment_threshold ) | ( F . col ( \"mode_pct\" ) . isNull ()) ) . select ( \"attribute\" ) . rdd . flatMap ( lambda x : x ) . collect () ) odf = idf . drop ( * remove_cols ) odf_print = odf_print . withColumnRenamed ( \"flagged\" , \"treated\" ) else : odf = idf if print_impact : odf_print . show ( len ( list_of_cols )) if treatment : print ( \"Removed Columns: \" , remove_cols ) return odf , odf_print def duplicate_detection ( spark, idf, list_of_cols='all', drop_cols=[], treatment=False, print_impact=False) As the name implies, this function detects duplication in the input dataset. This means, for a pair of duplicate rows, the values in each column coincide. Duplication check is confined to the list of columns passed in the arguments. As part of treatment, duplicated rows are removed. This function returns two dataframes in tuple format; the 1st dataframe is the input dataset after deduplication (if treated else the original dataset). The 2nd dataframe is of schema \u2013 metric, value and contains the total number of rows, number of unique rows, number of duplicate rows and percentage of duplicate rows in total. Parameters spark Spark Session idf Input Dataframe list_of_cols List of columns to analyse e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) treatment Boolean argument \u2013 True or False. If True, duplicate rows are removed from the input dataframe. (Default value = False) print_impact True, False This argument is to print out the statistics.(Default value = False) Returns odf :\u2002 DataFrame de-duplicated dataframe if treated, else original input dataframe. odf_print :\u2002 DataFrame schema [metric, value] and contains metrics - number of rows, number of unique rows, number of duplicate rows and percentage of duplicate rows in total. Expand source code def duplicate_detection ( spark , idf , list_of_cols = \"all\" , drop_cols = [], treatment = False , print_impact = False ): \"\"\" As the name implies, this function detects duplication in the input dataset. This means, for a pair of duplicate rows, the values in each column coincide. Duplication check is confined to the list of columns passed in the arguments. As part of treatment, duplicated rows are removed. This function returns two dataframes in tuple format; the 1st dataframe is the input dataset after deduplication (if treated else the original dataset). The 2nd dataframe is of schema \u2013 metric, value and contains the total number of rows, number of unique rows, number of duplicate rows and percentage of duplicate rows in total. Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of columns to analyse e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) treatment Boolean argument \u2013 True or False. If True, duplicate rows are removed from the input dataframe. (Default value = False) print_impact True, False This argument is to print out the statistics.(Default value = False) Returns ------- odf : DataFrame de-duplicated dataframe if treated, else original input dataframe. odf_print : DataFrame schema [metric, value] and contains metrics - number of rows, number of unique rows, number of duplicate rows and percentage of duplicate rows in total. \"\"\" if list_of_cols == \"all\" : num_cols , cat_cols , other_cols = attributeType_segregation ( idf ) list_of_cols = num_cols + cat_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in drop_cols ])) if any ( x not in idf . columns for x in list_of_cols ) | ( len ( list_of_cols ) == 0 ): raise TypeError ( \"Invalid input for Column(s)\" ) if str ( treatment ) . lower () == \"true\" : treatment = True elif str ( treatment ) . lower () == \"false\" : treatment = False else : raise TypeError ( \"Non-Boolean input for treatment\" ) odf_tmp = idf . drop_duplicates ( subset = list_of_cols ) odf = odf_tmp if treatment else idf odf_print = spark . createDataFrame ( [ [ \"rows_count\" , float ( idf . count ())], [ \"unique_rows_count\" , float ( odf_tmp . count ())], [ \"duplicate_rows\" , float ( idf . count () - odf_tmp . count ())], [ \"duplicate_pct\" , round (( idf . count () - odf_tmp . count ()) / idf . count (), 4 )], ], schema = [ \"metric\" , \"value\" ], ) if print_impact : print ( \"No. of Rows: \" + str ( idf . count ())) print ( \"No. of UNIQUE Rows: \" + str ( odf_tmp . count ())) print ( \"No. of Duplicate Rows: \" + str ( idf . count () - odf_tmp . count ())) print ( \"Percentage of Duplicate Rows: \" + str ( round (( idf . count () - odf_tmp . count ()) / idf . count (), 4 )) ) return odf , odf_print def invalidEntries_detection ( spark, idf, list_of_cols='all', drop_cols=[], detection_type='auto', invalid_entries=[], valid_entries=[], partial_match=False, treatment=False, treatment_method='null_replacement', treatment_configs={}, stats_missing={}, stats_unique={}, stats_mode={}, output_mode='replace', print_impact=False) This function checks for certain suspicious patterns in attributes\u2019 values. Patterns that are considered for this quality check: Missing Values: The function checks for all column values which directly or indirectly indicate the missing value in an attribute such as 'nan', 'null', 'na', 'inf', 'n/a', 'not defined' etc. The function also check for special characters. Repetitive Characters: Certain attributes\u2019 values with repetitive characters may be default value or system error, rather than being a legit value etc xx, zzzzz, 99999 etc. Such values are flagged for the user to take an appropriate action. There may be certain false positive which are legit values. Consecutive Characters: Similar to repetitive characters, consecutive characters (at least 3 characters long) such as abc, 1234 etc may not be legit values, and hence flagged. There may be certain false positive which are legit values. This function returns two dataframes in tuple format \u2013 1st dataframe is input dataset after treating the invalid values (or the original dataset if no treatment) and 2nd dataframe is of schema \u2013 attribute, invalid_entries, invalid_count, invalid_pct. All potential invalid values (separated by delimiter pipe \u201c|\u201d) are shown under invalid_entries column. Total number of rows impacted by these entries for each attribute is shown under invalid_count. invalid_pct is invalid_count divided by number of rows. Parameters spark Spark Session idf Input Dataframe list_of_cols List of Discrete (Categorical + Integer) columns to analyse e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all discrete columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) detection_type \"auto\",\"manual\",\"both\" (Default value = \"auto\") invalid_entries List of values or regex patterns to be classified as invalid. Valid only for \"auto\" or \"both\" detection type. (Default value = []) valid_entries List of values or regex patterns such that a value will be classified as invalid if it does not match any value or regex pattern in it. Valid only for \"auto\" or \"both\" detection type. (Default value = []) partial_match Boolean argument \u2013 True or False. If True, values with substring same as invalid_entries is declared invalid. (Default value = False) treatment Boolean argument \u2013 True or False. If True, outliers are treated as per treatment_method argument. (Default value = False) treatment_method \"MMM\", \"null_replacement\", \"column_removal\" (more methods to be added soon). MMM (Mean Median Mode) replaces invalid value by the measure of central tendency (mode for categorical features and mean or median for numerical features). null_replacement removes all values with any invalid values as null. column_removal remove a column if % of rows with invalid value is above a threshold (defined by key \"treatment_threshold\" under treatment_configs argument). (Default value = \"null_replacement\") treatment_configs Takes input in dictionary format. For column_removal treatment, key \u2018treatment_threshold\u2019 is provided with a value between 0 to 1. For value replacement, by MMM, arguments corresponding to imputation_MMM function (transformer module) are provided, where each key is an argument from imputation_MMM function. For null_replacement, this argument can be skipped. (Default value = {}) stats_missing Takes arguments for read_dataset (data_ingest module) function in a dictionary format to read pre-saved statistics on missing count/pct i.e. if measures_of_counts or missingCount_computation (data_analyzer.stats_generator module) has been computed & saved before. (Default value = {}) stats_unique Takes arguments for read_dataset (data_ingest module) function in a dictionary format to read pre-saved statistics on unique value count i.e. if measures_of_cardinality or uniqueCount_computation (data_analyzer.stats_generator module) has been computed & saved before. (Default value = {}) stats_mode Takes arguments for read_dataset (data_ingest module) function in a dictionary format to read pre-saved statistics on most frequently seen values i.e. if measures_of_centralTendency or mode_computation (data_analyzer.stats_generator module) has been computed & saved before. (Default value = {}) output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with treated column. \u201cappend\u201d option append treated column to the input dataset with a postfix \"_invalid\" e.g. column X is appended as X_invalid. (Default value = \"replace\") print_impact True, False This argument is to print out the statistics.(Default value = False) Returns odf :\u2002 DataFrame Dataframe after treatment if applicable, else original input dataframe. odf_print :\u2002 DataFrame schema [attribute, invalid_entries, invalid_count, invalid_pct]. invalid_entries are all potential invalid values (separated by delimiter pipe \u201c|\u201d), invalid_count is no. of rows which are impacted by invalid entries, and invalid_pct is invalid_count divided by no of rows. Expand source code def invalidEntries_detection ( spark , idf , list_of_cols = \"all\" , drop_cols = [], detection_type = \"auto\" , invalid_entries = [], valid_entries = [], partial_match = False , treatment = False , treatment_method = \"null_replacement\" , treatment_configs = {}, stats_missing = {}, stats_unique = {}, stats_mode = {}, output_mode = \"replace\" , print_impact = False , ): \"\"\" This function checks for certain suspicious patterns in attributes\u2019 values. Patterns that are considered for this quality check: - Missing Values: The function checks for all column values which directly or indirectly indicate the missing value in an attribute such as 'nan', 'null', 'na', 'inf', 'n/a', 'not defined' etc. The function also check for special characters. - Repetitive Characters: Certain attributes\u2019 values with repetitive characters may be default value or system error, rather than being a legit value etc xx, zzzzz, 99999 etc. Such values are flagged for the user to take an appropriate action. There may be certain false positive which are legit values. - Consecutive Characters: Similar to repetitive characters, consecutive characters (at least 3 characters long) such as abc, 1234 etc may not be legit values, and hence flagged. There may be certain false positive which are legit values. This function returns two dataframes in tuple format \u2013 1st dataframe is input dataset after treating the invalid values (or the original dataset if no treatment) and 2nd dataframe is of schema \u2013 attribute, invalid_entries, invalid_count, invalid_pct. All potential invalid values (separated by delimiter pipe \u201c|\u201d) are shown under invalid_entries column. Total number of rows impacted by these entries for each attribute is shown under invalid_count. invalid_pct is invalid_count divided by number of rows. Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of Discrete (Categorical + Integer) columns to analyse e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all discrete columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) detection_type \"auto\",\"manual\",\"both\" (Default value = \"auto\") invalid_entries List of values or regex patterns to be classified as invalid. Valid only for \"auto\" or \"both\" detection type. (Default value = []) valid_entries List of values or regex patterns such that a value will be classified as invalid if it does not match any value or regex pattern in it. Valid only for \"auto\" or \"both\" detection type. (Default value = []) partial_match Boolean argument \u2013 True or False. If True, values with substring same as invalid_entries is declared invalid. (Default value = False) treatment Boolean argument \u2013 True or False. If True, outliers are treated as per treatment_method argument. (Default value = False) treatment_method \"MMM\", \"null_replacement\", \"column_removal\" (more methods to be added soon). MMM (Mean Median Mode) replaces invalid value by the measure of central tendency (mode for categorical features and mean or median for numerical features). null_replacement removes all values with any invalid values as null. column_removal remove a column if % of rows with invalid value is above a threshold (defined by key \"treatment_threshold\" under treatment_configs argument). (Default value = \"null_replacement\") treatment_configs Takes input in dictionary format. For column_removal treatment, key \u2018treatment_threshold\u2019 is provided with a value between 0 to 1. For value replacement, by MMM, arguments corresponding to imputation_MMM function (transformer module) are provided, where each key is an argument from imputation_MMM function. For null_replacement, this argument can be skipped. (Default value = {}) stats_missing Takes arguments for read_dataset (data_ingest module) function in a dictionary format to read pre-saved statistics on missing count/pct i.e. if measures_of_counts or missingCount_computation (data_analyzer.stats_generator module) has been computed & saved before. (Default value = {}) stats_unique Takes arguments for read_dataset (data_ingest module) function in a dictionary format to read pre-saved statistics on unique value count i.e. if measures_of_cardinality or uniqueCount_computation (data_analyzer.stats_generator module) has been computed & saved before. (Default value = {}) stats_mode Takes arguments for read_dataset (data_ingest module) function in a dictionary format to read pre-saved statistics on most frequently seen values i.e. if measures_of_centralTendency or mode_computation (data_analyzer.stats_generator module) has been computed & saved before. (Default value = {}) output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with treated column. \u201cappend\u201d option append treated column to the input dataset with a postfix \"_invalid\" e.g. column X is appended as X_invalid. (Default value = \"replace\") print_impact True, False This argument is to print out the statistics.(Default value = False) Returns ------- odf : DataFrame Dataframe after treatment if applicable, else original input dataframe. odf_print : DataFrame schema [attribute, invalid_entries, invalid_count, invalid_pct]. invalid_entries are all potential invalid values (separated by delimiter pipe \u201c|\u201d), invalid_count is no. of rows which are impacted by invalid entries, and invalid_pct is invalid_count divided by no of rows. \"\"\" if list_of_cols == \"all\" : list_of_cols = [] for i in idf . dtypes : if i [ 1 ] in ( \"string\" , \"int\" , \"bigint\" , \"long\" ): list_of_cols . append ( i [ 0 ]) if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in drop_cols ])) if any ( x not in idf . columns for x in list_of_cols ): raise TypeError ( \"Invalid input for Column(s)\" ) if len ( list_of_cols ) == 0 : warnings . warn ( \"No Invalid Entries Check - No discrete column(s) to analyze\" ) odf = idf schema = T . StructType ( [ T . StructField ( \"attribute\" , T . StringType (), True ), T . StructField ( \"invalid_entries\" , T . StringType (), True ), T . StructField ( \"invalid_count\" , T . StringType (), True ), T . StructField ( \"invalid_pct\" , T . StringType (), True ), ] ) odf_print = spark . sparkContext . emptyRDD () . toDF ( schema ) return odf , odf_print if output_mode not in ( \"replace\" , \"append\" ): raise TypeError ( \"Invalid input for output_mode\" ) if str ( treatment ) . lower () == \"true\" : treatment = True elif str ( treatment ) . lower () == \"false\" : treatment = False else : raise TypeError ( \"Non-Boolean input for treatment\" ) if treatment_method not in ( \"MMM\" , \"null_replacement\" , \"column_removal\" ): raise TypeError ( \"Invalid input for method_type\" ) treatment_threshold = treatment_configs . pop ( \"treatment_threshold\" , None ) if treatment_threshold : treatment_threshold = float ( treatment_threshold ) else : if treatment_method == \"column_removal\" : raise TypeError ( \"Invalid input for column removal threshold\" ) null_vocab = [ \"\" , \" \" , \"nan\" , \"null\" , \"na\" , \"inf\" , \"n/a\" , \"not defined\" , \"none\" , \"undefined\" , \"blank\" , \"unknown\" , ] special_chars_vocab = [ \"&\" , \"$\" , \";\" , \":\" , \".\" , \",\" , \"*\" , \"#\" , \"@\" , \"_\" , \"?\" , \"%\" , \"!\" , \"^\" , \"(\" , \")\" , \"-\" , \"/\" , \"'\" , ] def detect ( * v ): output = [] for idx , e in enumerate ( v ): if e is None : output . append ( None ) continue if detection_type in ( \"auto\" , \"both\" ): e = str ( e ) . lower () . strip () # Null & Special Chars Search if e in ( null_vocab + special_chars_vocab ): output . append ( 1 ) continue # Consecutive Identical Chars Search regex = \" \\\\ b([a-zA-Z0-9]) \\\\ 1 \\\\ 1+ \\\\ b\" p = re . compile ( regex ) if re . search ( p , e ): output . append ( 1 ) continue # Ordered Chars Search l = len ( e ) check = 0 if l >= 3 : for i in range ( 1 , l ): if ord ( e [ i ]) - ord ( e [ i - 1 ]) != 1 : check = 1 break if check == 0 : output . append ( 1 ) continue check = 0 if detection_type in ( \"manual\" , \"both\" ): e = str ( e ) . lower () . strip () for regex in invalid_entries : p = re . compile ( regex ) if partial_match : if re . search ( p , e ): check = 1 output . append ( 1 ) break else : if p . fullmatch ( e ): check = 1 output . append ( 1 ) break match_valid_entries = [] for regex in valid_entries : p = re . compile ( regex ) if partial_match : if re . search ( p , e ): match_valid_entries . append ( 1 ) else : match_valid_entries . append ( 0 ) else : if p . fullmatch ( e ): match_valid_entries . append ( 1 ) else : match_valid_entries . append ( 0 ) if ( len ( match_valid_entries ) > 0 ) & ( sum ( match_valid_entries ) == 0 ): check = 1 output . append ( 1 ) if check == 0 : output . append ( 0 ) return output f_detect = F . udf ( detect , T . ArrayType ( T . LongType ())) odf = idf . withColumn ( \"invalid\" , f_detect ( * list_of_cols )) odf . persist () output_print = [] for index , i in enumerate ( list_of_cols ): tmp = odf . withColumn ( i + \"_invalid\" , F . col ( \"invalid\" )[ index ]) invalid = ( tmp . where ( F . col ( i + \"_invalid\" ) == 1 ) . select ( i ) . distinct () . rdd . flatMap ( lambda x : x ) . collect () ) invalid = [ str ( x ) for x in invalid ] invalid_count = tmp . where ( F . col ( i + \"_invalid\" ) == 1 ) . count () output_print . append ( [ i , \"|\" . join ( invalid ), invalid_count , round ( invalid_count / idf . count (), 4 )] ) odf_print = spark . createDataFrame ( output_print , schema = [ \"attribute\" , \"invalid_entries\" , \"invalid_count\" , \"invalid_pct\" ], ) if treatment : if treatment_threshold : threshold_cols = ( odf_print . where ( F . col ( \"attribute\" ) . isin ( list_of_cols )) . where ( F . col ( \"invalid_pct\" ) > treatment_threshold ) . select ( \"attribute\" ) . rdd . flatMap ( lambda x : x ) . collect () ) if treatment_method in ( \"null_replacement\" , \"MMM\" ): for index , i in enumerate ( list_of_cols ): if treatment_threshold : if i not in threshold_cols : odf = odf . drop ( i + \"_invalid\" ) continue odf = odf . withColumn ( i + \"_invalid\" , F . when ( F . col ( \"invalid\" )[ index ] == 1 , None ) . otherwise ( F . col ( i )), ) if output_mode == \"replace\" : odf = odf . drop ( i ) . withColumnRenamed ( i + \"_invalid\" , i ) else : if ( odf_print . where ( F . col ( \"attribute\" ) == i ) . select ( \"invalid_pct\" ) . collect ()[ 0 ][ 0 ] == 0.0 ): odf = odf . drop ( i + \"_invalid\" ) odf = odf . drop ( \"invalid\" ) if treatment_method == \"column_removal\" : odf = idf . drop ( * threshold_cols ) if print_impact : print ( \"Removed Columns: \" , threshold_cols ) if treatment_method == \"MMM\" : if stats_unique == {} or output_mode == \"append\" : remove_cols = ( uniqueCount_computation ( spark , odf , list_of_cols ) . where ( F . col ( \"unique_values\" ) < 2 ) . select ( \"attribute\" ) . rdd . flatMap ( lambda x : x ) . collect () ) else : remove_cols = ( read_dataset ( spark , ** stats_unique ) . where ( F . col ( \"unique_values\" ) < 2 ) . select ( \"attribute\" ) . rdd . flatMap ( lambda x : x ) . collect () ) list_of_cols = [ e for e in list_of_cols if e not in remove_cols ] if treatment_threshold : list_of_cols = [ e for e in threshold_cols if e not in remove_cols ] if output_mode == \"append\" : if len ( list_of_cols ) > 0 : list_of_cols = [ e + \"_invalid\" for e in list_of_cols ] odf = imputation_MMM ( spark , odf , list_of_cols , ** treatment_configs , stats_missing = stats_missing , stats_mode = stats_mode , print_impact = print_impact ) else : odf = idf if print_impact : odf_print . show ( len ( list_of_cols )) return odf , odf_print def nullColumns_detection ( spark, idf, list_of_cols='missing', drop_cols=[], treatment=False, treatment_method='row_removal', treatment_configs={}, stats_missing={}, stats_unique={}, stats_mode={}, print_impact=False) This function inspects the column quality and computes the number of rows that are missing for a column. This function also leverages statistics computed as part of the State Generator module. Statistics are not computed twice if already available. As part of treatments, it currently supports the following methods \u2013 Mean Median Mode (MMM), row_removal, column_removal, KNN, regression, Matrix Factorization (MF), auto imputation (auto). - MMM replaces null value with the measure of central tendency (mode for categorical features and mean/median for numerical features). - row_removal removes all rows with any missing value (output of this treatment is same as nullRows_detection with treatment_threshold of 0). - column_removal remove a column if %rows with a missing value is above treatment_threshold. - KNN/regression create an imputation model for every to-be-imputed column based on the rest of columns in the list_of_cols columns. KNN leverages sklearn.impute.KNNImputer and regression sklearn.impute.IterativeImputer. Since sklearn algorithms are not scalable, we create imputation model on sample dataset and apply that model on the whole dataset in distributed manner using pyspark pandas udf. - Matrix Factorization leverages pyspark.ml.recommendation.ALS algorithm. - auto imputation compares all imputation methods and select the best imputation method based on the least RMSE. This function returns two dataframes in tuple format \u2013 1st dataframe is input dataset after imputation (if treated else the original dataset) and 2nd dataframe is of schema \u2013 attribute, missing_count, missing_pct. Parameters spark Spark Session idf Input Dataframe list_of_cols List of columns to inspect e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all (non-array) columns for analysis. This is super useful instead of specifying all column names manually. \"missing\" (default) can be passed to include only those columns with missing values. One of the usecases where \"all\" may be preferable over \"missing\" is when the user wants to save the imputation model for the future use e.g. a column may not have missing value in the training dataset but missing values may possibly appear in the prediction dataset. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) treatment Boolean argument \u2013 True or False. If True, missing values are treated as per treatment_method argument. (Default value = False) treatment_method \"MMM\", \"row_removal\", \"column_removal\", \"KNN\", \"regression\", \"MF\", \"auto\". (Default value = \"row_removal\") treatment_configs Takes input in dictionary format. For column_removal treatment, key \u2018treatment_threshold\u2019 is provided with a value between 0 to 1 (remove column if % of rows with missing value is above this threshold) For row_removal, this argument can be skipped. For MMM, arguments corresponding to imputation_MMM function (transformer module) are provided, where each key is an argument from imputation_MMM function. For KNN, arguments corresponding to imputation_sklearn function (transformer module) are provided, where each key is an argument from imputation_sklearn function. method_type should be \"KNN\" For regression, arguments corresponding to imputation_sklearn function (transformer module) are provided, where each key is an argument from imputation_sklearn function. method_type should be \"regression\" For MF, arguments corresponding to imputation_matrixFactorization function (transformer module) are provided, where each key is an argument from imputation_matrixFactorization function. For auto, arguments corresponding to auto_imputation function (transformer module) are provided, where each key is an argument from auto_imputation function. (Default value = {}) stats_missing Takes arguments for read_dataset (data_ingest module) function in a dictionary format to read pre-saved statistics on missing count/pct i.e. if measures_of_counts or missingCount_computation (data_analyzer.stats_generator module) has been computed & saved before. (Default value = {}) stats_unique Takes arguments for read_dataset (data_ingest module) function in a dictionary format to read pre-saved statistics on unique value count i.e. if measures_of_cardinality or uniqueCount_computation (data_analyzer.stats_generator module) has been computed & saved before. (Default value = {}) stats_mode Takes arguments for read_dataset (data_ingest module) function in a dictionary format to read pre-saved statistics on most frequently seen values i.e. if measures_of_centralTendency or mode_computation (data_analyzer.stats_generator module) has been computed & saved before. (Default value = {}) print_impact True, False This argument is to print out the statistics or the impact of imputation (if applicable).(Default value = False) Returns odf :\u2002 DataFrame Imputed dataframe if treated, else original input dataframe. odf_print :\u2002 DataFrame schema [attribute, missing_count, missing_pct]. missing_count is number of rows with null values for an attribute, and missing_pct is missing_count divided by number of rows. Expand source code def nullColumns_detection ( spark , idf , list_of_cols = \"missing\" , drop_cols = [], treatment = False , treatment_method = \"row_removal\" , treatment_configs = {}, stats_missing = {}, stats_unique = {}, stats_mode = {}, print_impact = False , ): \"\"\" This function inspects the column quality and computes the number of rows that are missing for a column. This function also leverages statistics computed as part of the State Generator module. Statistics are not computed twice if already available. As part of treatments, it currently supports the following methods \u2013 Mean Median Mode (MMM), row_removal, column_removal, KNN, regression, Matrix Factorization (MF), auto imputation (auto). - MMM replaces null value with the measure of central tendency (mode for categorical features and mean/median for numerical features). - row_removal removes all rows with any missing value (output of this treatment is same as nullRows_detection with treatment_threshold of 0). - column_removal remove a column if %rows with a missing value is above treatment_threshold. - KNN/regression create an imputation model for every to-be-imputed column based on the rest of columns in the list_of_cols columns. KNN leverages sklearn.impute.KNNImputer and regression sklearn.impute.IterativeImputer. Since sklearn algorithms are not scalable, we create imputation model on sample dataset and apply that model on the whole dataset in distributed manner using pyspark pandas udf. - Matrix Factorization leverages pyspark.ml.recommendation.ALS algorithm. - auto imputation compares all imputation methods and select the best imputation method based on the least RMSE. This function returns two dataframes in tuple format \u2013 1st dataframe is input dataset after imputation (if treated else the original dataset) and 2nd dataframe is of schema \u2013 attribute, missing_count, missing_pct. Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of columns to inspect e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all (non-array) columns for analysis. This is super useful instead of specifying all column names manually. \"missing\" (default) can be passed to include only those columns with missing values. One of the usecases where \"all\" may be preferable over \"missing\" is when the user wants to save the imputation model for the future use e.g. a column may not have missing value in the training dataset but missing values may possibly appear in the prediction dataset. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) treatment Boolean argument \u2013 True or False. If True, missing values are treated as per treatment_method argument. (Default value = False) treatment_method \"MMM\", \"row_removal\", \"column_removal\", \"KNN\", \"regression\", \"MF\", \"auto\". (Default value = \"row_removal\") treatment_configs Takes input in dictionary format. For column_removal treatment, key \u2018treatment_threshold\u2019 is provided with a value between 0 to 1 (remove column if % of rows with missing value is above this threshold) For row_removal, this argument can be skipped. For MMM, arguments corresponding to imputation_MMM function (transformer module) are provided, where each key is an argument from imputation_MMM function. For KNN, arguments corresponding to imputation_sklearn function (transformer module) are provided, where each key is an argument from imputation_sklearn function. method_type should be \"KNN\" For regression, arguments corresponding to imputation_sklearn function (transformer module) are provided, where each key is an argument from imputation_sklearn function. method_type should be \"regression\" For MF, arguments corresponding to imputation_matrixFactorization function (transformer module) are provided, where each key is an argument from imputation_matrixFactorization function. For auto, arguments corresponding to auto_imputation function (transformer module) are provided, where each key is an argument from auto_imputation function. (Default value = {}) stats_missing Takes arguments for read_dataset (data_ingest module) function in a dictionary format to read pre-saved statistics on missing count/pct i.e. if measures_of_counts or missingCount_computation (data_analyzer.stats_generator module) has been computed & saved before. (Default value = {}) stats_unique Takes arguments for read_dataset (data_ingest module) function in a dictionary format to read pre-saved statistics on unique value count i.e. if measures_of_cardinality or uniqueCount_computation (data_analyzer.stats_generator module) has been computed & saved before. (Default value = {}) stats_mode Takes arguments for read_dataset (data_ingest module) function in a dictionary format to read pre-saved statistics on most frequently seen values i.e. if measures_of_centralTendency or mode_computation (data_analyzer.stats_generator module) has been computed & saved before. (Default value = {}) print_impact True, False This argument is to print out the statistics or the impact of imputation (if applicable).(Default value = False) Returns ------- odf : DataFrame Imputed dataframe if treated, else original input dataframe. odf_print : DataFrame schema [attribute, missing_count, missing_pct]. missing_count is number of rows with null values for an attribute, and missing_pct is missing_count divided by number of rows. \"\"\" if stats_missing == {}: odf_print = missingCount_computation ( spark , idf ) else : odf_print = read_dataset ( spark , ** stats_missing ) . select ( \"attribute\" , \"missing_count\" , \"missing_pct\" ) missing_cols = ( odf_print . where ( F . col ( \"missing_count\" ) > 0 ) . select ( \"attribute\" ) . rdd . flatMap ( lambda x : x ) . collect () ) if list_of_cols == \"all\" : num_cols , cat_cols , other_cols = attributeType_segregation ( idf ) list_of_cols = num_cols + cat_cols if list_of_cols == \"missing\" : list_of_cols = missing_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in drop_cols ])) if len ( list_of_cols ) == 0 : warnings . warn ( \"No Null Detection - No column(s) to analyze\" ) odf = idf schema = T . StructType ( [ T . StructField ( \"attribute\" , T . StringType (), True ), T . StructField ( \"missing_count\" , T . StringType (), True ), T . StructField ( \"missing_pct\" , T . StringType (), True ), ] ) odf_print = spark . sparkContext . emptyRDD () . toDF ( schema ) return odf , odf_print if any ( x not in idf . columns for x in list_of_cols ): raise TypeError ( \"Invalid input for Column(s)\" ) if str ( treatment ) . lower () == \"true\" : treatment = True elif str ( treatment ) . lower () == \"false\" : treatment = False else : raise TypeError ( \"Non-Boolean input for treatment\" ) if treatment_method not in ( \"MMM\" , \"row_removal\" , \"column_removal\" , \"KNN\" , \"regression\" , \"MF\" , \"auto\" , ): raise TypeError ( \"Invalid input for method_type\" ) treatment_threshold = treatment_configs . pop ( \"treatment_threshold\" , None ) if treatment_threshold : treatment_threshold = float ( treatment_threshold ) else : if treatment_method == \"column_removal\" : raise TypeError ( \"Invalid input for column removal threshold\" ) odf_print = odf_print . where ( F . col ( \"attribute\" ) . isin ( list_of_cols )) if treatment : if treatment_threshold : threshold_cols = ( odf_print . where ( F . col ( \"attribute\" ) . isin ( list_of_cols )) . where ( F . col ( \"missing_pct\" ) > treatment_threshold ) . select ( \"attribute\" ) . rdd . flatMap ( lambda x : x ) . collect () ) if treatment_method == \"column_removal\" : odf = idf . drop ( * threshold_cols ) if print_impact : odf_print . show ( len ( list_of_cols )) print ( \"Removed Columns: \" , threshold_cols ) if treatment_method == \"row_removal\" : remove_cols = ( odf_print . where ( F . col ( \"attribute\" ) . isin ( list_of_cols )) . where ( F . col ( \"missing_pct\" ) == 1.0 ) . select ( \"attribute\" ) . rdd . flatMap ( lambda x : x ) . collect () ) list_of_cols = [ e for e in list_of_cols if e not in remove_cols ] if treatment_threshold : list_of_cols = [ e for e in threshold_cols if e not in remove_cols ] odf = idf . dropna ( subset = list_of_cols ) if print_impact : odf_print . show ( len ( list_of_cols )) print ( \"Before Count: \" + str ( idf . count ())) print ( \"After Count: \" + str ( odf . count ())) if treatment_method == \"MMM\" : if stats_unique == {}: remove_cols = ( uniqueCount_computation ( spark , idf , list_of_cols ) . where ( F . col ( \"unique_values\" ) < 2 ) . select ( \"attribute\" ) . rdd . flatMap ( lambda x : x ) . collect () ) else : remove_cols = ( read_dataset ( spark , ** stats_unique ) . where ( F . col ( \"unique_values\" ) < 2 ) . select ( \"attribute\" ) . rdd . flatMap ( lambda x : x ) . collect () ) list_of_cols = [ e for e in list_of_cols if e not in remove_cols ] if treatment_threshold : list_of_cols = [ e for e in threshold_cols if e not in remove_cols ] odf = imputation_MMM ( spark , idf , list_of_cols , ** treatment_configs , stats_missing = stats_missing , stats_mode = stats_mode , print_impact = print_impact ) if treatment_method in ( \"KNN\" , \"regression\" , \"MF\" , \"auto\" ): if treatment_threshold : list_of_cols = threshold_cols list_of_cols = [ e for e in list_of_cols if e in num_cols ] func_mapping = { \"KNN\" : imputation_sklearn , \"regression\" : imputation_sklearn , \"MF\" : imputation_matrixFactorization , \"auto\" : auto_imputation , } func = func_mapping [ treatment_method ] odf = func ( spark , idf , list_of_cols , ** treatment_configs , stats_missing = stats_missing , print_impact = print_impact ) else : odf = idf if print_impact : odf_print . show ( len ( list_of_cols )) return odf , odf_print def nullRows_detection ( spark, idf, list_of_cols='all', drop_cols=[], treatment=False, treatment_threshold=0.8, print_impact=False) This function inspects the row quality and computes the number of columns that are missing for a row. This metric is further aggregated to check how many columns are missing for how many rows (or % rows). Intuition is if too many columns are missing for a row, removing it from the modeling may give better results than relying on its imputed values. Therefore as part of the treatment, rows with missing columns above the specified threshold are removed. This function returns two dataframes in tuple format; the 1st dataframe is the input dataset after filtering rows with a high number of missing columns (if treated else the original dataframe). The 2nd dataframe is of schema \u2013 null_cols_count, row_count, row_pct, flagged/treated. null_cols_count row_count row_pct flagged 5 11 3.0E-4 0 7 1306 0.0401 1 Interpretation: 1306 rows (4.01% of total rows) have 7 missing columns and flagged for are removal because null_cols_count is above the threshold. If treatment is True, then flagged column is renamed as treated to show rows which has been removed. Parameters spark Spark Session idf Input Dataframe list_of_cols List of columns to analyse e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) treatment Boolean argument \u2013 True or False. If True, rows with high no. of null columns (defined by treatment_threshold argument) are removed from the input dataframe. (Default value = False) treatment_threshold Defines % of columns allowed to be Null per row and takes value between 0 to 1. If % of null columns is above the threshold for a row, it is removed from the dataframe. There is no row removal if the threshold is 1.0. And if the threshold is 0, all rows with null value are removed. (Default value = 0.8) print_impact True, False This argument is to print out the statistics.(Default value = False) Returns odf :\u2002 DataFrame Dataframe after row removal if treated, else original input dataframe. odf_print :\u2002 DataFrame schema [null_cols_count, row_count, row_pct, flagged/treated]. null_cols_count is defined as no. of missing columns in a row. row_count is no. of rows with null_cols_count missing columns. row_pct is row_count divided by number of rows. flagged/treated is 1 if null_cols_count is more than (threshold X Number of Columns), else 0. Expand source code def nullRows_detection ( spark , idf , list_of_cols = \"all\" , drop_cols = [], treatment = False , treatment_threshold = 0.8 , print_impact = False , ): \"\"\" This function inspects the row quality and computes the number of columns that are missing for a row. This metric is further aggregated to check how many columns are missing for how many rows (or % rows). Intuition is if too many columns are missing for a row, removing it from the modeling may give better results than relying on its imputed values. Therefore as part of the treatment, rows with missing columns above the specified threshold are removed. This function returns two dataframes in tuple format; the 1st dataframe is the input dataset after filtering rows with a high number of missing columns (if treated else the original dataframe). The 2nd dataframe is of schema \u2013 null_cols_count, row_count, row_pct, flagged/treated. | null_cols_count | row_count | row_pct | flagged | |-----------------|-----------|---------|---------| | 5 | 11 | 3.0E-4 | 0 | | 7 | 1306 | 0.0401 | 1 | Interpretation: 1306 rows (4.01% of total rows) have 7 missing columns and flagged for are removal because null_cols_count is above the threshold. If treatment is True, then flagged column is renamed as treated to show rows which has been removed. Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of columns to analyse e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) treatment Boolean argument \u2013 True or False. If True, rows with high no. of null columns (defined by treatment_threshold argument) are removed from the input dataframe. (Default value = False) treatment_threshold Defines % of columns allowed to be Null per row and takes value between 0 to 1. If % of null columns is above the threshold for a row, it is removed from the dataframe. There is no row removal if the threshold is 1.0. And if the threshold is 0, all rows with null value are removed. (Default value = 0.8) print_impact True, False This argument is to print out the statistics.(Default value = False) Returns ------- odf : DataFrame Dataframe after row removal if treated, else original input dataframe. odf_print : DataFrame schema [null_cols_count, row_count, row_pct, flagged/treated]. null_cols_count is defined as no. of missing columns in a row. row_count is no. of rows with null_cols_count missing columns. row_pct is row_count divided by number of rows. flagged/treated is 1 if null_cols_count is more than (threshold X Number of Columns), else 0. \"\"\" if list_of_cols == \"all\" : num_cols , cat_cols , other_cols = attributeType_segregation ( idf ) list_of_cols = num_cols + cat_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in drop_cols ])) if any ( x not in idf . columns for x in list_of_cols ) | ( len ( list_of_cols ) == 0 ): raise TypeError ( \"Invalid input for Column(s)\" ) if str ( treatment ) . lower () == \"true\" : treatment = True elif str ( treatment ) . lower () == \"false\" : treatment = False else : raise TypeError ( \"Non-Boolean input for treatment\" ) treatment_threshold = float ( treatment_threshold ) if ( treatment_threshold < 0 ) | ( treatment_threshold > 1 ): raise TypeError ( \"Invalid input for Treatment Threshold Value\" ) def null_count ( * cols ): return cols . count ( None ) f_null_count = F . udf ( null_count , T . LongType ()) odf_tmp = idf . withColumn ( \"null_cols_count\" , f_null_count ( * list_of_cols )) . withColumn ( \"flagged\" , F . when ( F . col ( \"null_cols_count\" ) > ( len ( list_of_cols ) * treatment_threshold ), 1 ) . otherwise ( 0 ), ) if treatment_threshold == 1 : odf_tmp = odf_tmp . withColumn ( \"flagged\" , F . when ( F . col ( \"null_cols_count\" ) == len ( list_of_cols ), 1 ) . otherwise ( 0 ), ) odf_print = ( odf_tmp . groupBy ( \"null_cols_count\" , \"flagged\" ) . agg ( F . count ( F . lit ( 1 )) . alias ( \"row_count\" )) . withColumn ( \"row_pct\" , F . round ( F . col ( \"row_count\" ) / float ( idf . count ()), 4 )) . select ( \"null_cols_count\" , \"row_count\" , \"row_pct\" , \"flagged\" ) . orderBy ( \"null_cols_count\" ) ) if treatment : odf = odf_tmp . where ( F . col ( \"flagged\" ) == 0 ) . drop ( * [ \"null_cols_count\" , \"flagged\" ]) odf_print = odf_print . withColumnRenamed ( \"flagged\" , \"treated\" ) else : odf = idf if print_impact : odf_print . show ( odf . count ()) return odf , odf_print def outlier_detection ( spark, idf, list_of_cols='all', drop_cols=[], detection_side='upper', detection_configs={'pctile_lower': 0.05, 'pctile_upper': 0.95, 'stdev_lower': 3.0, 'stdev_upper': 3.0, 'IQR_lower': 1.5, 'IQR_upper': 1.5, 'min_validation': 2}, treatment=False, treatment_method='value_replacement', pre_existing_model=False, model_path='NA', output_mode='replace', stats_unique={}, print_impact=False) In Machine Learning, outlier detection identifies values that deviate drastically from the rest of the attribute values. An outlier may be caused simply by chance, measurement error, or inherent heavy-tailed distribution. This function identifies extreme values in both directions (or any direction provided by the user via detection_side argument). Outlier is identified by 3 different methodologies and tagged an outlier only if it is validated by at least 2 methods (can be changed by the user via min_validation under detection_configs argument). Percentile Method: In this methodology, a value higher than a certain (default 95th) percentile value is considered as an outlier. Similarly, a value lower than a certain (default 5th) percentile value is considered as an outlier. Standard Deviation Method: In this methodology, if a value is a certain number of standard deviations (default 3) away from the mean, it is identified as an outlier. Interquartile Range (IQR) Method: A value below Q1 \u2013 1.5 IQR or above Q3 + 1.5 IQR are identified as outliers, where Q1 is in first quantile/25th percentile, Q3 is in third quantile/75th percentile, and IQR is the difference between third quantile & first quantile. This function also leverages statistics which were computed as the part of the State Generator module so that statistics are not computed twice if already available. As part of treatments available, outlier values can be replaced by null so that it can be imputed by a reliable imputation methodology (null_replacement). It can also be replaced by maximum or minimum permissible by above methodologies (value_replacement). Lastly, rows can be removed if it is identified with any outlier (row_removal). This function returns two dataframes in tuple format \u2013 1st dataframe is input dataset after treating outlier (the original dataset if no treatment) and 2nd dataframe is of schema \u2013 attribute, lower_outliers, upper_outliers. If outliers are checked only for upper end, then lower_outliers column will be shown all zero. Similarly if checked only for lower end, then upper_outliers will be zero for all attributes. Parameters spark Spark Session idf Input Dataframe list_of_cols List of numerical columns to analyse e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all numerical columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) detection_side \"upper\", \"lower\", \"both\". \"lower\" detects outliers in the lower spectrum of the column range, whereas \"upper\" detects in the upper spectrum. \"Both\" detects in both upper and lower end of the spectrum. (Default value = \"upper\") detection_configs Takes input in dictionary format with keys representing upper & lower parameter for three outlier detection methodologies. a) Percentile Method: lower and upper percentile threshold can be set via \"pctile_lower\" & \"pctile_upper\" (default 0.05 & 0.95) Any value above \"pctile_upper\" is considered as an outlier. Similarly, a value lower than \"pctile_lower\" is considered as an outlier. b) Standard Deviation Method: In this methodology, if a value which is below (mean - \"stdev_lower\" * standard deviation) or above (mean + \"stdev_upper\" * standard deviation), then it is identified as an outlier (default 3.0 & 3.0). c) Interquartile Range (IQR) Method: A value which is below (Q1 \u2013 \"IQR_lower\" * IQR) or above (Q3 + \"IQR_lower\" * IQR) is identified as outliers, where Q1 is first quartile/25th percentile, Q3 is third quartile/75th percentile and IQR is difference between third quartile & first quartile (default 1.5 & 1.5). If an attribute value is less (more) than its derived lower (upper) bound value, it is considered as outlier by a methodology. A attribute value is considered as outlier if it is declared as outlier by atleast 'min_validation' methodologies (default 2). treatment Boolean argument \u2013 True or False. If True, outliers are treated as per treatment_method argument. (Default value = False) treatment_method \"null_replacement\", \"row_removal\", \"value_replacement\". In \"null_replacement\", outlier values are replaced by null so that it can be imputed by a reliable imputation methodology. In \"value_replacement\", outlier values are replaced by maximum or minimum permissible value by above methodologies. Lastly in \"row_removal\", rows are removed if it is found with any outlier. (Default value = \"value_replacement\") pre_existing_model Boolean argument \u2013 True or False. True if the model with upper/lower permissible values for each attribute exists already to be used, False otherwise. (Default value = False) model_path If pre_existing_model is True, this argument is path for the pre-saved model. If pre_existing_model is False, this field can be used for saving the model. Default \"NA\" means there is neither pre-existing model nor there is a need to save one. output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with treated column. \u201cappend\u201d option append treated column to the input dataset with a postfix \"_outliered\" e.g. column X is appended as X_outliered. (Default value = \"replace\") stats_unique Takes arguments for read_dataset (data_ingest module) function in a dictionary format to read pre-saved statistics on unique value count i.e. if measures_of_cardinality or uniqueCount_computation (data_analyzer.stats_generator module) has been computed & saved before. (Default value = {}) print_impact True, False This argument is to print out the statistics and the impact of treatment (if applicable).(Default value = False) Returns odf :\u2002 DataFrame Imputed dataframe if treated, else original input dataframe. odf_print :\u2002 DataFrame schema [attribute, lower_outliers, upper_outliers]. lower_outliers is no. of outliers found in the lower spectrum of the attribute range, and upper_outliers is outlier count in the upper spectrum. Expand source code def outlier_detection ( spark , idf , list_of_cols = \"all\" , drop_cols = [], detection_side = \"upper\" , detection_configs = { \"pctile_lower\" : 0.05 , \"pctile_upper\" : 0.95 , \"stdev_lower\" : 3.0 , \"stdev_upper\" : 3.0 , \"IQR_lower\" : 1.5 , \"IQR_upper\" : 1.5 , \"min_validation\" : 2 , }, treatment = False , treatment_method = \"value_replacement\" , pre_existing_model = False , model_path = \"NA\" , output_mode = \"replace\" , stats_unique = {}, print_impact = False , ): \"\"\" In Machine Learning, outlier detection identifies values that deviate drastically from the rest of the attribute values. An outlier may be caused simply by chance, measurement error, or inherent heavy-tailed distribution. This function identifies extreme values in both directions (or any direction provided by the user via detection_side argument). Outlier is identified by 3 different methodologies and tagged an outlier only if it is validated by at least 2 methods (can be changed by the user via min_validation under detection_configs argument). - Percentile Method: In this methodology, a value higher than a certain (default 95th) percentile value is considered as an outlier. Similarly, a value lower than a certain (default 5th) percentile value is considered as an outlier. - Standard Deviation Method: In this methodology, if a value is a certain number of standard deviations (default 3) away from the mean, it is identified as an outlier. - Interquartile Range (IQR) Method: A value below Q1 \u2013 1.5 IQR or above Q3 + 1.5 IQR are identified as outliers, where Q1 is in first quantile/25th percentile, Q3 is in third quantile/75th percentile, and IQR is the difference between third quantile & first quantile. This function also leverages statistics which were computed as the part of the State Generator module so that statistics are not computed twice if already available. As part of treatments available, outlier values can be replaced by null so that it can be imputed by a reliable imputation methodology (null_replacement). It can also be replaced by maximum or minimum permissible by above methodologies (value_replacement). Lastly, rows can be removed if it is identified with any outlier (row_removal). This function returns two dataframes in tuple format \u2013 1st dataframe is input dataset after treating outlier (the original dataset if no treatment) and 2nd dataframe is of schema \u2013 attribute, lower_outliers, upper_outliers. If outliers are checked only for upper end, then lower_outliers column will be shown all zero. Similarly if checked only for lower end, then upper_outliers will be zero for all attributes. Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of numerical columns to analyse e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all numerical columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) detection_side \"upper\", \"lower\", \"both\". \"lower\" detects outliers in the lower spectrum of the column range, whereas \"upper\" detects in the upper spectrum. \"Both\" detects in both upper and lower end of the spectrum. (Default value = \"upper\") detection_configs Takes input in dictionary format with keys representing upper & lower parameter for three outlier detection methodologies. a) Percentile Method: lower and upper percentile threshold can be set via \"pctile_lower\" & \"pctile_upper\" (default 0.05 & 0.95) Any value above \"pctile_upper\" is considered as an outlier. Similarly, a value lower than \"pctile_lower\" is considered as an outlier. b) Standard Deviation Method: In this methodology, if a value which is below (mean - \"stdev_lower\" * standard deviation) or above (mean + \"stdev_upper\" * standard deviation), then it is identified as an outlier (default 3.0 & 3.0). c) Interquartile Range (IQR) Method: A value which is below (Q1 \u2013 \"IQR_lower\" * IQR) or above (Q3 + \"IQR_lower\" * IQR) is identified as outliers, where Q1 is first quartile/25th percentile, Q3 is third quartile/75th percentile and IQR is difference between third quartile & first quartile (default 1.5 & 1.5). If an attribute value is less (more) than its derived lower (upper) bound value, it is considered as outlier by a methodology. A attribute value is considered as outlier if it is declared as outlier by atleast 'min_validation' methodologies (default 2). treatment Boolean argument \u2013 True or False. If True, outliers are treated as per treatment_method argument. (Default value = False) treatment_method \"null_replacement\", \"row_removal\", \"value_replacement\". In \"null_replacement\", outlier values are replaced by null so that it can be imputed by a reliable imputation methodology. In \"value_replacement\", outlier values are replaced by maximum or minimum permissible value by above methodologies. Lastly in \"row_removal\", rows are removed if it is found with any outlier. (Default value = \"value_replacement\") pre_existing_model Boolean argument \u2013 True or False. True if the model with upper/lower permissible values for each attribute exists already to be used, False otherwise. (Default value = False) model_path If pre_existing_model is True, this argument is path for the pre-saved model. If pre_existing_model is False, this field can be used for saving the model. Default \"NA\" means there is neither pre-existing model nor there is a need to save one. output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with treated column. \u201cappend\u201d option append treated column to the input dataset with a postfix \"_outliered\" e.g. column X is appended as X_outliered. (Default value = \"replace\") stats_unique Takes arguments for read_dataset (data_ingest module) function in a dictionary format to read pre-saved statistics on unique value count i.e. if measures_of_cardinality or uniqueCount_computation (data_analyzer.stats_generator module) has been computed & saved before. (Default value = {}) print_impact True, False This argument is to print out the statistics and the impact of treatment (if applicable).(Default value = False) Returns ------- odf : DataFrame Imputed dataframe if treated, else original input dataframe. odf_print : DataFrame schema [attribute, lower_outliers, upper_outliers]. lower_outliers is no. of outliers found in the lower spectrum of the attribute range, and upper_outliers is outlier count in the upper spectrum. \"\"\" num_cols = attributeType_segregation ( idf )[ 0 ] if len ( num_cols ) == 0 : warnings . warn ( \"No Outlier Check - No numerical column(s) to analyse\" ) odf = idf schema = T . StructType ( [ T . StructField ( \"attribute\" , T . StringType (), True ), T . StructField ( \"lower_outliers\" , T . StringType (), True ), T . StructField ( \"upper_outliers\" , T . StringType (), True ), ] ) odf_print = spark . sparkContext . emptyRDD () . toDF ( schema ) return odf , odf_print if list_of_cols == \"all\" : list_of_cols = num_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] if stats_unique == {}: remove_cols = ( uniqueCount_computation ( spark , idf , list_of_cols ) . where ( F . col ( \"unique_values\" ) < 2 ) . select ( \"attribute\" ) . rdd . flatMap ( lambda x : x ) . collect () ) else : remove_cols = ( read_dataset ( spark , ** stats_unique ) . where ( F . col ( \"unique_values\" ) < 2 ) . select ( \"attribute\" ) . rdd . flatMap ( lambda x : x ) . collect () ) list_of_cols = list ( set ([ e for e in list_of_cols if e not in ( drop_cols + remove_cols )]) ) if any ( x not in num_cols for x in list_of_cols ): raise TypeError ( \"Invalid input for Column(s)\" ) if detection_side not in ( \"upper\" , \"lower\" , \"both\" ): raise TypeError ( \"Invalid input for detection_side\" ) if treatment_method not in ( \"null_replacement\" , \"row_removal\" , \"value_replacement\" ): raise TypeError ( \"Invalid input for treatment_method\" ) if output_mode not in ( \"replace\" , \"append\" ): raise TypeError ( \"Invalid input for output_mode\" ) if str ( treatment ) . lower () == \"true\" : treatment = True elif str ( treatment ) . lower () == \"false\" : treatment = False else : raise TypeError ( \"Non-Boolean input for treatment\" ) if str ( pre_existing_model ) . lower () == \"true\" : pre_existing_model = True elif str ( pre_existing_model ) . lower () == \"false\" : pre_existing_model = False else : raise TypeError ( \"Non-Boolean input for pre_existing_model\" ) for arg in [ \"pctile_lower\" , \"pctile_upper\" ]: if arg in detection_configs : if ( detection_configs [ arg ] < 0 ) | ( detection_configs [ arg ] > 1 ): raise TypeError ( \"Invalid input for \" + arg ) recast_cols = [] recast_type = [] for i in list_of_cols : if get_dtype ( idf , i ) . startswith ( \"decimal\" ): idf = idf . withColumn ( i , F . col ( i ) . cast ( T . DoubleType ())) recast_cols . append ( i ) recast_type . append ( get_dtype ( idf , i )) if pre_existing_model : df_model = spark . read . parquet ( model_path + \"/outlier_numcols\" ) params = [] for i in list_of_cols : mapped_value = ( df_model . where ( F . col ( \"attribute\" ) == i ) . select ( \"parameters\" ) . rdd . flatMap ( lambda x : x ) . collect ()[ 0 ] ) params . append ( mapped_value ) pctile_params = idf . approxQuantile ( list_of_cols , [ detection_configs . get ( \"pctile_lower\" , 0.05 ), detection_configs . get ( \"pctile_upper\" , 0.95 ), ], 0.01 , ) skewed_cols = [] for i , p in zip ( list_of_cols , pctile_params ): if p [ 0 ] == p [ 1 ]: skewed_cols . append ( i ) else : detection_configs [ \"pctile_lower\" ] = detection_configs [ \"pctile_lower\" ] or 0.0 detection_configs [ \"pctile_upper\" ] = detection_configs [ \"pctile_upper\" ] or 1.0 pctile_params = idf . approxQuantile ( list_of_cols , [ detection_configs [ \"pctile_lower\" ], detection_configs [ \"pctile_upper\" ]], 0.01 , ) skewed_cols = [] for i , p in zip ( list_of_cols , pctile_params ): if p [ 0 ] == p [ 1 ]: skewed_cols . append ( i ) detection_configs [ \"stdev_lower\" ] = ( detection_configs [ \"stdev_lower\" ] or detection_configs [ \"stdev_upper\" ] ) detection_configs [ \"stdev_upper\" ] = ( detection_configs [ \"stdev_upper\" ] or detection_configs [ \"stdev_lower\" ] ) stdev_params = [] for i in list_of_cols : mean , stdev = idf . select ( F . mean ( i ), F . stddev ( i )) . first () stdev_params . append ( [ mean - detection_configs [ \"stdev_lower\" ] * stdev , mean + detection_configs [ \"stdev_upper\" ] * stdev , ] ) detection_configs [ \"IQR_lower\" ] = ( detection_configs [ \"IQR_lower\" ] or detection_configs [ \"IQR_upper\" ] ) detection_configs [ \"IQR_upper\" ] = ( detection_configs [ \"IQR_upper\" ] or detection_configs [ \"IQR_lower\" ] ) quantiles = idf . approxQuantile ( list_of_cols , [ 0.25 , 0.75 ], 0.01 ) IQR_params = [ [ e [ 0 ] - detection_configs [ \"IQR_lower\" ] * ( e [ 1 ] - e [ 0 ]), e [ 1 ] + detection_configs [ \"IQR_upper\" ] * ( e [ 1 ] - e [ 0 ]), ] for e in quantiles ] n = detection_configs [ \"min_validation\" ] params = [ [ sorted ([ x [ 0 ], y [ 0 ], z [ 0 ]], reverse = True )[ n - 1 ], sorted ([ x [ 1 ], y [ 1 ], z [ 1 ]])[ n - 1 ], ] for x , y , z in list ( zip ( pctile_params , stdev_params , IQR_params )) ] # Saving model File if required if model_path != \"NA\" : df_model = spark . createDataFrame ( zip ( list_of_cols , params ), schema = [ \"attribute\" , \"parameters\" ] ) df_model . coalesce ( 1 ) . write . parquet ( model_path + \"/outlier_numcols\" , mode = \"overwrite\" ) for i , j in zip ( recast_cols , recast_type ): idf = idf . withColumn ( i , F . col ( i ) . cast ( j )) def composite_outlier ( * v ): output = [] for idx , e in enumerate ( v ): if e is None : output . append ( None ) continue if detection_side in ( \"upper\" , \"both\" ): if e > params [ idx ][ 1 ]: output . append ( 1 ) continue if detection_side in ( \"lower\" , \"both\" ): if e < params [ idx ][ 0 ]: output . append ( - 1 ) continue output . append ( 0 ) return output f_composite_outlier = F . udf ( composite_outlier , T . ArrayType ( T . IntegerType ())) odf = idf . withColumn ( \"outliered\" , f_composite_outlier ( * list_of_cols )) odf . persist () output_print = [] for index , i in enumerate ( list_of_cols ): odf = odf . withColumn ( i + \"_outliered\" , F . col ( \"outliered\" )[ index ]) output_print . append ( [ i , odf . where ( F . col ( i + \"_outliered\" ) == - 1 ) . count (), odf . where ( F . col ( i + \"_outliered\" ) == 1 ) . count (), ] ) if treatment & ( treatment_method in ( \"value_replacement\" , \"null_replacement\" )): if skewed_cols : warnings . warn ( \"Columns dropped from outlier treatment due to highly skewed distribution: \" + ( \",\" ) . join ( skewed_cols ) ) if i not in skewed_cols : replace_vals = { \"value_replacement\" : [ params [ index ][ 0 ], params [ index ][ 1 ]], \"null_replacement\" : [ None , None ], } odf = odf . withColumn ( i + \"_outliered\" , F . when ( F . col ( i + \"_outliered\" ) == 1 , replace_vals [ treatment_method ][ 1 ] ) . otherwise ( F . when ( F . col ( i + \"_outliered\" ) == - 1 , replace_vals [ treatment_method ][ 0 ], ) . otherwise ( F . col ( i )) ), ) if output_mode == \"replace\" : odf = odf . drop ( i ) . withColumnRenamed ( i + \"_outliered\" , i ) else : odf = odf . drop ( i + \"_outliered\" ) odf = odf . drop ( \"outliered\" ) if treatment & ( treatment_method == \"row_removal\" ): if skewed_cols : warnings . warn ( \"Columns dropped from outlier treatment due to highly skewed distribution: \" + ( \",\" ) . join ( skewed_cols ) ) for index , i in enumerate ( list_of_cols ): if i not in skewed_cols : odf = odf . where ( ( F . col ( i + \"_outliered\" ) == 0 ) | ( F . col ( i + \"_outliered\" ) . isNull ()) ) . drop ( i + \"_outliered\" ) else : odf = odf . drop ( i + \"_outliered\" ) if not treatment : odf = idf odf_print = spark . createDataFrame ( output_print , schema = [ \"attribute\" , \"lower_outliers\" , \"upper_outliers\" ] ) if print_impact : odf_print . show ( len ( list_of_cols )) return odf , odf_print","title":"<code>quality_checker</code>"},{"location":"api/data_analyzer/quality_checker.html#quality_checker","text":"This submodule focuses on assessing the data quality at both row-level and column-level and also provides an appropriate treatment option to fix quality issues. At the row level, the following checks are done: duplicate_detection nullRows_detection At the column level, the following checks are done: nullColumns_detection outlier_detection IDness_detection biasedness_detection invalidEntries_detection Expand source code # coding=utf-8 \"\"\" This submodule focuses on assessing the data quality at both row-level and column-level and also provides an appropriate treatment option to fix quality issues. At the row level, the following checks are done: - duplicate_detection - nullRows_detection At the column level, the following checks are done: - nullColumns_detection - outlier_detection - IDness_detection - biasedness_detection - invalidEntries_detection \"\"\" import re import warnings from pyspark.sql import functions as F from pyspark.sql import types as T from anovos.data_analyzer.stats_generator import ( uniqueCount_computation , missingCount_computation , mode_computation , measures_of_cardinality , ) from anovos.data_ingest.data_ingest import read_dataset from anovos.data_transformer.transformers import ( imputation_MMM , imputation_sklearn , imputation_matrixFactorization , auto_imputation , ) from anovos.shared.utils import ( attributeType_segregation , transpose_dataframe , get_dtype , ) def duplicate_detection ( spark , idf , list_of_cols = \"all\" , drop_cols = [], treatment = False , print_impact = False ): \"\"\" As the name implies, this function detects duplication in the input dataset. This means, for a pair of duplicate rows, the values in each column coincide. Duplication check is confined to the list of columns passed in the arguments. As part of treatment, duplicated rows are removed. This function returns two dataframes in tuple format; the 1st dataframe is the input dataset after deduplication (if treated else the original dataset). The 2nd dataframe is of schema \u2013 metric, value and contains the total number of rows, number of unique rows, number of duplicate rows and percentage of duplicate rows in total. Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of columns to analyse e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) treatment Boolean argument \u2013 True or False. If True, duplicate rows are removed from the input dataframe. (Default value = False) print_impact True, False This argument is to print out the statistics.(Default value = False) Returns ------- odf : DataFrame de-duplicated dataframe if treated, else original input dataframe. odf_print : DataFrame schema [metric, value] and contains metrics - number of rows, number of unique rows, number of duplicate rows and percentage of duplicate rows in total. \"\"\" if list_of_cols == \"all\" : num_cols , cat_cols , other_cols = attributeType_segregation ( idf ) list_of_cols = num_cols + cat_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in drop_cols ])) if any ( x not in idf . columns for x in list_of_cols ) | ( len ( list_of_cols ) == 0 ): raise TypeError ( \"Invalid input for Column(s)\" ) if str ( treatment ) . lower () == \"true\" : treatment = True elif str ( treatment ) . lower () == \"false\" : treatment = False else : raise TypeError ( \"Non-Boolean input for treatment\" ) odf_tmp = idf . drop_duplicates ( subset = list_of_cols ) odf = odf_tmp if treatment else idf odf_print = spark . createDataFrame ( [ [ \"rows_count\" , float ( idf . count ())], [ \"unique_rows_count\" , float ( odf_tmp . count ())], [ \"duplicate_rows\" , float ( idf . count () - odf_tmp . count ())], [ \"duplicate_pct\" , round (( idf . count () - odf_tmp . count ()) / idf . count (), 4 )], ], schema = [ \"metric\" , \"value\" ], ) if print_impact : print ( \"No. of Rows: \" + str ( idf . count ())) print ( \"No. of UNIQUE Rows: \" + str ( odf_tmp . count ())) print ( \"No. of Duplicate Rows: \" + str ( idf . count () - odf_tmp . count ())) print ( \"Percentage of Duplicate Rows: \" + str ( round (( idf . count () - odf_tmp . count ()) / idf . count (), 4 )) ) return odf , odf_print def nullRows_detection ( spark , idf , list_of_cols = \"all\" , drop_cols = [], treatment = False , treatment_threshold = 0.8 , print_impact = False , ): \"\"\" This function inspects the row quality and computes the number of columns that are missing for a row. This metric is further aggregated to check how many columns are missing for how many rows (or % rows). Intuition is if too many columns are missing for a row, removing it from the modeling may give better results than relying on its imputed values. Therefore as part of the treatment, rows with missing columns above the specified threshold are removed. This function returns two dataframes in tuple format; the 1st dataframe is the input dataset after filtering rows with a high number of missing columns (if treated else the original dataframe). The 2nd dataframe is of schema \u2013 null_cols_count, row_count, row_pct, flagged/treated. | null_cols_count | row_count | row_pct | flagged | |-----------------|-----------|---------|---------| | 5 | 11 | 3.0E-4 | 0 | | 7 | 1306 | 0.0401 | 1 | Interpretation: 1306 rows (4.01% of total rows) have 7 missing columns and flagged for are removal because null_cols_count is above the threshold. If treatment is True, then flagged column is renamed as treated to show rows which has been removed. Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of columns to analyse e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) treatment Boolean argument \u2013 True or False. If True, rows with high no. of null columns (defined by treatment_threshold argument) are removed from the input dataframe. (Default value = False) treatment_threshold Defines % of columns allowed to be Null per row and takes value between 0 to 1. If % of null columns is above the threshold for a row, it is removed from the dataframe. There is no row removal if the threshold is 1.0. And if the threshold is 0, all rows with null value are removed. (Default value = 0.8) print_impact True, False This argument is to print out the statistics.(Default value = False) Returns ------- odf : DataFrame Dataframe after row removal if treated, else original input dataframe. odf_print : DataFrame schema [null_cols_count, row_count, row_pct, flagged/treated]. null_cols_count is defined as no. of missing columns in a row. row_count is no. of rows with null_cols_count missing columns. row_pct is row_count divided by number of rows. flagged/treated is 1 if null_cols_count is more than (threshold X Number of Columns), else 0. \"\"\" if list_of_cols == \"all\" : num_cols , cat_cols , other_cols = attributeType_segregation ( idf ) list_of_cols = num_cols + cat_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in drop_cols ])) if any ( x not in idf . columns for x in list_of_cols ) | ( len ( list_of_cols ) == 0 ): raise TypeError ( \"Invalid input for Column(s)\" ) if str ( treatment ) . lower () == \"true\" : treatment = True elif str ( treatment ) . lower () == \"false\" : treatment = False else : raise TypeError ( \"Non-Boolean input for treatment\" ) treatment_threshold = float ( treatment_threshold ) if ( treatment_threshold < 0 ) | ( treatment_threshold > 1 ): raise TypeError ( \"Invalid input for Treatment Threshold Value\" ) def null_count ( * cols ): return cols . count ( None ) f_null_count = F . udf ( null_count , T . LongType ()) odf_tmp = idf . withColumn ( \"null_cols_count\" , f_null_count ( * list_of_cols )) . withColumn ( \"flagged\" , F . when ( F . col ( \"null_cols_count\" ) > ( len ( list_of_cols ) * treatment_threshold ), 1 ) . otherwise ( 0 ), ) if treatment_threshold == 1 : odf_tmp = odf_tmp . withColumn ( \"flagged\" , F . when ( F . col ( \"null_cols_count\" ) == len ( list_of_cols ), 1 ) . otherwise ( 0 ), ) odf_print = ( odf_tmp . groupBy ( \"null_cols_count\" , \"flagged\" ) . agg ( F . count ( F . lit ( 1 )) . alias ( \"row_count\" )) . withColumn ( \"row_pct\" , F . round ( F . col ( \"row_count\" ) / float ( idf . count ()), 4 )) . select ( \"null_cols_count\" , \"row_count\" , \"row_pct\" , \"flagged\" ) . orderBy ( \"null_cols_count\" ) ) if treatment : odf = odf_tmp . where ( F . col ( \"flagged\" ) == 0 ) . drop ( * [ \"null_cols_count\" , \"flagged\" ]) odf_print = odf_print . withColumnRenamed ( \"flagged\" , \"treated\" ) else : odf = idf if print_impact : odf_print . show ( odf . count ()) return odf , odf_print def nullColumns_detection ( spark , idf , list_of_cols = \"missing\" , drop_cols = [], treatment = False , treatment_method = \"row_removal\" , treatment_configs = {}, stats_missing = {}, stats_unique = {}, stats_mode = {}, print_impact = False , ): \"\"\" This function inspects the column quality and computes the number of rows that are missing for a column. This function also leverages statistics computed as part of the State Generator module. Statistics are not computed twice if already available. As part of treatments, it currently supports the following methods \u2013 Mean Median Mode (MMM), row_removal, column_removal, KNN, regression, Matrix Factorization (MF), auto imputation (auto). - MMM replaces null value with the measure of central tendency (mode for categorical features and mean/median for numerical features). - row_removal removes all rows with any missing value (output of this treatment is same as nullRows_detection with treatment_threshold of 0). - column_removal remove a column if %rows with a missing value is above treatment_threshold. - KNN/regression create an imputation model for every to-be-imputed column based on the rest of columns in the list_of_cols columns. KNN leverages sklearn.impute.KNNImputer and regression sklearn.impute.IterativeImputer. Since sklearn algorithms are not scalable, we create imputation model on sample dataset and apply that model on the whole dataset in distributed manner using pyspark pandas udf. - Matrix Factorization leverages pyspark.ml.recommendation.ALS algorithm. - auto imputation compares all imputation methods and select the best imputation method based on the least RMSE. This function returns two dataframes in tuple format \u2013 1st dataframe is input dataset after imputation (if treated else the original dataset) and 2nd dataframe is of schema \u2013 attribute, missing_count, missing_pct. Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of columns to inspect e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all (non-array) columns for analysis. This is super useful instead of specifying all column names manually. \"missing\" (default) can be passed to include only those columns with missing values. One of the usecases where \"all\" may be preferable over \"missing\" is when the user wants to save the imputation model for the future use e.g. a column may not have missing value in the training dataset but missing values may possibly appear in the prediction dataset. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) treatment Boolean argument \u2013 True or False. If True, missing values are treated as per treatment_method argument. (Default value = False) treatment_method \"MMM\", \"row_removal\", \"column_removal\", \"KNN\", \"regression\", \"MF\", \"auto\". (Default value = \"row_removal\") treatment_configs Takes input in dictionary format. For column_removal treatment, key \u2018treatment_threshold\u2019 is provided with a value between 0 to 1 (remove column if % of rows with missing value is above this threshold) For row_removal, this argument can be skipped. For MMM, arguments corresponding to imputation_MMM function (transformer module) are provided, where each key is an argument from imputation_MMM function. For KNN, arguments corresponding to imputation_sklearn function (transformer module) are provided, where each key is an argument from imputation_sklearn function. method_type should be \"KNN\" For regression, arguments corresponding to imputation_sklearn function (transformer module) are provided, where each key is an argument from imputation_sklearn function. method_type should be \"regression\" For MF, arguments corresponding to imputation_matrixFactorization function (transformer module) are provided, where each key is an argument from imputation_matrixFactorization function. For auto, arguments corresponding to auto_imputation function (transformer module) are provided, where each key is an argument from auto_imputation function. (Default value = {}) stats_missing Takes arguments for read_dataset (data_ingest module) function in a dictionary format to read pre-saved statistics on missing count/pct i.e. if measures_of_counts or missingCount_computation (data_analyzer.stats_generator module) has been computed & saved before. (Default value = {}) stats_unique Takes arguments for read_dataset (data_ingest module) function in a dictionary format to read pre-saved statistics on unique value count i.e. if measures_of_cardinality or uniqueCount_computation (data_analyzer.stats_generator module) has been computed & saved before. (Default value = {}) stats_mode Takes arguments for read_dataset (data_ingest module) function in a dictionary format to read pre-saved statistics on most frequently seen values i.e. if measures_of_centralTendency or mode_computation (data_analyzer.stats_generator module) has been computed & saved before. (Default value = {}) print_impact True, False This argument is to print out the statistics or the impact of imputation (if applicable).(Default value = False) Returns ------- odf : DataFrame Imputed dataframe if treated, else original input dataframe. odf_print : DataFrame schema [attribute, missing_count, missing_pct]. missing_count is number of rows with null values for an attribute, and missing_pct is missing_count divided by number of rows. \"\"\" if stats_missing == {}: odf_print = missingCount_computation ( spark , idf ) else : odf_print = read_dataset ( spark , ** stats_missing ) . select ( \"attribute\" , \"missing_count\" , \"missing_pct\" ) missing_cols = ( odf_print . where ( F . col ( \"missing_count\" ) > 0 ) . select ( \"attribute\" ) . rdd . flatMap ( lambda x : x ) . collect () ) if list_of_cols == \"all\" : num_cols , cat_cols , other_cols = attributeType_segregation ( idf ) list_of_cols = num_cols + cat_cols if list_of_cols == \"missing\" : list_of_cols = missing_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in drop_cols ])) if len ( list_of_cols ) == 0 : warnings . warn ( \"No Null Detection - No column(s) to analyze\" ) odf = idf schema = T . StructType ( [ T . StructField ( \"attribute\" , T . StringType (), True ), T . StructField ( \"missing_count\" , T . StringType (), True ), T . StructField ( \"missing_pct\" , T . StringType (), True ), ] ) odf_print = spark . sparkContext . emptyRDD () . toDF ( schema ) return odf , odf_print if any ( x not in idf . columns for x in list_of_cols ): raise TypeError ( \"Invalid input for Column(s)\" ) if str ( treatment ) . lower () == \"true\" : treatment = True elif str ( treatment ) . lower () == \"false\" : treatment = False else : raise TypeError ( \"Non-Boolean input for treatment\" ) if treatment_method not in ( \"MMM\" , \"row_removal\" , \"column_removal\" , \"KNN\" , \"regression\" , \"MF\" , \"auto\" , ): raise TypeError ( \"Invalid input for method_type\" ) treatment_threshold = treatment_configs . pop ( \"treatment_threshold\" , None ) if treatment_threshold : treatment_threshold = float ( treatment_threshold ) else : if treatment_method == \"column_removal\" : raise TypeError ( \"Invalid input for column removal threshold\" ) odf_print = odf_print . where ( F . col ( \"attribute\" ) . isin ( list_of_cols )) if treatment : if treatment_threshold : threshold_cols = ( odf_print . where ( F . col ( \"attribute\" ) . isin ( list_of_cols )) . where ( F . col ( \"missing_pct\" ) > treatment_threshold ) . select ( \"attribute\" ) . rdd . flatMap ( lambda x : x ) . collect () ) if treatment_method == \"column_removal\" : odf = idf . drop ( * threshold_cols ) if print_impact : odf_print . show ( len ( list_of_cols )) print ( \"Removed Columns: \" , threshold_cols ) if treatment_method == \"row_removal\" : remove_cols = ( odf_print . where ( F . col ( \"attribute\" ) . isin ( list_of_cols )) . where ( F . col ( \"missing_pct\" ) == 1.0 ) . select ( \"attribute\" ) . rdd . flatMap ( lambda x : x ) . collect () ) list_of_cols = [ e for e in list_of_cols if e not in remove_cols ] if treatment_threshold : list_of_cols = [ e for e in threshold_cols if e not in remove_cols ] odf = idf . dropna ( subset = list_of_cols ) if print_impact : odf_print . show ( len ( list_of_cols )) print ( \"Before Count: \" + str ( idf . count ())) print ( \"After Count: \" + str ( odf . count ())) if treatment_method == \"MMM\" : if stats_unique == {}: remove_cols = ( uniqueCount_computation ( spark , idf , list_of_cols ) . where ( F . col ( \"unique_values\" ) < 2 ) . select ( \"attribute\" ) . rdd . flatMap ( lambda x : x ) . collect () ) else : remove_cols = ( read_dataset ( spark , ** stats_unique ) . where ( F . col ( \"unique_values\" ) < 2 ) . select ( \"attribute\" ) . rdd . flatMap ( lambda x : x ) . collect () ) list_of_cols = [ e for e in list_of_cols if e not in remove_cols ] if treatment_threshold : list_of_cols = [ e for e in threshold_cols if e not in remove_cols ] odf = imputation_MMM ( spark , idf , list_of_cols , ** treatment_configs , stats_missing = stats_missing , stats_mode = stats_mode , print_impact = print_impact ) if treatment_method in ( \"KNN\" , \"regression\" , \"MF\" , \"auto\" ): if treatment_threshold : list_of_cols = threshold_cols list_of_cols = [ e for e in list_of_cols if e in num_cols ] func_mapping = { \"KNN\" : imputation_sklearn , \"regression\" : imputation_sklearn , \"MF\" : imputation_matrixFactorization , \"auto\" : auto_imputation , } func = func_mapping [ treatment_method ] odf = func ( spark , idf , list_of_cols , ** treatment_configs , stats_missing = stats_missing , print_impact = print_impact ) else : odf = idf if print_impact : odf_print . show ( len ( list_of_cols )) return odf , odf_print def outlier_detection ( spark , idf , list_of_cols = \"all\" , drop_cols = [], detection_side = \"upper\" , detection_configs = { \"pctile_lower\" : 0.05 , \"pctile_upper\" : 0.95 , \"stdev_lower\" : 3.0 , \"stdev_upper\" : 3.0 , \"IQR_lower\" : 1.5 , \"IQR_upper\" : 1.5 , \"min_validation\" : 2 , }, treatment = False , treatment_method = \"value_replacement\" , pre_existing_model = False , model_path = \"NA\" , output_mode = \"replace\" , stats_unique = {}, print_impact = False , ): \"\"\" In Machine Learning, outlier detection identifies values that deviate drastically from the rest of the attribute values. An outlier may be caused simply by chance, measurement error, or inherent heavy-tailed distribution. This function identifies extreme values in both directions (or any direction provided by the user via detection_side argument). Outlier is identified by 3 different methodologies and tagged an outlier only if it is validated by at least 2 methods (can be changed by the user via min_validation under detection_configs argument). - Percentile Method: In this methodology, a value higher than a certain (default 95th) percentile value is considered as an outlier. Similarly, a value lower than a certain (default 5th) percentile value is considered as an outlier. - Standard Deviation Method: In this methodology, if a value is a certain number of standard deviations (default 3) away from the mean, it is identified as an outlier. - Interquartile Range (IQR) Method: A value below Q1 \u2013 1.5 IQR or above Q3 + 1.5 IQR are identified as outliers, where Q1 is in first quantile/25th percentile, Q3 is in third quantile/75th percentile, and IQR is the difference between third quantile & first quantile. This function also leverages statistics which were computed as the part of the State Generator module so that statistics are not computed twice if already available. As part of treatments available, outlier values can be replaced by null so that it can be imputed by a reliable imputation methodology (null_replacement). It can also be replaced by maximum or minimum permissible by above methodologies (value_replacement). Lastly, rows can be removed if it is identified with any outlier (row_removal). This function returns two dataframes in tuple format \u2013 1st dataframe is input dataset after treating outlier (the original dataset if no treatment) and 2nd dataframe is of schema \u2013 attribute, lower_outliers, upper_outliers. If outliers are checked only for upper end, then lower_outliers column will be shown all zero. Similarly if checked only for lower end, then upper_outliers will be zero for all attributes. Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of numerical columns to analyse e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all numerical columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) detection_side \"upper\", \"lower\", \"both\". \"lower\" detects outliers in the lower spectrum of the column range, whereas \"upper\" detects in the upper spectrum. \"Both\" detects in both upper and lower end of the spectrum. (Default value = \"upper\") detection_configs Takes input in dictionary format with keys representing upper & lower parameter for three outlier detection methodologies. a) Percentile Method: lower and upper percentile threshold can be set via \"pctile_lower\" & \"pctile_upper\" (default 0.05 & 0.95) Any value above \"pctile_upper\" is considered as an outlier. Similarly, a value lower than \"pctile_lower\" is considered as an outlier. b) Standard Deviation Method: In this methodology, if a value which is below (mean - \"stdev_lower\" * standard deviation) or above (mean + \"stdev_upper\" * standard deviation), then it is identified as an outlier (default 3.0 & 3.0). c) Interquartile Range (IQR) Method: A value which is below (Q1 \u2013 \"IQR_lower\" * IQR) or above (Q3 + \"IQR_lower\" * IQR) is identified as outliers, where Q1 is first quartile/25th percentile, Q3 is third quartile/75th percentile and IQR is difference between third quartile & first quartile (default 1.5 & 1.5). If an attribute value is less (more) than its derived lower (upper) bound value, it is considered as outlier by a methodology. A attribute value is considered as outlier if it is declared as outlier by atleast 'min_validation' methodologies (default 2). treatment Boolean argument \u2013 True or False. If True, outliers are treated as per treatment_method argument. (Default value = False) treatment_method \"null_replacement\", \"row_removal\", \"value_replacement\". In \"null_replacement\", outlier values are replaced by null so that it can be imputed by a reliable imputation methodology. In \"value_replacement\", outlier values are replaced by maximum or minimum permissible value by above methodologies. Lastly in \"row_removal\", rows are removed if it is found with any outlier. (Default value = \"value_replacement\") pre_existing_model Boolean argument \u2013 True or False. True if the model with upper/lower permissible values for each attribute exists already to be used, False otherwise. (Default value = False) model_path If pre_existing_model is True, this argument is path for the pre-saved model. If pre_existing_model is False, this field can be used for saving the model. Default \"NA\" means there is neither pre-existing model nor there is a need to save one. output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with treated column. \u201cappend\u201d option append treated column to the input dataset with a postfix \"_outliered\" e.g. column X is appended as X_outliered. (Default value = \"replace\") stats_unique Takes arguments for read_dataset (data_ingest module) function in a dictionary format to read pre-saved statistics on unique value count i.e. if measures_of_cardinality or uniqueCount_computation (data_analyzer.stats_generator module) has been computed & saved before. (Default value = {}) print_impact True, False This argument is to print out the statistics and the impact of treatment (if applicable).(Default value = False) Returns ------- odf : DataFrame Imputed dataframe if treated, else original input dataframe. odf_print : DataFrame schema [attribute, lower_outliers, upper_outliers]. lower_outliers is no. of outliers found in the lower spectrum of the attribute range, and upper_outliers is outlier count in the upper spectrum. \"\"\" num_cols = attributeType_segregation ( idf )[ 0 ] if len ( num_cols ) == 0 : warnings . warn ( \"No Outlier Check - No numerical column(s) to analyse\" ) odf = idf schema = T . StructType ( [ T . StructField ( \"attribute\" , T . StringType (), True ), T . StructField ( \"lower_outliers\" , T . StringType (), True ), T . StructField ( \"upper_outliers\" , T . StringType (), True ), ] ) odf_print = spark . sparkContext . emptyRDD () . toDF ( schema ) return odf , odf_print if list_of_cols == \"all\" : list_of_cols = num_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] if stats_unique == {}: remove_cols = ( uniqueCount_computation ( spark , idf , list_of_cols ) . where ( F . col ( \"unique_values\" ) < 2 ) . select ( \"attribute\" ) . rdd . flatMap ( lambda x : x ) . collect () ) else : remove_cols = ( read_dataset ( spark , ** stats_unique ) . where ( F . col ( \"unique_values\" ) < 2 ) . select ( \"attribute\" ) . rdd . flatMap ( lambda x : x ) . collect () ) list_of_cols = list ( set ([ e for e in list_of_cols if e not in ( drop_cols + remove_cols )]) ) if any ( x not in num_cols for x in list_of_cols ): raise TypeError ( \"Invalid input for Column(s)\" ) if detection_side not in ( \"upper\" , \"lower\" , \"both\" ): raise TypeError ( \"Invalid input for detection_side\" ) if treatment_method not in ( \"null_replacement\" , \"row_removal\" , \"value_replacement\" ): raise TypeError ( \"Invalid input for treatment_method\" ) if output_mode not in ( \"replace\" , \"append\" ): raise TypeError ( \"Invalid input for output_mode\" ) if str ( treatment ) . lower () == \"true\" : treatment = True elif str ( treatment ) . lower () == \"false\" : treatment = False else : raise TypeError ( \"Non-Boolean input for treatment\" ) if str ( pre_existing_model ) . lower () == \"true\" : pre_existing_model = True elif str ( pre_existing_model ) . lower () == \"false\" : pre_existing_model = False else : raise TypeError ( \"Non-Boolean input for pre_existing_model\" ) for arg in [ \"pctile_lower\" , \"pctile_upper\" ]: if arg in detection_configs : if ( detection_configs [ arg ] < 0 ) | ( detection_configs [ arg ] > 1 ): raise TypeError ( \"Invalid input for \" + arg ) recast_cols = [] recast_type = [] for i in list_of_cols : if get_dtype ( idf , i ) . startswith ( \"decimal\" ): idf = idf . withColumn ( i , F . col ( i ) . cast ( T . DoubleType ())) recast_cols . append ( i ) recast_type . append ( get_dtype ( idf , i )) if pre_existing_model : df_model = spark . read . parquet ( model_path + \"/outlier_numcols\" ) params = [] for i in list_of_cols : mapped_value = ( df_model . where ( F . col ( \"attribute\" ) == i ) . select ( \"parameters\" ) . rdd . flatMap ( lambda x : x ) . collect ()[ 0 ] ) params . append ( mapped_value ) pctile_params = idf . approxQuantile ( list_of_cols , [ detection_configs . get ( \"pctile_lower\" , 0.05 ), detection_configs . get ( \"pctile_upper\" , 0.95 ), ], 0.01 , ) skewed_cols = [] for i , p in zip ( list_of_cols , pctile_params ): if p [ 0 ] == p [ 1 ]: skewed_cols . append ( i ) else : detection_configs [ \"pctile_lower\" ] = detection_configs [ \"pctile_lower\" ] or 0.0 detection_configs [ \"pctile_upper\" ] = detection_configs [ \"pctile_upper\" ] or 1.0 pctile_params = idf . approxQuantile ( list_of_cols , [ detection_configs [ \"pctile_lower\" ], detection_configs [ \"pctile_upper\" ]], 0.01 , ) skewed_cols = [] for i , p in zip ( list_of_cols , pctile_params ): if p [ 0 ] == p [ 1 ]: skewed_cols . append ( i ) detection_configs [ \"stdev_lower\" ] = ( detection_configs [ \"stdev_lower\" ] or detection_configs [ \"stdev_upper\" ] ) detection_configs [ \"stdev_upper\" ] = ( detection_configs [ \"stdev_upper\" ] or detection_configs [ \"stdev_lower\" ] ) stdev_params = [] for i in list_of_cols : mean , stdev = idf . select ( F . mean ( i ), F . stddev ( i )) . first () stdev_params . append ( [ mean - detection_configs [ \"stdev_lower\" ] * stdev , mean + detection_configs [ \"stdev_upper\" ] * stdev , ] ) detection_configs [ \"IQR_lower\" ] = ( detection_configs [ \"IQR_lower\" ] or detection_configs [ \"IQR_upper\" ] ) detection_configs [ \"IQR_upper\" ] = ( detection_configs [ \"IQR_upper\" ] or detection_configs [ \"IQR_lower\" ] ) quantiles = idf . approxQuantile ( list_of_cols , [ 0.25 , 0.75 ], 0.01 ) IQR_params = [ [ e [ 0 ] - detection_configs [ \"IQR_lower\" ] * ( e [ 1 ] - e [ 0 ]), e [ 1 ] + detection_configs [ \"IQR_upper\" ] * ( e [ 1 ] - e [ 0 ]), ] for e in quantiles ] n = detection_configs [ \"min_validation\" ] params = [ [ sorted ([ x [ 0 ], y [ 0 ], z [ 0 ]], reverse = True )[ n - 1 ], sorted ([ x [ 1 ], y [ 1 ], z [ 1 ]])[ n - 1 ], ] for x , y , z in list ( zip ( pctile_params , stdev_params , IQR_params )) ] # Saving model File if required if model_path != \"NA\" : df_model = spark . createDataFrame ( zip ( list_of_cols , params ), schema = [ \"attribute\" , \"parameters\" ] ) df_model . coalesce ( 1 ) . write . parquet ( model_path + \"/outlier_numcols\" , mode = \"overwrite\" ) for i , j in zip ( recast_cols , recast_type ): idf = idf . withColumn ( i , F . col ( i ) . cast ( j )) def composite_outlier ( * v ): output = [] for idx , e in enumerate ( v ): if e is None : output . append ( None ) continue if detection_side in ( \"upper\" , \"both\" ): if e > params [ idx ][ 1 ]: output . append ( 1 ) continue if detection_side in ( \"lower\" , \"both\" ): if e < params [ idx ][ 0 ]: output . append ( - 1 ) continue output . append ( 0 ) return output f_composite_outlier = F . udf ( composite_outlier , T . ArrayType ( T . IntegerType ())) odf = idf . withColumn ( \"outliered\" , f_composite_outlier ( * list_of_cols )) odf . persist () output_print = [] for index , i in enumerate ( list_of_cols ): odf = odf . withColumn ( i + \"_outliered\" , F . col ( \"outliered\" )[ index ]) output_print . append ( [ i , odf . where ( F . col ( i + \"_outliered\" ) == - 1 ) . count (), odf . where ( F . col ( i + \"_outliered\" ) == 1 ) . count (), ] ) if treatment & ( treatment_method in ( \"value_replacement\" , \"null_replacement\" )): if skewed_cols : warnings . warn ( \"Columns dropped from outlier treatment due to highly skewed distribution: \" + ( \",\" ) . join ( skewed_cols ) ) if i not in skewed_cols : replace_vals = { \"value_replacement\" : [ params [ index ][ 0 ], params [ index ][ 1 ]], \"null_replacement\" : [ None , None ], } odf = odf . withColumn ( i + \"_outliered\" , F . when ( F . col ( i + \"_outliered\" ) == 1 , replace_vals [ treatment_method ][ 1 ] ) . otherwise ( F . when ( F . col ( i + \"_outliered\" ) == - 1 , replace_vals [ treatment_method ][ 0 ], ) . otherwise ( F . col ( i )) ), ) if output_mode == \"replace\" : odf = odf . drop ( i ) . withColumnRenamed ( i + \"_outliered\" , i ) else : odf = odf . drop ( i + \"_outliered\" ) odf = odf . drop ( \"outliered\" ) if treatment & ( treatment_method == \"row_removal\" ): if skewed_cols : warnings . warn ( \"Columns dropped from outlier treatment due to highly skewed distribution: \" + ( \",\" ) . join ( skewed_cols ) ) for index , i in enumerate ( list_of_cols ): if i not in skewed_cols : odf = odf . where ( ( F . col ( i + \"_outliered\" ) == 0 ) | ( F . col ( i + \"_outliered\" ) . isNull ()) ) . drop ( i + \"_outliered\" ) else : odf = odf . drop ( i + \"_outliered\" ) if not treatment : odf = idf odf_print = spark . createDataFrame ( output_print , schema = [ \"attribute\" , \"lower_outliers\" , \"upper_outliers\" ] ) if print_impact : odf_print . show ( len ( list_of_cols )) return odf , odf_print def IDness_detection ( spark , idf , list_of_cols = \"all\" , drop_cols = [], treatment = False , treatment_threshold = 0.8 , stats_unique = {}, print_impact = False , ): \"\"\" IDness of an attribute is defined as the ratio of number of unique values seen in an attribute by number of non-null rows. It varies between 0 to 100% where IDness of 100% means there are as many unique values as number of rows (primary key in the input dataset). IDness is computed only for discrete features. This function leverages the statistics from Measures of Cardinality function and flag the columns if IDness is above a certain threshold. Such columns can be deleted from the modelling analysis if directed for a treatment. This function returns two dataframes in tuple format \u2013 1st dataframe is input dataset after removing high IDness columns (if treated else the original dataset) and 2nd dataframe is of schema \u2013 attribute, unique_values, IDness. Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of Discrete (Categorical + Integer) columns to analyse e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all discrete columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) treatment Boolean argument \u2013 True or False. If True, columns with high IDness (defined by treatment_threshold argument) are removed from the input dataframe. (Default value = False) treatment_threshold Defines acceptable level of IDness (calculated as no. of unique values divided by no. of non-null values) for a column and takes value between 0 to 1. Default threshold of 0.8 can be interpreted as remove column if its unique values count is more than 80% of total rows (after excluding null values). stats_unique Takes arguments for read_dataset (data_ingest module) function in a dictionary format to read pre-saved statistics on unique value count i.e. if measures_of_cardinality or uniqueCount_computation (data_analyzer.stats_generator module) has been computed & saved before. (Default value = {}) print_impact True, False This argument is to print out the statistics and the impact of treatment (if applicable).(Default value = False) Returns ------- odf : DataFrame Dataframe after column removal if treated, else original input dataframe. odf_print : DataFrame schema [attribute, unique_values, IDness, flagged/treated]. unique_values is no. of distinct values in a column, IDness is unique_values divided by no. of non-null values. A column is flagged 1 if IDness is above the threshold, else 0. \"\"\" if list_of_cols == \"all\" : num_cols , cat_cols , other_cols = attributeType_segregation ( idf ) list_of_cols = num_cols + cat_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in drop_cols ])) for i in idf . select ( list_of_cols ) . dtypes : if i [ 1 ] not in ( \"string\" , \"int\" , \"bigint\" , \"long\" ): list_of_cols . remove ( i [ 0 ]) if any ( x not in idf . columns for x in list_of_cols ): raise TypeError ( \"Invalid input for Column(s)\" ) if len ( list_of_cols ) == 0 : warnings . warn ( \"No IDness Check - No discrete column(s) to analyze\" ) odf = idf schema = T . StructType ( [ T . StructField ( \"attribute\" , T . StringType (), True ), T . StructField ( \"unique_values\" , T . StringType (), True ), T . StructField ( \"IDness\" , T . StringType (), True ), T . StructField ( \"flagged\" , T . StringType (), True ), ] ) odf_print = spark . sparkContext . emptyRDD () . toDF ( schema ) return odf , odf_print treatment_threshold = float ( treatment_threshold ) if ( treatment_threshold < 0 ) | ( treatment_threshold > 1 ): raise TypeError ( \"Invalid input for Treatment Threshold Value\" ) if str ( treatment ) . lower () == \"true\" : treatment = True elif str ( treatment ) . lower () == \"false\" : treatment = False else : raise TypeError ( \"Non-Boolean input for treatment\" ) if stats_unique == {}: odf_print = measures_of_cardinality ( spark , idf , list_of_cols ) else : odf_print = read_dataset ( spark , ** stats_unique ) . where ( F . col ( \"attribute\" ) . isin ( list_of_cols ) ) odf_print = odf_print . withColumn ( \"flagged\" , F . when ( F . col ( \"IDness\" ) >= treatment_threshold , 1 ) . otherwise ( 0 ) ) if treatment : remove_cols = ( odf_print . where ( F . col ( \"flagged\" ) == 1 ) . select ( \"attribute\" ) . rdd . flatMap ( lambda x : x ) . collect () ) odf = idf . drop ( * remove_cols ) odf_print = odf_print . withColumnRenamed ( \"flagged\" , \"treated\" ) else : odf = idf if print_impact : odf_print . show ( len ( list_of_cols )) if treatment : print ( \"Removed Columns: \" , remove_cols ) return odf , odf_print def biasedness_detection ( spark , idf , list_of_cols = \"all\" , drop_cols = [], treatment = False , treatment_threshold = 0.8 , stats_mode = {}, print_impact = False , ): \"\"\" This function flags column if they are biased or skewed towards one specific value and leverages mode_pct computation from Measures of Central Tendency i.e. number of rows with mode value (most frequently seen value) divided by number of non-null values. It varies between 0 to 100% where biasedness of 100% means there is only a single value (other than null). The function flags a column if its biasedness is above a certain threshold. Such columns can be deleted from the modelling analysis, if required. This function returns two dataframes in tuple format \u2013 1st dataframe is input dataset after removing high biased columns (the original dataset if no treatment) and 2nd dataframe is of schema \u2013 attribute, mode, mode_pct. Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of Discrete (Categorical + Integer) columns to analyse e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all discrete columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) treatment Boolean argument \u2013 True or False. If True, columns with high biasedness (defined by treatment_threshold argument) are removed from the input dataframe. (Default value = False) treatment_threshold Defines acceptable level of biasedness (frequency of most-frequently seen value)for a column and takes value between 0 to 1. Default threshold of 0.8 can be interpreted as remove column if the number of rows with most-frequently seen value is more than 80% of total rows (after excluding null values). stats_mode Takes arguments for read_dataset (data_ingest module) function in a dictionary format to read pre-saved statistics on most frequently seen values i.e. if measures_of_centralTendency or mode_computation (data_analyzer.stats_generator module) has been computed & saved before. (Default value = {}) print_impact True, False This argument is to print out the statistics and the impact of treatment (if applicable).(Default value = False) Returns ------- odf : DataFrame Dataframe after column removal if treated, else original input dataframe. odf_print : DataFrame schema [attribute, mode, mode_rows, mode_pct, flagged/treated]. mode is the most frequently seen value, mode_rows is number of rows with mode value, and mode_pct is number of rows with mode value divided by non-null values. A column is flagged 1 if mode_pct is above the threshold else 0. \"\"\" if list_of_cols == \"all\" : num_cols , cat_cols , other_cols = attributeType_segregation ( idf ) list_of_cols = num_cols + cat_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in drop_cols ])) for i in idf . select ( list_of_cols ) . dtypes : if i [ 1 ] not in ( \"string\" , \"int\" , \"bigint\" , \"long\" ): list_of_cols . remove ( i [ 0 ]) if any ( x not in idf . columns for x in list_of_cols ): raise TypeError ( \"Invalid input for Column(s)\" ) if len ( list_of_cols ) == 0 : warnings . warn ( \"No biasedness Check - No discrete column(s) to analyze\" ) odf = idf schema = T . StructType ( [ T . StructField ( \"attribute\" , T . StringType (), True ), T . StructField ( \"mode\" , T . StringType (), True ), T . StructField ( \"mode_rows\" , T . StringType (), True ), T . StructField ( \"mode_pct\" , T . StringType (), True ), T . StructField ( \"flagged\" , T . StringType (), True ), ] ) odf_print = spark . sparkContext . emptyRDD () . toDF ( schema ) return odf , odf_print if ( treatment_threshold < 0 ) | ( treatment_threshold > 1 ): raise TypeError ( \"Invalid input for Treatment Threshold Value\" ) if str ( treatment ) . lower () == \"true\" : treatment = True elif str ( treatment ) . lower () == \"false\" : treatment = False else : raise TypeError ( \"Non-Boolean input for treatment\" ) if stats_mode == {}: odf_print = ( transpose_dataframe ( idf . select ( list_of_cols ) . summary ( \"count\" ), \"summary\" ) . withColumnRenamed ( \"key\" , \"attribute\" ) . join ( mode_computation ( spark , idf , list_of_cols ), \"attribute\" , \"full_outer\" ) . withColumn ( \"mode_pct\" , F . round ( F . col ( \"mode_rows\" ) / F . col ( \"count\" ) . cast ( T . DoubleType ()), 4 ), ) . select ( \"attribute\" , \"mode\" , \"mode_rows\" , \"mode_pct\" ) ) else : odf_print = ( read_dataset ( spark , ** stats_mode ) . select ( \"attribute\" , \"mode\" , \"mode_rows\" , \"mode_pct\" ) . where ( F . col ( \"attribute\" ) . isin ( list_of_cols )) ) odf_print = odf_print . withColumn ( \"flagged\" , F . when ( ( F . col ( \"mode_pct\" ) >= treatment_threshold ) | ( F . col ( \"mode_pct\" ) . isNull ()), 1 ) . otherwise ( 0 ), ) if treatment : remove_cols = ( odf_print . where ( ( F . col ( \"mode_pct\" ) >= treatment_threshold ) | ( F . col ( \"mode_pct\" ) . isNull ()) ) . select ( \"attribute\" ) . rdd . flatMap ( lambda x : x ) . collect () ) odf = idf . drop ( * remove_cols ) odf_print = odf_print . withColumnRenamed ( \"flagged\" , \"treated\" ) else : odf = idf if print_impact : odf_print . show ( len ( list_of_cols )) if treatment : print ( \"Removed Columns: \" , remove_cols ) return odf , odf_print def invalidEntries_detection ( spark , idf , list_of_cols = \"all\" , drop_cols = [], detection_type = \"auto\" , invalid_entries = [], valid_entries = [], partial_match = False , treatment = False , treatment_method = \"null_replacement\" , treatment_configs = {}, stats_missing = {}, stats_unique = {}, stats_mode = {}, output_mode = \"replace\" , print_impact = False , ): \"\"\" This function checks for certain suspicious patterns in attributes\u2019 values. Patterns that are considered for this quality check: - Missing Values: The function checks for all column values which directly or indirectly indicate the missing value in an attribute such as 'nan', 'null', 'na', 'inf', 'n/a', 'not defined' etc. The function also check for special characters. - Repetitive Characters: Certain attributes\u2019 values with repetitive characters may be default value or system error, rather than being a legit value etc xx, zzzzz, 99999 etc. Such values are flagged for the user to take an appropriate action. There may be certain false positive which are legit values. - Consecutive Characters: Similar to repetitive characters, consecutive characters (at least 3 characters long) such as abc, 1234 etc may not be legit values, and hence flagged. There may be certain false positive which are legit values. This function returns two dataframes in tuple format \u2013 1st dataframe is input dataset after treating the invalid values (or the original dataset if no treatment) and 2nd dataframe is of schema \u2013 attribute, invalid_entries, invalid_count, invalid_pct. All potential invalid values (separated by delimiter pipe \u201c|\u201d) are shown under invalid_entries column. Total number of rows impacted by these entries for each attribute is shown under invalid_count. invalid_pct is invalid_count divided by number of rows. Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of Discrete (Categorical + Integer) columns to analyse e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all discrete columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) detection_type \"auto\",\"manual\",\"both\" (Default value = \"auto\") invalid_entries List of values or regex patterns to be classified as invalid. Valid only for \"auto\" or \"both\" detection type. (Default value = []) valid_entries List of values or regex patterns such that a value will be classified as invalid if it does not match any value or regex pattern in it. Valid only for \"auto\" or \"both\" detection type. (Default value = []) partial_match Boolean argument \u2013 True or False. If True, values with substring same as invalid_entries is declared invalid. (Default value = False) treatment Boolean argument \u2013 True or False. If True, outliers are treated as per treatment_method argument. (Default value = False) treatment_method \"MMM\", \"null_replacement\", \"column_removal\" (more methods to be added soon). MMM (Mean Median Mode) replaces invalid value by the measure of central tendency (mode for categorical features and mean or median for numerical features). null_replacement removes all values with any invalid values as null. column_removal remove a column if % of rows with invalid value is above a threshold (defined by key \"treatment_threshold\" under treatment_configs argument). (Default value = \"null_replacement\") treatment_configs Takes input in dictionary format. For column_removal treatment, key \u2018treatment_threshold\u2019 is provided with a value between 0 to 1. For value replacement, by MMM, arguments corresponding to imputation_MMM function (transformer module) are provided, where each key is an argument from imputation_MMM function. For null_replacement, this argument can be skipped. (Default value = {}) stats_missing Takes arguments for read_dataset (data_ingest module) function in a dictionary format to read pre-saved statistics on missing count/pct i.e. if measures_of_counts or missingCount_computation (data_analyzer.stats_generator module) has been computed & saved before. (Default value = {}) stats_unique Takes arguments for read_dataset (data_ingest module) function in a dictionary format to read pre-saved statistics on unique value count i.e. if measures_of_cardinality or uniqueCount_computation (data_analyzer.stats_generator module) has been computed & saved before. (Default value = {}) stats_mode Takes arguments for read_dataset (data_ingest module) function in a dictionary format to read pre-saved statistics on most frequently seen values i.e. if measures_of_centralTendency or mode_computation (data_analyzer.stats_generator module) has been computed & saved before. (Default value = {}) output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with treated column. \u201cappend\u201d option append treated column to the input dataset with a postfix \"_invalid\" e.g. column X is appended as X_invalid. (Default value = \"replace\") print_impact True, False This argument is to print out the statistics.(Default value = False) Returns ------- odf : DataFrame Dataframe after treatment if applicable, else original input dataframe. odf_print : DataFrame schema [attribute, invalid_entries, invalid_count, invalid_pct]. invalid_entries are all potential invalid values (separated by delimiter pipe \u201c|\u201d), invalid_count is no. of rows which are impacted by invalid entries, and invalid_pct is invalid_count divided by no of rows. \"\"\" if list_of_cols == \"all\" : list_of_cols = [] for i in idf . dtypes : if i [ 1 ] in ( \"string\" , \"int\" , \"bigint\" , \"long\" ): list_of_cols . append ( i [ 0 ]) if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in drop_cols ])) if any ( x not in idf . columns for x in list_of_cols ): raise TypeError ( \"Invalid input for Column(s)\" ) if len ( list_of_cols ) == 0 : warnings . warn ( \"No Invalid Entries Check - No discrete column(s) to analyze\" ) odf = idf schema = T . StructType ( [ T . StructField ( \"attribute\" , T . StringType (), True ), T . StructField ( \"invalid_entries\" , T . StringType (), True ), T . StructField ( \"invalid_count\" , T . StringType (), True ), T . StructField ( \"invalid_pct\" , T . StringType (), True ), ] ) odf_print = spark . sparkContext . emptyRDD () . toDF ( schema ) return odf , odf_print if output_mode not in ( \"replace\" , \"append\" ): raise TypeError ( \"Invalid input for output_mode\" ) if str ( treatment ) . lower () == \"true\" : treatment = True elif str ( treatment ) . lower () == \"false\" : treatment = False else : raise TypeError ( \"Non-Boolean input for treatment\" ) if treatment_method not in ( \"MMM\" , \"null_replacement\" , \"column_removal\" ): raise TypeError ( \"Invalid input for method_type\" ) treatment_threshold = treatment_configs . pop ( \"treatment_threshold\" , None ) if treatment_threshold : treatment_threshold = float ( treatment_threshold ) else : if treatment_method == \"column_removal\" : raise TypeError ( \"Invalid input for column removal threshold\" ) null_vocab = [ \"\" , \" \" , \"nan\" , \"null\" , \"na\" , \"inf\" , \"n/a\" , \"not defined\" , \"none\" , \"undefined\" , \"blank\" , \"unknown\" , ] special_chars_vocab = [ \"&\" , \"$\" , \";\" , \":\" , \".\" , \",\" , \"*\" , \"#\" , \"@\" , \"_\" , \"?\" , \"%\" , \"!\" , \"^\" , \"(\" , \")\" , \"-\" , \"/\" , \"'\" , ] def detect ( * v ): output = [] for idx , e in enumerate ( v ): if e is None : output . append ( None ) continue if detection_type in ( \"auto\" , \"both\" ): e = str ( e ) . lower () . strip () # Null & Special Chars Search if e in ( null_vocab + special_chars_vocab ): output . append ( 1 ) continue # Consecutive Identical Chars Search regex = \" \\\\ b([a-zA-Z0-9]) \\\\ 1 \\\\ 1+ \\\\ b\" p = re . compile ( regex ) if re . search ( p , e ): output . append ( 1 ) continue # Ordered Chars Search l = len ( e ) check = 0 if l >= 3 : for i in range ( 1 , l ): if ord ( e [ i ]) - ord ( e [ i - 1 ]) != 1 : check = 1 break if check == 0 : output . append ( 1 ) continue check = 0 if detection_type in ( \"manual\" , \"both\" ): e = str ( e ) . lower () . strip () for regex in invalid_entries : p = re . compile ( regex ) if partial_match : if re . search ( p , e ): check = 1 output . append ( 1 ) break else : if p . fullmatch ( e ): check = 1 output . append ( 1 ) break match_valid_entries = [] for regex in valid_entries : p = re . compile ( regex ) if partial_match : if re . search ( p , e ): match_valid_entries . append ( 1 ) else : match_valid_entries . append ( 0 ) else : if p . fullmatch ( e ): match_valid_entries . append ( 1 ) else : match_valid_entries . append ( 0 ) if ( len ( match_valid_entries ) > 0 ) & ( sum ( match_valid_entries ) == 0 ): check = 1 output . append ( 1 ) if check == 0 : output . append ( 0 ) return output f_detect = F . udf ( detect , T . ArrayType ( T . LongType ())) odf = idf . withColumn ( \"invalid\" , f_detect ( * list_of_cols )) odf . persist () output_print = [] for index , i in enumerate ( list_of_cols ): tmp = odf . withColumn ( i + \"_invalid\" , F . col ( \"invalid\" )[ index ]) invalid = ( tmp . where ( F . col ( i + \"_invalid\" ) == 1 ) . select ( i ) . distinct () . rdd . flatMap ( lambda x : x ) . collect () ) invalid = [ str ( x ) for x in invalid ] invalid_count = tmp . where ( F . col ( i + \"_invalid\" ) == 1 ) . count () output_print . append ( [ i , \"|\" . join ( invalid ), invalid_count , round ( invalid_count / idf . count (), 4 )] ) odf_print = spark . createDataFrame ( output_print , schema = [ \"attribute\" , \"invalid_entries\" , \"invalid_count\" , \"invalid_pct\" ], ) if treatment : if treatment_threshold : threshold_cols = ( odf_print . where ( F . col ( \"attribute\" ) . isin ( list_of_cols )) . where ( F . col ( \"invalid_pct\" ) > treatment_threshold ) . select ( \"attribute\" ) . rdd . flatMap ( lambda x : x ) . collect () ) if treatment_method in ( \"null_replacement\" , \"MMM\" ): for index , i in enumerate ( list_of_cols ): if treatment_threshold : if i not in threshold_cols : odf = odf . drop ( i + \"_invalid\" ) continue odf = odf . withColumn ( i + \"_invalid\" , F . when ( F . col ( \"invalid\" )[ index ] == 1 , None ) . otherwise ( F . col ( i )), ) if output_mode == \"replace\" : odf = odf . drop ( i ) . withColumnRenamed ( i + \"_invalid\" , i ) else : if ( odf_print . where ( F . col ( \"attribute\" ) == i ) . select ( \"invalid_pct\" ) . collect ()[ 0 ][ 0 ] == 0.0 ): odf = odf . drop ( i + \"_invalid\" ) odf = odf . drop ( \"invalid\" ) if treatment_method == \"column_removal\" : odf = idf . drop ( * threshold_cols ) if print_impact : print ( \"Removed Columns: \" , threshold_cols ) if treatment_method == \"MMM\" : if stats_unique == {} or output_mode == \"append\" : remove_cols = ( uniqueCount_computation ( spark , odf , list_of_cols ) . where ( F . col ( \"unique_values\" ) < 2 ) . select ( \"attribute\" ) . rdd . flatMap ( lambda x : x ) . collect () ) else : remove_cols = ( read_dataset ( spark , ** stats_unique ) . where ( F . col ( \"unique_values\" ) < 2 ) . select ( \"attribute\" ) . rdd . flatMap ( lambda x : x ) . collect () ) list_of_cols = [ e for e in list_of_cols if e not in remove_cols ] if treatment_threshold : list_of_cols = [ e for e in threshold_cols if e not in remove_cols ] if output_mode == \"append\" : if len ( list_of_cols ) > 0 : list_of_cols = [ e + \"_invalid\" for e in list_of_cols ] odf = imputation_MMM ( spark , odf , list_of_cols , ** treatment_configs , stats_missing = stats_missing , stats_mode = stats_mode , print_impact = print_impact ) else : odf = idf if print_impact : odf_print . show ( len ( list_of_cols )) return odf , odf_print","title":"quality_checker"},{"location":"api/data_analyzer/quality_checker.html#functions","text":"def IDness_detection ( spark, idf, list_of_cols='all', drop_cols=[], treatment=False, treatment_threshold=0.8, stats_unique={}, print_impact=False) IDness of an attribute is defined as the ratio of number of unique values seen in an attribute by number of non-null rows. It varies between 0 to 100% where IDness of 100% means there are as many unique values as number of rows (primary key in the input dataset). IDness is computed only for discrete features. This function leverages the statistics from Measures of Cardinality function and flag the columns if IDness is above a certain threshold. Such columns can be deleted from the modelling analysis if directed for a treatment. This function returns two dataframes in tuple format \u2013 1st dataframe is input dataset after removing high IDness columns (if treated else the original dataset) and 2nd dataframe is of schema \u2013 attribute, unique_values, IDness.","title":"Functions"},{"location":"api/data_analyzer/stats_generator.html","text":"stats_generator This module generates all the descriptive statistics related to the ingested data. Descriptive statistics are split into different metric types, and each function below corresponds to one metric type: global_summary measures_of_counts measures_of_centralTendency measures_of_cardinality measures_of_dispersion measures_of_percentiles measures_of_shape Above primary functions are supported by below functions, which can be used independently as well: missingCount_computation nonzeroCount_computation mode_computation uniqueCount_computation Expand source code # coding=utf-8 \"\"\" This module generates all the descriptive statistics related to the ingested data. Descriptive statistics are split into different metric types, and each function below corresponds to one metric type: - global_summary - measures_of_counts - measures_of_centralTendency - measures_of_cardinality - measures_of_dispersion - measures_of_percentiles - measures_of_shape Above primary functions are supported by below functions, which can be used independently as well: - missingCount_computation - nonzeroCount_computation - mode_computation - uniqueCount_computation \"\"\" import warnings from pyspark.mllib.linalg import Vectors from pyspark.mllib.stat import Statistics from pyspark.sql import functions as F from pyspark.sql import types as T from anovos.shared.utils import transpose_dataframe , attributeType_segregation def global_summary ( spark , idf , list_of_cols = \"all\" , drop_cols = [], print_impact = False ): \"\"\" The global summary function computes the universal statistics/metrics and returns a Spark DataFrame with schema \u2013 metric, value. The metrics computed in this function - No. of rows, No. of columns, No. of categorical columns along with column names, No. of numerical columns along with the column names, No. of non-numerical non-categorical columns such as date type, array type etc. along with column names. Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of columns to analyse e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) print_impact True, False This argument is to print out the statistics.(Default value = False) Returns ------- DataFrame [metric, value] \"\"\" if list_of_cols == \"all\" : list_of_cols = idf . columns if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in drop_cols ])) if any ( x not in idf . columns for x in list_of_cols ) | ( len ( list_of_cols ) == 0 ): raise TypeError ( \"Invalid input for Column(s)\" ) row_count = idf . count () col_count = len ( list_of_cols ) num_cols , cat_cols , other_cols = attributeType_segregation ( idf . select ( list_of_cols )) numcol_count = len ( num_cols ) catcol_count = len ( cat_cols ) othercol_count = len ( other_cols ) if print_impact : print ( \"No. of Rows: %s \" % \" {0:,} \" . format ( row_count )) print ( \"No. of Columns: %s \" % \" {0:,} \" . format ( col_count )) print ( \"Numerical Columns: %s \" % \" {0:,} \" . format ( numcol_count )) if numcol_count > 0 : print ( num_cols ) print ( \"Categorical Columns: %s \" % \" {0:,} \" . format ( catcol_count )) if catcol_count > 0 : print ( cat_cols ) if othercol_count > 0 : print ( \"Other Columns: %s \" % \" {0:,} \" . format ( othercol_count )) print ( other_cols ) odf = spark . createDataFrame ( [ [ \"rows_count\" , str ( row_count )], [ \"columns_count\" , str ( col_count )], [ \"numcols_count\" , str ( numcol_count )], [ \"numcols_name\" , \", \" . join ( num_cols )], [ \"catcols_count\" , str ( catcol_count )], [ \"catcols_name\" , \", \" . join ( cat_cols )], [ \"othercols_count\" , str ( othercol_count )], [ \"othercols_name\" , \", \" . join ( other_cols )], ], schema = [ \"metric\" , \"value\" ], ) return odf def missingCount_computation ( spark , idf , list_of_cols = \"all\" , drop_cols = [], print_impact = False ): \"\"\" Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of columns to analyse e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) print_impact True, False This argument is to print out the statistics.(Default value = False) Returns ------- DataFrame [attribute, missing_count, missing_pct] \"\"\" if list_of_cols == \"all\" : num_cols , cat_cols , other_cols = attributeType_segregation ( idf ) list_of_cols = num_cols + cat_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in drop_cols ])) if any ( x not in idf . columns for x in list_of_cols ) | ( len ( list_of_cols ) == 0 ): raise TypeError ( \"Invalid input for Column(s)\" ) idf_stats = idf . select ( list_of_cols ) . summary ( \"count\" ) odf = ( transpose_dataframe ( idf_stats , \"summary\" ) . withColumn ( \"missing_count\" , F . lit ( idf . count ()) - F . col ( \"count\" ) . cast ( T . LongType ()) ) . withColumn ( \"missing_pct\" , F . round ( F . col ( \"missing_count\" ) / F . lit ( idf . count ()), 4 ) ) . select ( F . col ( \"key\" ) . alias ( \"attribute\" ), \"missing_count\" , \"missing_pct\" ) ) if print_impact : odf . show ( len ( list_of_cols )) return odf def nonzeroCount_computation ( spark , idf , list_of_cols = \"all\" , drop_cols = [], print_impact = False ): \"\"\" Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of numerical columns to analyse e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all numerical columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) print_impact True, False This argument is to print out the statistics.(Default value = False) Returns ------- DataFrame [attribute, nonzero_count, nonzero_pct] \"\"\" num_cols = attributeType_segregation ( idf )[ 0 ] if list_of_cols == \"all\" : list_of_cols = num_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in drop_cols ])) if any ( x not in num_cols for x in list_of_cols ): raise TypeError ( \"Invalid input for Column(s)\" ) if len ( list_of_cols ) == 0 : warnings . warn ( \"No Non-Zero Count Computation - No numerical column(s) to analyze\" ) schema = T . StructType ( [ T . StructField ( \"attribute\" , T . StringType (), True ), T . StructField ( \"nonzero_count\" , T . StringType (), True ), T . StructField ( \"nonzero_pct\" , T . StringType (), True ), ] ) odf = spark . sparkContext . emptyRDD () . toDF ( schema ) return odf tmp = idf . select ( list_of_cols ) . fillna ( 0 ) . rdd . map ( lambda row : Vectors . dense ( row )) nonzero_count = Statistics . colStats ( tmp ) . numNonzeros () odf = spark . createDataFrame ( zip ( list_of_cols , [ int ( i ) for i in nonzero_count ]), schema = ( \"attribute\" , \"nonzero_count\" ), ) . withColumn ( \"nonzero_pct\" , F . round ( F . col ( \"nonzero_count\" ) / F . lit ( idf . count ()), 4 )) if print_impact : odf . show ( len ( list_of_cols )) return odf def measures_of_counts ( spark , idf , list_of_cols = \"all\" , drop_cols = [], print_impact = False ): \"\"\" The Measures of Counts function computes different count metrics for each column. It returns a Spark DataFrame with schema \u2013 attribute, fill_count, fill_pct, missing_count, missing_pct, nonzero_count, nonzero_pct. - Fill Count/Rate is defined as number of rows with non-null values in a column both in terms of absolute count and its proportion to row count. It leverages count statistic from summary functionality of Spark SQL. - Missing Count/Rate is defined as null (or missing) values seen in a column both in terms of absolute count and its proportion to row count. It is directly derivable from Fill Count/Rate. - Non Zero Count/Rate is defined as non-zero values seen in a numerical column both in terms of absolute count and its proportion to row count. For categorical column, it will show null value. Also, it uses a supporting function nonzeroCount_computation. Under the hood, it leverage Multivariate Statistical Summary of Spark MLlib. Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of columns to analyse e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) print_impact True, False This argument is to print out the statistics.(Default value = False) Returns ------- DataFrame [attribute, fill_count, fill_pct, missing_count, missing_pct, nonzero_count, nonzero_pct] \"\"\" if list_of_cols == \"all\" : num_cols , cat_cols , other_cols = attributeType_segregation ( idf ) list_of_cols = num_cols + cat_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in drop_cols ])) num_cols = attributeType_segregation ( idf . select ( list_of_cols ))[ 0 ] if any ( x not in idf . columns for x in list_of_cols ) | ( len ( list_of_cols ) == 0 ): raise TypeError ( \"Invalid input for Column(s)\" ) odf = ( transpose_dataframe ( idf . select ( list_of_cols ) . summary ( \"count\" ), \"summary\" ) . select ( F . col ( \"key\" ) . alias ( \"attribute\" ), F . col ( \"count\" ) . cast ( T . LongType ()) . alias ( \"fill_count\" ), ) . withColumn ( \"fill_pct\" , F . round ( F . col ( \"fill_count\" ) / F . lit ( idf . count ()), 4 )) . withColumn ( \"missing_count\" , F . lit ( idf . count ()) - F . col ( \"fill_count\" ) . cast ( T . LongType ()) ) . withColumn ( \"missing_pct\" , F . round ( 1 - F . col ( \"fill_pct\" ), 4 )) . join ( nonzeroCount_computation ( spark , idf , num_cols ), \"attribute\" , \"full_outer\" ) ) if print_impact : odf . show ( len ( list_of_cols )) return odf def mode_computation ( spark , idf , list_of_cols = \"all\" , drop_cols = [], print_impact = False ): \"\"\" Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of Discrete (Categorical + Integer) columns to analyse e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all discrete columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) print_impact True, False This argument is to print out the statistics.(Default value = False) Returns ------- DataFrame [attribute, mode, mode_rows] In case there is tie between multiple values, one value is randomly picked as mode. \"\"\" if list_of_cols == \"all\" : num_cols , cat_cols , other_cols = attributeType_segregation ( idf ) list_of_cols = num_cols + cat_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in drop_cols ])) for i in idf . select ( list_of_cols ) . dtypes : if i [ 1 ] not in ( \"string\" , \"int\" , \"bigint\" , \"long\" ): list_of_cols . remove ( i [ 0 ]) if any ( x not in idf . columns for x in list_of_cols ): raise TypeError ( \"Invalid input for Column(s)\" ) if len ( list_of_cols ) == 0 : warnings . warn ( \"No Mode Computation - No discrete column(s) to analyze\" ) schema = T . StructType ( [ T . StructField ( \"attribute\" , T . StringType (), True ), T . StructField ( \"mode\" , T . StringType (), True ), T . StructField ( \"mode_rows\" , T . StringType (), True ), ] ) odf = spark . sparkContext . emptyRDD () . toDF ( schema ) return odf mode = [ list ( idf . select ( i ) . dropna () . groupby ( i ) . count () . orderBy ( \"count\" , ascending = False ) . first () or [ None , None ] ) for i in list_of_cols ] mode = [( str ( i ), str ( j )) for i , j in mode ] odf = spark . createDataFrame ( zip ( list_of_cols , mode ), schema = ( \"attribute\" , \"metric\" ) ) . select ( \"attribute\" , ( F . col ( \"metric\" )[ \"_1\" ]) . alias ( \"mode\" ), ( F . col ( \"metric\" )[ \"_2\" ]) . cast ( \"long\" ) . alias ( \"mode_rows\" ), ) if print_impact : odf . show ( len ( list_of_cols )) return odf def measures_of_centralTendency ( spark , idf , list_of_cols = \"all\" , drop_cols = [], print_impact = False ): \"\"\" The Measures of Central Tendency function provides summary statistics that represents the centre point or most likely value of an attribute. It returns a Spark DataFrame with schema \u2013 attribute, mean, median, mode, mode_rows, mode_pct. - Mean is arithmetic average of a column i.e. sum of all values seen in the column divided by the number of rows. It leverage mean statistic from summary functionality of Spark SQL. Mean is calculated only for numerical columns. - Median is 50th percentile or middle value in a column when the values are arranged in ascending or descending order. It leverage \u201850%\u2019 statistic from summary functionality of Spark SQL. Median is calculated only for numerical columns. - Mode is most frequently seen value in a column. Mode is calculated only for discrete columns (categorical + Integer/Long columns). - Mode Rows is the numer of rows seen with Mode value. Mode Rows is calculated only for discrete columns (categorical + Integer/Long columns). - Mode Pct is defined as Mode Rows divided by non-null values seen in a column. Mode Pct is calculated only for discrete columns (categorical + Integer/Long columns). Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of columns to analyse e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) print_impact True, False This argument is to print out the statistics.(Default value = False) Returns ------- DataFrame [attribute, mean, median, mode, mode_rows, mode_pct] \"\"\" num_cols , cat_cols , other_cols = attributeType_segregation ( idf ) if list_of_cols == \"all\" : list_of_cols = num_cols + cat_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in drop_cols ])) if any ( x not in idf . columns for x in list_of_cols ) | ( len ( list_of_cols ) == 0 ): raise TypeError ( \"Invalid input for Column(s)\" ) odf = ( transpose_dataframe ( idf . select ( list_of_cols ) . summary ( \"mean\" , \"50%\" , \"count\" ), \"summary\" ) . withColumn ( \"mean\" , F . when ( F . col ( \"key\" ) . isin ( num_cols ), F . round ( F . col ( \"mean\" ) . cast ( T . DoubleType ()), 4 ), ) . otherwise ( None ), ) . withColumn ( \"median\" , F . when ( F . col ( \"key\" ) . isin ( num_cols ), F . round ( F . col ( \"50%\" ) . cast ( T . DoubleType ()), 4 ), ) . otherwise ( None ), ) . withColumnRenamed ( \"key\" , \"attribute\" ) . join ( mode_computation ( spark , idf , list_of_cols ), \"attribute\" , \"full_outer\" ) . withColumn ( \"mode_pct\" , F . round ( F . col ( \"mode_rows\" ) / F . col ( \"count\" ) . cast ( T . DoubleType ()), 4 ), ) . select ( \"attribute\" , \"mean\" , \"median\" , \"mode\" , \"mode_rows\" , \"mode_pct\" ) ) if print_impact : odf . show ( len ( list_of_cols )) return odf def uniqueCount_computation ( spark , idf , list_of_cols = \"all\" , drop_cols = [], print_impact = False ): \"\"\" Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of Discrete (Categorical + Integer) columns to analyse e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all discrete columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) print_impact True, False This argument is to print out the statistics.(Default value = False) Returns ------- DataFrame [attribute, unique_values] \"\"\" if list_of_cols == \"all\" : list_of_cols = [] for i in idf . dtypes : if i [ 1 ] in ( \"string\" , \"int\" , \"bigint\" , \"long\" ): list_of_cols . append ( i [ 0 ]) if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in drop_cols ])) if any ( x not in idf . columns for x in list_of_cols ): raise TypeError ( \"Invalid input for Column(s)\" ) if len ( list_of_cols ) == 0 : warnings . warn ( \"No Unique Count Computation - No discrete column(s) to analyze\" ) schema = T . StructType ( [ T . StructField ( \"attribute\" , T . StringType (), True ), T . StructField ( \"unique_values\" , T . StringType (), True ), ] ) odf = spark . sparkContext . emptyRDD () . toDF ( schema ) return odf uniquevalue_count = idf . agg ( * ( F . countDistinct ( F . col ( i )) . alias ( i ) for i in list_of_cols ) ) odf = spark . createDataFrame ( zip ( list_of_cols , uniquevalue_count . rdd . map ( list ) . collect ()[ 0 ]), schema = ( \"attribute\" , \"unique_values\" ), ) if print_impact : odf . show ( len ( list_of_cols )) return odf def measures_of_cardinality ( spark , idf , list_of_cols = \"all\" , drop_cols = [], print_impact = False ): \"\"\" The Measures of Cardinality function provides statistics that are related to unique values seen in an attribute. These statistics are calculated only for discrete columns (categorical + Integer/Long columns). It returns a Spark Dataframe with schema \u2013 attribute, unique_values, IDness. - Unique Value is defined as a distinct value count of a column. It relies on a supporting function uniqueCount_computation for its computation and leverages the countDistinct functionality of Spark SQL. - IDness is calculated as Unique Values divided by non-null values seen in a column. Non-null values count is used instead of total count because too many null values can give misleading results even if the column have all unique values (except null). It uses supporting functions - uniqueCount_computation and missingCount_computation. Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of Discrete (Categorical + Integer) columns to analyse e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all discrete columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) print_impact True, False This argument is to print out the statistics.(Default value = False) Returns ------- DataFrame [attribute, unique_values, IDness] \"\"\" if list_of_cols == \"all\" : list_of_cols = [] for i in idf . dtypes : if i [ 1 ] in ( \"string\" , \"int\" , \"bigint\" , \"long\" ): list_of_cols . append ( i [ 0 ]) if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in drop_cols ])) if any ( x not in idf . columns for x in list_of_cols ): raise TypeError ( \"Invalid input for Column(s)\" ) if len ( list_of_cols ) == 0 : warnings . warn ( \"No Cardinality Computation - No discrete column(s) to analyze\" ) schema = T . StructType ( [ T . StructField ( \"attribute\" , T . StringType (), True ), T . StructField ( \"unique_values\" , T . StringType (), True ), T . StructField ( \"IDness\" , T . StringType (), True ), ] ) odf = spark . sparkContext . emptyRDD () . toDF ( schema ) return odf odf = ( uniqueCount_computation ( spark , idf , list_of_cols ) . join ( missingCount_computation ( spark , idf , list_of_cols ), \"attribute\" , \"full_outer\" , ) . withColumn ( \"IDness\" , F . round ( F . col ( \"unique_values\" ) / ( F . lit ( idf . count ()) - F . col ( \"missing_count\" )), 4 , ), ) . select ( \"attribute\" , \"unique_values\" , \"IDness\" ) ) if print_impact : odf . show ( len ( list_of_cols )) return odf def measures_of_dispersion ( spark , idf , list_of_cols = \"all\" , drop_cols = [], print_impact = False ): \"\"\" The Measures of Dispersion function provides statistics that describe the spread of a numerical attribute. Alternatively, these statistics are also known as measures of spread. It returns a Spark DataFrame with schema \u2013 attribute, stddev, variance, cov, IQR, range. - Standard Deviation (stddev) measures how concentrated an attribute is around the mean or average. It leverages \u2018stddev\u2019 statistic from summary functionality of Spark SQL. - Variance is the squared value of Standard Deviation. - Coefficient of Variance (cov) is computed as ratio of Standard Deviation & Mean. It leverages \u2018stddev\u2019 and \u2018mean\u2019 statistic from the summary functionality of Spark SQL. - Interquartile Range (IQR): It describes the difference between the third quartile (75th percentile) and the first quartile (25th percentile), telling us about the range where middle half values are seen. It leverage \u201825%\u2019 and \u201875%\u2019 statistics from the summary functionality of Spark SQL. - Range is simply the difference between the maximum value and the minimum value. It leverage \u2018min\u2019 and \u2018max\u2019 statistics from the summary functionality of Spark Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of numerical columns to analyse e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all numerical columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) print_impact True, False This argument is to print out the statistics.(Default value = False) Returns ------- DataFrame [attribute, stddev, variance, cov, IQR, range] \"\"\" num_cols = attributeType_segregation ( idf )[ 0 ] if list_of_cols == \"all\" : list_of_cols = num_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in drop_cols ])) if any ( x not in num_cols for x in list_of_cols ): raise TypeError ( \"Invalid input for Column(s)\" ) if len ( list_of_cols ) == 0 : warnings . warn ( \"No Dispersion Computation - No numerical column(s) to analyze\" ) schema = T . StructType ( [ T . StructField ( \"attribute\" , T . StringType (), True ), T . StructField ( \"stddev\" , T . StringType (), True ), T . StructField ( \"variance\" , T . StringType (), True ), T . StructField ( \"cov\" , T . StringType (), True ), T . StructField ( \"IQR\" , T . StringType (), True ), T . StructField ( \"range\" , T . StringType (), True ), ] ) odf = spark . sparkContext . emptyRDD () . toDF ( schema ) return odf odf = ( transpose_dataframe ( idf . select ( list_of_cols ) . summary ( \"stddev\" , \"min\" , \"max\" , \"mean\" , \"25%\" , \"75%\" ), \"summary\" , ) . withColumn ( \"stddev\" , F . round ( F . col ( \"stddev\" ) . cast ( T . DoubleType ()), 4 )) . withColumn ( \"variance\" , F . round ( F . col ( \"stddev\" ) * F . col ( \"stddev\" ), 4 )) . withColumn ( \"range\" , F . round ( F . col ( \"max\" ) - F . col ( \"min\" ), 4 )) . withColumn ( \"cov\" , F . round ( F . col ( \"stddev\" ) / F . col ( \"mean\" ), 4 )) . withColumn ( \"IQR\" , F . round ( F . col ( \"75%\" ) - F . col ( \"25%\" ), 4 )) . select ( F . col ( \"key\" ) . alias ( \"attribute\" ), \"stddev\" , \"variance\" , \"cov\" , \"IQR\" , \"range\" ) ) if print_impact : odf . show ( len ( list_of_cols )) return odf def measures_of_percentiles ( spark , idf , list_of_cols = \"all\" , drop_cols = [], print_impact = False ): \"\"\" The Measures of Percentiles function provides statistics at different percentiles. Nth percentile can be interpreted as N% of rows having values lesser than or equal to Nth percentile value. It is prominently used for quick detection of skewness or outlier. Alternatively, these statistics are also known as measures of position. These statistics are computed only for numerical attributes. It returns a Spark Dataframe with schema \u2013 attribute, min, 1%, 5%, 10%, 25%, 50%, 75%, 90%, 95%, 99%, max. It leverage \u2018N%\u2019 statistics from summary functionality of Spark SQL where N is 0 for min and 100 for max. Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of numerical columns to analyse e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all numerical columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) print_impact True, False This argument is to print out the statistics.(Default value = False) Returns ------- DataFrame [attribute, min, 1%, 5%, 10%, 25%, 50%, 75%, 90%, 95%, 99%, max] \"\"\" num_cols = attributeType_segregation ( idf )[ 0 ] if list_of_cols == \"all\" : list_of_cols = num_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in drop_cols ])) if any ( x not in num_cols for x in list_of_cols ): raise TypeError ( \"Invalid input for Column(s)\" ) if len ( list_of_cols ) == 0 : warnings . warn ( \"No Percentiles Computation - No numerical column(s) to analyze\" ) schema = T . StructType ( [ T . StructField ( \"attribute\" , T . StringType (), True ), T . StructField ( \"min\" , T . StringType (), True ), T . StructField ( \"1%\" , T . StringType (), True ), T . StructField ( \"5%\" , T . StringType (), True ), T . StructField ( \"10%\" , T . StringType (), True ), T . StructField ( \"25%\" , T . StringType (), True ), T . StructField ( \"50%\" , T . StringType (), True ), T . StructField ( \"75%\" , T . StringType (), True ), T . StructField ( \"90%\" , T . StringType (), True ), T . StructField ( \"95%\" , T . StringType (), True ), T . StructField ( \"99%\" , T . StringType (), True ), T . StructField ( \"max\" , T . StringType (), True ), ] ) odf = spark . sparkContext . emptyRDD () . toDF ( schema ) return odf stats = [ \"min\" , \"1%\" , \"5%\" , \"10%\" , \"25%\" , \"50%\" , \"75%\" , \"90%\" , \"95%\" , \"99%\" , \"max\" ] odf = transpose_dataframe ( idf . select ( list_of_cols ) . summary ( * stats ), \"summary\" ) . withColumnRenamed ( \"key\" , \"attribute\" ) for i in odf . columns : if i != \"attribute\" : odf = odf . withColumn ( i , F . round ( F . col ( i ) . cast ( \"Double\" ), 4 )) odf = odf . select ([ \"attribute\" ] + stats ) if print_impact : odf . show ( len ( list_of_cols )) return odf def measures_of_shape ( spark , idf , list_of_cols = \"all\" , drop_cols = [], print_impact = False ): \"\"\" The Measures of Shapes function provides statistics related to the shape of an attribute's distribution. Alternatively, these statistics are also known as measures of the moment and are computed only for numerical attributes. It returns a Spark Dataframe with schema \u2013 attribute, skewness, kurtosis. - Skewness describes how much-skewed values are relative to a perfect bell curve observed in normal distribution and the direction of skew. If the majority of the values are at the left and the right tail is longer, we say that the distribution is skewed right or positively skewed; if the peak is toward the right and the left tail is longer, we say that the distribution is skewed left or negatively skewed. It leverage skewness functionality of Spark SQL. - (Excess) Kurtosis describes how tall and sharp the central peak is relative to a perfect bell curve observed in the normal distribution. The reference standard is a normal distribution, which has a kurtosis of 3. In token of this, often, the excess kurtosis is presented: excess kurtosis is simply kurtosis\u22123. Higher (positive) values indicate a higher, sharper peak; lower (negative) values indicate a less distinct peak. It leverages kurtosis functionality of Spark SQL. Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of numerical columns to analyse e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all numerical columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) print_impact True, False This argument is to print out the statistics.(Default value = False) Returns ------- DataFrame [attribute, skewness, kurtosis] \"\"\" num_cols = attributeType_segregation ( idf )[ 0 ] if list_of_cols == \"all\" : list_of_cols = num_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in drop_cols ])) if any ( x not in num_cols for x in list_of_cols ): raise TypeError ( \"Invalid input for Column(s)\" ) if len ( list_of_cols ) == 0 : warnings . warn ( \"No Skewness/Kurtosis Computation - No numerical column(s) to analyze\" ) schema = T . StructType ( [ T . StructField ( \"attribute\" , T . StringType (), True ), T . StructField ( \"skewness\" , T . StringType (), True ), T . StructField ( \"kurtosis\" , T . StringType (), True ), ] ) odf = spark . sparkContext . emptyRDD () . toDF ( schema ) return odf shapes = [] for i in list_of_cols : s , k = idf . select ( F . skewness ( i ), F . kurtosis ( i )) . first () shapes . append ([ i , s , k ]) odf = ( spark . createDataFrame ( shapes , schema = ( \"attribute\" , \"skewness\" , \"kurtosis\" )) . withColumn ( \"skewness\" , F . round ( F . col ( \"skewness\" ), 4 )) . withColumn ( \"kurtosis\" , F . round ( F . col ( \"kurtosis\" ), 4 )) ) if print_impact : odf . show ( len ( list_of_cols )) return odf Functions def global_summary ( spark, idf, list_of_cols='all', drop_cols=[], print_impact=False) The global summary function computes the universal statistics/metrics and returns a Spark DataFrame with schema \u2013 metric, value. The metrics computed in this function - No. of rows, No. of columns, No. of categorical columns along with column names, No. of numerical columns along with the column names, No. of non-numerical non-categorical columns such as date type, array type etc. along with column names. Parameters spark Spark Session idf Input Dataframe list_of_cols List of columns to analyse e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) print_impact True, False This argument is to print out the statistics.(Default value = False) Returns DataFrame [metric, value] Expand source code def global_summary ( spark , idf , list_of_cols = \"all\" , drop_cols = [], print_impact = False ): \"\"\" The global summary function computes the universal statistics/metrics and returns a Spark DataFrame with schema \u2013 metric, value. The metrics computed in this function - No. of rows, No. of columns, No. of categorical columns along with column names, No. of numerical columns along with the column names, No. of non-numerical non-categorical columns such as date type, array type etc. along with column names. Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of columns to analyse e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) print_impact True, False This argument is to print out the statistics.(Default value = False) Returns ------- DataFrame [metric, value] \"\"\" if list_of_cols == \"all\" : list_of_cols = idf . columns if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in drop_cols ])) if any ( x not in idf . columns for x in list_of_cols ) | ( len ( list_of_cols ) == 0 ): raise TypeError ( \"Invalid input for Column(s)\" ) row_count = idf . count () col_count = len ( list_of_cols ) num_cols , cat_cols , other_cols = attributeType_segregation ( idf . select ( list_of_cols )) numcol_count = len ( num_cols ) catcol_count = len ( cat_cols ) othercol_count = len ( other_cols ) if print_impact : print ( \"No. of Rows: %s \" % \" {0:,} \" . format ( row_count )) print ( \"No. of Columns: %s \" % \" {0:,} \" . format ( col_count )) print ( \"Numerical Columns: %s \" % \" {0:,} \" . format ( numcol_count )) if numcol_count > 0 : print ( num_cols ) print ( \"Categorical Columns: %s \" % \" {0:,} \" . format ( catcol_count )) if catcol_count > 0 : print ( cat_cols ) if othercol_count > 0 : print ( \"Other Columns: %s \" % \" {0:,} \" . format ( othercol_count )) print ( other_cols ) odf = spark . createDataFrame ( [ [ \"rows_count\" , str ( row_count )], [ \"columns_count\" , str ( col_count )], [ \"numcols_count\" , str ( numcol_count )], [ \"numcols_name\" , \", \" . join ( num_cols )], [ \"catcols_count\" , str ( catcol_count )], [ \"catcols_name\" , \", \" . join ( cat_cols )], [ \"othercols_count\" , str ( othercol_count )], [ \"othercols_name\" , \", \" . join ( other_cols )], ], schema = [ \"metric\" , \"value\" ], ) return odf def measures_of_cardinality ( spark, idf, list_of_cols='all', drop_cols=[], print_impact=False) The Measures of Cardinality function provides statistics that are related to unique values seen in an attribute. These statistics are calculated only for discrete columns (categorical + Integer/Long columns). It returns a Spark Dataframe with schema \u2013 attribute, unique_values, IDness. Unique Value is defined as a distinct value count of a column. It relies on a supporting function uniqueCount_computation for its computation and leverages the countDistinct functionality of Spark SQL. IDness is calculated as Unique Values divided by non-null values seen in a column. Non-null values count is used instead of total count because too many null values can give misleading results even if the column have all unique values (except null). It uses supporting functions - uniqueCount_computation and missingCount_computation. Parameters spark Spark Session idf Input Dataframe list_of_cols List of Discrete (Categorical + Integer) columns to analyse e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all discrete columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) print_impact True, False This argument is to print out the statistics.(Default value = False) Returns DataFrame [attribute, unique_values, IDness] Expand source code def measures_of_cardinality ( spark , idf , list_of_cols = \"all\" , drop_cols = [], print_impact = False ): \"\"\" The Measures of Cardinality function provides statistics that are related to unique values seen in an attribute. These statistics are calculated only for discrete columns (categorical + Integer/Long columns). It returns a Spark Dataframe with schema \u2013 attribute, unique_values, IDness. - Unique Value is defined as a distinct value count of a column. It relies on a supporting function uniqueCount_computation for its computation and leverages the countDistinct functionality of Spark SQL. - IDness is calculated as Unique Values divided by non-null values seen in a column. Non-null values count is used instead of total count because too many null values can give misleading results even if the column have all unique values (except null). It uses supporting functions - uniqueCount_computation and missingCount_computation. Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of Discrete (Categorical + Integer) columns to analyse e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all discrete columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) print_impact True, False This argument is to print out the statistics.(Default value = False) Returns ------- DataFrame [attribute, unique_values, IDness] \"\"\" if list_of_cols == \"all\" : list_of_cols = [] for i in idf . dtypes : if i [ 1 ] in ( \"string\" , \"int\" , \"bigint\" , \"long\" ): list_of_cols . append ( i [ 0 ]) if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in drop_cols ])) if any ( x not in idf . columns for x in list_of_cols ): raise TypeError ( \"Invalid input for Column(s)\" ) if len ( list_of_cols ) == 0 : warnings . warn ( \"No Cardinality Computation - No discrete column(s) to analyze\" ) schema = T . StructType ( [ T . StructField ( \"attribute\" , T . StringType (), True ), T . StructField ( \"unique_values\" , T . StringType (), True ), T . StructField ( \"IDness\" , T . StringType (), True ), ] ) odf = spark . sparkContext . emptyRDD () . toDF ( schema ) return odf odf = ( uniqueCount_computation ( spark , idf , list_of_cols ) . join ( missingCount_computation ( spark , idf , list_of_cols ), \"attribute\" , \"full_outer\" , ) . withColumn ( \"IDness\" , F . round ( F . col ( \"unique_values\" ) / ( F . lit ( idf . count ()) - F . col ( \"missing_count\" )), 4 , ), ) . select ( \"attribute\" , \"unique_values\" , \"IDness\" ) ) if print_impact : odf . show ( len ( list_of_cols )) return odf def measures_of_centralTendency ( spark, idf, list_of_cols='all', drop_cols=[], print_impact=False) The Measures of Central Tendency function provides summary statistics that represents the centre point or most likely value of an attribute. It returns a Spark DataFrame with schema \u2013 attribute, mean, median, mode, mode_rows, mode_pct. Mean is arithmetic average of a column i.e. sum of all values seen in the column divided by the number of rows. It leverage mean statistic from summary functionality of Spark SQL. Mean is calculated only for numerical columns. Median is 50th percentile or middle value in a column when the values are arranged in ascending or descending order. It leverage \u201850%\u2019 statistic from summary functionality of Spark SQL. Median is calculated only for numerical columns. Mode is most frequently seen value in a column. Mode is calculated only for discrete columns (categorical + Integer/Long columns). Mode Rows is the numer of rows seen with Mode value. Mode Rows is calculated only for discrete columns (categorical + Integer/Long columns). Mode Pct is defined as Mode Rows divided by non-null values seen in a column. Mode Pct is calculated only for discrete columns (categorical + Integer/Long columns). Parameters spark Spark Session idf Input Dataframe list_of_cols List of columns to analyse e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) print_impact True, False This argument is to print out the statistics.(Default value = False) Returns DataFrame [attribute, mean, median, mode, mode_rows, mode_pct] Expand source code def measures_of_centralTendency ( spark , idf , list_of_cols = \"all\" , drop_cols = [], print_impact = False ): \"\"\" The Measures of Central Tendency function provides summary statistics that represents the centre point or most likely value of an attribute. It returns a Spark DataFrame with schema \u2013 attribute, mean, median, mode, mode_rows, mode_pct. - Mean is arithmetic average of a column i.e. sum of all values seen in the column divided by the number of rows. It leverage mean statistic from summary functionality of Spark SQL. Mean is calculated only for numerical columns. - Median is 50th percentile or middle value in a column when the values are arranged in ascending or descending order. It leverage \u201850%\u2019 statistic from summary functionality of Spark SQL. Median is calculated only for numerical columns. - Mode is most frequently seen value in a column. Mode is calculated only for discrete columns (categorical + Integer/Long columns). - Mode Rows is the numer of rows seen with Mode value. Mode Rows is calculated only for discrete columns (categorical + Integer/Long columns). - Mode Pct is defined as Mode Rows divided by non-null values seen in a column. Mode Pct is calculated only for discrete columns (categorical + Integer/Long columns). Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of columns to analyse e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) print_impact True, False This argument is to print out the statistics.(Default value = False) Returns ------- DataFrame [attribute, mean, median, mode, mode_rows, mode_pct] \"\"\" num_cols , cat_cols , other_cols = attributeType_segregation ( idf ) if list_of_cols == \"all\" : list_of_cols = num_cols + cat_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in drop_cols ])) if any ( x not in idf . columns for x in list_of_cols ) | ( len ( list_of_cols ) == 0 ): raise TypeError ( \"Invalid input for Column(s)\" ) odf = ( transpose_dataframe ( idf . select ( list_of_cols ) . summary ( \"mean\" , \"50%\" , \"count\" ), \"summary\" ) . withColumn ( \"mean\" , F . when ( F . col ( \"key\" ) . isin ( num_cols ), F . round ( F . col ( \"mean\" ) . cast ( T . DoubleType ()), 4 ), ) . otherwise ( None ), ) . withColumn ( \"median\" , F . when ( F . col ( \"key\" ) . isin ( num_cols ), F . round ( F . col ( \"50%\" ) . cast ( T . DoubleType ()), 4 ), ) . otherwise ( None ), ) . withColumnRenamed ( \"key\" , \"attribute\" ) . join ( mode_computation ( spark , idf , list_of_cols ), \"attribute\" , \"full_outer\" ) . withColumn ( \"mode_pct\" , F . round ( F . col ( \"mode_rows\" ) / F . col ( \"count\" ) . cast ( T . DoubleType ()), 4 ), ) . select ( \"attribute\" , \"mean\" , \"median\" , \"mode\" , \"mode_rows\" , \"mode_pct\" ) ) if print_impact : odf . show ( len ( list_of_cols )) return odf def measures_of_counts ( spark, idf, list_of_cols='all', drop_cols=[], print_impact=False) The Measures of Counts function computes different count metrics for each column. It returns a Spark DataFrame with schema \u2013 attribute, fill_count, fill_pct, missing_count, missing_pct, nonzero_count, nonzero_pct. Fill Count/Rate is defined as number of rows with non-null values in a column both in terms of absolute count and its proportion to row count. It leverages count statistic from summary functionality of Spark SQL. Missing Count/Rate is defined as null (or missing) values seen in a column both in terms of absolute count and its proportion to row count. It is directly derivable from Fill Count/Rate. Non Zero Count/Rate is defined as non-zero values seen in a numerical column both in terms of absolute count and its proportion to row count. For categorical column, it will show null value. Also, it uses a supporting function nonzeroCount_computation. Under the hood, it leverage Multivariate Statistical Summary of Spark MLlib. Parameters spark Spark Session idf Input Dataframe list_of_cols List of columns to analyse e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) print_impact True, False This argument is to print out the statistics.(Default value = False) Returns DataFrame [attribute, fill_count, fill_pct, missing_count, missing_pct, nonzero_count, nonzero_pct] Expand source code def measures_of_counts ( spark , idf , list_of_cols = \"all\" , drop_cols = [], print_impact = False ): \"\"\" The Measures of Counts function computes different count metrics for each column. It returns a Spark DataFrame with schema \u2013 attribute, fill_count, fill_pct, missing_count, missing_pct, nonzero_count, nonzero_pct. - Fill Count/Rate is defined as number of rows with non-null values in a column both in terms of absolute count and its proportion to row count. It leverages count statistic from summary functionality of Spark SQL. - Missing Count/Rate is defined as null (or missing) values seen in a column both in terms of absolute count and its proportion to row count. It is directly derivable from Fill Count/Rate. - Non Zero Count/Rate is defined as non-zero values seen in a numerical column both in terms of absolute count and its proportion to row count. For categorical column, it will show null value. Also, it uses a supporting function nonzeroCount_computation. Under the hood, it leverage Multivariate Statistical Summary of Spark MLlib. Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of columns to analyse e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) print_impact True, False This argument is to print out the statistics.(Default value = False) Returns ------- DataFrame [attribute, fill_count, fill_pct, missing_count, missing_pct, nonzero_count, nonzero_pct] \"\"\" if list_of_cols == \"all\" : num_cols , cat_cols , other_cols = attributeType_segregation ( idf ) list_of_cols = num_cols + cat_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in drop_cols ])) num_cols = attributeType_segregation ( idf . select ( list_of_cols ))[ 0 ] if any ( x not in idf . columns for x in list_of_cols ) | ( len ( list_of_cols ) == 0 ): raise TypeError ( \"Invalid input for Column(s)\" ) odf = ( transpose_dataframe ( idf . select ( list_of_cols ) . summary ( \"count\" ), \"summary\" ) . select ( F . col ( \"key\" ) . alias ( \"attribute\" ), F . col ( \"count\" ) . cast ( T . LongType ()) . alias ( \"fill_count\" ), ) . withColumn ( \"fill_pct\" , F . round ( F . col ( \"fill_count\" ) / F . lit ( idf . count ()), 4 )) . withColumn ( \"missing_count\" , F . lit ( idf . count ()) - F . col ( \"fill_count\" ) . cast ( T . LongType ()) ) . withColumn ( \"missing_pct\" , F . round ( 1 - F . col ( \"fill_pct\" ), 4 )) . join ( nonzeroCount_computation ( spark , idf , num_cols ), \"attribute\" , \"full_outer\" ) ) if print_impact : odf . show ( len ( list_of_cols )) return odf def measures_of_dispersion ( spark, idf, list_of_cols='all', drop_cols=[], print_impact=False) The Measures of Dispersion function provides statistics that describe the spread of a numerical attribute. Alternatively, these statistics are also known as measures of spread. It returns a Spark DataFrame with schema \u2013 attribute, stddev, variance, cov, IQR, range. Standard Deviation (stddev) measures how concentrated an attribute is around the mean or average. It leverages \u2018stddev\u2019 statistic from summary functionality of Spark SQL. Variance is the squared value of Standard Deviation. Coefficient of Variance (cov) is computed as ratio of Standard Deviation & Mean. It leverages \u2018stddev\u2019 and \u2018mean\u2019 statistic from the summary functionality of Spark SQL. Interquartile Range (IQR): It describes the difference between the third quartile (75th percentile) and the first quartile (25th percentile), telling us about the range where middle half values are seen. It leverage \u201825%\u2019 and \u201875%\u2019 statistics from the summary functionality of Spark SQL. Range is simply the difference between the maximum value and the minimum value. It leverage \u2018min\u2019 and \u2018max\u2019 statistics from the summary functionality of Spark Parameters spark Spark Session idf Input Dataframe list_of_cols List of numerical columns to analyse e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all numerical columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) print_impact True, False This argument is to print out the statistics.(Default value = False) Returns DataFrame [attribute, stddev, variance, cov, IQR, range] Expand source code def measures_of_dispersion ( spark , idf , list_of_cols = \"all\" , drop_cols = [], print_impact = False ): \"\"\" The Measures of Dispersion function provides statistics that describe the spread of a numerical attribute. Alternatively, these statistics are also known as measures of spread. It returns a Spark DataFrame with schema \u2013 attribute, stddev, variance, cov, IQR, range. - Standard Deviation (stddev) measures how concentrated an attribute is around the mean or average. It leverages \u2018stddev\u2019 statistic from summary functionality of Spark SQL. - Variance is the squared value of Standard Deviation. - Coefficient of Variance (cov) is computed as ratio of Standard Deviation & Mean. It leverages \u2018stddev\u2019 and \u2018mean\u2019 statistic from the summary functionality of Spark SQL. - Interquartile Range (IQR): It describes the difference between the third quartile (75th percentile) and the first quartile (25th percentile), telling us about the range where middle half values are seen. It leverage \u201825%\u2019 and \u201875%\u2019 statistics from the summary functionality of Spark SQL. - Range is simply the difference between the maximum value and the minimum value. It leverage \u2018min\u2019 and \u2018max\u2019 statistics from the summary functionality of Spark Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of numerical columns to analyse e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all numerical columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) print_impact True, False This argument is to print out the statistics.(Default value = False) Returns ------- DataFrame [attribute, stddev, variance, cov, IQR, range] \"\"\" num_cols = attributeType_segregation ( idf )[ 0 ] if list_of_cols == \"all\" : list_of_cols = num_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in drop_cols ])) if any ( x not in num_cols for x in list_of_cols ): raise TypeError ( \"Invalid input for Column(s)\" ) if len ( list_of_cols ) == 0 : warnings . warn ( \"No Dispersion Computation - No numerical column(s) to analyze\" ) schema = T . StructType ( [ T . StructField ( \"attribute\" , T . StringType (), True ), T . StructField ( \"stddev\" , T . StringType (), True ), T . StructField ( \"variance\" , T . StringType (), True ), T . StructField ( \"cov\" , T . StringType (), True ), T . StructField ( \"IQR\" , T . StringType (), True ), T . StructField ( \"range\" , T . StringType (), True ), ] ) odf = spark . sparkContext . emptyRDD () . toDF ( schema ) return odf odf = ( transpose_dataframe ( idf . select ( list_of_cols ) . summary ( \"stddev\" , \"min\" , \"max\" , \"mean\" , \"25%\" , \"75%\" ), \"summary\" , ) . withColumn ( \"stddev\" , F . round ( F . col ( \"stddev\" ) . cast ( T . DoubleType ()), 4 )) . withColumn ( \"variance\" , F . round ( F . col ( \"stddev\" ) * F . col ( \"stddev\" ), 4 )) . withColumn ( \"range\" , F . round ( F . col ( \"max\" ) - F . col ( \"min\" ), 4 )) . withColumn ( \"cov\" , F . round ( F . col ( \"stddev\" ) / F . col ( \"mean\" ), 4 )) . withColumn ( \"IQR\" , F . round ( F . col ( \"75%\" ) - F . col ( \"25%\" ), 4 )) . select ( F . col ( \"key\" ) . alias ( \"attribute\" ), \"stddev\" , \"variance\" , \"cov\" , \"IQR\" , \"range\" ) ) if print_impact : odf . show ( len ( list_of_cols )) return odf def measures_of_percentiles ( spark, idf, list_of_cols='all', drop_cols=[], print_impact=False) The Measures of Percentiles function provides statistics at different percentiles. Nth percentile can be interpreted as N% of rows having values lesser than or equal to Nth percentile value. It is prominently used for quick detection of skewness or outlier. Alternatively, these statistics are also known as measures of position. These statistics are computed only for numerical attributes. It returns a Spark Dataframe with schema \u2013 attribute, min, 1%, 5%, 10%, 25%, 50%, 75%, 90%, 95%, 99%, max. It leverage \u2018N%\u2019 statistics from summary functionality of Spark SQL where N is 0 for min and 100 for max. Parameters spark Spark Session idf Input Dataframe list_of_cols List of numerical columns to analyse e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all numerical columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) print_impact True, False This argument is to print out the statistics.(Default value = False) Returns DataFrame [attribute, min, 1%, 5%, 10%, 25%, 50%, 75%, 90%, 95%, 99%, max] Expand source code def measures_of_percentiles ( spark , idf , list_of_cols = \"all\" , drop_cols = [], print_impact = False ): \"\"\" The Measures of Percentiles function provides statistics at different percentiles. Nth percentile can be interpreted as N% of rows having values lesser than or equal to Nth percentile value. It is prominently used for quick detection of skewness or outlier. Alternatively, these statistics are also known as measures of position. These statistics are computed only for numerical attributes. It returns a Spark Dataframe with schema \u2013 attribute, min, 1%, 5%, 10%, 25%, 50%, 75%, 90%, 95%, 99%, max. It leverage \u2018N%\u2019 statistics from summary functionality of Spark SQL where N is 0 for min and 100 for max. Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of numerical columns to analyse e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all numerical columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) print_impact True, False This argument is to print out the statistics.(Default value = False) Returns ------- DataFrame [attribute, min, 1%, 5%, 10%, 25%, 50%, 75%, 90%, 95%, 99%, max] \"\"\" num_cols = attributeType_segregation ( idf )[ 0 ] if list_of_cols == \"all\" : list_of_cols = num_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in drop_cols ])) if any ( x not in num_cols for x in list_of_cols ): raise TypeError ( \"Invalid input for Column(s)\" ) if len ( list_of_cols ) == 0 : warnings . warn ( \"No Percentiles Computation - No numerical column(s) to analyze\" ) schema = T . StructType ( [ T . StructField ( \"attribute\" , T . StringType (), True ), T . StructField ( \"min\" , T . StringType (), True ), T . StructField ( \"1%\" , T . StringType (), True ), T . StructField ( \"5%\" , T . StringType (), True ), T . StructField ( \"10%\" , T . StringType (), True ), T . StructField ( \"25%\" , T . StringType (), True ), T . StructField ( \"50%\" , T . StringType (), True ), T . StructField ( \"75%\" , T . StringType (), True ), T . StructField ( \"90%\" , T . StringType (), True ), T . StructField ( \"95%\" , T . StringType (), True ), T . StructField ( \"99%\" , T . StringType (), True ), T . StructField ( \"max\" , T . StringType (), True ), ] ) odf = spark . sparkContext . emptyRDD () . toDF ( schema ) return odf stats = [ \"min\" , \"1%\" , \"5%\" , \"10%\" , \"25%\" , \"50%\" , \"75%\" , \"90%\" , \"95%\" , \"99%\" , \"max\" ] odf = transpose_dataframe ( idf . select ( list_of_cols ) . summary ( * stats ), \"summary\" ) . withColumnRenamed ( \"key\" , \"attribute\" ) for i in odf . columns : if i != \"attribute\" : odf = odf . withColumn ( i , F . round ( F . col ( i ) . cast ( \"Double\" ), 4 )) odf = odf . select ([ \"attribute\" ] + stats ) if print_impact : odf . show ( len ( list_of_cols )) return odf def measures_of_shape ( spark, idf, list_of_cols='all', drop_cols=[], print_impact=False) The Measures of Shapes function provides statistics related to the shape of an attribute's distribution. Alternatively, these statistics are also known as measures of the moment and are computed only for numerical attributes. It returns a Spark Dataframe with schema \u2013 attribute, skewness, kurtosis. Skewness describes how much-skewed values are relative to a perfect bell curve observed in normal distribution and the direction of skew. If the majority of the values are at the left and the right tail is longer, we say that the distribution is skewed right or positively skewed; if the peak is toward the right and the left tail is longer, we say that the distribution is skewed left or negatively skewed. It leverage skewness functionality of Spark SQL. (Excess) Kurtosis describes how tall and sharp the central peak is relative to a perfect bell curve observed in the normal distribution. The reference standard is a normal distribution, which has a kurtosis of 3. In token of this, often, the excess kurtosis is presented: excess kurtosis is simply kurtosis\u22123. Higher (positive) values indicate a higher, sharper peak; lower (negative) values indicate a less distinct peak. It leverages kurtosis functionality of Spark SQL. Parameters spark Spark Session idf Input Dataframe list_of_cols List of numerical columns to analyse e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all numerical columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) print_impact True, False This argument is to print out the statistics.(Default value = False) Returns DataFrame [attribute, skewness, kurtosis] Expand source code def measures_of_shape ( spark , idf , list_of_cols = \"all\" , drop_cols = [], print_impact = False ): \"\"\" The Measures of Shapes function provides statistics related to the shape of an attribute's distribution. Alternatively, these statistics are also known as measures of the moment and are computed only for numerical attributes. It returns a Spark Dataframe with schema \u2013 attribute, skewness, kurtosis. - Skewness describes how much-skewed values are relative to a perfect bell curve observed in normal distribution and the direction of skew. If the majority of the values are at the left and the right tail is longer, we say that the distribution is skewed right or positively skewed; if the peak is toward the right and the left tail is longer, we say that the distribution is skewed left or negatively skewed. It leverage skewness functionality of Spark SQL. - (Excess) Kurtosis describes how tall and sharp the central peak is relative to a perfect bell curve observed in the normal distribution. The reference standard is a normal distribution, which has a kurtosis of 3. In token of this, often, the excess kurtosis is presented: excess kurtosis is simply kurtosis\u22123. Higher (positive) values indicate a higher, sharper peak; lower (negative) values indicate a less distinct peak. It leverages kurtosis functionality of Spark SQL. Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of numerical columns to analyse e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all numerical columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) print_impact True, False This argument is to print out the statistics.(Default value = False) Returns ------- DataFrame [attribute, skewness, kurtosis] \"\"\" num_cols = attributeType_segregation ( idf )[ 0 ] if list_of_cols == \"all\" : list_of_cols = num_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in drop_cols ])) if any ( x not in num_cols for x in list_of_cols ): raise TypeError ( \"Invalid input for Column(s)\" ) if len ( list_of_cols ) == 0 : warnings . warn ( \"No Skewness/Kurtosis Computation - No numerical column(s) to analyze\" ) schema = T . StructType ( [ T . StructField ( \"attribute\" , T . StringType (), True ), T . StructField ( \"skewness\" , T . StringType (), True ), T . StructField ( \"kurtosis\" , T . StringType (), True ), ] ) odf = spark . sparkContext . emptyRDD () . toDF ( schema ) return odf shapes = [] for i in list_of_cols : s , k = idf . select ( F . skewness ( i ), F . kurtosis ( i )) . first () shapes . append ([ i , s , k ]) odf = ( spark . createDataFrame ( shapes , schema = ( \"attribute\" , \"skewness\" , \"kurtosis\" )) . withColumn ( \"skewness\" , F . round ( F . col ( \"skewness\" ), 4 )) . withColumn ( \"kurtosis\" , F . round ( F . col ( \"kurtosis\" ), 4 )) ) if print_impact : odf . show ( len ( list_of_cols )) return odf def missingCount_computation ( spark, idf, list_of_cols='all', drop_cols=[], print_impact=False) Parameters spark Spark Session idf Input Dataframe list_of_cols List of columns to analyse e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) print_impact True, False This argument is to print out the statistics.(Default value = False) Returns DataFrame [attribute, missing_count, missing_pct] Expand source code def missingCount_computation ( spark , idf , list_of_cols = \"all\" , drop_cols = [], print_impact = False ): \"\"\" Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of columns to analyse e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) print_impact True, False This argument is to print out the statistics.(Default value = False) Returns ------- DataFrame [attribute, missing_count, missing_pct] \"\"\" if list_of_cols == \"all\" : num_cols , cat_cols , other_cols = attributeType_segregation ( idf ) list_of_cols = num_cols + cat_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in drop_cols ])) if any ( x not in idf . columns for x in list_of_cols ) | ( len ( list_of_cols ) == 0 ): raise TypeError ( \"Invalid input for Column(s)\" ) idf_stats = idf . select ( list_of_cols ) . summary ( \"count\" ) odf = ( transpose_dataframe ( idf_stats , \"summary\" ) . withColumn ( \"missing_count\" , F . lit ( idf . count ()) - F . col ( \"count\" ) . cast ( T . LongType ()) ) . withColumn ( \"missing_pct\" , F . round ( F . col ( \"missing_count\" ) / F . lit ( idf . count ()), 4 ) ) . select ( F . col ( \"key\" ) . alias ( \"attribute\" ), \"missing_count\" , \"missing_pct\" ) ) if print_impact : odf . show ( len ( list_of_cols )) return odf def mode_computation ( spark, idf, list_of_cols='all', drop_cols=[], print_impact=False) Parameters spark Spark Session idf Input Dataframe list_of_cols List of Discrete (Categorical + Integer) columns to analyse e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all discrete columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) print_impact True, False This argument is to print out the statistics.(Default value = False) Returns DataFrame [attribute, mode, mode_rows] In case there is tie between multiple values, one value is randomly picked as mode. Expand source code def mode_computation ( spark , idf , list_of_cols = \"all\" , drop_cols = [], print_impact = False ): \"\"\" Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of Discrete (Categorical + Integer) columns to analyse e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all discrete columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) print_impact True, False This argument is to print out the statistics.(Default value = False) Returns ------- DataFrame [attribute, mode, mode_rows] In case there is tie between multiple values, one value is randomly picked as mode. \"\"\" if list_of_cols == \"all\" : num_cols , cat_cols , other_cols = attributeType_segregation ( idf ) list_of_cols = num_cols + cat_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in drop_cols ])) for i in idf . select ( list_of_cols ) . dtypes : if i [ 1 ] not in ( \"string\" , \"int\" , \"bigint\" , \"long\" ): list_of_cols . remove ( i [ 0 ]) if any ( x not in idf . columns for x in list_of_cols ): raise TypeError ( \"Invalid input for Column(s)\" ) if len ( list_of_cols ) == 0 : warnings . warn ( \"No Mode Computation - No discrete column(s) to analyze\" ) schema = T . StructType ( [ T . StructField ( \"attribute\" , T . StringType (), True ), T . StructField ( \"mode\" , T . StringType (), True ), T . StructField ( \"mode_rows\" , T . StringType (), True ), ] ) odf = spark . sparkContext . emptyRDD () . toDF ( schema ) return odf mode = [ list ( idf . select ( i ) . dropna () . groupby ( i ) . count () . orderBy ( \"count\" , ascending = False ) . first () or [ None , None ] ) for i in list_of_cols ] mode = [( str ( i ), str ( j )) for i , j in mode ] odf = spark . createDataFrame ( zip ( list_of_cols , mode ), schema = ( \"attribute\" , \"metric\" ) ) . select ( \"attribute\" , ( F . col ( \"metric\" )[ \"_1\" ]) . alias ( \"mode\" ), ( F . col ( \"metric\" )[ \"_2\" ]) . cast ( \"long\" ) . alias ( \"mode_rows\" ), ) if print_impact : odf . show ( len ( list_of_cols )) return odf def nonzeroCount_computation ( spark, idf, list_of_cols='all', drop_cols=[], print_impact=False) Parameters spark Spark Session idf Input Dataframe list_of_cols List of numerical columns to analyse e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all numerical columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) print_impact True, False This argument is to print out the statistics.(Default value = False) Returns DataFrame [attribute, nonzero_count, nonzero_pct] Expand source code def nonzeroCount_computation ( spark , idf , list_of_cols = \"all\" , drop_cols = [], print_impact = False ): \"\"\" Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of numerical columns to analyse e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all numerical columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) print_impact True, False This argument is to print out the statistics.(Default value = False) Returns ------- DataFrame [attribute, nonzero_count, nonzero_pct] \"\"\" num_cols = attributeType_segregation ( idf )[ 0 ] if list_of_cols == \"all\" : list_of_cols = num_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in drop_cols ])) if any ( x not in num_cols for x in list_of_cols ): raise TypeError ( \"Invalid input for Column(s)\" ) if len ( list_of_cols ) == 0 : warnings . warn ( \"No Non-Zero Count Computation - No numerical column(s) to analyze\" ) schema = T . StructType ( [ T . StructField ( \"attribute\" , T . StringType (), True ), T . StructField ( \"nonzero_count\" , T . StringType (), True ), T . StructField ( \"nonzero_pct\" , T . StringType (), True ), ] ) odf = spark . sparkContext . emptyRDD () . toDF ( schema ) return odf tmp = idf . select ( list_of_cols ) . fillna ( 0 ) . rdd . map ( lambda row : Vectors . dense ( row )) nonzero_count = Statistics . colStats ( tmp ) . numNonzeros () odf = spark . createDataFrame ( zip ( list_of_cols , [ int ( i ) for i in nonzero_count ]), schema = ( \"attribute\" , \"nonzero_count\" ), ) . withColumn ( \"nonzero_pct\" , F . round ( F . col ( \"nonzero_count\" ) / F . lit ( idf . count ()), 4 )) if print_impact : odf . show ( len ( list_of_cols )) return odf def uniqueCount_computation ( spark, idf, list_of_cols='all', drop_cols=[], print_impact=False) Parameters spark Spark Session idf Input Dataframe list_of_cols List of Discrete (Categorical + Integer) columns to analyse e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all discrete columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) print_impact True, False This argument is to print out the statistics.(Default value = False) Returns DataFrame [attribute, unique_values] Expand source code def uniqueCount_computation ( spark , idf , list_of_cols = \"all\" , drop_cols = [], print_impact = False ): \"\"\" Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of Discrete (Categorical + Integer) columns to analyse e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all discrete columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) print_impact True, False This argument is to print out the statistics.(Default value = False) Returns ------- DataFrame [attribute, unique_values] \"\"\" if list_of_cols == \"all\" : list_of_cols = [] for i in idf . dtypes : if i [ 1 ] in ( \"string\" , \"int\" , \"bigint\" , \"long\" ): list_of_cols . append ( i [ 0 ]) if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in drop_cols ])) if any ( x not in idf . columns for x in list_of_cols ): raise TypeError ( \"Invalid input for Column(s)\" ) if len ( list_of_cols ) == 0 : warnings . warn ( \"No Unique Count Computation - No discrete column(s) to analyze\" ) schema = T . StructType ( [ T . StructField ( \"attribute\" , T . StringType (), True ), T . StructField ( \"unique_values\" , T . StringType (), True ), ] ) odf = spark . sparkContext . emptyRDD () . toDF ( schema ) return odf uniquevalue_count = idf . agg ( * ( F . countDistinct ( F . col ( i )) . alias ( i ) for i in list_of_cols ) ) odf = spark . createDataFrame ( zip ( list_of_cols , uniquevalue_count . rdd . map ( list ) . collect ()[ 0 ]), schema = ( \"attribute\" , \"unique_values\" ), ) if print_impact : odf . show ( len ( list_of_cols )) return odf","title":"<code>stats_generator</code>"},{"location":"api/data_analyzer/stats_generator.html#stats_generator","text":"This module generates all the descriptive statistics related to the ingested data. Descriptive statistics are split into different metric types, and each function below corresponds to one metric type: global_summary measures_of_counts measures_of_centralTendency measures_of_cardinality measures_of_dispersion measures_of_percentiles measures_of_shape Above primary functions are supported by below functions, which can be used independently as well: missingCount_computation nonzeroCount_computation mode_computation uniqueCount_computation Expand source code # coding=utf-8 \"\"\" This module generates all the descriptive statistics related to the ingested data. Descriptive statistics are split into different metric types, and each function below corresponds to one metric type: - global_summary - measures_of_counts - measures_of_centralTendency - measures_of_cardinality - measures_of_dispersion - measures_of_percentiles - measures_of_shape Above primary functions are supported by below functions, which can be used independently as well: - missingCount_computation - nonzeroCount_computation - mode_computation - uniqueCount_computation \"\"\" import warnings from pyspark.mllib.linalg import Vectors from pyspark.mllib.stat import Statistics from pyspark.sql import functions as F from pyspark.sql import types as T from anovos.shared.utils import transpose_dataframe , attributeType_segregation def global_summary ( spark , idf , list_of_cols = \"all\" , drop_cols = [], print_impact = False ): \"\"\" The global summary function computes the universal statistics/metrics and returns a Spark DataFrame with schema \u2013 metric, value. The metrics computed in this function - No. of rows, No. of columns, No. of categorical columns along with column names, No. of numerical columns along with the column names, No. of non-numerical non-categorical columns such as date type, array type etc. along with column names. Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of columns to analyse e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) print_impact True, False This argument is to print out the statistics.(Default value = False) Returns ------- DataFrame [metric, value] \"\"\" if list_of_cols == \"all\" : list_of_cols = idf . columns if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in drop_cols ])) if any ( x not in idf . columns for x in list_of_cols ) | ( len ( list_of_cols ) == 0 ): raise TypeError ( \"Invalid input for Column(s)\" ) row_count = idf . count () col_count = len ( list_of_cols ) num_cols , cat_cols , other_cols = attributeType_segregation ( idf . select ( list_of_cols )) numcol_count = len ( num_cols ) catcol_count = len ( cat_cols ) othercol_count = len ( other_cols ) if print_impact : print ( \"No. of Rows: %s \" % \" {0:,} \" . format ( row_count )) print ( \"No. of Columns: %s \" % \" {0:,} \" . format ( col_count )) print ( \"Numerical Columns: %s \" % \" {0:,} \" . format ( numcol_count )) if numcol_count > 0 : print ( num_cols ) print ( \"Categorical Columns: %s \" % \" {0:,} \" . format ( catcol_count )) if catcol_count > 0 : print ( cat_cols ) if othercol_count > 0 : print ( \"Other Columns: %s \" % \" {0:,} \" . format ( othercol_count )) print ( other_cols ) odf = spark . createDataFrame ( [ [ \"rows_count\" , str ( row_count )], [ \"columns_count\" , str ( col_count )], [ \"numcols_count\" , str ( numcol_count )], [ \"numcols_name\" , \", \" . join ( num_cols )], [ \"catcols_count\" , str ( catcol_count )], [ \"catcols_name\" , \", \" . join ( cat_cols )], [ \"othercols_count\" , str ( othercol_count )], [ \"othercols_name\" , \", \" . join ( other_cols )], ], schema = [ \"metric\" , \"value\" ], ) return odf def missingCount_computation ( spark , idf , list_of_cols = \"all\" , drop_cols = [], print_impact = False ): \"\"\" Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of columns to analyse e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) print_impact True, False This argument is to print out the statistics.(Default value = False) Returns ------- DataFrame [attribute, missing_count, missing_pct] \"\"\" if list_of_cols == \"all\" : num_cols , cat_cols , other_cols = attributeType_segregation ( idf ) list_of_cols = num_cols + cat_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in drop_cols ])) if any ( x not in idf . columns for x in list_of_cols ) | ( len ( list_of_cols ) == 0 ): raise TypeError ( \"Invalid input for Column(s)\" ) idf_stats = idf . select ( list_of_cols ) . summary ( \"count\" ) odf = ( transpose_dataframe ( idf_stats , \"summary\" ) . withColumn ( \"missing_count\" , F . lit ( idf . count ()) - F . col ( \"count\" ) . cast ( T . LongType ()) ) . withColumn ( \"missing_pct\" , F . round ( F . col ( \"missing_count\" ) / F . lit ( idf . count ()), 4 ) ) . select ( F . col ( \"key\" ) . alias ( \"attribute\" ), \"missing_count\" , \"missing_pct\" ) ) if print_impact : odf . show ( len ( list_of_cols )) return odf def nonzeroCount_computation ( spark , idf , list_of_cols = \"all\" , drop_cols = [], print_impact = False ): \"\"\" Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of numerical columns to analyse e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all numerical columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) print_impact True, False This argument is to print out the statistics.(Default value = False) Returns ------- DataFrame [attribute, nonzero_count, nonzero_pct] \"\"\" num_cols = attributeType_segregation ( idf )[ 0 ] if list_of_cols == \"all\" : list_of_cols = num_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in drop_cols ])) if any ( x not in num_cols for x in list_of_cols ): raise TypeError ( \"Invalid input for Column(s)\" ) if len ( list_of_cols ) == 0 : warnings . warn ( \"No Non-Zero Count Computation - No numerical column(s) to analyze\" ) schema = T . StructType ( [ T . StructField ( \"attribute\" , T . StringType (), True ), T . StructField ( \"nonzero_count\" , T . StringType (), True ), T . StructField ( \"nonzero_pct\" , T . StringType (), True ), ] ) odf = spark . sparkContext . emptyRDD () . toDF ( schema ) return odf tmp = idf . select ( list_of_cols ) . fillna ( 0 ) . rdd . map ( lambda row : Vectors . dense ( row )) nonzero_count = Statistics . colStats ( tmp ) . numNonzeros () odf = spark . createDataFrame ( zip ( list_of_cols , [ int ( i ) for i in nonzero_count ]), schema = ( \"attribute\" , \"nonzero_count\" ), ) . withColumn ( \"nonzero_pct\" , F . round ( F . col ( \"nonzero_count\" ) / F . lit ( idf . count ()), 4 )) if print_impact : odf . show ( len ( list_of_cols )) return odf def measures_of_counts ( spark , idf , list_of_cols = \"all\" , drop_cols = [], print_impact = False ): \"\"\" The Measures of Counts function computes different count metrics for each column. It returns a Spark DataFrame with schema \u2013 attribute, fill_count, fill_pct, missing_count, missing_pct, nonzero_count, nonzero_pct. - Fill Count/Rate is defined as number of rows with non-null values in a column both in terms of absolute count and its proportion to row count. It leverages count statistic from summary functionality of Spark SQL. - Missing Count/Rate is defined as null (or missing) values seen in a column both in terms of absolute count and its proportion to row count. It is directly derivable from Fill Count/Rate. - Non Zero Count/Rate is defined as non-zero values seen in a numerical column both in terms of absolute count and its proportion to row count. For categorical column, it will show null value. Also, it uses a supporting function nonzeroCount_computation. Under the hood, it leverage Multivariate Statistical Summary of Spark MLlib. Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of columns to analyse e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) print_impact True, False This argument is to print out the statistics.(Default value = False) Returns ------- DataFrame [attribute, fill_count, fill_pct, missing_count, missing_pct, nonzero_count, nonzero_pct] \"\"\" if list_of_cols == \"all\" : num_cols , cat_cols , other_cols = attributeType_segregation ( idf ) list_of_cols = num_cols + cat_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in drop_cols ])) num_cols = attributeType_segregation ( idf . select ( list_of_cols ))[ 0 ] if any ( x not in idf . columns for x in list_of_cols ) | ( len ( list_of_cols ) == 0 ): raise TypeError ( \"Invalid input for Column(s)\" ) odf = ( transpose_dataframe ( idf . select ( list_of_cols ) . summary ( \"count\" ), \"summary\" ) . select ( F . col ( \"key\" ) . alias ( \"attribute\" ), F . col ( \"count\" ) . cast ( T . LongType ()) . alias ( \"fill_count\" ), ) . withColumn ( \"fill_pct\" , F . round ( F . col ( \"fill_count\" ) / F . lit ( idf . count ()), 4 )) . withColumn ( \"missing_count\" , F . lit ( idf . count ()) - F . col ( \"fill_count\" ) . cast ( T . LongType ()) ) . withColumn ( \"missing_pct\" , F . round ( 1 - F . col ( \"fill_pct\" ), 4 )) . join ( nonzeroCount_computation ( spark , idf , num_cols ), \"attribute\" , \"full_outer\" ) ) if print_impact : odf . show ( len ( list_of_cols )) return odf def mode_computation ( spark , idf , list_of_cols = \"all\" , drop_cols = [], print_impact = False ): \"\"\" Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of Discrete (Categorical + Integer) columns to analyse e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all discrete columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) print_impact True, False This argument is to print out the statistics.(Default value = False) Returns ------- DataFrame [attribute, mode, mode_rows] In case there is tie between multiple values, one value is randomly picked as mode. \"\"\" if list_of_cols == \"all\" : num_cols , cat_cols , other_cols = attributeType_segregation ( idf ) list_of_cols = num_cols + cat_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in drop_cols ])) for i in idf . select ( list_of_cols ) . dtypes : if i [ 1 ] not in ( \"string\" , \"int\" , \"bigint\" , \"long\" ): list_of_cols . remove ( i [ 0 ]) if any ( x not in idf . columns for x in list_of_cols ): raise TypeError ( \"Invalid input for Column(s)\" ) if len ( list_of_cols ) == 0 : warnings . warn ( \"No Mode Computation - No discrete column(s) to analyze\" ) schema = T . StructType ( [ T . StructField ( \"attribute\" , T . StringType (), True ), T . StructField ( \"mode\" , T . StringType (), True ), T . StructField ( \"mode_rows\" , T . StringType (), True ), ] ) odf = spark . sparkContext . emptyRDD () . toDF ( schema ) return odf mode = [ list ( idf . select ( i ) . dropna () . groupby ( i ) . count () . orderBy ( \"count\" , ascending = False ) . first () or [ None , None ] ) for i in list_of_cols ] mode = [( str ( i ), str ( j )) for i , j in mode ] odf = spark . createDataFrame ( zip ( list_of_cols , mode ), schema = ( \"attribute\" , \"metric\" ) ) . select ( \"attribute\" , ( F . col ( \"metric\" )[ \"_1\" ]) . alias ( \"mode\" ), ( F . col ( \"metric\" )[ \"_2\" ]) . cast ( \"long\" ) . alias ( \"mode_rows\" ), ) if print_impact : odf . show ( len ( list_of_cols )) return odf def measures_of_centralTendency ( spark , idf , list_of_cols = \"all\" , drop_cols = [], print_impact = False ): \"\"\" The Measures of Central Tendency function provides summary statistics that represents the centre point or most likely value of an attribute. It returns a Spark DataFrame with schema \u2013 attribute, mean, median, mode, mode_rows, mode_pct. - Mean is arithmetic average of a column i.e. sum of all values seen in the column divided by the number of rows. It leverage mean statistic from summary functionality of Spark SQL. Mean is calculated only for numerical columns. - Median is 50th percentile or middle value in a column when the values are arranged in ascending or descending order. It leverage \u201850%\u2019 statistic from summary functionality of Spark SQL. Median is calculated only for numerical columns. - Mode is most frequently seen value in a column. Mode is calculated only for discrete columns (categorical + Integer/Long columns). - Mode Rows is the numer of rows seen with Mode value. Mode Rows is calculated only for discrete columns (categorical + Integer/Long columns). - Mode Pct is defined as Mode Rows divided by non-null values seen in a column. Mode Pct is calculated only for discrete columns (categorical + Integer/Long columns). Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of columns to analyse e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) print_impact True, False This argument is to print out the statistics.(Default value = False) Returns ------- DataFrame [attribute, mean, median, mode, mode_rows, mode_pct] \"\"\" num_cols , cat_cols , other_cols = attributeType_segregation ( idf ) if list_of_cols == \"all\" : list_of_cols = num_cols + cat_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in drop_cols ])) if any ( x not in idf . columns for x in list_of_cols ) | ( len ( list_of_cols ) == 0 ): raise TypeError ( \"Invalid input for Column(s)\" ) odf = ( transpose_dataframe ( idf . select ( list_of_cols ) . summary ( \"mean\" , \"50%\" , \"count\" ), \"summary\" ) . withColumn ( \"mean\" , F . when ( F . col ( \"key\" ) . isin ( num_cols ), F . round ( F . col ( \"mean\" ) . cast ( T . DoubleType ()), 4 ), ) . otherwise ( None ), ) . withColumn ( \"median\" , F . when ( F . col ( \"key\" ) . isin ( num_cols ), F . round ( F . col ( \"50%\" ) . cast ( T . DoubleType ()), 4 ), ) . otherwise ( None ), ) . withColumnRenamed ( \"key\" , \"attribute\" ) . join ( mode_computation ( spark , idf , list_of_cols ), \"attribute\" , \"full_outer\" ) . withColumn ( \"mode_pct\" , F . round ( F . col ( \"mode_rows\" ) / F . col ( \"count\" ) . cast ( T . DoubleType ()), 4 ), ) . select ( \"attribute\" , \"mean\" , \"median\" , \"mode\" , \"mode_rows\" , \"mode_pct\" ) ) if print_impact : odf . show ( len ( list_of_cols )) return odf def uniqueCount_computation ( spark , idf , list_of_cols = \"all\" , drop_cols = [], print_impact = False ): \"\"\" Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of Discrete (Categorical + Integer) columns to analyse e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all discrete columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) print_impact True, False This argument is to print out the statistics.(Default value = False) Returns ------- DataFrame [attribute, unique_values] \"\"\" if list_of_cols == \"all\" : list_of_cols = [] for i in idf . dtypes : if i [ 1 ] in ( \"string\" , \"int\" , \"bigint\" , \"long\" ): list_of_cols . append ( i [ 0 ]) if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in drop_cols ])) if any ( x not in idf . columns for x in list_of_cols ): raise TypeError ( \"Invalid input for Column(s)\" ) if len ( list_of_cols ) == 0 : warnings . warn ( \"No Unique Count Computation - No discrete column(s) to analyze\" ) schema = T . StructType ( [ T . StructField ( \"attribute\" , T . StringType (), True ), T . StructField ( \"unique_values\" , T . StringType (), True ), ] ) odf = spark . sparkContext . emptyRDD () . toDF ( schema ) return odf uniquevalue_count = idf . agg ( * ( F . countDistinct ( F . col ( i )) . alias ( i ) for i in list_of_cols ) ) odf = spark . createDataFrame ( zip ( list_of_cols , uniquevalue_count . rdd . map ( list ) . collect ()[ 0 ]), schema = ( \"attribute\" , \"unique_values\" ), ) if print_impact : odf . show ( len ( list_of_cols )) return odf def measures_of_cardinality ( spark , idf , list_of_cols = \"all\" , drop_cols = [], print_impact = False ): \"\"\" The Measures of Cardinality function provides statistics that are related to unique values seen in an attribute. These statistics are calculated only for discrete columns (categorical + Integer/Long columns). It returns a Spark Dataframe with schema \u2013 attribute, unique_values, IDness. - Unique Value is defined as a distinct value count of a column. It relies on a supporting function uniqueCount_computation for its computation and leverages the countDistinct functionality of Spark SQL. - IDness is calculated as Unique Values divided by non-null values seen in a column. Non-null values count is used instead of total count because too many null values can give misleading results even if the column have all unique values (except null). It uses supporting functions - uniqueCount_computation and missingCount_computation. Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of Discrete (Categorical + Integer) columns to analyse e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all discrete columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) print_impact True, False This argument is to print out the statistics.(Default value = False) Returns ------- DataFrame [attribute, unique_values, IDness] \"\"\" if list_of_cols == \"all\" : list_of_cols = [] for i in idf . dtypes : if i [ 1 ] in ( \"string\" , \"int\" , \"bigint\" , \"long\" ): list_of_cols . append ( i [ 0 ]) if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in drop_cols ])) if any ( x not in idf . columns for x in list_of_cols ): raise TypeError ( \"Invalid input for Column(s)\" ) if len ( list_of_cols ) == 0 : warnings . warn ( \"No Cardinality Computation - No discrete column(s) to analyze\" ) schema = T . StructType ( [ T . StructField ( \"attribute\" , T . StringType (), True ), T . StructField ( \"unique_values\" , T . StringType (), True ), T . StructField ( \"IDness\" , T . StringType (), True ), ] ) odf = spark . sparkContext . emptyRDD () . toDF ( schema ) return odf odf = ( uniqueCount_computation ( spark , idf , list_of_cols ) . join ( missingCount_computation ( spark , idf , list_of_cols ), \"attribute\" , \"full_outer\" , ) . withColumn ( \"IDness\" , F . round ( F . col ( \"unique_values\" ) / ( F . lit ( idf . count ()) - F . col ( \"missing_count\" )), 4 , ), ) . select ( \"attribute\" , \"unique_values\" , \"IDness\" ) ) if print_impact : odf . show ( len ( list_of_cols )) return odf def measures_of_dispersion ( spark , idf , list_of_cols = \"all\" , drop_cols = [], print_impact = False ): \"\"\" The Measures of Dispersion function provides statistics that describe the spread of a numerical attribute. Alternatively, these statistics are also known as measures of spread. It returns a Spark DataFrame with schema \u2013 attribute, stddev, variance, cov, IQR, range. - Standard Deviation (stddev) measures how concentrated an attribute is around the mean or average. It leverages \u2018stddev\u2019 statistic from summary functionality of Spark SQL. - Variance is the squared value of Standard Deviation. - Coefficient of Variance (cov) is computed as ratio of Standard Deviation & Mean. It leverages \u2018stddev\u2019 and \u2018mean\u2019 statistic from the summary functionality of Spark SQL. - Interquartile Range (IQR): It describes the difference between the third quartile (75th percentile) and the first quartile (25th percentile), telling us about the range where middle half values are seen. It leverage \u201825%\u2019 and \u201875%\u2019 statistics from the summary functionality of Spark SQL. - Range is simply the difference between the maximum value and the minimum value. It leverage \u2018min\u2019 and \u2018max\u2019 statistics from the summary functionality of Spark Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of numerical columns to analyse e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all numerical columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) print_impact True, False This argument is to print out the statistics.(Default value = False) Returns ------- DataFrame [attribute, stddev, variance, cov, IQR, range] \"\"\" num_cols = attributeType_segregation ( idf )[ 0 ] if list_of_cols == \"all\" : list_of_cols = num_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in drop_cols ])) if any ( x not in num_cols for x in list_of_cols ): raise TypeError ( \"Invalid input for Column(s)\" ) if len ( list_of_cols ) == 0 : warnings . warn ( \"No Dispersion Computation - No numerical column(s) to analyze\" ) schema = T . StructType ( [ T . StructField ( \"attribute\" , T . StringType (), True ), T . StructField ( \"stddev\" , T . StringType (), True ), T . StructField ( \"variance\" , T . StringType (), True ), T . StructField ( \"cov\" , T . StringType (), True ), T . StructField ( \"IQR\" , T . StringType (), True ), T . StructField ( \"range\" , T . StringType (), True ), ] ) odf = spark . sparkContext . emptyRDD () . toDF ( schema ) return odf odf = ( transpose_dataframe ( idf . select ( list_of_cols ) . summary ( \"stddev\" , \"min\" , \"max\" , \"mean\" , \"25%\" , \"75%\" ), \"summary\" , ) . withColumn ( \"stddev\" , F . round ( F . col ( \"stddev\" ) . cast ( T . DoubleType ()), 4 )) . withColumn ( \"variance\" , F . round ( F . col ( \"stddev\" ) * F . col ( \"stddev\" ), 4 )) . withColumn ( \"range\" , F . round ( F . col ( \"max\" ) - F . col ( \"min\" ), 4 )) . withColumn ( \"cov\" , F . round ( F . col ( \"stddev\" ) / F . col ( \"mean\" ), 4 )) . withColumn ( \"IQR\" , F . round ( F . col ( \"75%\" ) - F . col ( \"25%\" ), 4 )) . select ( F . col ( \"key\" ) . alias ( \"attribute\" ), \"stddev\" , \"variance\" , \"cov\" , \"IQR\" , \"range\" ) ) if print_impact : odf . show ( len ( list_of_cols )) return odf def measures_of_percentiles ( spark , idf , list_of_cols = \"all\" , drop_cols = [], print_impact = False ): \"\"\" The Measures of Percentiles function provides statistics at different percentiles. Nth percentile can be interpreted as N% of rows having values lesser than or equal to Nth percentile value. It is prominently used for quick detection of skewness or outlier. Alternatively, these statistics are also known as measures of position. These statistics are computed only for numerical attributes. It returns a Spark Dataframe with schema \u2013 attribute, min, 1%, 5%, 10%, 25%, 50%, 75%, 90%, 95%, 99%, max. It leverage \u2018N%\u2019 statistics from summary functionality of Spark SQL where N is 0 for min and 100 for max. Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of numerical columns to analyse e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all numerical columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) print_impact True, False This argument is to print out the statistics.(Default value = False) Returns ------- DataFrame [attribute, min, 1%, 5%, 10%, 25%, 50%, 75%, 90%, 95%, 99%, max] \"\"\" num_cols = attributeType_segregation ( idf )[ 0 ] if list_of_cols == \"all\" : list_of_cols = num_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in drop_cols ])) if any ( x not in num_cols for x in list_of_cols ): raise TypeError ( \"Invalid input for Column(s)\" ) if len ( list_of_cols ) == 0 : warnings . warn ( \"No Percentiles Computation - No numerical column(s) to analyze\" ) schema = T . StructType ( [ T . StructField ( \"attribute\" , T . StringType (), True ), T . StructField ( \"min\" , T . StringType (), True ), T . StructField ( \"1%\" , T . StringType (), True ), T . StructField ( \"5%\" , T . StringType (), True ), T . StructField ( \"10%\" , T . StringType (), True ), T . StructField ( \"25%\" , T . StringType (), True ), T . StructField ( \"50%\" , T . StringType (), True ), T . StructField ( \"75%\" , T . StringType (), True ), T . StructField ( \"90%\" , T . StringType (), True ), T . StructField ( \"95%\" , T . StringType (), True ), T . StructField ( \"99%\" , T . StringType (), True ), T . StructField ( \"max\" , T . StringType (), True ), ] ) odf = spark . sparkContext . emptyRDD () . toDF ( schema ) return odf stats = [ \"min\" , \"1%\" , \"5%\" , \"10%\" , \"25%\" , \"50%\" , \"75%\" , \"90%\" , \"95%\" , \"99%\" , \"max\" ] odf = transpose_dataframe ( idf . select ( list_of_cols ) . summary ( * stats ), \"summary\" ) . withColumnRenamed ( \"key\" , \"attribute\" ) for i in odf . columns : if i != \"attribute\" : odf = odf . withColumn ( i , F . round ( F . col ( i ) . cast ( \"Double\" ), 4 )) odf = odf . select ([ \"attribute\" ] + stats ) if print_impact : odf . show ( len ( list_of_cols )) return odf def measures_of_shape ( spark , idf , list_of_cols = \"all\" , drop_cols = [], print_impact = False ): \"\"\" The Measures of Shapes function provides statistics related to the shape of an attribute's distribution. Alternatively, these statistics are also known as measures of the moment and are computed only for numerical attributes. It returns a Spark Dataframe with schema \u2013 attribute, skewness, kurtosis. - Skewness describes how much-skewed values are relative to a perfect bell curve observed in normal distribution and the direction of skew. If the majority of the values are at the left and the right tail is longer, we say that the distribution is skewed right or positively skewed; if the peak is toward the right and the left tail is longer, we say that the distribution is skewed left or negatively skewed. It leverage skewness functionality of Spark SQL. - (Excess) Kurtosis describes how tall and sharp the central peak is relative to a perfect bell curve observed in the normal distribution. The reference standard is a normal distribution, which has a kurtosis of 3. In token of this, often, the excess kurtosis is presented: excess kurtosis is simply kurtosis\u22123. Higher (positive) values indicate a higher, sharper peak; lower (negative) values indicate a less distinct peak. It leverages kurtosis functionality of Spark SQL. Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of numerical columns to analyse e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all numerical columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) print_impact True, False This argument is to print out the statistics.(Default value = False) Returns ------- DataFrame [attribute, skewness, kurtosis] \"\"\" num_cols = attributeType_segregation ( idf )[ 0 ] if list_of_cols == \"all\" : list_of_cols = num_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in drop_cols ])) if any ( x not in num_cols for x in list_of_cols ): raise TypeError ( \"Invalid input for Column(s)\" ) if len ( list_of_cols ) == 0 : warnings . warn ( \"No Skewness/Kurtosis Computation - No numerical column(s) to analyze\" ) schema = T . StructType ( [ T . StructField ( \"attribute\" , T . StringType (), True ), T . StructField ( \"skewness\" , T . StringType (), True ), T . StructField ( \"kurtosis\" , T . StringType (), True ), ] ) odf = spark . sparkContext . emptyRDD () . toDF ( schema ) return odf shapes = [] for i in list_of_cols : s , k = idf . select ( F . skewness ( i ), F . kurtosis ( i )) . first () shapes . append ([ i , s , k ]) odf = ( spark . createDataFrame ( shapes , schema = ( \"attribute\" , \"skewness\" , \"kurtosis\" )) . withColumn ( \"skewness\" , F . round ( F . col ( \"skewness\" ), 4 )) . withColumn ( \"kurtosis\" , F . round ( F . col ( \"kurtosis\" ), 4 )) ) if print_impact : odf . show ( len ( list_of_cols )) return odf","title":"stats_generator"},{"location":"api/data_analyzer/stats_generator.html#functions","text":"def global_summary ( spark, idf, list_of_cols='all', drop_cols=[], print_impact=False) The global summary function computes the universal statistics/metrics and returns a Spark DataFrame with schema \u2013 metric, value. The metrics computed in this function - No. of rows, No. of columns, No. of categorical columns along with column names, No. of numerical columns along with the column names, No. of non-numerical non-categorical columns such as date type, array type etc. along with column names.","title":"Functions"},{"location":"api/data_analyzer/ts_analyzer.html","text":"ts_analyzer This module generates the intermediate output specific to the inspection of Time series analysis. As a part of generation of final output, there are various functions created such as - ts_processed_feats ts_eligiblity_check ts_viz_data ts_analyzer daypart_cat Respective functions have sections containing the detailed definition of the parameters used for computing. Expand source code # coding=utf-8 \"\"\"This module generates the intermediate output specific to the inspection of Time series analysis. As a part of generation of final output, there are various functions created such as - - ts_processed_feats - ts_eligiblity_check - ts_viz_data - ts_analyzer - daypart_cat Respective functions have sections containing the detailed definition of the parameters used for computing. \"\"\" import pyspark import datetime from pyspark.sql import functions as F from pyspark.sql import types as T from pyspark.sql import Window from loguru import logger import calendar from anovos.shared.utils import attributeType_segregation , ends_with , output_to_local from anovos.data_analyzer.stats_generator import measures_of_percentiles from anovos.data_ingest.ts_auto_detection import ts_preprocess from anovos.data_transformer.datetime import ( timeUnits_extraction , unix_to_timestamp , lagged_ts , ) import csv import io import os import re import warnings import subprocess from pathlib import Path import dateutil.parser from statsmodels.tsa.seasonal import seasonal_decompose import pandas as pd import numpy as np def daypart_cat ( column ): \"\"\" This functioin helps to convert the input hour part into the respective day parts. The different dayparts are Early Hours, Work Hours, Late Hours, Commuting Hours, Other Hours based on the hour value. Parameters ---------- column Reads the column containing the hour part and converts into respective day part Returns ------- String \"\"\" # calculate hour buckets after adding local timezone if column is None : return \"Missing_NA\" elif ( column >= 4 ) and ( column < 7 ): return \"early_hours\" elif ( column >= 10 ) and ( column < 17 ): return \"work_hours\" elif ( column >= 23 ) or ( column < 4 ): return \"late_hours\" elif (( column >= 7 ) and ( column < 10 )) or (( column >= 17 ) and ( column < 20 )): return \"commuting_hours\" else : return \"other_hours\" f_daypart_cat = F . udf ( daypart_cat , T . StringType ()) def ts_processed_feats ( idf , col , id_col , tz , cnt_row , cnt_unique_id ): \"\"\" This function helps to extract time units from the input dataframe on a processed column being timestamp / date. Parameters ---------- idf Input dataframe col Column belonging to timestamp / date id_col ID column tz Timezone offset cnt_row Count of rows present in the Input dataframe cnt_unique_id Count of unique records present in the Input dataframe Returns ------- DataFrame \"\"\" if cnt_row == cnt_unique_id : odf = ( timeUnits_extraction ( idf , col , \"all\" , output_mode = \"append\" , ) . withColumn ( \"yyyymmdd_col\" , F . to_date ( col )) . orderBy ( \"yyyymmdd_col\" ) . withColumn ( \"daypart_cat\" , f_daypart_cat ( F . col ( col + \"_hour\" ))) . withColumn ( \"week_cat\" , F . when ( F . col ( col + \"_dayofweek\" ) > 5 , F . lit ( \"weekend\" )) . otherwise ( \"weekday\" ), ) . withColumnRenamed ( col + \"_dayofweek\" , \"dow\" ) ) return odf else : odf = ( timeUnits_extraction ( idf , col , \"all\" , output_mode = \"append\" , ) . withColumn ( \"yyyymmdd_col\" , F . to_date ( col )) . orderBy ( id_col , \"yyyymmdd_col\" ) . withColumn ( \"daypart_cat\" , f_daypart_cat ( F . col ( col + \"_hour\" ))) . withColumn ( \"week_cat\" , F . when ( F . col ( col + \"_dayofweek\" ) > 5 , F . lit ( \"weekend\" )) . otherwise ( \"weekday\" ), ) . withColumnRenamed ( col + \"_dayofweek\" , \"dow\" ) ) return odf def ts_eligiblity_check ( spark , idf , id_col , opt = 1 , tz_offset = \"local\" ): \"\"\" This function helps to extract various metrics which can help to understand the nature of timestamp / date column for a given dataset. Parameters ---------- spark Spark session idf Input dataframe id_col ID Column opt Option to choose between [1,2]. 1 is kept as default. Based on the user input, the specific aggregation of data will happen. tz_offset Timezone offset (Option to chose between options like Local, GMT, UTC, etc.). Default option is set as \"Local\". Returns ------- DataFrame \"\"\" lagged_df = lagged_ts ( idf . select ( \"yyyymmdd_col\" ) . distinct () . orderBy ( \"yyyymmdd_col\" ), \"yyyymmdd_col\" , lag = 1 , tsdiff_unit = \"days\" , output_mode = \"append\" , ) . orderBy ( \"yyyymmdd_col\" ) diff_lagged_df = list ( np . around ( lagged_df . withColumn ( \"daydiff\" , F . datediff ( \"yyyymmdd_col\" , \"yyyymmdd_col_lag1\" ) ) . where ( F . col ( \"daydiff\" ) . isNotNull ()) . groupBy () . agg ( F . mean ( \"daydiff\" ) . alias ( \"mean\" ), F . variance ( \"daydiff\" ) . alias ( \"variance\" ), F . stddev ( \"daydiff\" ) . alias ( \"stdev\" ), ) . withColumn ( \"coef_of_var_lag\" , F . col ( \"stdev\" ) / F . col ( \"mean\" )) . rdd . flatMap ( lambda x : x ) . collect (), 3 , ) ) p1 = measures_of_percentiles ( spark , idf . groupBy ( id_col ) . agg ( F . countDistinct ( \"yyyymmdd_col\" ) . alias ( \"id_date_pair\" )), list_of_cols = \"id_date_pair\" , ) p2 = measures_of_percentiles ( spark , idf . groupBy ( \"yyyymmdd_col\" ) . agg ( F . countDistinct ( id_col ) . alias ( \"date_id_pair\" )), list_of_cols = \"date_id_pair\" , ) if opt == 1 : odf = p1 . union ( p2 ) . toPandas () return odf else : odf = idf m = ( odf . groupBy ( \"yyyymmdd_col\" ) . count () . orderBy ( \"count\" , ascending = False ) . collect () ) mode = str ( m [ 0 ][ 0 ]) + \" [\" + str ( m [ 0 ][ 1 ]) + \"]\" missing_vals = odf . where ( F . col ( \"yyyymmdd_col\" ) . isNull ()) . count () odf = ( odf . groupBy () . agg ( F . countDistinct ( \"yyyymmdd_col\" ) . alias ( \"count_unique_dates\" ), F . min ( \"yyyymmdd_col\" ) . alias ( \"min_date\" ), F . max ( \"yyyymmdd_col\" ) . alias ( \"max_date\" ), ) . withColumn ( \"modal_date\" , F . lit ( mode )) . withColumn ( \"date_diff\" , F . datediff ( \"max_date\" , \"min_date\" )) . withColumn ( \"missing_date\" , F . lit ( missing_vals )) . withColumn ( \"mean\" , F . lit ( diff_lagged_df [ 0 ])) . withColumn ( \"variance\" , F . lit ( diff_lagged_df [ 1 ])) . withColumn ( \"stdev\" , F . lit ( diff_lagged_df [ 2 ])) . withColumn ( \"cov\" , F . lit ( diff_lagged_df [ 3 ])) . toPandas () ) return odf def ts_viz_data ( idf , x_col , y_col , id_col , tz_offset = \"local\" , output_mode = \"append\" , output_type = \"daily\" , n_cat = 10 , ): \"\"\" This function helps to produce the processed dataframe with the relevant aggregation at the time frequency chosen for a given column as seen against the timestamp / date column. Parameters ---------- idf Input Dataframe x_col Timestamp / Date column as set in the X-Axis y_col Numerical & Categorical column as set in the Y-Axis id_col ID Column tz_offset Timezone offset (Option to chose between options like Local, GMT, UTC, etc.). Default option is set as \"Local\". output_mode Option to choose between Append or Replace. If the option Append is selected, the column names are Appended by \"_ts\" else it's replaced by the original column name output_type Option to choose between \"Daily\" or \"Weekly\" or \"Hourly\". Daily is chosen as default. If \"Daily\" is selected as the output type, the daily view is populated ; If it's \"Hourly\", the view is shown at a Day part level. However, if it's \"Weekly\", then the display it per individual week days (1-7) as captured. n_cat For categorical columns whose cardinality is beyond N, the Top N categories are chosen, beyond which the categories are grouped as Others. Returns ------- DataFrame \"\"\" y_col_org = y_col y_col = y_col . replace ( \"-\" , \"_\" ) idf = idf . withColumnRenamed ( y_col_org , y_col ) for i in idf . dtypes : if y_col == i [ 0 ]: y_col_dtype = i [ 1 ] if y_col_dtype == \"string\" : top_cat = list ( idf . groupBy ( y_col ) . count () . orderBy ( \"count\" , ascending = False ) . limit ( int ( n_cat )) . select ( y_col ) . toPandas ()[ y_col ] . values ) idf = idf . withColumn ( y_col , F . when ( F . col ( y_col ) . isin ( top_cat ), F . col ( y_col )) . otherwise ( F . lit ( \"Others\" )), ) if output_type == \"daily\" : odf = ( idf . groupBy ( y_col , \"yyyymmdd_col\" ) . agg ( F . count ( y_col ) . alias ( \"count\" )) . orderBy ( \"yyyymmdd_col\" ) . withColumnRenamed ( \"yyyymmdd_col\" , x_col ) . withColumnRenamed ( y_col , y_col_org ) . toPandas () ) elif output_type == \"hourly\" : odf = ( idf . groupBy ( y_col , \"daypart_cat\" ) . agg ( F . count ( y_col ) . alias ( \"count\" )) . orderBy ( \"daypart_cat\" ) . withColumnRenamed ( y_col , y_col_org ) . toPandas () ) elif output_type == \"weekly\" : odf = ( idf . groupBy ( y_col , \"dow\" ) . agg ( F . count ( y_col ) . alias ( \"count\" )) . orderBy ( \"dow\" ) . withColumnRenamed ( y_col , y_col_org ) . toPandas () ) return odf else : if output_type == \"daily\" : odf = ( idf . groupBy ( \"yyyymmdd_col\" ) . agg ( F . min ( y_col ) . alias ( \"min\" ), F . max ( y_col ) . alias ( \"max\" ), F . mean ( y_col ) . alias ( \"mean\" ), F . expr ( \"percentile(\" + y_col + \", array(0.5))\" )[ 0 ] . alias ( \"median\" ), ) . orderBy ( \"yyyymmdd_col\" ) . withColumnRenamed ( \"yyyymmdd_col\" , x_col ) . withColumnRenamed ( y_col , y_col_org ) . toPandas () ) elif output_type == \"hourly\" : odf = ( idf . groupBy ( \"daypart_cat\" ) . agg ( F . min ( y_col ) . alias ( \"min\" ), F . max ( y_col ) . alias ( \"max\" ), F . mean ( y_col ) . alias ( \"mean\" ), F . expr ( \"percentile(\" + y_col + \", array(0.5))\" )[ 0 ] . alias ( \"median\" ), ) . orderBy ( \"daypart_cat\" ) . withColumnRenamed ( y_col , y_col_org ) . toPandas () ) elif output_type == \"weekly\" : odf = ( idf . groupBy ( \"dow\" ) . agg ( F . min ( y_col ) . alias ( \"min\" ), F . max ( y_col ) . alias ( \"max\" ), F . mean ( y_col ) . alias ( \"mean\" ), F . expr ( \"percentile(\" + y_col + \", array(0.5))\" )[ 0 ] . alias ( \"median\" ), ) . orderBy ( \"dow\" ) . withColumnRenamed ( y_col , y_col_org ) . toPandas () ) return odf def ts_analyzer ( spark , idf , id_col , max_days , output_path , output_type = \"daily\" , tz_offset = \"local\" , run_type = \"local\" , ): \"\"\" This function helps to produce the processed output in an aggregate form considering the input dataframe with processed timestamp / date column. The aggregation happens across Mean, Median, Min & Max for the Numerical / Categorical column. Parameters ---------- spark Spark session idf Input Dataframe id_col ID Column max_days Max days upto which the data will be aggregated. If we've a dataset containing a timestamp / date field with very high number of unique dates (Let's say beyond 20 years worth of daily data), a maximum days value chosen basis which the latest output is displayed. output_path Output path where the intermediate data is going to be saved output_type Option to choose between \"Daily\" or \"Weekly\" or \"Hourly\". Daily is chosen as default. If \"Daily\" is selected as the output type, the daily view is populated ; If it's \"Hourly\", the view is shown at a Day part level. However, if it's \"Weekly\", then the display it per individual week days (1-7) as captured. tz_offset Timezone offset (Option to chose between options like Local, GMT, UTC, etc.). Default option is set as \"Local\". run_type Option to choose between run type \"Local\" or \"EMR\" or \"Azure\" basis the user flexibility. Default option is set as \"Local\". Returns ------- Output[CSV] \"\"\" if run_type == \"local\" : local_path = output_path elif run_type == \"databricks\" : local_path = output_to_local ( output_path ) elif run_type == \"emr\" : local_path = \"report_stats\" else : raise ValueError ( \"Invalid run_type\" ) Path ( local_path ) . mkdir ( parents = True , exist_ok = True ) num_cols , cat_cols , other_cols = attributeType_segregation ( idf ) num_cols = [ x for x in num_cols if x not in [ id_col ]] cat_cols = [ x for x in cat_cols if x not in [ id_col ]] ts_loop_cols_post = [ x [ 0 ] for x in idf . dtypes if x [ 1 ] in [ \"timestamp\" , \"date\" ]] cnt_row = idf . count () cnt_unique_id = idf . select ( id_col ) . distinct () . count () for i in ts_loop_cols_post : ts_processed_feat_df = ts_processed_feats ( idf , i , id_col , tz_offset , cnt_row , cnt_unique_id ) ts_processed_feat_df . persist ( pyspark . StorageLevel . MEMORY_AND_DISK ) # for j in range(1, 3): # f = ts_eligiblity_check( # spark, # ts_processed_feat_df, # id_col=id_col, # opt=j, # tz_offset=tz_offset, # ) # f.to_csv( # ends_with(local_path) + \"stats_\" + str(i) + \"_\" + str(j) + \".csv\", # index=False, # ) f1 = ts_eligiblity_check ( spark , ts_processed_feat_df , id_col = id_col , opt = 1 , tz_offset = tz_offset ) f1 . to_csv ( ends_with ( local_path ) + \"stats_\" + str ( i ) + \"_\" + str ( 1 ) + \".csv\" , index = False , ) f2 = ts_eligiblity_check ( spark , ts_processed_feat_df , id_col = id_col , opt = 2 , tz_offset = tz_offset ) f2 . to_csv ( ends_with ( local_path ) + \"stats_\" + str ( i ) + \"_\" + str ( 2 ) + \".csv\" , index = False , ) for k in [ num_cols , cat_cols ]: for l in k : for m in [ output_type ]: f = ( ts_viz_data ( ts_processed_feat_df , i , l , id_col = id_col , tz_offset = tz_offset , output_mode = \"append\" , output_type = m , n_cat = 10 , ) . tail ( int ( max_days )) . dropna () ) f . to_csv ( ends_with ( local_path ) + i + \"_\" + l + \"_\" + m + \".csv\" , index = False , ) ts_processed_feat_df . unpersist () if run_type == \"emr\" : bash_cmd = ( \"aws s3 cp --recursive \" + ends_with ( local_path ) + \" \" + ends_with ( output_path ) ) output = subprocess . check_output ([ \"bash\" , \"-c\" , bash_cmd ]) Functions def daypart_cat ( column) This functioin helps to convert the input hour part into the respective day parts. The different dayparts are Early Hours, Work Hours, Late Hours, Commuting Hours, Other Hours based on the hour value. Parameters column Reads the column containing the hour part and converts into respective day part Returns String Expand source code def daypart_cat ( column ): \"\"\" This functioin helps to convert the input hour part into the respective day parts. The different dayparts are Early Hours, Work Hours, Late Hours, Commuting Hours, Other Hours based on the hour value. Parameters ---------- column Reads the column containing the hour part and converts into respective day part Returns ------- String \"\"\" # calculate hour buckets after adding local timezone if column is None : return \"Missing_NA\" elif ( column >= 4 ) and ( column < 7 ): return \"early_hours\" elif ( column >= 10 ) and ( column < 17 ): return \"work_hours\" elif ( column >= 23 ) or ( column < 4 ): return \"late_hours\" elif (( column >= 7 ) and ( column < 10 )) or (( column >= 17 ) and ( column < 20 )): return \"commuting_hours\" else : return \"other_hours\" def f_daypart_cat ( column) This functioin helps to convert the input hour part into the respective day parts. The different dayparts are Early Hours, Work Hours, Late Hours, Commuting Hours, Other Hours based on the hour value. Parameters column Reads the column containing the hour part and converts into respective day part Returns String Expand source code def daypart_cat ( column ): \"\"\" This functioin helps to convert the input hour part into the respective day parts. The different dayparts are Early Hours, Work Hours, Late Hours, Commuting Hours, Other Hours based on the hour value. Parameters ---------- column Reads the column containing the hour part and converts into respective day part Returns ------- String \"\"\" # calculate hour buckets after adding local timezone if column is None : return \"Missing_NA\" elif ( column >= 4 ) and ( column < 7 ): return \"early_hours\" elif ( column >= 10 ) and ( column < 17 ): return \"work_hours\" elif ( column >= 23 ) or ( column < 4 ): return \"late_hours\" elif (( column >= 7 ) and ( column < 10 )) or (( column >= 17 ) and ( column < 20 )): return \"commuting_hours\" else : return \"other_hours\" def ts_analyzer ( spark, idf, id_col, max_days, output_path, output_type='daily', tz_offset='local', run_type='local') This function helps to produce the processed output in an aggregate form considering the input dataframe with processed timestamp / date column. The aggregation happens across Mean, Median, Min & Max for the Numerical / Categorical column. Parameters spark Spark session idf Input Dataframe id_col ID Column max_days Max days upto which the data will be aggregated. If we've a dataset containing a timestamp / date field with very high number of unique dates (Let's say beyond 20 years worth of daily data), a maximum days value chosen basis which the latest output is displayed. output_path Output path where the intermediate data is going to be saved output_type Option to choose between \"Daily\" or \"Weekly\" or \"Hourly\". Daily is chosen as default. If \"Daily\" is selected as the output type, the daily view is populated ; If it's \"Hourly\", the view is shown at a Day part level. However, if it's \"Weekly\", then the display it per individual week days (1-7) as captured. tz_offset Timezone offset (Option to chose between options like Local, GMT, UTC, etc.). Default option is set as \"Local\". run_type Option to choose between run type \"Local\" or \"EMR\" or \"Azure\" basis the user flexibility. Default option is set as \"Local\". Returns Output[CSV] Expand source code def ts_analyzer ( spark , idf , id_col , max_days , output_path , output_type = \"daily\" , tz_offset = \"local\" , run_type = \"local\" , ): \"\"\" This function helps to produce the processed output in an aggregate form considering the input dataframe with processed timestamp / date column. The aggregation happens across Mean, Median, Min & Max for the Numerical / Categorical column. Parameters ---------- spark Spark session idf Input Dataframe id_col ID Column max_days Max days upto which the data will be aggregated. If we've a dataset containing a timestamp / date field with very high number of unique dates (Let's say beyond 20 years worth of daily data), a maximum days value chosen basis which the latest output is displayed. output_path Output path where the intermediate data is going to be saved output_type Option to choose between \"Daily\" or \"Weekly\" or \"Hourly\". Daily is chosen as default. If \"Daily\" is selected as the output type, the daily view is populated ; If it's \"Hourly\", the view is shown at a Day part level. However, if it's \"Weekly\", then the display it per individual week days (1-7) as captured. tz_offset Timezone offset (Option to chose between options like Local, GMT, UTC, etc.). Default option is set as \"Local\". run_type Option to choose between run type \"Local\" or \"EMR\" or \"Azure\" basis the user flexibility. Default option is set as \"Local\". Returns ------- Output[CSV] \"\"\" if run_type == \"local\" : local_path = output_path elif run_type == \"databricks\" : local_path = output_to_local ( output_path ) elif run_type == \"emr\" : local_path = \"report_stats\" else : raise ValueError ( \"Invalid run_type\" ) Path ( local_path ) . mkdir ( parents = True , exist_ok = True ) num_cols , cat_cols , other_cols = attributeType_segregation ( idf ) num_cols = [ x for x in num_cols if x not in [ id_col ]] cat_cols = [ x for x in cat_cols if x not in [ id_col ]] ts_loop_cols_post = [ x [ 0 ] for x in idf . dtypes if x [ 1 ] in [ \"timestamp\" , \"date\" ]] cnt_row = idf . count () cnt_unique_id = idf . select ( id_col ) . distinct () . count () for i in ts_loop_cols_post : ts_processed_feat_df = ts_processed_feats ( idf , i , id_col , tz_offset , cnt_row , cnt_unique_id ) ts_processed_feat_df . persist ( pyspark . StorageLevel . MEMORY_AND_DISK ) # for j in range(1, 3): # f = ts_eligiblity_check( # spark, # ts_processed_feat_df, # id_col=id_col, # opt=j, # tz_offset=tz_offset, # ) # f.to_csv( # ends_with(local_path) + \"stats_\" + str(i) + \"_\" + str(j) + \".csv\", # index=False, # ) f1 = ts_eligiblity_check ( spark , ts_processed_feat_df , id_col = id_col , opt = 1 , tz_offset = tz_offset ) f1 . to_csv ( ends_with ( local_path ) + \"stats_\" + str ( i ) + \"_\" + str ( 1 ) + \".csv\" , index = False , ) f2 = ts_eligiblity_check ( spark , ts_processed_feat_df , id_col = id_col , opt = 2 , tz_offset = tz_offset ) f2 . to_csv ( ends_with ( local_path ) + \"stats_\" + str ( i ) + \"_\" + str ( 2 ) + \".csv\" , index = False , ) for k in [ num_cols , cat_cols ]: for l in k : for m in [ output_type ]: f = ( ts_viz_data ( ts_processed_feat_df , i , l , id_col = id_col , tz_offset = tz_offset , output_mode = \"append\" , output_type = m , n_cat = 10 , ) . tail ( int ( max_days )) . dropna () ) f . to_csv ( ends_with ( local_path ) + i + \"_\" + l + \"_\" + m + \".csv\" , index = False , ) ts_processed_feat_df . unpersist () if run_type == \"emr\" : bash_cmd = ( \"aws s3 cp --recursive \" + ends_with ( local_path ) + \" \" + ends_with ( output_path ) ) output = subprocess . check_output ([ \"bash\" , \"-c\" , bash_cmd ]) def ts_eligiblity_check ( spark, idf, id_col, opt=1, tz_offset='local') This function helps to extract various metrics which can help to understand the nature of timestamp / date column for a given dataset. Parameters spark Spark session idf Input dataframe id_col ID Column opt Option to choose between [1,2]. 1 is kept as default. Based on the user input, the specific aggregation of data will happen. tz_offset Timezone offset (Option to chose between options like Local, GMT, UTC, etc.). Default option is set as \"Local\". Returns DataFrame Expand source code def ts_eligiblity_check ( spark , idf , id_col , opt = 1 , tz_offset = \"local\" ): \"\"\" This function helps to extract various metrics which can help to understand the nature of timestamp / date column for a given dataset. Parameters ---------- spark Spark session idf Input dataframe id_col ID Column opt Option to choose between [1,2]. 1 is kept as default. Based on the user input, the specific aggregation of data will happen. tz_offset Timezone offset (Option to chose between options like Local, GMT, UTC, etc.). Default option is set as \"Local\". Returns ------- DataFrame \"\"\" lagged_df = lagged_ts ( idf . select ( \"yyyymmdd_col\" ) . distinct () . orderBy ( \"yyyymmdd_col\" ), \"yyyymmdd_col\" , lag = 1 , tsdiff_unit = \"days\" , output_mode = \"append\" , ) . orderBy ( \"yyyymmdd_col\" ) diff_lagged_df = list ( np . around ( lagged_df . withColumn ( \"daydiff\" , F . datediff ( \"yyyymmdd_col\" , \"yyyymmdd_col_lag1\" ) ) . where ( F . col ( \"daydiff\" ) . isNotNull ()) . groupBy () . agg ( F . mean ( \"daydiff\" ) . alias ( \"mean\" ), F . variance ( \"daydiff\" ) . alias ( \"variance\" ), F . stddev ( \"daydiff\" ) . alias ( \"stdev\" ), ) . withColumn ( \"coef_of_var_lag\" , F . col ( \"stdev\" ) / F . col ( \"mean\" )) . rdd . flatMap ( lambda x : x ) . collect (), 3 , ) ) p1 = measures_of_percentiles ( spark , idf . groupBy ( id_col ) . agg ( F . countDistinct ( \"yyyymmdd_col\" ) . alias ( \"id_date_pair\" )), list_of_cols = \"id_date_pair\" , ) p2 = measures_of_percentiles ( spark , idf . groupBy ( \"yyyymmdd_col\" ) . agg ( F . countDistinct ( id_col ) . alias ( \"date_id_pair\" )), list_of_cols = \"date_id_pair\" , ) if opt == 1 : odf = p1 . union ( p2 ) . toPandas () return odf else : odf = idf m = ( odf . groupBy ( \"yyyymmdd_col\" ) . count () . orderBy ( \"count\" , ascending = False ) . collect () ) mode = str ( m [ 0 ][ 0 ]) + \" [\" + str ( m [ 0 ][ 1 ]) + \"]\" missing_vals = odf . where ( F . col ( \"yyyymmdd_col\" ) . isNull ()) . count () odf = ( odf . groupBy () . agg ( F . countDistinct ( \"yyyymmdd_col\" ) . alias ( \"count_unique_dates\" ), F . min ( \"yyyymmdd_col\" ) . alias ( \"min_date\" ), F . max ( \"yyyymmdd_col\" ) . alias ( \"max_date\" ), ) . withColumn ( \"modal_date\" , F . lit ( mode )) . withColumn ( \"date_diff\" , F . datediff ( \"max_date\" , \"min_date\" )) . withColumn ( \"missing_date\" , F . lit ( missing_vals )) . withColumn ( \"mean\" , F . lit ( diff_lagged_df [ 0 ])) . withColumn ( \"variance\" , F . lit ( diff_lagged_df [ 1 ])) . withColumn ( \"stdev\" , F . lit ( diff_lagged_df [ 2 ])) . withColumn ( \"cov\" , F . lit ( diff_lagged_df [ 3 ])) . toPandas () ) return odf def ts_processed_feats ( idf, col, id_col, tz, cnt_row, cnt_unique_id) This function helps to extract time units from the input dataframe on a processed column being timestamp / date. Parameters idf Input dataframe col Column belonging to timestamp / date id_col ID column tz Timezone offset cnt_row Count of rows present in the Input dataframe cnt_unique_id Count of unique records present in the Input dataframe Returns DataFrame Expand source code def ts_processed_feats ( idf , col , id_col , tz , cnt_row , cnt_unique_id ): \"\"\" This function helps to extract time units from the input dataframe on a processed column being timestamp / date. Parameters ---------- idf Input dataframe col Column belonging to timestamp / date id_col ID column tz Timezone offset cnt_row Count of rows present in the Input dataframe cnt_unique_id Count of unique records present in the Input dataframe Returns ------- DataFrame \"\"\" if cnt_row == cnt_unique_id : odf = ( timeUnits_extraction ( idf , col , \"all\" , output_mode = \"append\" , ) . withColumn ( \"yyyymmdd_col\" , F . to_date ( col )) . orderBy ( \"yyyymmdd_col\" ) . withColumn ( \"daypart_cat\" , f_daypart_cat ( F . col ( col + \"_hour\" ))) . withColumn ( \"week_cat\" , F . when ( F . col ( col + \"_dayofweek\" ) > 5 , F . lit ( \"weekend\" )) . otherwise ( \"weekday\" ), ) . withColumnRenamed ( col + \"_dayofweek\" , \"dow\" ) ) return odf else : odf = ( timeUnits_extraction ( idf , col , \"all\" , output_mode = \"append\" , ) . withColumn ( \"yyyymmdd_col\" , F . to_date ( col )) . orderBy ( id_col , \"yyyymmdd_col\" ) . withColumn ( \"daypart_cat\" , f_daypart_cat ( F . col ( col + \"_hour\" ))) . withColumn ( \"week_cat\" , F . when ( F . col ( col + \"_dayofweek\" ) > 5 , F . lit ( \"weekend\" )) . otherwise ( \"weekday\" ), ) . withColumnRenamed ( col + \"_dayofweek\" , \"dow\" ) ) return odf def ts_viz_data ( idf, x_col, y_col, id_col, tz_offset='local', output_mode='append', output_type='daily', n_cat=10) This function helps to produce the processed dataframe with the relevant aggregation at the time frequency chosen for a given column as seen against the timestamp / date column. Parameters idf Input Dataframe x_col Timestamp / Date column as set in the X-Axis y_col Numerical & Categorical column as set in the Y-Axis id_col ID Column tz_offset Timezone offset (Option to chose between options like Local, GMT, UTC, etc.). Default option is set as \"Local\". output_mode Option to choose between Append or Replace. If the option Append is selected, the column names are Appended by \"_ts\" else it's replaced by the original column name output_type Option to choose between \"Daily\" or \"Weekly\" or \"Hourly\". Daily is chosen as default. If \"Daily\" is selected as the output type, the daily view is populated ; If it's \"Hourly\", the view is shown at a Day part level. However, if it's \"Weekly\", then the display it per individual week days (1-7) as captured. n_cat For categorical columns whose cardinality is beyond N, the Top N categories are chosen, beyond which the categories are grouped as Others. Returns DataFrame Expand source code def ts_viz_data ( idf , x_col , y_col , id_col , tz_offset = \"local\" , output_mode = \"append\" , output_type = \"daily\" , n_cat = 10 , ): \"\"\" This function helps to produce the processed dataframe with the relevant aggregation at the time frequency chosen for a given column as seen against the timestamp / date column. Parameters ---------- idf Input Dataframe x_col Timestamp / Date column as set in the X-Axis y_col Numerical & Categorical column as set in the Y-Axis id_col ID Column tz_offset Timezone offset (Option to chose between options like Local, GMT, UTC, etc.). Default option is set as \"Local\". output_mode Option to choose between Append or Replace. If the option Append is selected, the column names are Appended by \"_ts\" else it's replaced by the original column name output_type Option to choose between \"Daily\" or \"Weekly\" or \"Hourly\". Daily is chosen as default. If \"Daily\" is selected as the output type, the daily view is populated ; If it's \"Hourly\", the view is shown at a Day part level. However, if it's \"Weekly\", then the display it per individual week days (1-7) as captured. n_cat For categorical columns whose cardinality is beyond N, the Top N categories are chosen, beyond which the categories are grouped as Others. Returns ------- DataFrame \"\"\" y_col_org = y_col y_col = y_col . replace ( \"-\" , \"_\" ) idf = idf . withColumnRenamed ( y_col_org , y_col ) for i in idf . dtypes : if y_col == i [ 0 ]: y_col_dtype = i [ 1 ] if y_col_dtype == \"string\" : top_cat = list ( idf . groupBy ( y_col ) . count () . orderBy ( \"count\" , ascending = False ) . limit ( int ( n_cat )) . select ( y_col ) . toPandas ()[ y_col ] . values ) idf = idf . withColumn ( y_col , F . when ( F . col ( y_col ) . isin ( top_cat ), F . col ( y_col )) . otherwise ( F . lit ( \"Others\" )), ) if output_type == \"daily\" : odf = ( idf . groupBy ( y_col , \"yyyymmdd_col\" ) . agg ( F . count ( y_col ) . alias ( \"count\" )) . orderBy ( \"yyyymmdd_col\" ) . withColumnRenamed ( \"yyyymmdd_col\" , x_col ) . withColumnRenamed ( y_col , y_col_org ) . toPandas () ) elif output_type == \"hourly\" : odf = ( idf . groupBy ( y_col , \"daypart_cat\" ) . agg ( F . count ( y_col ) . alias ( \"count\" )) . orderBy ( \"daypart_cat\" ) . withColumnRenamed ( y_col , y_col_org ) . toPandas () ) elif output_type == \"weekly\" : odf = ( idf . groupBy ( y_col , \"dow\" ) . agg ( F . count ( y_col ) . alias ( \"count\" )) . orderBy ( \"dow\" ) . withColumnRenamed ( y_col , y_col_org ) . toPandas () ) return odf else : if output_type == \"daily\" : odf = ( idf . groupBy ( \"yyyymmdd_col\" ) . agg ( F . min ( y_col ) . alias ( \"min\" ), F . max ( y_col ) . alias ( \"max\" ), F . mean ( y_col ) . alias ( \"mean\" ), F . expr ( \"percentile(\" + y_col + \", array(0.5))\" )[ 0 ] . alias ( \"median\" ), ) . orderBy ( \"yyyymmdd_col\" ) . withColumnRenamed ( \"yyyymmdd_col\" , x_col ) . withColumnRenamed ( y_col , y_col_org ) . toPandas () ) elif output_type == \"hourly\" : odf = ( idf . groupBy ( \"daypart_cat\" ) . agg ( F . min ( y_col ) . alias ( \"min\" ), F . max ( y_col ) . alias ( \"max\" ), F . mean ( y_col ) . alias ( \"mean\" ), F . expr ( \"percentile(\" + y_col + \", array(0.5))\" )[ 0 ] . alias ( \"median\" ), ) . orderBy ( \"daypart_cat\" ) . withColumnRenamed ( y_col , y_col_org ) . toPandas () ) elif output_type == \"weekly\" : odf = ( idf . groupBy ( \"dow\" ) . agg ( F . min ( y_col ) . alias ( \"min\" ), F . max ( y_col ) . alias ( \"max\" ), F . mean ( y_col ) . alias ( \"mean\" ), F . expr ( \"percentile(\" + y_col + \", array(0.5))\" )[ 0 ] . alias ( \"median\" ), ) . orderBy ( \"dow\" ) . withColumnRenamed ( y_col , y_col_org ) . toPandas () ) return odf","title":"<code>ts_analyzer</code>"},{"location":"api/data_analyzer/ts_analyzer.html#ts_analyzer","text":"This module generates the intermediate output specific to the inspection of Time series analysis. As a part of generation of final output, there are various functions created such as - ts_processed_feats ts_eligiblity_check ts_viz_data ts_analyzer daypart_cat Respective functions have sections containing the detailed definition of the parameters used for computing. Expand source code # coding=utf-8 \"\"\"This module generates the intermediate output specific to the inspection of Time series analysis. As a part of generation of final output, there are various functions created such as - - ts_processed_feats - ts_eligiblity_check - ts_viz_data - ts_analyzer - daypart_cat Respective functions have sections containing the detailed definition of the parameters used for computing. \"\"\" import pyspark import datetime from pyspark.sql import functions as F from pyspark.sql import types as T from pyspark.sql import Window from loguru import logger import calendar from anovos.shared.utils import attributeType_segregation , ends_with , output_to_local from anovos.data_analyzer.stats_generator import measures_of_percentiles from anovos.data_ingest.ts_auto_detection import ts_preprocess from anovos.data_transformer.datetime import ( timeUnits_extraction , unix_to_timestamp , lagged_ts , ) import csv import io import os import re import warnings import subprocess from pathlib import Path import dateutil.parser from statsmodels.tsa.seasonal import seasonal_decompose import pandas as pd import numpy as np def daypart_cat ( column ): \"\"\" This functioin helps to convert the input hour part into the respective day parts. The different dayparts are Early Hours, Work Hours, Late Hours, Commuting Hours, Other Hours based on the hour value. Parameters ---------- column Reads the column containing the hour part and converts into respective day part Returns ------- String \"\"\" # calculate hour buckets after adding local timezone if column is None : return \"Missing_NA\" elif ( column >= 4 ) and ( column < 7 ): return \"early_hours\" elif ( column >= 10 ) and ( column < 17 ): return \"work_hours\" elif ( column >= 23 ) or ( column < 4 ): return \"late_hours\" elif (( column >= 7 ) and ( column < 10 )) or (( column >= 17 ) and ( column < 20 )): return \"commuting_hours\" else : return \"other_hours\" f_daypart_cat = F . udf ( daypart_cat , T . StringType ()) def ts_processed_feats ( idf , col , id_col , tz , cnt_row , cnt_unique_id ): \"\"\" This function helps to extract time units from the input dataframe on a processed column being timestamp / date. Parameters ---------- idf Input dataframe col Column belonging to timestamp / date id_col ID column tz Timezone offset cnt_row Count of rows present in the Input dataframe cnt_unique_id Count of unique records present in the Input dataframe Returns ------- DataFrame \"\"\" if cnt_row == cnt_unique_id : odf = ( timeUnits_extraction ( idf , col , \"all\" , output_mode = \"append\" , ) . withColumn ( \"yyyymmdd_col\" , F . to_date ( col )) . orderBy ( \"yyyymmdd_col\" ) . withColumn ( \"daypart_cat\" , f_daypart_cat ( F . col ( col + \"_hour\" ))) . withColumn ( \"week_cat\" , F . when ( F . col ( col + \"_dayofweek\" ) > 5 , F . lit ( \"weekend\" )) . otherwise ( \"weekday\" ), ) . withColumnRenamed ( col + \"_dayofweek\" , \"dow\" ) ) return odf else : odf = ( timeUnits_extraction ( idf , col , \"all\" , output_mode = \"append\" , ) . withColumn ( \"yyyymmdd_col\" , F . to_date ( col )) . orderBy ( id_col , \"yyyymmdd_col\" ) . withColumn ( \"daypart_cat\" , f_daypart_cat ( F . col ( col + \"_hour\" ))) . withColumn ( \"week_cat\" , F . when ( F . col ( col + \"_dayofweek\" ) > 5 , F . lit ( \"weekend\" )) . otherwise ( \"weekday\" ), ) . withColumnRenamed ( col + \"_dayofweek\" , \"dow\" ) ) return odf def ts_eligiblity_check ( spark , idf , id_col , opt = 1 , tz_offset = \"local\" ): \"\"\" This function helps to extract various metrics which can help to understand the nature of timestamp / date column for a given dataset. Parameters ---------- spark Spark session idf Input dataframe id_col ID Column opt Option to choose between [1,2]. 1 is kept as default. Based on the user input, the specific aggregation of data will happen. tz_offset Timezone offset (Option to chose between options like Local, GMT, UTC, etc.). Default option is set as \"Local\". Returns ------- DataFrame \"\"\" lagged_df = lagged_ts ( idf . select ( \"yyyymmdd_col\" ) . distinct () . orderBy ( \"yyyymmdd_col\" ), \"yyyymmdd_col\" , lag = 1 , tsdiff_unit = \"days\" , output_mode = \"append\" , ) . orderBy ( \"yyyymmdd_col\" ) diff_lagged_df = list ( np . around ( lagged_df . withColumn ( \"daydiff\" , F . datediff ( \"yyyymmdd_col\" , \"yyyymmdd_col_lag1\" ) ) . where ( F . col ( \"daydiff\" ) . isNotNull ()) . groupBy () . agg ( F . mean ( \"daydiff\" ) . alias ( \"mean\" ), F . variance ( \"daydiff\" ) . alias ( \"variance\" ), F . stddev ( \"daydiff\" ) . alias ( \"stdev\" ), ) . withColumn ( \"coef_of_var_lag\" , F . col ( \"stdev\" ) / F . col ( \"mean\" )) . rdd . flatMap ( lambda x : x ) . collect (), 3 , ) ) p1 = measures_of_percentiles ( spark , idf . groupBy ( id_col ) . agg ( F . countDistinct ( \"yyyymmdd_col\" ) . alias ( \"id_date_pair\" )), list_of_cols = \"id_date_pair\" , ) p2 = measures_of_percentiles ( spark , idf . groupBy ( \"yyyymmdd_col\" ) . agg ( F . countDistinct ( id_col ) . alias ( \"date_id_pair\" )), list_of_cols = \"date_id_pair\" , ) if opt == 1 : odf = p1 . union ( p2 ) . toPandas () return odf else : odf = idf m = ( odf . groupBy ( \"yyyymmdd_col\" ) . count () . orderBy ( \"count\" , ascending = False ) . collect () ) mode = str ( m [ 0 ][ 0 ]) + \" [\" + str ( m [ 0 ][ 1 ]) + \"]\" missing_vals = odf . where ( F . col ( \"yyyymmdd_col\" ) . isNull ()) . count () odf = ( odf . groupBy () . agg ( F . countDistinct ( \"yyyymmdd_col\" ) . alias ( \"count_unique_dates\" ), F . min ( \"yyyymmdd_col\" ) . alias ( \"min_date\" ), F . max ( \"yyyymmdd_col\" ) . alias ( \"max_date\" ), ) . withColumn ( \"modal_date\" , F . lit ( mode )) . withColumn ( \"date_diff\" , F . datediff ( \"max_date\" , \"min_date\" )) . withColumn ( \"missing_date\" , F . lit ( missing_vals )) . withColumn ( \"mean\" , F . lit ( diff_lagged_df [ 0 ])) . withColumn ( \"variance\" , F . lit ( diff_lagged_df [ 1 ])) . withColumn ( \"stdev\" , F . lit ( diff_lagged_df [ 2 ])) . withColumn ( \"cov\" , F . lit ( diff_lagged_df [ 3 ])) . toPandas () ) return odf def ts_viz_data ( idf , x_col , y_col , id_col , tz_offset = \"local\" , output_mode = \"append\" , output_type = \"daily\" , n_cat = 10 , ): \"\"\" This function helps to produce the processed dataframe with the relevant aggregation at the time frequency chosen for a given column as seen against the timestamp / date column. Parameters ---------- idf Input Dataframe x_col Timestamp / Date column as set in the X-Axis y_col Numerical & Categorical column as set in the Y-Axis id_col ID Column tz_offset Timezone offset (Option to chose between options like Local, GMT, UTC, etc.). Default option is set as \"Local\". output_mode Option to choose between Append or Replace. If the option Append is selected, the column names are Appended by \"_ts\" else it's replaced by the original column name output_type Option to choose between \"Daily\" or \"Weekly\" or \"Hourly\". Daily is chosen as default. If \"Daily\" is selected as the output type, the daily view is populated ; If it's \"Hourly\", the view is shown at a Day part level. However, if it's \"Weekly\", then the display it per individual week days (1-7) as captured. n_cat For categorical columns whose cardinality is beyond N, the Top N categories are chosen, beyond which the categories are grouped as Others. Returns ------- DataFrame \"\"\" y_col_org = y_col y_col = y_col . replace ( \"-\" , \"_\" ) idf = idf . withColumnRenamed ( y_col_org , y_col ) for i in idf . dtypes : if y_col == i [ 0 ]: y_col_dtype = i [ 1 ] if y_col_dtype == \"string\" : top_cat = list ( idf . groupBy ( y_col ) . count () . orderBy ( \"count\" , ascending = False ) . limit ( int ( n_cat )) . select ( y_col ) . toPandas ()[ y_col ] . values ) idf = idf . withColumn ( y_col , F . when ( F . col ( y_col ) . isin ( top_cat ), F . col ( y_col )) . otherwise ( F . lit ( \"Others\" )), ) if output_type == \"daily\" : odf = ( idf . groupBy ( y_col , \"yyyymmdd_col\" ) . agg ( F . count ( y_col ) . alias ( \"count\" )) . orderBy ( \"yyyymmdd_col\" ) . withColumnRenamed ( \"yyyymmdd_col\" , x_col ) . withColumnRenamed ( y_col , y_col_org ) . toPandas () ) elif output_type == \"hourly\" : odf = ( idf . groupBy ( y_col , \"daypart_cat\" ) . agg ( F . count ( y_col ) . alias ( \"count\" )) . orderBy ( \"daypart_cat\" ) . withColumnRenamed ( y_col , y_col_org ) . toPandas () ) elif output_type == \"weekly\" : odf = ( idf . groupBy ( y_col , \"dow\" ) . agg ( F . count ( y_col ) . alias ( \"count\" )) . orderBy ( \"dow\" ) . withColumnRenamed ( y_col , y_col_org ) . toPandas () ) return odf else : if output_type == \"daily\" : odf = ( idf . groupBy ( \"yyyymmdd_col\" ) . agg ( F . min ( y_col ) . alias ( \"min\" ), F . max ( y_col ) . alias ( \"max\" ), F . mean ( y_col ) . alias ( \"mean\" ), F . expr ( \"percentile(\" + y_col + \", array(0.5))\" )[ 0 ] . alias ( \"median\" ), ) . orderBy ( \"yyyymmdd_col\" ) . withColumnRenamed ( \"yyyymmdd_col\" , x_col ) . withColumnRenamed ( y_col , y_col_org ) . toPandas () ) elif output_type == \"hourly\" : odf = ( idf . groupBy ( \"daypart_cat\" ) . agg ( F . min ( y_col ) . alias ( \"min\" ), F . max ( y_col ) . alias ( \"max\" ), F . mean ( y_col ) . alias ( \"mean\" ), F . expr ( \"percentile(\" + y_col + \", array(0.5))\" )[ 0 ] . alias ( \"median\" ), ) . orderBy ( \"daypart_cat\" ) . withColumnRenamed ( y_col , y_col_org ) . toPandas () ) elif output_type == \"weekly\" : odf = ( idf . groupBy ( \"dow\" ) . agg ( F . min ( y_col ) . alias ( \"min\" ), F . max ( y_col ) . alias ( \"max\" ), F . mean ( y_col ) . alias ( \"mean\" ), F . expr ( \"percentile(\" + y_col + \", array(0.5))\" )[ 0 ] . alias ( \"median\" ), ) . orderBy ( \"dow\" ) . withColumnRenamed ( y_col , y_col_org ) . toPandas () ) return odf def ts_analyzer ( spark , idf , id_col , max_days , output_path , output_type = \"daily\" , tz_offset = \"local\" , run_type = \"local\" , ): \"\"\" This function helps to produce the processed output in an aggregate form considering the input dataframe with processed timestamp / date column. The aggregation happens across Mean, Median, Min & Max for the Numerical / Categorical column. Parameters ---------- spark Spark session idf Input Dataframe id_col ID Column max_days Max days upto which the data will be aggregated. If we've a dataset containing a timestamp / date field with very high number of unique dates (Let's say beyond 20 years worth of daily data), a maximum days value chosen basis which the latest output is displayed. output_path Output path where the intermediate data is going to be saved output_type Option to choose between \"Daily\" or \"Weekly\" or \"Hourly\". Daily is chosen as default. If \"Daily\" is selected as the output type, the daily view is populated ; If it's \"Hourly\", the view is shown at a Day part level. However, if it's \"Weekly\", then the display it per individual week days (1-7) as captured. tz_offset Timezone offset (Option to chose between options like Local, GMT, UTC, etc.). Default option is set as \"Local\". run_type Option to choose between run type \"Local\" or \"EMR\" or \"Azure\" basis the user flexibility. Default option is set as \"Local\". Returns ------- Output[CSV] \"\"\" if run_type == \"local\" : local_path = output_path elif run_type == \"databricks\" : local_path = output_to_local ( output_path ) elif run_type == \"emr\" : local_path = \"report_stats\" else : raise ValueError ( \"Invalid run_type\" ) Path ( local_path ) . mkdir ( parents = True , exist_ok = True ) num_cols , cat_cols , other_cols = attributeType_segregation ( idf ) num_cols = [ x for x in num_cols if x not in [ id_col ]] cat_cols = [ x for x in cat_cols if x not in [ id_col ]] ts_loop_cols_post = [ x [ 0 ] for x in idf . dtypes if x [ 1 ] in [ \"timestamp\" , \"date\" ]] cnt_row = idf . count () cnt_unique_id = idf . select ( id_col ) . distinct () . count () for i in ts_loop_cols_post : ts_processed_feat_df = ts_processed_feats ( idf , i , id_col , tz_offset , cnt_row , cnt_unique_id ) ts_processed_feat_df . persist ( pyspark . StorageLevel . MEMORY_AND_DISK ) # for j in range(1, 3): # f = ts_eligiblity_check( # spark, # ts_processed_feat_df, # id_col=id_col, # opt=j, # tz_offset=tz_offset, # ) # f.to_csv( # ends_with(local_path) + \"stats_\" + str(i) + \"_\" + str(j) + \".csv\", # index=False, # ) f1 = ts_eligiblity_check ( spark , ts_processed_feat_df , id_col = id_col , opt = 1 , tz_offset = tz_offset ) f1 . to_csv ( ends_with ( local_path ) + \"stats_\" + str ( i ) + \"_\" + str ( 1 ) + \".csv\" , index = False , ) f2 = ts_eligiblity_check ( spark , ts_processed_feat_df , id_col = id_col , opt = 2 , tz_offset = tz_offset ) f2 . to_csv ( ends_with ( local_path ) + \"stats_\" + str ( i ) + \"_\" + str ( 2 ) + \".csv\" , index = False , ) for k in [ num_cols , cat_cols ]: for l in k : for m in [ output_type ]: f = ( ts_viz_data ( ts_processed_feat_df , i , l , id_col = id_col , tz_offset = tz_offset , output_mode = \"append\" , output_type = m , n_cat = 10 , ) . tail ( int ( max_days )) . dropna () ) f . to_csv ( ends_with ( local_path ) + i + \"_\" + l + \"_\" + m + \".csv\" , index = False , ) ts_processed_feat_df . unpersist () if run_type == \"emr\" : bash_cmd = ( \"aws s3 cp --recursive \" + ends_with ( local_path ) + \" \" + ends_with ( output_path ) ) output = subprocess . check_output ([ \"bash\" , \"-c\" , bash_cmd ])","title":"ts_analyzer"},{"location":"api/data_analyzer/ts_analyzer.html#functions","text":"def daypart_cat ( column) This functioin helps to convert the input hour part into the respective day parts. The different dayparts are Early Hours, Work Hours, Late Hours, Commuting Hours, Other Hours based on the hour value.","title":"Functions"},{"location":"api/data_ingest/_index.html","text":"Overview Sub-modules anovos.data_ingest.data_ingest This module consists of functions to read the dataset as Spark DataFrame, concatenate/join with other functions (if required), and perform some basic \u2026 anovos.data_ingest.ts_auto_detection This module help produce the output containing a transformation through auto timestamp / date detection by reading the ingested dataframe from source \u2026","title":"Overview"},{"location":"api/data_ingest/_index.html#overview","text":"","title":"Overview"},{"location":"api/data_ingest/_index.html#sub-modules","text":"anovos.data_ingest.data_ingest This module consists of functions to read the dataset as Spark DataFrame, concatenate/join with other functions (if required), and perform some basic \u2026 anovos.data_ingest.ts_auto_detection This module help produce the output containing a transformation through auto timestamp / date detection by reading the ingested dataframe from source \u2026","title":"Sub-modules"},{"location":"api/data_ingest/data_ingest.html","text":"data_ingest This module consists of functions to read the dataset as Spark DataFrame, concatenate/join with other functions (if required), and perform some basic ETL actions such as selecting, deleting, renaming and/or recasting columns. List of functions included in this module are: - read_dataset - write_dataset - concatenate_dataset - join_dataset - delete_column - select_column - rename_column - recast_column Expand source code # coding=utf-8 \"\"\" This module consists of functions to read the dataset as Spark DataFrame, concatenate/join with other functions (if required), and perform some basic ETL actions such as selecting, deleting, renaming and/or recasting columns. List of functions included in this module are: - read_dataset - write_dataset - concatenate_dataset - join_dataset - delete_column - select_column - rename_column - recast_column \"\"\" import pyspark.sql.functions as F from pyspark.sql import DataFrame from anovos.shared.utils import pairwise_reduce def read_dataset ( spark , file_path , file_type , file_configs = {}): \"\"\" This function reads the input data path and return a Spark DataFrame. Under the hood, this function is based on generic Load functionality of Spark SQL. Parameters ---------- spark Spark Session file_path Path to input data (directory or filename). Compatible with local path and s3 path (when running in AWS environment). file_type \"csv\", \"parquet\", \"avro\", \"json\". Avro data source requires an external package to run, which can be configured with spark-submit (--packages org.apache.spark:spark-avro_2.11:2.4.0). file_configs This optional argument is passed in a dictionary format as key/value pairs e.g. {\"header\": \"True\",\"delimiter\": \"|\",\"inferSchema\": \"True\"} for csv files. All the key/value pairs in this argument are passed as options to DataFrameReader, which is created using SparkSession.read. (Default value = {}) Returns ------- DataFrame \"\"\" odf = spark . read . format ( file_type ) . options ( ** file_configs ) . load ( file_path ) return odf def write_dataset ( idf , file_path , file_type , file_configs = {}, column_order = []): \"\"\" This function saves the Spark DataFrame in the user-provided output path. Like read_dataset, this function is based on the generic Save functionality of Spark SQL. Parameters ---------- idf Input Dataframe i.e. Spark DataFrame to be saved file_path Path to output data (directory or filename). Compatible with local path and s3 path (when running in AWS environment). file_type \"csv\", \"parquet\", \"avro\", \"json\". Avro data source requires an external package to run, which can be configured with spark-submit (--packages org.apache.spark:spark-avro_2.11:2.4.0). file_configs This argument is passed in dictionary format as key/value pairs. Some of the potential keys are header, delimiter, mode, compression, repartition. compression options - uncompressed, gzip (doesn't work with avro), snappy (only valid for parquet) mode options - error (default), overwrite, append repartition - None (automatic partitioning) or an integer value () e.g. {\"header\":\"True\", \"delimiter\":\",\",'compression':'snappy','mode':'overwrite','repartition':'10'}. All the key/value pairs (except repartition, mode) written in this argument are passed as options to DataFrameWriter is available using Dataset.write operator. If the number of repartitions mentioned through this argument is less than the existing DataFrame partitions, then the coalesce operation is used instead of the repartition operation to make the execution work. This is because the coalesce operation doesn\u2019t require any shuffling like repartition which is known to be an expensive step. column_order list of columns in the order in which Dataframe is to be written. If None or [] is specified, then the default order is applied. \"\"\" if not column_order : column_order = idf . columns else : if not isinstance ( column_order , list ): raise TypeError ( \"Invalid input type for column_order argument\" ) if len ( column_order ) != len ( idf . columns ): raise ValueError ( \"Count of column(s) specified in column_order argument do not match Dataframe\" ) diff_cols = [ x for x in column_order if x not in set ( idf . columns )] if diff_cols : raise ValueError ( \"Column(s) specified in column_order argument not found in Dataframe: \" + str ( diff_cols ) ) mode = file_configs [ \"mode\" ] if \"mode\" in file_configs else \"error\" repartition = ( int ( file_configs [ \"repartition\" ]) if \"repartition\" in file_configs else None ) if repartition is None : idf . select ( column_order ) . write . format ( file_type ) . options ( ** file_configs ) . save ( file_path , mode = mode ) else : exist_parts = idf . rdd . getNumPartitions () req_parts = int ( repartition ) if req_parts > exist_parts : idf . select ( column_order ) . repartition ( req_parts ) . write . format ( file_type ) . options ( ** file_configs ) . save ( file_path , mode = mode ) else : idf . select ( column_order ) . coalesce ( req_parts ) . write . format ( file_type ) . options ( ** file_configs ) . save ( file_path , mode = mode ) def concatenate_dataset ( * idfs , method_type = \"name\" ): \"\"\" This function combines multiple dataframes into a single dataframe. A pairwise concatenation is performed on the dataframes, instead of adding one dataframe at a time to the bigger dataframe. This function leverages union functionality of Spark SQL. Parameters ---------- *idfs All dataframes to be concatenated (with the first dataframe columns) method_type \"index\", \"name\". This argument needs to be passed as a keyword argument. The \u201cindex\u201d method concatenates the dataframes by the column index (without shuffling columns). If the sequence of column is not fixed among the dataframe, this method should be avoided. The \u201cname\u201d method concatenates after shuffling and arranging columns as per the first dataframe order. First dataframe passed under idfs will define the final columns in the concatenated dataframe, and will throw error if any column in first dataframe is not available in any of other dataframes. (Default value = \"name\") Returns ------- DataFrame Concatenated dataframe \"\"\" if method_type not in [ \"index\" , \"name\" ]: raise TypeError ( \"Invalid input for concatenate_dataset method\" ) if method_type == \"name\" : odf = pairwise_reduce ( lambda idf1 , idf2 : idf1 . union ( idf2 . select ( idf1 . columns )), idfs ) # odf = reduce(DataFrame.unionByName, idfs) # only if exact no. of columns else : odf = pairwise_reduce ( DataFrame . union , idfs ) return odf def join_dataset ( * idfs , join_cols , join_type ): \"\"\" This function joins multiple dataframes into a single dataframe by joining key column(s). For optimization, Pairwise joining is done on the dataframes, instead of joining individual dataframes to the bigger dataframe. This function leverages join functionality of Spark SQL. Parameters ---------- idfs All dataframes to be joined join_cols Key column(s) to join all dataframes together. In case of multiple key columns to join, they can be passed in a list format or a string format where different column names are separated by pipe delimiter \u201c|\u201d e.g. \"col1|col2\". join_type \"inner\", \u201cfull\u201d, \u201cleft\u201d, \u201cright\u201d, \u201cleft_semi\u201d, \u201cleft_anti\u201d Returns ------- DataFrame Joined dataframe \"\"\" if isinstance ( join_cols , str ): join_cols = [ x . strip () for x in join_cols . split ( \"|\" )] list_of_df_cols = [ x . columns for x in idfs ] list_of_all_cols = [ x for sublist in list_of_df_cols for x in sublist ] list_of_nonjoin_cols = [ x for x in list_of_all_cols if x not in join_cols ] if len ( list_of_nonjoin_cols ) != ( len ( list_of_all_cols ) - ( len ( list_of_df_cols ) * len ( join_cols )) ): raise ValueError ( \"Specified join_cols do not match all the Input Dataframe(s)\" ) if len ( list_of_nonjoin_cols ) != len ( set ( list_of_nonjoin_cols )): raise ValueError ( \"Duplicate column(s) present in non joining column(s) in Input Dataframe(s)\" ) odf = pairwise_reduce ( lambda idf1 , idf2 : idf1 . join ( idf2 , join_cols , join_type ), idfs ) return odf def delete_column ( idf , list_of_cols , print_impact = False ): \"\"\" This function is used to delete specific columns from the input data. It is executed using drop functionality of Spark SQL. It is advisable to use this function if the number of columns to delete is lesser than the number of columns to select; otherwise, it is recommended to use select_column. Parameters ---------- idf Input Dataframe list_of_cols List of columns to delete e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". print_impact True, False This argument is to compare number of columns before and after the operation.(Default value = False) Returns ------- DataFrame Dataframe after dropping columns \"\"\" if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] list_of_cols = list ( set ( list_of_cols )) odf = idf . drop ( * list_of_cols ) if print_impact : print ( \"Before: \\n No. of Columns- \" , len ( idf . columns )) print ( idf . columns ) print ( \"After: \\n No. of Columns- \" , len ( odf . columns )) print ( odf . columns ) return odf def select_column ( idf , list_of_cols , print_impact = False ): \"\"\" This function is used to select specific columns from the input data. It is executed using select operation of spark dataframe. It is advisable to use this function if the number of columns to select is lesser than the number of columns to drop; otherwise, it is recommended to use delete_column. Parameters ---------- idf Input Dataframe list_of_cols List of columns to select e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". print_impact True, False This argument is to compare number of columns before and after the operation.(Default value = False) Returns ------- DataFrame Dataframe with the selected columns \"\"\" if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] list_of_cols = list ( set ( list_of_cols )) odf = idf . select ( list_of_cols ) if print_impact : print ( \"Before: \\n No. of Columns-\" , len ( idf . columns )) print ( idf . columns ) print ( \" \\n After: \\n No. of Columns-\" , len ( odf . columns )) print ( odf . columns ) return odf def rename_column ( idf , list_of_cols , list_of_newcols , print_impact = False ): \"\"\" This function is used to rename the columns of the input data. Multiple columns can be renamed; however, the sequence they passed as an argument is critical and must be consistent between list_of_cols and list_of_newcols. Parameters ---------- idf Input Dataframe list_of_cols List of old column names e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". list_of_newcols List of corresponding new column names e.g., [\"newcol1\",\"newcol2\"]. Alternatively, new column names can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"newcol1|newcol2\". First element in list_of_cols will be original column name, and corresponding first column in list_of_newcols will be new column name. print_impact True, False This argument is to compare column names before and after the operation. (Default value = False) Returns ------- DataFrame Dataframe with revised column names \"\"\" if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( list_of_newcols , str ): list_of_newcols = [ x . strip () for x in list_of_newcols . split ( \"|\" )] mapping = dict ( zip ( list_of_cols , list_of_newcols )) odf = idf . select ([ F . col ( i ) . alias ( mapping . get ( i , i )) for i in idf . columns ]) if print_impact : print ( \"Before: \\n No. of Columns- \" , len ( idf . columns )) print ( idf . columns ) print ( \"After: \\n No. of Columns- \" , len ( odf . columns )) print ( odf . columns ) return odf def recast_column ( idf , list_of_cols , list_of_dtypes , print_impact = False ): \"\"\" This function is used to modify the datatype of columns. Multiple columns can be cast; however, the sequence they passed as argument is critical and needs to be consistent between list_of_cols and list_of_dtypes. Parameters ---------- idf Input Dataframe list_of_cols List of columns to cast e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". list_of_dtypes List of corresponding datatypes e.g., [\"type1\",\"type2\"]. Alternatively, datatypes can be specified in a string format, where they are separated by pipe delimiter \u201c|\u201d e.g., \"type1|type2\". First element in list_of_cols will column name and corresponding element in list_of_dtypes will be new datatypes such as \"float\", \"integer\", \"long\", \"string\", \"double\", decimal\" etc. Datatypes are case insensitive e.g. float or Float are treated as same. print_impact True, False This argument is to compare schema before and after the operation. (Default value = False) Returns ------- DataFrame Dataframe with revised datatypes \"\"\" if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( list_of_dtypes , str ): list_of_dtypes = [ x . strip () for x in list_of_dtypes . split ( \"|\" )] odf = idf for i , j in zip ( list_of_cols , list_of_dtypes ): odf = odf . withColumn ( i , F . col ( i ) . cast ( j )) if print_impact : print ( \"Before: \" ) idf . printSchema () print ( \"After: \" ) odf . printSchema () return odf Functions def concatenate_dataset ( *idfs, method_type='name') This function combines multiple dataframes into a single dataframe. A pairwise concatenation is performed on the dataframes, instead of adding one dataframe at a time to the bigger dataframe. This function leverages union functionality of Spark SQL. Parameters *idfs All dataframes to be concatenated (with the first dataframe columns) method_type \"index\", \"name\". This argument needs to be passed as a keyword argument. The \u201cindex\u201d method concatenates the dataframes by the column index (without shuffling columns). If the sequence of column is not fixed among the dataframe, this method should be avoided. The \u201cname\u201d method concatenates after shuffling and arranging columns as per the first dataframe order. First dataframe passed under idfs will define the final columns in the concatenated dataframe, and will throw error if any column in first dataframe is not available in any of other dataframes. (Default value = \"name\") Returns DataFrame Concatenated dataframe Expand source code def concatenate_dataset ( * idfs , method_type = \"name\" ): \"\"\" This function combines multiple dataframes into a single dataframe. A pairwise concatenation is performed on the dataframes, instead of adding one dataframe at a time to the bigger dataframe. This function leverages union functionality of Spark SQL. Parameters ---------- *idfs All dataframes to be concatenated (with the first dataframe columns) method_type \"index\", \"name\". This argument needs to be passed as a keyword argument. The \u201cindex\u201d method concatenates the dataframes by the column index (without shuffling columns). If the sequence of column is not fixed among the dataframe, this method should be avoided. The \u201cname\u201d method concatenates after shuffling and arranging columns as per the first dataframe order. First dataframe passed under idfs will define the final columns in the concatenated dataframe, and will throw error if any column in first dataframe is not available in any of other dataframes. (Default value = \"name\") Returns ------- DataFrame Concatenated dataframe \"\"\" if method_type not in [ \"index\" , \"name\" ]: raise TypeError ( \"Invalid input for concatenate_dataset method\" ) if method_type == \"name\" : odf = pairwise_reduce ( lambda idf1 , idf2 : idf1 . union ( idf2 . select ( idf1 . columns )), idfs ) # odf = reduce(DataFrame.unionByName, idfs) # only if exact no. of columns else : odf = pairwise_reduce ( DataFrame . union , idfs ) return odf def delete_column ( idf, list_of_cols, print_impact=False) This function is used to delete specific columns from the input data. It is executed using drop functionality of Spark SQL. It is advisable to use this function if the number of columns to delete is lesser than the number of columns to select; otherwise, it is recommended to use select_column. Parameters idf Input Dataframe list_of_cols List of columns to delete e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". print_impact True, False This argument is to compare number of columns before and after the operation.(Default value = False) Returns DataFrame Dataframe after dropping columns Expand source code def delete_column ( idf , list_of_cols , print_impact = False ): \"\"\" This function is used to delete specific columns from the input data. It is executed using drop functionality of Spark SQL. It is advisable to use this function if the number of columns to delete is lesser than the number of columns to select; otherwise, it is recommended to use select_column. Parameters ---------- idf Input Dataframe list_of_cols List of columns to delete e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". print_impact True, False This argument is to compare number of columns before and after the operation.(Default value = False) Returns ------- DataFrame Dataframe after dropping columns \"\"\" if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] list_of_cols = list ( set ( list_of_cols )) odf = idf . drop ( * list_of_cols ) if print_impact : print ( \"Before: \\n No. of Columns- \" , len ( idf . columns )) print ( idf . columns ) print ( \"After: \\n No. of Columns- \" , len ( odf . columns )) print ( odf . columns ) return odf def join_dataset ( *idfs, join_cols, join_type) This function joins multiple dataframes into a single dataframe by joining key column(s). For optimization, Pairwise joining is done on the dataframes, instead of joining individual dataframes to the bigger dataframe. This function leverages join functionality of Spark SQL. Parameters idfs All dataframes to be joined join_cols Key column(s) to join all dataframes together. In case of multiple key columns to join, they can be passed in a list format or a string format where different column names are separated by pipe delimiter \u201c|\u201d e.g. \"col1|col2\". join_type \"inner\", \u201cfull\u201d, \u201cleft\u201d, \u201cright\u201d, \u201cleft_semi\u201d, \u201cleft_anti\u201d Returns DataFrame Joined dataframe Expand source code def join_dataset ( * idfs , join_cols , join_type ): \"\"\" This function joins multiple dataframes into a single dataframe by joining key column(s). For optimization, Pairwise joining is done on the dataframes, instead of joining individual dataframes to the bigger dataframe. This function leverages join functionality of Spark SQL. Parameters ---------- idfs All dataframes to be joined join_cols Key column(s) to join all dataframes together. In case of multiple key columns to join, they can be passed in a list format or a string format where different column names are separated by pipe delimiter \u201c|\u201d e.g. \"col1|col2\". join_type \"inner\", \u201cfull\u201d, \u201cleft\u201d, \u201cright\u201d, \u201cleft_semi\u201d, \u201cleft_anti\u201d Returns ------- DataFrame Joined dataframe \"\"\" if isinstance ( join_cols , str ): join_cols = [ x . strip () for x in join_cols . split ( \"|\" )] list_of_df_cols = [ x . columns for x in idfs ] list_of_all_cols = [ x for sublist in list_of_df_cols for x in sublist ] list_of_nonjoin_cols = [ x for x in list_of_all_cols if x not in join_cols ] if len ( list_of_nonjoin_cols ) != ( len ( list_of_all_cols ) - ( len ( list_of_df_cols ) * len ( join_cols )) ): raise ValueError ( \"Specified join_cols do not match all the Input Dataframe(s)\" ) if len ( list_of_nonjoin_cols ) != len ( set ( list_of_nonjoin_cols )): raise ValueError ( \"Duplicate column(s) present in non joining column(s) in Input Dataframe(s)\" ) odf = pairwise_reduce ( lambda idf1 , idf2 : idf1 . join ( idf2 , join_cols , join_type ), idfs ) return odf def read_dataset ( spark, file_path, file_type, file_configs={}) This function reads the input data path and return a Spark DataFrame. Under the hood, this function is based on generic Load functionality of Spark SQL. Parameters spark Spark Session file_path Path to input data (directory or filename). Compatible with local path and s3 path (when running in AWS environment). file_type \"csv\", \"parquet\", \"avro\", \"json\". Avro data source requires an external package to run, which can be configured with spark-submit (\u2013packages org.apache.spark:spark-avro_2.11:2.4.0). file_configs This optional argument is passed in a dictionary format as key/value pairs e.g. {\"header\": \"True\",\"delimiter\": \"|\",\"inferSchema\": \"True\"} for csv files. All the key/value pairs in this argument are passed as options to DataFrameReader, which is created using SparkSession.read. (Default value = {}) Returns DataFrame Expand source code def read_dataset ( spark , file_path , file_type , file_configs = {}): \"\"\" This function reads the input data path and return a Spark DataFrame. Under the hood, this function is based on generic Load functionality of Spark SQL. Parameters ---------- spark Spark Session file_path Path to input data (directory or filename). Compatible with local path and s3 path (when running in AWS environment). file_type \"csv\", \"parquet\", \"avro\", \"json\". Avro data source requires an external package to run, which can be configured with spark-submit (--packages org.apache.spark:spark-avro_2.11:2.4.0). file_configs This optional argument is passed in a dictionary format as key/value pairs e.g. {\"header\": \"True\",\"delimiter\": \"|\",\"inferSchema\": \"True\"} for csv files. All the key/value pairs in this argument are passed as options to DataFrameReader, which is created using SparkSession.read. (Default value = {}) Returns ------- DataFrame \"\"\" odf = spark . read . format ( file_type ) . options ( ** file_configs ) . load ( file_path ) return odf def recast_column ( idf, list_of_cols, list_of_dtypes, print_impact=False) This function is used to modify the datatype of columns. Multiple columns can be cast; however, the sequence they passed as argument is critical and needs to be consistent between list_of_cols and list_of_dtypes. Parameters idf Input Dataframe list_of_cols List of columns to cast e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". list_of_dtypes List of corresponding datatypes e.g., [\"type1\",\"type2\"]. Alternatively, datatypes can be specified in a string format, where they are separated by pipe delimiter \u201c|\u201d e.g., \"type1|type2\". First element in list_of_cols will column name and corresponding element in list_of_dtypes will be new datatypes such as \"float\", \"integer\", \"long\", \"string\", \"double\", decimal\" etc. Datatypes are case insensitive e.g. float or Float are treated as same. print_impact True, False This argument is to compare schema before and after the operation. (Default value = False) Returns DataFrame Dataframe with revised datatypes Expand source code def recast_column ( idf , list_of_cols , list_of_dtypes , print_impact = False ): \"\"\" This function is used to modify the datatype of columns. Multiple columns can be cast; however, the sequence they passed as argument is critical and needs to be consistent between list_of_cols and list_of_dtypes. Parameters ---------- idf Input Dataframe list_of_cols List of columns to cast e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". list_of_dtypes List of corresponding datatypes e.g., [\"type1\",\"type2\"]. Alternatively, datatypes can be specified in a string format, where they are separated by pipe delimiter \u201c|\u201d e.g., \"type1|type2\". First element in list_of_cols will column name and corresponding element in list_of_dtypes will be new datatypes such as \"float\", \"integer\", \"long\", \"string\", \"double\", decimal\" etc. Datatypes are case insensitive e.g. float or Float are treated as same. print_impact True, False This argument is to compare schema before and after the operation. (Default value = False) Returns ------- DataFrame Dataframe with revised datatypes \"\"\" if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( list_of_dtypes , str ): list_of_dtypes = [ x . strip () for x in list_of_dtypes . split ( \"|\" )] odf = idf for i , j in zip ( list_of_cols , list_of_dtypes ): odf = odf . withColumn ( i , F . col ( i ) . cast ( j )) if print_impact : print ( \"Before: \" ) idf . printSchema () print ( \"After: \" ) odf . printSchema () return odf def rename_column ( idf, list_of_cols, list_of_newcols, print_impact=False) This function is used to rename the columns of the input data. Multiple columns can be renamed; however, the sequence they passed as an argument is critical and must be consistent between list_of_cols and list_of_newcols. Parameters idf Input Dataframe list_of_cols List of old column names e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". list_of_newcols List of corresponding new column names e.g., [\"newcol1\",\"newcol2\"]. Alternatively, new column names can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"newcol1|newcol2\". First element in list_of_cols will be original column name, and corresponding first column in list_of_newcols will be new column name. print_impact True, False This argument is to compare column names before and after the operation. (Default value = False) Returns DataFrame Dataframe with revised column names Expand source code def rename_column ( idf , list_of_cols , list_of_newcols , print_impact = False ): \"\"\" This function is used to rename the columns of the input data. Multiple columns can be renamed; however, the sequence they passed as an argument is critical and must be consistent between list_of_cols and list_of_newcols. Parameters ---------- idf Input Dataframe list_of_cols List of old column names e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". list_of_newcols List of corresponding new column names e.g., [\"newcol1\",\"newcol2\"]. Alternatively, new column names can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"newcol1|newcol2\". First element in list_of_cols will be original column name, and corresponding first column in list_of_newcols will be new column name. print_impact True, False This argument is to compare column names before and after the operation. (Default value = False) Returns ------- DataFrame Dataframe with revised column names \"\"\" if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( list_of_newcols , str ): list_of_newcols = [ x . strip () for x in list_of_newcols . split ( \"|\" )] mapping = dict ( zip ( list_of_cols , list_of_newcols )) odf = idf . select ([ F . col ( i ) . alias ( mapping . get ( i , i )) for i in idf . columns ]) if print_impact : print ( \"Before: \\n No. of Columns- \" , len ( idf . columns )) print ( idf . columns ) print ( \"After: \\n No. of Columns- \" , len ( odf . columns )) print ( odf . columns ) return odf def select_column ( idf, list_of_cols, print_impact=False) This function is used to select specific columns from the input data. It is executed using select operation of spark dataframe. It is advisable to use this function if the number of columns to select is lesser than the number of columns to drop; otherwise, it is recommended to use delete_column. Parameters idf Input Dataframe list_of_cols List of columns to select e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". print_impact True, False This argument is to compare number of columns before and after the operation.(Default value = False) Returns DataFrame Dataframe with the selected columns Expand source code def select_column ( idf , list_of_cols , print_impact = False ): \"\"\" This function is used to select specific columns from the input data. It is executed using select operation of spark dataframe. It is advisable to use this function if the number of columns to select is lesser than the number of columns to drop; otherwise, it is recommended to use delete_column. Parameters ---------- idf Input Dataframe list_of_cols List of columns to select e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". print_impact True, False This argument is to compare number of columns before and after the operation.(Default value = False) Returns ------- DataFrame Dataframe with the selected columns \"\"\" if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] list_of_cols = list ( set ( list_of_cols )) odf = idf . select ( list_of_cols ) if print_impact : print ( \"Before: \\n No. of Columns-\" , len ( idf . columns )) print ( idf . columns ) print ( \" \\n After: \\n No. of Columns-\" , len ( odf . columns )) print ( odf . columns ) return odf def write_dataset ( idf, file_path, file_type, file_configs={}, column_order=[]) This function saves the Spark DataFrame in the user-provided output path. Like read_dataset, this function is based on the generic Save functionality of Spark SQL. Parameters idf Input Dataframe i.e. Spark DataFrame to be saved file_path Path to output data (directory or filename). Compatible with local path and s3 path (when running in AWS environment). file_type \"csv\", \"parquet\", \"avro\", \"json\". Avro data source requires an external package to run, which can be configured with spark-submit (\u2013packages org.apache.spark:spark-avro_2.11:2.4.0). file_configs This argument is passed in dictionary format as key/value pairs. Some of the potential keys are header, delimiter, mode, compression, repartition. compression options - uncompressed, gzip (doesn't work with avro), snappy (only valid for parquet) mode options - error (default), overwrite, append repartition - None (automatic partitioning) or an integer value () e.g. {\"header\":\"True\", \"delimiter\":\",\",'compression':'snappy','mode':'overwrite','repartition':'10'}. All the key/value pairs (except repartition, mode) written in this argument are passed as options to DataFrameWriter is available using Dataset.write operator. If the number of repartitions mentioned through this argument is less than the existing DataFrame partitions, then the coalesce operation is used instead of the repartition operation to make the execution work. This is because the coalesce operation doesn\u2019t require any shuffling like repartition which is known to be an expensive step. column_order list of columns in the order in which Dataframe is to be written. If None or [] is specified, then the default order is applied. Expand source code def write_dataset ( idf , file_path , file_type , file_configs = {}, column_order = []): \"\"\" This function saves the Spark DataFrame in the user-provided output path. Like read_dataset, this function is based on the generic Save functionality of Spark SQL. Parameters ---------- idf Input Dataframe i.e. Spark DataFrame to be saved file_path Path to output data (directory or filename). Compatible with local path and s3 path (when running in AWS environment). file_type \"csv\", \"parquet\", \"avro\", \"json\". Avro data source requires an external package to run, which can be configured with spark-submit (--packages org.apache.spark:spark-avro_2.11:2.4.0). file_configs This argument is passed in dictionary format as key/value pairs. Some of the potential keys are header, delimiter, mode, compression, repartition. compression options - uncompressed, gzip (doesn't work with avro), snappy (only valid for parquet) mode options - error (default), overwrite, append repartition - None (automatic partitioning) or an integer value () e.g. {\"header\":\"True\", \"delimiter\":\",\",'compression':'snappy','mode':'overwrite','repartition':'10'}. All the key/value pairs (except repartition, mode) written in this argument are passed as options to DataFrameWriter is available using Dataset.write operator. If the number of repartitions mentioned through this argument is less than the existing DataFrame partitions, then the coalesce operation is used instead of the repartition operation to make the execution work. This is because the coalesce operation doesn\u2019t require any shuffling like repartition which is known to be an expensive step. column_order list of columns in the order in which Dataframe is to be written. If None or [] is specified, then the default order is applied. \"\"\" if not column_order : column_order = idf . columns else : if not isinstance ( column_order , list ): raise TypeError ( \"Invalid input type for column_order argument\" ) if len ( column_order ) != len ( idf . columns ): raise ValueError ( \"Count of column(s) specified in column_order argument do not match Dataframe\" ) diff_cols = [ x for x in column_order if x not in set ( idf . columns )] if diff_cols : raise ValueError ( \"Column(s) specified in column_order argument not found in Dataframe: \" + str ( diff_cols ) ) mode = file_configs [ \"mode\" ] if \"mode\" in file_configs else \"error\" repartition = ( int ( file_configs [ \"repartition\" ]) if \"repartition\" in file_configs else None ) if repartition is None : idf . select ( column_order ) . write . format ( file_type ) . options ( ** file_configs ) . save ( file_path , mode = mode ) else : exist_parts = idf . rdd . getNumPartitions () req_parts = int ( repartition ) if req_parts > exist_parts : idf . select ( column_order ) . repartition ( req_parts ) . write . format ( file_type ) . options ( ** file_configs ) . save ( file_path , mode = mode ) else : idf . select ( column_order ) . coalesce ( req_parts ) . write . format ( file_type ) . options ( ** file_configs ) . save ( file_path , mode = mode )","title":"<code>data_ingest</code>"},{"location":"api/data_ingest/data_ingest.html#data_ingest","text":"This module consists of functions to read the dataset as Spark DataFrame, concatenate/join with other functions (if required), and perform some basic ETL actions such as selecting, deleting, renaming and/or recasting columns. List of functions included in this module are: - read_dataset - write_dataset - concatenate_dataset - join_dataset - delete_column - select_column - rename_column - recast_column Expand source code # coding=utf-8 \"\"\" This module consists of functions to read the dataset as Spark DataFrame, concatenate/join with other functions (if required), and perform some basic ETL actions such as selecting, deleting, renaming and/or recasting columns. List of functions included in this module are: - read_dataset - write_dataset - concatenate_dataset - join_dataset - delete_column - select_column - rename_column - recast_column \"\"\" import pyspark.sql.functions as F from pyspark.sql import DataFrame from anovos.shared.utils import pairwise_reduce def read_dataset ( spark , file_path , file_type , file_configs = {}): \"\"\" This function reads the input data path and return a Spark DataFrame. Under the hood, this function is based on generic Load functionality of Spark SQL. Parameters ---------- spark Spark Session file_path Path to input data (directory or filename). Compatible with local path and s3 path (when running in AWS environment). file_type \"csv\", \"parquet\", \"avro\", \"json\". Avro data source requires an external package to run, which can be configured with spark-submit (--packages org.apache.spark:spark-avro_2.11:2.4.0). file_configs This optional argument is passed in a dictionary format as key/value pairs e.g. {\"header\": \"True\",\"delimiter\": \"|\",\"inferSchema\": \"True\"} for csv files. All the key/value pairs in this argument are passed as options to DataFrameReader, which is created using SparkSession.read. (Default value = {}) Returns ------- DataFrame \"\"\" odf = spark . read . format ( file_type ) . options ( ** file_configs ) . load ( file_path ) return odf def write_dataset ( idf , file_path , file_type , file_configs = {}, column_order = []): \"\"\" This function saves the Spark DataFrame in the user-provided output path. Like read_dataset, this function is based on the generic Save functionality of Spark SQL. Parameters ---------- idf Input Dataframe i.e. Spark DataFrame to be saved file_path Path to output data (directory or filename). Compatible with local path and s3 path (when running in AWS environment). file_type \"csv\", \"parquet\", \"avro\", \"json\". Avro data source requires an external package to run, which can be configured with spark-submit (--packages org.apache.spark:spark-avro_2.11:2.4.0). file_configs This argument is passed in dictionary format as key/value pairs. Some of the potential keys are header, delimiter, mode, compression, repartition. compression options - uncompressed, gzip (doesn't work with avro), snappy (only valid for parquet) mode options - error (default), overwrite, append repartition - None (automatic partitioning) or an integer value () e.g. {\"header\":\"True\", \"delimiter\":\",\",'compression':'snappy','mode':'overwrite','repartition':'10'}. All the key/value pairs (except repartition, mode) written in this argument are passed as options to DataFrameWriter is available using Dataset.write operator. If the number of repartitions mentioned through this argument is less than the existing DataFrame partitions, then the coalesce operation is used instead of the repartition operation to make the execution work. This is because the coalesce operation doesn\u2019t require any shuffling like repartition which is known to be an expensive step. column_order list of columns in the order in which Dataframe is to be written. If None or [] is specified, then the default order is applied. \"\"\" if not column_order : column_order = idf . columns else : if not isinstance ( column_order , list ): raise TypeError ( \"Invalid input type for column_order argument\" ) if len ( column_order ) != len ( idf . columns ): raise ValueError ( \"Count of column(s) specified in column_order argument do not match Dataframe\" ) diff_cols = [ x for x in column_order if x not in set ( idf . columns )] if diff_cols : raise ValueError ( \"Column(s) specified in column_order argument not found in Dataframe: \" + str ( diff_cols ) ) mode = file_configs [ \"mode\" ] if \"mode\" in file_configs else \"error\" repartition = ( int ( file_configs [ \"repartition\" ]) if \"repartition\" in file_configs else None ) if repartition is None : idf . select ( column_order ) . write . format ( file_type ) . options ( ** file_configs ) . save ( file_path , mode = mode ) else : exist_parts = idf . rdd . getNumPartitions () req_parts = int ( repartition ) if req_parts > exist_parts : idf . select ( column_order ) . repartition ( req_parts ) . write . format ( file_type ) . options ( ** file_configs ) . save ( file_path , mode = mode ) else : idf . select ( column_order ) . coalesce ( req_parts ) . write . format ( file_type ) . options ( ** file_configs ) . save ( file_path , mode = mode ) def concatenate_dataset ( * idfs , method_type = \"name\" ): \"\"\" This function combines multiple dataframes into a single dataframe. A pairwise concatenation is performed on the dataframes, instead of adding one dataframe at a time to the bigger dataframe. This function leverages union functionality of Spark SQL. Parameters ---------- *idfs All dataframes to be concatenated (with the first dataframe columns) method_type \"index\", \"name\". This argument needs to be passed as a keyword argument. The \u201cindex\u201d method concatenates the dataframes by the column index (without shuffling columns). If the sequence of column is not fixed among the dataframe, this method should be avoided. The \u201cname\u201d method concatenates after shuffling and arranging columns as per the first dataframe order. First dataframe passed under idfs will define the final columns in the concatenated dataframe, and will throw error if any column in first dataframe is not available in any of other dataframes. (Default value = \"name\") Returns ------- DataFrame Concatenated dataframe \"\"\" if method_type not in [ \"index\" , \"name\" ]: raise TypeError ( \"Invalid input for concatenate_dataset method\" ) if method_type == \"name\" : odf = pairwise_reduce ( lambda idf1 , idf2 : idf1 . union ( idf2 . select ( idf1 . columns )), idfs ) # odf = reduce(DataFrame.unionByName, idfs) # only if exact no. of columns else : odf = pairwise_reduce ( DataFrame . union , idfs ) return odf def join_dataset ( * idfs , join_cols , join_type ): \"\"\" This function joins multiple dataframes into a single dataframe by joining key column(s). For optimization, Pairwise joining is done on the dataframes, instead of joining individual dataframes to the bigger dataframe. This function leverages join functionality of Spark SQL. Parameters ---------- idfs All dataframes to be joined join_cols Key column(s) to join all dataframes together. In case of multiple key columns to join, they can be passed in a list format or a string format where different column names are separated by pipe delimiter \u201c|\u201d e.g. \"col1|col2\". join_type \"inner\", \u201cfull\u201d, \u201cleft\u201d, \u201cright\u201d, \u201cleft_semi\u201d, \u201cleft_anti\u201d Returns ------- DataFrame Joined dataframe \"\"\" if isinstance ( join_cols , str ): join_cols = [ x . strip () for x in join_cols . split ( \"|\" )] list_of_df_cols = [ x . columns for x in idfs ] list_of_all_cols = [ x for sublist in list_of_df_cols for x in sublist ] list_of_nonjoin_cols = [ x for x in list_of_all_cols if x not in join_cols ] if len ( list_of_nonjoin_cols ) != ( len ( list_of_all_cols ) - ( len ( list_of_df_cols ) * len ( join_cols )) ): raise ValueError ( \"Specified join_cols do not match all the Input Dataframe(s)\" ) if len ( list_of_nonjoin_cols ) != len ( set ( list_of_nonjoin_cols )): raise ValueError ( \"Duplicate column(s) present in non joining column(s) in Input Dataframe(s)\" ) odf = pairwise_reduce ( lambda idf1 , idf2 : idf1 . join ( idf2 , join_cols , join_type ), idfs ) return odf def delete_column ( idf , list_of_cols , print_impact = False ): \"\"\" This function is used to delete specific columns from the input data. It is executed using drop functionality of Spark SQL. It is advisable to use this function if the number of columns to delete is lesser than the number of columns to select; otherwise, it is recommended to use select_column. Parameters ---------- idf Input Dataframe list_of_cols List of columns to delete e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". print_impact True, False This argument is to compare number of columns before and after the operation.(Default value = False) Returns ------- DataFrame Dataframe after dropping columns \"\"\" if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] list_of_cols = list ( set ( list_of_cols )) odf = idf . drop ( * list_of_cols ) if print_impact : print ( \"Before: \\n No. of Columns- \" , len ( idf . columns )) print ( idf . columns ) print ( \"After: \\n No. of Columns- \" , len ( odf . columns )) print ( odf . columns ) return odf def select_column ( idf , list_of_cols , print_impact = False ): \"\"\" This function is used to select specific columns from the input data. It is executed using select operation of spark dataframe. It is advisable to use this function if the number of columns to select is lesser than the number of columns to drop; otherwise, it is recommended to use delete_column. Parameters ---------- idf Input Dataframe list_of_cols List of columns to select e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". print_impact True, False This argument is to compare number of columns before and after the operation.(Default value = False) Returns ------- DataFrame Dataframe with the selected columns \"\"\" if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] list_of_cols = list ( set ( list_of_cols )) odf = idf . select ( list_of_cols ) if print_impact : print ( \"Before: \\n No. of Columns-\" , len ( idf . columns )) print ( idf . columns ) print ( \" \\n After: \\n No. of Columns-\" , len ( odf . columns )) print ( odf . columns ) return odf def rename_column ( idf , list_of_cols , list_of_newcols , print_impact = False ): \"\"\" This function is used to rename the columns of the input data. Multiple columns can be renamed; however, the sequence they passed as an argument is critical and must be consistent between list_of_cols and list_of_newcols. Parameters ---------- idf Input Dataframe list_of_cols List of old column names e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". list_of_newcols List of corresponding new column names e.g., [\"newcol1\",\"newcol2\"]. Alternatively, new column names can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"newcol1|newcol2\". First element in list_of_cols will be original column name, and corresponding first column in list_of_newcols will be new column name. print_impact True, False This argument is to compare column names before and after the operation. (Default value = False) Returns ------- DataFrame Dataframe with revised column names \"\"\" if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( list_of_newcols , str ): list_of_newcols = [ x . strip () for x in list_of_newcols . split ( \"|\" )] mapping = dict ( zip ( list_of_cols , list_of_newcols )) odf = idf . select ([ F . col ( i ) . alias ( mapping . get ( i , i )) for i in idf . columns ]) if print_impact : print ( \"Before: \\n No. of Columns- \" , len ( idf . columns )) print ( idf . columns ) print ( \"After: \\n No. of Columns- \" , len ( odf . columns )) print ( odf . columns ) return odf def recast_column ( idf , list_of_cols , list_of_dtypes , print_impact = False ): \"\"\" This function is used to modify the datatype of columns. Multiple columns can be cast; however, the sequence they passed as argument is critical and needs to be consistent between list_of_cols and list_of_dtypes. Parameters ---------- idf Input Dataframe list_of_cols List of columns to cast e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". list_of_dtypes List of corresponding datatypes e.g., [\"type1\",\"type2\"]. Alternatively, datatypes can be specified in a string format, where they are separated by pipe delimiter \u201c|\u201d e.g., \"type1|type2\". First element in list_of_cols will column name and corresponding element in list_of_dtypes will be new datatypes such as \"float\", \"integer\", \"long\", \"string\", \"double\", decimal\" etc. Datatypes are case insensitive e.g. float or Float are treated as same. print_impact True, False This argument is to compare schema before and after the operation. (Default value = False) Returns ------- DataFrame Dataframe with revised datatypes \"\"\" if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( list_of_dtypes , str ): list_of_dtypes = [ x . strip () for x in list_of_dtypes . split ( \"|\" )] odf = idf for i , j in zip ( list_of_cols , list_of_dtypes ): odf = odf . withColumn ( i , F . col ( i ) . cast ( j )) if print_impact : print ( \"Before: \" ) idf . printSchema () print ( \"After: \" ) odf . printSchema () return odf","title":"data_ingest"},{"location":"api/data_ingest/data_ingest.html#functions","text":"def concatenate_dataset ( *idfs, method_type='name') This function combines multiple dataframes into a single dataframe. A pairwise concatenation is performed on the dataframes, instead of adding one dataframe at a time to the bigger dataframe. This function leverages union functionality of Spark SQL.","title":"Functions"},{"location":"api/data_ingest/ts_auto_detection.html","text":"ts_auto_detection This module help produce the output containing a transformation through auto timestamp / date detection by reading the ingested dataframe from source. As a part of generation of the auto detection output, there are various functions created such as - regex_date_time_parser ts_loop_cols_pre ts_preprocess Respective functions have sections containing the detailed definition of the parameters used for computing. Expand source code # coding=utf-8 \"\"\"This module help produce the output containing a transformation through auto timestamp / date detection by reading the ingested dataframe from source. As a part of generation of the auto detection output, there are various functions created such as - - regex_date_time_parser - ts_loop_cols_pre - ts_preprocess Respective functions have sections containing the detailed definition of the parameters used for computing. \"\"\" import pyspark import datetime import csv import io import os import re import warnings import subprocess from pyspark.sql import functions as F from pyspark.sql import types as T from pyspark.sql import Window from loguru import logger import calendar from anovos.shared.utils import attributeType_segregation , ends_with , output_to_local from anovos.data_analyzer.stats_generator import measures_of_percentiles from anovos.data_transformer.datetime import ( timeUnits_extraction , unix_to_timestamp , lagged_ts , ) from pathlib import Path import dateutil.parser import pandas as pd import numpy as np ###regex based ts parser function def regex_date_time_parser ( spark , idf , id_col , col , tz , val_unique_cat , trans_cat , save_output = None , output_mode = \"replace\" , ): \"\"\" This function helps to produce the transformed output (if applicable) based on the auto-detection of timestamp / date type. The output from this function is decoupled as a part of ingestion. Parameters ---------- spark Spark session idf Input dataframe id_col ID Column col Column passed for Auto detection of Timestamp / date type tz Timezone offset (Option to chose between options like Local, GMT, UTC). Default option is set as \"Local\". val_unique_cat Maximum character length of the field. trans_cat Custom data type basis which further processing will be conditioned. save_output Output path where the transformed ddata can be saved output_mode Option to choose between Append or Replace. If the option Append is selected, the column names are Appended by \"_ts\" else it's replaced by the original column name Returns ------- DataFrame \"\"\" REGEX_PARTS = { \"Y\" : r \"(?:19[4-9]\\d|20[0-3]\\d)\" , # 1940 to 2039 \"y\" : r \"(?:\\d\\d)\" , # 00 to 99 \"m\" : r \"(?:1[012]|0?[1-9])\" , # 0?1 to 12 \"mz\" : r \"(?:1[012]|0[1-9])\" , # 01 to 12 \"B\" : r \"(?:\" r \"D?JAN(?:UAR[IY])?|\" r \"[FP]EB(?:RUAR[IY])?|\" r \"MAC|MAR(?:CH|ET)?|MRT|\" r \"APR(?:IL)?|\" r \"M[EA]I|MAY|\" r \"JUNE?|D?JUNI?|\" r \"JUL(?:Y|AI)?|D?JULI?|\" r \"OG(?:OS)?|AUG(?:UST)?|AGT?(?:USTUS)?|\" r \"SEP(?:T(?:EMBER)?)?|\" r \"O[KC]T(?:OBER)?|\" r \"NO[VP](?:EMBER)?|\" r \"D[EI][SC](?:EMBER)?\" r \")\" , \"d\" : r \"(?:3[01]|[12]\\d|0?[1-9])\" , # 0?1 to 31 \"d_range\" : r \"(?:3[01]|[12]\\d|0?[1-9])(?: ?[-] ?(?:3[01]|[12]\\d|0?[1-9]))?\" , # 14-15 \"dz\" : r \"(?:3[01]|[12]\\d|0[1-9])\" , # 01 to 31 \"j\" : r \"(?:36[0-6]|3[0-5]\\d|[12]\\d\\d|0?[1-9]\\d|0?0?[1-9])\" , # 0?0?1 to 366 \"H\" : r \"(?:2[0-4]|[01]?\\d)\" , # 0?0 to 24 \"HZ\" : r \"(?:2[0-4]|[01]\\d)\" , # 0?0 to 24 \"I\" : r \"(?:1[012]|0?[1-9])\" , # 0?1 to 12 \"M\" : r \"(?:[1-5]\\d|0\\d)\" , # 00 to 59 \"S\" : r \"(?:6[01]|[0-5]\\d)\" , # 00 to 61 (leap second) \"p\" : r \"(?:MIDNI(?:GHT|TE)|AFTERNOON|MORNING|NOON|[MN]N|H(?:(?:OU)?RS?)?|[AP]\\.? ?M\\.?)\" , \"p2\" : r \"(?:MIDNI(?:GHT|TE)|NOON|[AP]\\.? ?M\\.?)\" , \"Z\" : r \"(?:A(?:C(?:DT|ST|T|WST)|DT|E(?:DT|ST)|FT|K(?:DT|ST)|M(?:ST|T)|RT|ST|WST\" r \"|Z(?:O(?:ST|T)|T))|B(?:DT|I(?:OT|T)|OT|R(?:ST|T)|ST|TT)|C(?:AT|CT|DT|E(\" r \"?:ST|T)|H(?:A(?:DT|ST)|O(?:ST|T)|ST|UT)|I(?:ST|T)|KT|L(?:ST|T)|O(?:ST|T\" r \")|ST|T|VT|WST|XT)|D(?:AVT|DUT|FT)|E(?:A(?:S(?:ST|T)|T)|CT|DT|E(?:ST|T)|\" r \"G(?:ST|T)|IT|ST)|F(?:ET|JT|K(?:ST|T)|NT)|G(?:A(?:LT|MT)|ET|FT|I(?:LT|T)\" r \"|MT|ST|YT)|H(?:AEC|DT|KT|MT|OV(?:ST|T)|ST)|I(?:CT|D(?:LW|T)|OT|R(?:DT|K\" r \"T|ST)|ST)|JST|K(?:ALT|GT|OST|RAT|ST)|L(?:HST|INT)|M(?:A(?:GT|RT|WT)|DT|\" r \"E(?:ST|T)|HT|I(?:ST|T)|MT|S(?:K|T)|UT|VT|YT)|N(?:CT|DT|FT|PT|ST|T|UT|Z(\" r \"?:DT|ST))|O(?:MST|RAT)|P(?:DT|ET(?:T)?|GT|H(?:OT|T)|KT|M(?:DT|ST)|ONT|S\" r \"T|Y(?:ST|T))|R(?:ET|OTT)|S(?:A(?:KT|MT|ST)|BT|CT|DT|GT|LST|R(?:ET|T)|ST\" r \"|YOT)|T(?:AHT|FT|HA|JT|KT|LT|MT|OT|RT|VT)|U(?:LA(?:ST|T)|TC|Y(?:ST|T)|Z\" r \"T)|V(?:ET|LAT|O(?:LT|ST)|UT)|W(?:A(?:KT|ST|T)|E(?:ST|T)|IT|ST)|Y(?:AKT|\" r \"EKT))\" , # FROM: en.wikipedia.org/wiki/List_of_time_zone_abbreviations \"z\" : r \"(?:[+-](?:0\\d|1[0-4]):?(?:00|15|30|45))\" , # [+-] 00:00 to 14:45 \"A\" : r \"(?:\" r \"MON(?:DAY)?|(?:IS|SE)N(?:[IE]N)?|\" r \"TUE(?:S(?:DAY)?)?|SEL(?:ASA)?|\" r \"WED(?:NESDAY)?|RABU?|\" r \"THU(?:RS(?:DAY)?)?|KH?A(?:M(?:IS)?)?|\" r \"FRI(?:DAY)?|JUM(?:[AM]A?T)?|\" r \"SAT(?:URDAY)?|SAB(?:TU)?|\" r \"SUN(?:DAY)?|AHA?D|MIN(?:GGU)?\" r \")\" , \"th\" : r \"(?:ST|ND|RD|TH|\u00ba)\" , } REGEX_PATTERNS_PARSERS = { # 14/8/1991 \"dd_mm_YYYY_1\" : r \"(?: {d} / {m} / {Y} )\" , \"dd_d2\" : r \"(?: {d} \\\\ {m} \\\\ {Y} )\" , \"dd_mm_YYYY_3\" : r \"(?: {d} [-] {m} [-] {Y} )\" , \"dd_mm_YYYY_4\" : r \"(?: {d} \\. {m} \\. {Y} )\" , # 'dd_mm_YYYY_5': r\"(?:{d}{m}{Y})\", # too many phone numbers \"dd_mm_YYYY_6\" : r \"(?: {d} ? {m} ? {Y} )\" , \"dd_mm_YYYY_7\" : r \"(?: {dz}{mz}{Y} )\" , # 14/8/91 \"dd_mm_yy_1\" : r \"(?: {d} / {m} / {y} )\" , \"dd_mm_yy_2\" : r \"(?: {d} \\\\ {m} \\\\ {y} )\" , \"dd_mm_yy_3\" : r \"(?: {d} [-] {m} [-] {y} )\" , \"dd_mm_yy_4\" : r \"(?: {d} \\. {m} \\. {y} )\" , # 'dd_mm_yy_5': r\"(?:{dz}{mz}{y})\", # too many phone numbers # 14 Aug, 1991 \"dd_mmm_YYYY_1\" : r \"(?: {d}{th} ? ?/ ? {B} ?/ ? {Y} )\" , \"dd_mmm_YYYY_2\" : r \"(?: {d}{th} ? ? \\\\ ? {B} ? \\\\ ? {Y} )\" , \"dd_mmm_YYYY_3\" : r \"(?: {d}{th} ? ?[-] ? {B} ?[ -] ? {Y} )\" , \"dd_mmm_YYYY_4\" : r \"(?: {d}{th} ? ?[ -]? ? {B} ?,? ? {Y} )\" , \"dd_mmm_YYYY_5\" : r \"(?: {d}{th} ? ?\\. ? {B} ?\\. ? {Y} )\" , # 14 Aug '91 \"dd_mmm_yy_1\" : r \"(?: {d}{th} ? ?/ ? {B} ?/ ?'? {y} )\" , \"dd_mmm_yy_2\" : r \"(?: {d}{th} ? ? \\\\ ? {B} ? \\\\ ?'? {y} )\" , \"dd_mmm_yy_3\" : r \"(?: {d}{th} ? ?[-] ? {B} ?[-] ?'? {y} )\" , \"dd_mmm_yy_4\" : r \"(?: {d}{th} ? ?[ -]? ? {B} ?,? ?'? {y} )\" , \"dd_mmm_yy_5\" : r \"(?: {d}{th} ? ?\\. ? {B} ?\\. ?'? {y} )\" , # 14th Aug \"dd_mmm\" : r \"(?: {d}{th} ? ?[/ \\\\ . -] ? {B} )\" , # 08/14/1991 # WARNING! dateutil set to day first \"mm_dd_YYYY_1\" : r \"(?: {m} / {d} / {Y} )\" , \"mm_dd_YYYY_2\" : r \"(?: {m} \\\\ {d} \\\\ {Y} )\" , \"mm_dd_YYYY_3\" : r \"(?: {m} [-] {d} [-] {Y} )\" , \"mm_dd_YYYY_4\" : r \"(?: {m} {d} {Y} )\" , \"mm_dd_YYYY_5\" : r \"(?: {m} \\. {d} \\. {Y} )\" , \"mm_dd_YYYY_6\" : r \"(?: {mz}{dz}{Y} )\" , # 8/14/91 # WARNING! dateutil set to day first \"mm_dd_yy_1\" : r \"(?: {m} / {d} / {y} )\" , \"mm_dd_yy_2\" : r \"(?: {m} \\\\ {d} \\\\ {y} )\" , \"mm_dd_yy_3\" : r \"(?: {m} [-] {d} [-] {y} )\" , \"mm_dd_yy_4\" : r \"(?: {m} \\. {d} \\. {y} )\" , # 'mm_dd_yy_5': r\"(?:{mz}{dz}{y})\", # too many phone numbers # Aug 14th, 1991 \"mmm_dd_YYYY_1\" : r \"(?: {B} ?/ ? {d}{th} ? ?/ ? {Y} )\" , \"mmm_dd_YYYY_2\" : r \"(?: {B} ? \\\\ ? {d}{th} ? ? \\\\ ? {Y} )\" , \"mmm_dd_YYYY_3\" : r \"(?: {B} ?[-] ? {d}{th} ? ?[ -] ? {Y} )\" , \"mmm_dd_YYYY_4\" : r \"(?: {B} ?[ -]? ? {d}{th} ? ?, ? {Y} )\" , \"mmm_dd_YYYY_5\" : r \"(?: {B} ?\\. ? {d}{th} ? ?\\. ? {Y} )\" , # Aug-14 '91 \"mmm_dd_yy_1\" : r \"(?: {B} ?/ ? {d}{th} ? ?/ ?'? {y} )\" , \"mmm_dd_yy_2\" : r \"(?: {B} ? \\\\ ? {d}{th} ? ? \\\\ ?'? {y} )\" , \"mmm_dd_yy_3\" : r \"(?: {B} ?[-] ? {d}{th} ? ?[-] ?'? {y} )\" , \"mmm_dd_yy_4\" : r \"(?: {B} ?[. -]? ? {d}{th} ?, '? {y} )\" , \"mmm_dd_yy_5\" : r \"(?: {B} ?\\. ? {d}{th} ? ?\\. ?'? {y} )\" , # Aug-14 # WARNING! dateutil assumes current year \"mmm_dd\" : r \"(?: {B} ?[/ \\\\ . -] ? {d}{th} ?)\" , # # Aug-91 # 'mmm_yy': r\"(?:{B} ?[/\\\\. -] ?'{y})\", # too many false positives # August 1991 \"mmm_YYYY\" : r \"(?: {B} ?[/ \\\\ . -] ? {Y} )\" , # many non-useful dates # 1991-8-14 \"YYYY_mm_dd_1\" : r \"(?: {Y} / {m} / {d} )\" , \"YYYY_mm_dd_2\" : r \"(?: {Y} \\\\ {m} \\\\ {d} )\" , \"YYYY_mm_dd_3\" : r \"(?: {Y} [-] {m} [-] {d} )\" , \"YYYY_mm_dd_4\" : r \"(?: {Y} {m} {d} )\" , \"YYYY_mm_dd_5\" : r \"(?: {Y} \\. {m} \\. {d} )\" , \"YYYY_mm_dd_6\" : r \"(?: {Y}{mz}{dz} )\" , # 910814 (ISO 8601) # 'yy_mm_dd_1': r\"(?:{y} {m} {d})\", # too many random numbers \"yy_mm_dd_2\" : r \"(?: {y} / {m} / {d} )\" , \"yy_mm_dd_3\" : r \"(?: {y} \\\\ {m} \\\\ {d} )\" , \"yy_mm_dd_4\" : r \"(?: {y} [-] {m} [-] {d} )\" , \"yy_mm_dd_5\" : r \"(?: {y} \\. {m} \\. {d} )\" , # 'yy_mm_dd_6': r\"(?:{y}{mz}{dz})\", # too many phone numbers # 1991-Aug-14 \"YYYY_mmm_dd_1\" : r \"(?: {Y} ?/ ? {B} ?/ ? {d} )\" , \"YYYY_mmm_dd_2\" : r \"(?: {Y} ? \\\\ ? {B} ? \\\\ ? {d} )\" , \"YYYY_mmm_dd_3\" : r \"(?: {Y} ?[-] ? {B} ?[-] ? {d} )\" , \"YYYY_mmm_dd_4\" : r \"(?: {Y} ? {B} ?[ -]? ? {d}{th} ?)\" , # 91-Aug-14 \"yy_mmm_dd_1\" : r \"(?:'? {y} ?/ ? {B} ?/ ? {d} )\" , \"yy_mmm_dd_2\" : r \"(?:'? {y} ? \\\\ ? {B} ? \\\\ ? {d} )\" , \"yy_mmm_dd_3\" : r \"(?:'? {y} ?[-] ? {B} ?[-] ? {d} )\" , \"yy_mmm_dd_4\" : r \"(?:'? {y} ? {B} ?[ -]? ? {d}{th} ?)\" , # # 1991.226 (Aug 14 = day 226 in 1991) # dateutil fails # 'YYYY_ddd_1': r\"(?:{Y}\\.{j})\", # too many random numbers # 'YYYY_ddd_2': r\"(?:{Y}[-]{j})\", # too many random numbers # time \"HH_MM_SS\" : r \"(?: {H} : {M} : {S} (?: ? {p} )?(?: ?(?:Z| {Z} | {z} ))?)\" , \"HH_MZ_pp_1\" : r \"(?: {H} : {M} (?: ? {p} )?(?: ?(?:Z| {Z} | {z} ))?)\" , \"HH_MZ_pp_1b\" : r \"(?: {H} [:. ] {M} (?: ? {p} )(?: ?(?:Z| {Z} | {z} ))?)\" , \"HH_MZ_pp_2\" : r \"(?:(?<!\\.) {HZ} [. ]? {M} (?: ? {p} )(?: ?(?:Z| {Z} | {z} ))?)\" , \"HH_pp\" : r \"(?:(?<!\\.) {H} ? {p2} (?: ?(?:Z| {Z} | {z} ))?)\" , # # 910814094500 (9:45am) # 'yy_mm_dd_HH_MM_SS': r\"(?:{y}{mz}{dz}{H}{M}{S})\", # too many phone numbers # 1991-08-14T09:45:00Z \"YYYY_mm_dd_HH_MM\" : r \"(?: {Y} [-] {m} [-] {d} [T ] {H} : {M} (?: ?(?:Z| {Z} | {z} ))?)\" , \"YYYY_mm_dd_HH_MM_SS_1\" : r \"(?: {Y} [-] {m} [-] {d} [T ] {H} : {M} : {S} (?: ?(?:Z| {Z} | {z} ))?)\" , \"YYYY_mm_dd_HH_MM_SS_2\" : r \"(?: {Y}{mz}{d} T? {H}{M}{S} (?: ?(?:Z| {Z} | {z} ))?)\" , \"YYYY_dd_mm_HH_MM_SS_3\" : r \"(?: {Y} [-] {d} [-] {m} [T ] {H} : {M} : {S} (?: ?(?:Z| {Z} | {z} ))?)\" , \"mm_dd_YYYY_HH_MM_SS_1\" : r \"(?: {m} [-] {d} [-] {Y} [T ] {H} : {M} : {S} (?: ?(?:Z| {Z} | {z} ))?)\" , \"dd_mm_YYYY_HH_MM_SS_1\" : r \"(?: {d} [-] {m} [-] {Y} [T ] {H} : {M} : {S} (?: ?(?:Z| {Z} | {z} ))?)\" , # # standalone # 'day': r\"{A}\", # too many false positives # 'month': r\"{B}\", # too many false positives # 'year': r\"{Y}\", # too many random numbers # 'timezone': r\"(?:Z|{Z}|{z})\", # too many malay words } # unicode fixes REGEX_FORMATTED = { label : \" \\\\ b\" + pattern . format ( ** REGEX_PARTS ) # fill in the chunks . replace ( \"-]\" , \" \\u2009\\u2010\\u2011\\u2012\\u2013\\u2014 -]\" ) # unicode dashes . replace ( \"'?\" , \"[' \\u2018\\u2019 ]?\" ) # unicode quotes + \" \\\\ b\" for label , pattern in REGEX_PATTERNS_PARSERS . items () } # match emails and urls to avoid returning chunks of them REGEX_FORMATTED [ \"eml\" ] = r \"\"\"[a-zA-Z0-9][^\\s`!@%$^= {} \\[\\]/\\\\\"',()<>:;]+(?:@|%40|\\s+at\\s+|\\s*<\\s*at\\s*>\\s*)[a-zA-Z0-9][-_a-zA-Z0-9~.]+\\.[a-zA-Z]{2,15}\"\"\" REGEX_FORMATTED [ \"url\" ] = r \"\\b(?:(?:https?|ftp|file)://|www\\d?\\.|ftp\\.)[-A-Z0-9+&@#/%=~_|$?!:,.]*[A-Z0-9+&@#/%=~_|$]\" REGEX_FORMATTED [ \"dot\" ] = r \"(?:\\d+\\.){3,}\\d+\" # compile all the regex patterns REGEX_COMPILED = { label : re . compile ( pattern , flags = re . I | re . U ) for label , pattern in REGEX_FORMATTED . items () } if trans_cat == \"dt\" : return idf elif ( trans_cat in [ \"long_c\" , \"bigint_c\" , \"int_c\" ]) & ( int ( val_unique_cat ) in [ 10 , 13 ] ): if int ( val_unique_cat ) == 10 : precision = \"s\" elif int ( val_unique_cat ) == 13 : precision = \"ms\" else : precision = \"ms\" output_df = unix_to_timestamp ( spark , idf , col , precision = precision , tz = tz , output_mode = output_mode ) . orderBy ( id_col , col ) if save_output is not None : output_df . write . parquet ( save_output , mode = \"overwrite\" ) else : return output_df elif trans_cat == \"string\" : list_dates = list ( set ( idf . select ( col ) . rdd . flatMap ( lambda x : x ) . collect ())) def regex_text ( text , longest = True , context_max_len = 999 , dayfirst = False ): # join multiple spaces, convert tabs, strip leading/trailing whitespace if isinstance ( text , str ): pass else : raise ValueError ( \"Incompatible Column Type!!\" ) text = \" \" . join ( text . split ()) matches = [] for regex_label , regex_obj in REGEX_COMPILED . items (): for m in regex_obj . finditer ( text ): context_start = max ( 0 , ( m . start () + m . end () - context_max_len ) // 2 ) context_end = min ( len ( text ), context_start + context_max_len ) context_str = text [ context_start : context_end ] if context_start != 0 : context_str = \" \\u2026 \" + context_str [ 1 :] if context_end != len ( text ): context_str = ( context_str [: - 1 ] + \" \\u2026 \" ) # this is the `...` character parsed_date = None try : with warnings . catch_warnings (): warnings . simplefilter ( \"ignore\" , category = dateutil . parser . UnknownTimezoneWarning , ) if \"HH\" in regex_label : if \"dd\" in regex_label or \"YYYY\" in regex_label : matched_text = re . sub ( r \"[ \\\\ ]\" , \"/\" , m . group ()) parsed_date = dateutil . parser . parse ( matched_text , dayfirst = dayfirst ) else : matched_text = re . sub ( r \"H(?:(?:OU)?RS?)?\" , \"\" , m . group (), flags = re . I ) matched_text = re . sub ( r \"MN\" , r \"AM\" , matched_text , flags = re . I ) matched_text = re . sub ( r \"NN\" , r \"PM\" , matched_text , flags = re . I ) matched_text = re . sub ( r \"(\\d)[. ](\\d)\" , r \"\\1:\\2\" , matched_text ) matched_text = f \"1970-01-01 { matched_text } \" parsed_date = dateutil . parser . parse ( matched_text , dayfirst = dayfirst ) elif \"dd\" in regex_label or \"YYYY\" in regex_label : matched_text = re . sub ( r \"[ \\\\ ]\" , \"/\" , m . group ()) parsed_date = dateutil . parser . parse ( matched_text , dayfirst = dayfirst ) except ValueError : pass matches . append ( { \"REGEX_LABEL\" : regex_label , \"MATCH\" : m . group (), \"START\" : m . start (), \"END\" : m . end (), \"MATCH_LEN\" : m . end () - m . start (), \"NORM_TEXT_LEN\" : len ( text ), \"CONTEXT\" : context_str , \"PARSED\" : parsed_date , } ) # narrow to longest match for match in matches : if not longest or all ( ( other [ \"START\" ] >= match [ \"START\" ] and other [ \"END\" ] <= match [ \"END\" ]) or other [ \"START\" ] > match [ \"END\" ] or other [ \"END\" ] < match [ \"START\" ] for other in matches ): # don't return emails or urls if match [ \"REGEX_LABEL\" ] not in { \"eml\" , \"url\" , \"dot\" }: yield match bl = [] file_lines = list_dates for line_num , line in enumerate ( file_lines ): bl_int = [] for match_info in regex_text ( line ): try : ye , mo , da , ho , mi , se = ( match_info [ \"PARSED\" ] . year , match_info [ \"PARSED\" ] . month , match_info [ \"PARSED\" ] . day , match_info [ \"PARSED\" ] . hour , match_info [ \"PARSED\" ] . minute , match_info [ \"PARSED\" ] . second , ) if len ( bl_int ) == 0 : bl_int = [ ye , mo , da , ho , mi , se ] else : if ye == 1970 and mo == 1 and da == 1 : pass if ho + mi + se == 0 : pass if ye > 1970 : bl_int [ 0 ] = ye if mo > 0 and ye != 1970 : bl_int [ 1 ] = mo if da > 0 and ye != 1970 : bl_int [ 2 ] = da if ho > 0 : bl_int [ 3 ] = ho if mi > 0 : bl_int [ 4 ] = mi if se > 0 : bl_int [ 5 ] = se else : pass except : pass bl . append ( [ match_info [ \"CONTEXT\" ], datetime . datetime ( bl_int [ 0 ], bl_int [ 1 ], bl_int [ 2 ], bl_int [ 3 ], bl_int [ 4 ], bl_int [ 5 ], ), ] ) if len ( bl ) >= 1 : columns = [ col , col + \"_ts\" ] # output_df = spark.createDataFrame(spark.parallelize(bl),columns) output_df = spark . createDataFrame ( pd . DataFrame ( bl , columns = columns )) else : return idf elif trans_cat in [ \"string_c\" , \"int_c\" ]: if int ( val_unique_cat ) == 4 : output_df = idf . select ( col ) . withColumn ( col + \"_ts\" , F . col ( col ) . cast ( \"string\" ) . cast ( \"date\" ) ) elif int ( val_unique_cat ) == 6 : output_df = ( idf . select ( col ) . withColumn ( col , F . concat ( col , F . lit ( \"01\" ))) . withColumn ( col + \"_ts\" , F . to_date ( col , \"yyyyMMdd\" )) ) elif int ( val_unique_cat ) == 8 : f = ( idf . select ( F . max ( F . substring ( col , 1 , 4 )), F . max ( F . substring ( col , 5 , 2 )), F . max ( F . substring ( col , 7 , 2 )), ) . rdd . flatMap ( lambda x : x ) . collect () ) if int ( f [ 1 ]) > 12 : frmt = \"yyyyddMM\" elif int ( f [ 2 ]) > 12 : frmt = \"yyyyMMdd\" elif ( ( int ( f [ 0 ]) > 1970 & int ( f [ 0 ]) < 2049 ) & ( int ( f [ 1 ]) > 0 & int ( f [ 1 ]) <= 12 ) & ( int ( f [ 2 ]) > 0 & int ( f [ 2 ]) <= 31 ) ): frmt = \"yyyyMMdd\" elif ( ( int ( f [ 0 ]) > 1970 & int ( f [ 0 ]) < 2049 ) & ( int ( f [ 1 ]) > 0 & int ( f [ 1 ]) <= 31 ) & ( int ( f [ 2 ]) > 0 & int ( f [ 2 ]) <= 12 ) ): frmt = \"yyyyddMM\" else : return idf output_df = idf . select ( F . col ( col ) . cast ( \"string\" )) . withColumn ( col + \"_ts\" , F . to_date ( col , frmt ) ) else : return idf else : return idf # if ((output_df.where(F.col(col + \"_ts\").isNull()).count()) / output_df.count()) > 0.9: # return idf # else: # pass if output_mode == \"replace\" : output_df = ( idf . join ( output_df , col , \"left_outer\" ) . drop ( col ) . withColumnRenamed ( col + \"_ts\" , col ) . orderBy ( id_col , col ) ) elif output_mode == \"append\" : output_df = idf . join ( output_df , col , \"left_outer\" ) . orderBy ( id_col , col + \"_ts\" ) else : return \"Incorrect Output Mode Selected\" if save_output : output_df . write . parquet ( save_output , mode = \"overwrite\" ) else : return output_df def ts_loop_cols_pre ( idf , id_col ): \"\"\" This function helps to analyze the potential columns which can be passed for tiime series check. The columns are passed on to the auto-detection block. Parameters ---------- idf Input dataframe id_col ID Column Returns ------- Three lists \"\"\" lc1 , lc2 , lc3 = [], [], [] for i in idf . dtypes : try : col_len = ( idf . select ( F . max ( F . length ( i [ 0 ]))) . rdd . flatMap ( lambda x : x ) . collect ()[ 0 ] ) except : col_len = 0 if idf . select ( i [ 0 ]) . dropna () . distinct () . count () == 0 : lc1 . append ( i [ 0 ]) lc2 . append ( \"NA\" ) lc3 . append ( col_len ) elif ( ( i [ 0 ] != id_col ) & ( idf . select ( F . length ( i [ 0 ])) . distinct () . count () == 1 ) & ( col_len in [ 4 , 6 , 8 , 10 , 13 ]) ): if i [ 1 ] == \"string\" : lc1 . append ( i [ 0 ]) lc2 . append ( \"string_c\" ) lc3 . append ( col_len ) elif i [ 1 ] == \"long\" : lc1 . append ( i [ 0 ]) lc2 . append ( \"long_c\" ) lc3 . append ( col_len ) elif i [ 1 ] == \"bigint\" : lc1 . append ( i [ 0 ]) lc2 . append ( \"bigint_c\" ) lc3 . append ( col_len ) elif i [ 1 ] == \"int\" : lc1 . append ( i [ 0 ]) lc2 . append ( \"int_c\" ) lc3 . append ( col_len ) elif ( i [ 0 ] != id_col ) & ( i [ 1 ] in [ \"string\" , \"object\" ]): lc1 . append ( i [ 0 ]) lc2 . append ( \"string\" ) lc3 . append ( col_len ) elif ( i [ 0 ] != id_col ) & ( i [ 1 ] in [ \"timestamp\" , \"date\" ]): lc1 . append ( i [ 0 ]) lc2 . append ( \"dt\" ) lc3 . append ( col_len ) else : lc1 . append ( i [ 0 ]) lc2 . append ( \"NA\" ) lc3 . append ( col_len ) return lc1 , lc2 , lc3 def ts_preprocess ( spark , idf , id_col , output_path , tz_offset = \"local\" , run_type = \"local\" ): \"\"\" This function helps to read the input spark dataframe as source and do all the necessary processing. All the intermediate data created through this step foro the Time Series Analyzer. Parameters ---------- spark Spark session idf Input dataframe id_col ID Column output_path Output path where the data would be saved tz_offset Timezone offset (Option to chose between options like Local, GMT, UTC, etc.). Default option is set as \"Local\". run_type Option to choose between run type \"Local\" or \"EMR\" or \"Azure\" basis the user flexibility. Default option is set as \"Local\". Returns ------- DataFrame,Output[CSV] \"\"\" if run_type == \"local\" : local_path = output_path elif run_type == \"databricks\" : local_path = output_to_local ( output_path ) elif run_type == \"emr\" : local_path = \"report_stats\" else : raise ValueError ( \"Invalid run_type\" ) Path ( local_path ) . mkdir ( parents = True , exist_ok = True ) num_cols , cat_cols , other_cols = attributeType_segregation ( idf ) ts_loop_col_dtls = ts_loop_cols_pre ( idf , id_col ) l1 , l2 = [], [] for index , i in enumerate ( ts_loop_col_dtls [ 1 ]): if i != \"NA\" : l1 . append ( index ) if i == \"dt\" : l2 . append ( index ) ts_loop_cols = [ ts_loop_col_dtls [ 0 ][ i ] for i in l1 ] pre_exist_ts_cols = [ ts_loop_col_dtls [ 0 ][ i ] for i in l2 ] for i in ts_loop_cols : try : idx = ts_loop_col_dtls [ 0 ] . index ( i ) val_unique_cat = ts_loop_col_dtls [ 2 ][ idx ] trans_cat = ts_loop_col_dtls [ 1 ][ idx ] idf = regex_date_time_parser ( spark , idf , id_col , i , tz_offset , val_unique_cat , trans_cat , save_output = None , output_mode = \"replace\" , ) idf . persist ( pyspark . StorageLevel . MEMORY_AND_DISK ) except : pass odf = idf . distinct () ts_loop_cols_post = [ x [ 0 ] for x in idf . dtypes if x [ 1 ] in [ \"timestamp\" , \"date\" ]] num_cols = [ x for x in num_cols if x not in [ id_col ] + ts_loop_cols_post ] cat_cols = [ x for x in cat_cols if x not in [ id_col ] + ts_loop_cols_post ] c1 = ts_loop_cols c2 = list ( set ( ts_loop_cols_post ) - set ( pre_exist_ts_cols )) c3 = pre_exist_ts_cols c4 = ts_loop_cols_post f = pd . DataFrame ( [ [ \",\" . join ( idf . columns )], [ \",\" . join ( c1 )], [ \",\" . join ( c2 )], [ \",\" . join ( c3 )], [ \",\" . join ( c4 )], [ \",\" . join ( num_cols )], [ \",\" . join ( cat_cols )], ], columns = [ \"cols\" ], ) f . to_csv ( ends_with ( local_path ) + \"ts_cols_stats.csv\" , index = False ) if run_type == \"emr\" : bash_cmd = ( \"aws s3 cp --recursive \" + ends_with ( local_path ) + \" \" + ends_with ( output_path ) ) output = subprocess . check_output ([ \"bash\" , \"-c\" , bash_cmd ]) return odf Functions def regex_date_time_parser ( spark, idf, id_col, col, tz, val_unique_cat, trans_cat, save_output=None, output_mode='replace') This function helps to produce the transformed output (if applicable) based on the auto-detection of timestamp / date type. The output from this function is decoupled as a part of ingestion. Parameters spark Spark session idf Input dataframe id_col ID Column col Column passed for Auto detection of Timestamp / date type tz Timezone offset (Option to chose between options like Local, GMT, UTC). Default option is set as \"Local\". val_unique_cat Maximum character length of the field. trans_cat Custom data type basis which further processing will be conditioned. save_output Output path where the transformed ddata can be saved output_mode Option to choose between Append or Replace. If the option Append is selected, the column names are Appended by \"_ts\" else it's replaced by the original column name Returns DataFrame Expand source code def regex_date_time_parser ( spark , idf , id_col , col , tz , val_unique_cat , trans_cat , save_output = None , output_mode = \"replace\" , ): \"\"\" This function helps to produce the transformed output (if applicable) based on the auto-detection of timestamp / date type. The output from this function is decoupled as a part of ingestion. Parameters ---------- spark Spark session idf Input dataframe id_col ID Column col Column passed for Auto detection of Timestamp / date type tz Timezone offset (Option to chose between options like Local, GMT, UTC). Default option is set as \"Local\". val_unique_cat Maximum character length of the field. trans_cat Custom data type basis which further processing will be conditioned. save_output Output path where the transformed ddata can be saved output_mode Option to choose between Append or Replace. If the option Append is selected, the column names are Appended by \"_ts\" else it's replaced by the original column name Returns ------- DataFrame \"\"\" REGEX_PARTS = { \"Y\" : r \"(?:19[4-9]\\d|20[0-3]\\d)\" , # 1940 to 2039 \"y\" : r \"(?:\\d\\d)\" , # 00 to 99 \"m\" : r \"(?:1[012]|0?[1-9])\" , # 0?1 to 12 \"mz\" : r \"(?:1[012]|0[1-9])\" , # 01 to 12 \"B\" : r \"(?:\" r \"D?JAN(?:UAR[IY])?|\" r \"[FP]EB(?:RUAR[IY])?|\" r \"MAC|MAR(?:CH|ET)?|MRT|\" r \"APR(?:IL)?|\" r \"M[EA]I|MAY|\" r \"JUNE?|D?JUNI?|\" r \"JUL(?:Y|AI)?|D?JULI?|\" r \"OG(?:OS)?|AUG(?:UST)?|AGT?(?:USTUS)?|\" r \"SEP(?:T(?:EMBER)?)?|\" r \"O[KC]T(?:OBER)?|\" r \"NO[VP](?:EMBER)?|\" r \"D[EI][SC](?:EMBER)?\" r \")\" , \"d\" : r \"(?:3[01]|[12]\\d|0?[1-9])\" , # 0?1 to 31 \"d_range\" : r \"(?:3[01]|[12]\\d|0?[1-9])(?: ?[-] ?(?:3[01]|[12]\\d|0?[1-9]))?\" , # 14-15 \"dz\" : r \"(?:3[01]|[12]\\d|0[1-9])\" , # 01 to 31 \"j\" : r \"(?:36[0-6]|3[0-5]\\d|[12]\\d\\d|0?[1-9]\\d|0?0?[1-9])\" , # 0?0?1 to 366 \"H\" : r \"(?:2[0-4]|[01]?\\d)\" , # 0?0 to 24 \"HZ\" : r \"(?:2[0-4]|[01]\\d)\" , # 0?0 to 24 \"I\" : r \"(?:1[012]|0?[1-9])\" , # 0?1 to 12 \"M\" : r \"(?:[1-5]\\d|0\\d)\" , # 00 to 59 \"S\" : r \"(?:6[01]|[0-5]\\d)\" , # 00 to 61 (leap second) \"p\" : r \"(?:MIDNI(?:GHT|TE)|AFTERNOON|MORNING|NOON|[MN]N|H(?:(?:OU)?RS?)?|[AP]\\.? ?M\\.?)\" , \"p2\" : r \"(?:MIDNI(?:GHT|TE)|NOON|[AP]\\.? ?M\\.?)\" , \"Z\" : r \"(?:A(?:C(?:DT|ST|T|WST)|DT|E(?:DT|ST)|FT|K(?:DT|ST)|M(?:ST|T)|RT|ST|WST\" r \"|Z(?:O(?:ST|T)|T))|B(?:DT|I(?:OT|T)|OT|R(?:ST|T)|ST|TT)|C(?:AT|CT|DT|E(\" r \"?:ST|T)|H(?:A(?:DT|ST)|O(?:ST|T)|ST|UT)|I(?:ST|T)|KT|L(?:ST|T)|O(?:ST|T\" r \")|ST|T|VT|WST|XT)|D(?:AVT|DUT|FT)|E(?:A(?:S(?:ST|T)|T)|CT|DT|E(?:ST|T)|\" r \"G(?:ST|T)|IT|ST)|F(?:ET|JT|K(?:ST|T)|NT)|G(?:A(?:LT|MT)|ET|FT|I(?:LT|T)\" r \"|MT|ST|YT)|H(?:AEC|DT|KT|MT|OV(?:ST|T)|ST)|I(?:CT|D(?:LW|T)|OT|R(?:DT|K\" r \"T|ST)|ST)|JST|K(?:ALT|GT|OST|RAT|ST)|L(?:HST|INT)|M(?:A(?:GT|RT|WT)|DT|\" r \"E(?:ST|T)|HT|I(?:ST|T)|MT|S(?:K|T)|UT|VT|YT)|N(?:CT|DT|FT|PT|ST|T|UT|Z(\" r \"?:DT|ST))|O(?:MST|RAT)|P(?:DT|ET(?:T)?|GT|H(?:OT|T)|KT|M(?:DT|ST)|ONT|S\" r \"T|Y(?:ST|T))|R(?:ET|OTT)|S(?:A(?:KT|MT|ST)|BT|CT|DT|GT|LST|R(?:ET|T)|ST\" r \"|YOT)|T(?:AHT|FT|HA|JT|KT|LT|MT|OT|RT|VT)|U(?:LA(?:ST|T)|TC|Y(?:ST|T)|Z\" r \"T)|V(?:ET|LAT|O(?:LT|ST)|UT)|W(?:A(?:KT|ST|T)|E(?:ST|T)|IT|ST)|Y(?:AKT|\" r \"EKT))\" , # FROM: en.wikipedia.org/wiki/List_of_time_zone_abbreviations \"z\" : r \"(?:[+-](?:0\\d|1[0-4]):?(?:00|15|30|45))\" , # [+-] 00:00 to 14:45 \"A\" : r \"(?:\" r \"MON(?:DAY)?|(?:IS|SE)N(?:[IE]N)?|\" r \"TUE(?:S(?:DAY)?)?|SEL(?:ASA)?|\" r \"WED(?:NESDAY)?|RABU?|\" r \"THU(?:RS(?:DAY)?)?|KH?A(?:M(?:IS)?)?|\" r \"FRI(?:DAY)?|JUM(?:[AM]A?T)?|\" r \"SAT(?:URDAY)?|SAB(?:TU)?|\" r \"SUN(?:DAY)?|AHA?D|MIN(?:GGU)?\" r \")\" , \"th\" : r \"(?:ST|ND|RD|TH|\u00ba)\" , } REGEX_PATTERNS_PARSERS = { # 14/8/1991 \"dd_mm_YYYY_1\" : r \"(?: {d} / {m} / {Y} )\" , \"dd_d2\" : r \"(?: {d} \\\\ {m} \\\\ {Y} )\" , \"dd_mm_YYYY_3\" : r \"(?: {d} [-] {m} [-] {Y} )\" , \"dd_mm_YYYY_4\" : r \"(?: {d} \\. {m} \\. {Y} )\" , # 'dd_mm_YYYY_5': r\"(?:{d}{m}{Y})\", # too many phone numbers \"dd_mm_YYYY_6\" : r \"(?: {d} ? {m} ? {Y} )\" , \"dd_mm_YYYY_7\" : r \"(?: {dz}{mz}{Y} )\" , # 14/8/91 \"dd_mm_yy_1\" : r \"(?: {d} / {m} / {y} )\" , \"dd_mm_yy_2\" : r \"(?: {d} \\\\ {m} \\\\ {y} )\" , \"dd_mm_yy_3\" : r \"(?: {d} [-] {m} [-] {y} )\" , \"dd_mm_yy_4\" : r \"(?: {d} \\. {m} \\. {y} )\" , # 'dd_mm_yy_5': r\"(?:{dz}{mz}{y})\", # too many phone numbers # 14 Aug, 1991 \"dd_mmm_YYYY_1\" : r \"(?: {d}{th} ? ?/ ? {B} ?/ ? {Y} )\" , \"dd_mmm_YYYY_2\" : r \"(?: {d}{th} ? ? \\\\ ? {B} ? \\\\ ? {Y} )\" , \"dd_mmm_YYYY_3\" : r \"(?: {d}{th} ? ?[-] ? {B} ?[ -] ? {Y} )\" , \"dd_mmm_YYYY_4\" : r \"(?: {d}{th} ? ?[ -]? ? {B} ?,? ? {Y} )\" , \"dd_mmm_YYYY_5\" : r \"(?: {d}{th} ? ?\\. ? {B} ?\\. ? {Y} )\" , # 14 Aug '91 \"dd_mmm_yy_1\" : r \"(?: {d}{th} ? ?/ ? {B} ?/ ?'? {y} )\" , \"dd_mmm_yy_2\" : r \"(?: {d}{th} ? ? \\\\ ? {B} ? \\\\ ?'? {y} )\" , \"dd_mmm_yy_3\" : r \"(?: {d}{th} ? ?[-] ? {B} ?[-] ?'? {y} )\" , \"dd_mmm_yy_4\" : r \"(?: {d}{th} ? ?[ -]? ? {B} ?,? ?'? {y} )\" , \"dd_mmm_yy_5\" : r \"(?: {d}{th} ? ?\\. ? {B} ?\\. ?'? {y} )\" , # 14th Aug \"dd_mmm\" : r \"(?: {d}{th} ? ?[/ \\\\ . -] ? {B} )\" , # 08/14/1991 # WARNING! dateutil set to day first \"mm_dd_YYYY_1\" : r \"(?: {m} / {d} / {Y} )\" , \"mm_dd_YYYY_2\" : r \"(?: {m} \\\\ {d} \\\\ {Y} )\" , \"mm_dd_YYYY_3\" : r \"(?: {m} [-] {d} [-] {Y} )\" , \"mm_dd_YYYY_4\" : r \"(?: {m} {d} {Y} )\" , \"mm_dd_YYYY_5\" : r \"(?: {m} \\. {d} \\. {Y} )\" , \"mm_dd_YYYY_6\" : r \"(?: {mz}{dz}{Y} )\" , # 8/14/91 # WARNING! dateutil set to day first \"mm_dd_yy_1\" : r \"(?: {m} / {d} / {y} )\" , \"mm_dd_yy_2\" : r \"(?: {m} \\\\ {d} \\\\ {y} )\" , \"mm_dd_yy_3\" : r \"(?: {m} [-] {d} [-] {y} )\" , \"mm_dd_yy_4\" : r \"(?: {m} \\. {d} \\. {y} )\" , # 'mm_dd_yy_5': r\"(?:{mz}{dz}{y})\", # too many phone numbers # Aug 14th, 1991 \"mmm_dd_YYYY_1\" : r \"(?: {B} ?/ ? {d}{th} ? ?/ ? {Y} )\" , \"mmm_dd_YYYY_2\" : r \"(?: {B} ? \\\\ ? {d}{th} ? ? \\\\ ? {Y} )\" , \"mmm_dd_YYYY_3\" : r \"(?: {B} ?[-] ? {d}{th} ? ?[ -] ? {Y} )\" , \"mmm_dd_YYYY_4\" : r \"(?: {B} ?[ -]? ? {d}{th} ? ?, ? {Y} )\" , \"mmm_dd_YYYY_5\" : r \"(?: {B} ?\\. ? {d}{th} ? ?\\. ? {Y} )\" , # Aug-14 '91 \"mmm_dd_yy_1\" : r \"(?: {B} ?/ ? {d}{th} ? ?/ ?'? {y} )\" , \"mmm_dd_yy_2\" : r \"(?: {B} ? \\\\ ? {d}{th} ? ? \\\\ ?'? {y} )\" , \"mmm_dd_yy_3\" : r \"(?: {B} ?[-] ? {d}{th} ? ?[-] ?'? {y} )\" , \"mmm_dd_yy_4\" : r \"(?: {B} ?[. -]? ? {d}{th} ?, '? {y} )\" , \"mmm_dd_yy_5\" : r \"(?: {B} ?\\. ? {d}{th} ? ?\\. ?'? {y} )\" , # Aug-14 # WARNING! dateutil assumes current year \"mmm_dd\" : r \"(?: {B} ?[/ \\\\ . -] ? {d}{th} ?)\" , # # Aug-91 # 'mmm_yy': r\"(?:{B} ?[/\\\\. -] ?'{y})\", # too many false positives # August 1991 \"mmm_YYYY\" : r \"(?: {B} ?[/ \\\\ . -] ? {Y} )\" , # many non-useful dates # 1991-8-14 \"YYYY_mm_dd_1\" : r \"(?: {Y} / {m} / {d} )\" , \"YYYY_mm_dd_2\" : r \"(?: {Y} \\\\ {m} \\\\ {d} )\" , \"YYYY_mm_dd_3\" : r \"(?: {Y} [-] {m} [-] {d} )\" , \"YYYY_mm_dd_4\" : r \"(?: {Y} {m} {d} )\" , \"YYYY_mm_dd_5\" : r \"(?: {Y} \\. {m} \\. {d} )\" , \"YYYY_mm_dd_6\" : r \"(?: {Y}{mz}{dz} )\" , # 910814 (ISO 8601) # 'yy_mm_dd_1': r\"(?:{y} {m} {d})\", # too many random numbers \"yy_mm_dd_2\" : r \"(?: {y} / {m} / {d} )\" , \"yy_mm_dd_3\" : r \"(?: {y} \\\\ {m} \\\\ {d} )\" , \"yy_mm_dd_4\" : r \"(?: {y} [-] {m} [-] {d} )\" , \"yy_mm_dd_5\" : r \"(?: {y} \\. {m} \\. {d} )\" , # 'yy_mm_dd_6': r\"(?:{y}{mz}{dz})\", # too many phone numbers # 1991-Aug-14 \"YYYY_mmm_dd_1\" : r \"(?: {Y} ?/ ? {B} ?/ ? {d} )\" , \"YYYY_mmm_dd_2\" : r \"(?: {Y} ? \\\\ ? {B} ? \\\\ ? {d} )\" , \"YYYY_mmm_dd_3\" : r \"(?: {Y} ?[-] ? {B} ?[-] ? {d} )\" , \"YYYY_mmm_dd_4\" : r \"(?: {Y} ? {B} ?[ -]? ? {d}{th} ?)\" , # 91-Aug-14 \"yy_mmm_dd_1\" : r \"(?:'? {y} ?/ ? {B} ?/ ? {d} )\" , \"yy_mmm_dd_2\" : r \"(?:'? {y} ? \\\\ ? {B} ? \\\\ ? {d} )\" , \"yy_mmm_dd_3\" : r \"(?:'? {y} ?[-] ? {B} ?[-] ? {d} )\" , \"yy_mmm_dd_4\" : r \"(?:'? {y} ? {B} ?[ -]? ? {d}{th} ?)\" , # # 1991.226 (Aug 14 = day 226 in 1991) # dateutil fails # 'YYYY_ddd_1': r\"(?:{Y}\\.{j})\", # too many random numbers # 'YYYY_ddd_2': r\"(?:{Y}[-]{j})\", # too many random numbers # time \"HH_MM_SS\" : r \"(?: {H} : {M} : {S} (?: ? {p} )?(?: ?(?:Z| {Z} | {z} ))?)\" , \"HH_MZ_pp_1\" : r \"(?: {H} : {M} (?: ? {p} )?(?: ?(?:Z| {Z} | {z} ))?)\" , \"HH_MZ_pp_1b\" : r \"(?: {H} [:. ] {M} (?: ? {p} )(?: ?(?:Z| {Z} | {z} ))?)\" , \"HH_MZ_pp_2\" : r \"(?:(?<!\\.) {HZ} [. ]? {M} (?: ? {p} )(?: ?(?:Z| {Z} | {z} ))?)\" , \"HH_pp\" : r \"(?:(?<!\\.) {H} ? {p2} (?: ?(?:Z| {Z} | {z} ))?)\" , # # 910814094500 (9:45am) # 'yy_mm_dd_HH_MM_SS': r\"(?:{y}{mz}{dz}{H}{M}{S})\", # too many phone numbers # 1991-08-14T09:45:00Z \"YYYY_mm_dd_HH_MM\" : r \"(?: {Y} [-] {m} [-] {d} [T ] {H} : {M} (?: ?(?:Z| {Z} | {z} ))?)\" , \"YYYY_mm_dd_HH_MM_SS_1\" : r \"(?: {Y} [-] {m} [-] {d} [T ] {H} : {M} : {S} (?: ?(?:Z| {Z} | {z} ))?)\" , \"YYYY_mm_dd_HH_MM_SS_2\" : r \"(?: {Y}{mz}{d} T? {H}{M}{S} (?: ?(?:Z| {Z} | {z} ))?)\" , \"YYYY_dd_mm_HH_MM_SS_3\" : r \"(?: {Y} [-] {d} [-] {m} [T ] {H} : {M} : {S} (?: ?(?:Z| {Z} | {z} ))?)\" , \"mm_dd_YYYY_HH_MM_SS_1\" : r \"(?: {m} [-] {d} [-] {Y} [T ] {H} : {M} : {S} (?: ?(?:Z| {Z} | {z} ))?)\" , \"dd_mm_YYYY_HH_MM_SS_1\" : r \"(?: {d} [-] {m} [-] {Y} [T ] {H} : {M} : {S} (?: ?(?:Z| {Z} | {z} ))?)\" , # # standalone # 'day': r\"{A}\", # too many false positives # 'month': r\"{B}\", # too many false positives # 'year': r\"{Y}\", # too many random numbers # 'timezone': r\"(?:Z|{Z}|{z})\", # too many malay words } # unicode fixes REGEX_FORMATTED = { label : \" \\\\ b\" + pattern . format ( ** REGEX_PARTS ) # fill in the chunks . replace ( \"-]\" , \" \\u2009\\u2010\\u2011\\u2012\\u2013\\u2014 -]\" ) # unicode dashes . replace ( \"'?\" , \"[' \\u2018\\u2019 ]?\" ) # unicode quotes + \" \\\\ b\" for label , pattern in REGEX_PATTERNS_PARSERS . items () } # match emails and urls to avoid returning chunks of them REGEX_FORMATTED [ \"eml\" ] = r \"\"\"[a-zA-Z0-9][^\\s`!@%$^= {} \\[\\]/\\\\\"',()<>:;]+(?:@|%40|\\s+at\\s+|\\s*<\\s*at\\s*>\\s*)[a-zA-Z0-9][-_a-zA-Z0-9~.]+\\.[a-zA-Z]{2,15}\"\"\" REGEX_FORMATTED [ \"url\" ] = r \"\\b(?:(?:https?|ftp|file)://|www\\d?\\.|ftp\\.)[-A-Z0-9+&@#/%=~_|$?!:,.]*[A-Z0-9+&@#/%=~_|$]\" REGEX_FORMATTED [ \"dot\" ] = r \"(?:\\d+\\.){3,}\\d+\" # compile all the regex patterns REGEX_COMPILED = { label : re . compile ( pattern , flags = re . I | re . U ) for label , pattern in REGEX_FORMATTED . items () } if trans_cat == \"dt\" : return idf elif ( trans_cat in [ \"long_c\" , \"bigint_c\" , \"int_c\" ]) & ( int ( val_unique_cat ) in [ 10 , 13 ] ): if int ( val_unique_cat ) == 10 : precision = \"s\" elif int ( val_unique_cat ) == 13 : precision = \"ms\" else : precision = \"ms\" output_df = unix_to_timestamp ( spark , idf , col , precision = precision , tz = tz , output_mode = output_mode ) . orderBy ( id_col , col ) if save_output is not None : output_df . write . parquet ( save_output , mode = \"overwrite\" ) else : return output_df elif trans_cat == \"string\" : list_dates = list ( set ( idf . select ( col ) . rdd . flatMap ( lambda x : x ) . collect ())) def regex_text ( text , longest = True , context_max_len = 999 , dayfirst = False ): # join multiple spaces, convert tabs, strip leading/trailing whitespace if isinstance ( text , str ): pass else : raise ValueError ( \"Incompatible Column Type!!\" ) text = \" \" . join ( text . split ()) matches = [] for regex_label , regex_obj in REGEX_COMPILED . items (): for m in regex_obj . finditer ( text ): context_start = max ( 0 , ( m . start () + m . end () - context_max_len ) // 2 ) context_end = min ( len ( text ), context_start + context_max_len ) context_str = text [ context_start : context_end ] if context_start != 0 : context_str = \" \\u2026 \" + context_str [ 1 :] if context_end != len ( text ): context_str = ( context_str [: - 1 ] + \" \\u2026 \" ) # this is the `...` character parsed_date = None try : with warnings . catch_warnings (): warnings . simplefilter ( \"ignore\" , category = dateutil . parser . UnknownTimezoneWarning , ) if \"HH\" in regex_label : if \"dd\" in regex_label or \"YYYY\" in regex_label : matched_text = re . sub ( r \"[ \\\\ ]\" , \"/\" , m . group ()) parsed_date = dateutil . parser . parse ( matched_text , dayfirst = dayfirst ) else : matched_text = re . sub ( r \"H(?:(?:OU)?RS?)?\" , \"\" , m . group (), flags = re . I ) matched_text = re . sub ( r \"MN\" , r \"AM\" , matched_text , flags = re . I ) matched_text = re . sub ( r \"NN\" , r \"PM\" , matched_text , flags = re . I ) matched_text = re . sub ( r \"(\\d)[. ](\\d)\" , r \"\\1:\\2\" , matched_text ) matched_text = f \"1970-01-01 { matched_text } \" parsed_date = dateutil . parser . parse ( matched_text , dayfirst = dayfirst ) elif \"dd\" in regex_label or \"YYYY\" in regex_label : matched_text = re . sub ( r \"[ \\\\ ]\" , \"/\" , m . group ()) parsed_date = dateutil . parser . parse ( matched_text , dayfirst = dayfirst ) except ValueError : pass matches . append ( { \"REGEX_LABEL\" : regex_label , \"MATCH\" : m . group (), \"START\" : m . start (), \"END\" : m . end (), \"MATCH_LEN\" : m . end () - m . start (), \"NORM_TEXT_LEN\" : len ( text ), \"CONTEXT\" : context_str , \"PARSED\" : parsed_date , } ) # narrow to longest match for match in matches : if not longest or all ( ( other [ \"START\" ] >= match [ \"START\" ] and other [ \"END\" ] <= match [ \"END\" ]) or other [ \"START\" ] > match [ \"END\" ] or other [ \"END\" ] < match [ \"START\" ] for other in matches ): # don't return emails or urls if match [ \"REGEX_LABEL\" ] not in { \"eml\" , \"url\" , \"dot\" }: yield match bl = [] file_lines = list_dates for line_num , line in enumerate ( file_lines ): bl_int = [] for match_info in regex_text ( line ): try : ye , mo , da , ho , mi , se = ( match_info [ \"PARSED\" ] . year , match_info [ \"PARSED\" ] . month , match_info [ \"PARSED\" ] . day , match_info [ \"PARSED\" ] . hour , match_info [ \"PARSED\" ] . minute , match_info [ \"PARSED\" ] . second , ) if len ( bl_int ) == 0 : bl_int = [ ye , mo , da , ho , mi , se ] else : if ye == 1970 and mo == 1 and da == 1 : pass if ho + mi + se == 0 : pass if ye > 1970 : bl_int [ 0 ] = ye if mo > 0 and ye != 1970 : bl_int [ 1 ] = mo if da > 0 and ye != 1970 : bl_int [ 2 ] = da if ho > 0 : bl_int [ 3 ] = ho if mi > 0 : bl_int [ 4 ] = mi if se > 0 : bl_int [ 5 ] = se else : pass except : pass bl . append ( [ match_info [ \"CONTEXT\" ], datetime . datetime ( bl_int [ 0 ], bl_int [ 1 ], bl_int [ 2 ], bl_int [ 3 ], bl_int [ 4 ], bl_int [ 5 ], ), ] ) if len ( bl ) >= 1 : columns = [ col , col + \"_ts\" ] # output_df = spark.createDataFrame(spark.parallelize(bl),columns) output_df = spark . createDataFrame ( pd . DataFrame ( bl , columns = columns )) else : return idf elif trans_cat in [ \"string_c\" , \"int_c\" ]: if int ( val_unique_cat ) == 4 : output_df = idf . select ( col ) . withColumn ( col + \"_ts\" , F . col ( col ) . cast ( \"string\" ) . cast ( \"date\" ) ) elif int ( val_unique_cat ) == 6 : output_df = ( idf . select ( col ) . withColumn ( col , F . concat ( col , F . lit ( \"01\" ))) . withColumn ( col + \"_ts\" , F . to_date ( col , \"yyyyMMdd\" )) ) elif int ( val_unique_cat ) == 8 : f = ( idf . select ( F . max ( F . substring ( col , 1 , 4 )), F . max ( F . substring ( col , 5 , 2 )), F . max ( F . substring ( col , 7 , 2 )), ) . rdd . flatMap ( lambda x : x ) . collect () ) if int ( f [ 1 ]) > 12 : frmt = \"yyyyddMM\" elif int ( f [ 2 ]) > 12 : frmt = \"yyyyMMdd\" elif ( ( int ( f [ 0 ]) > 1970 & int ( f [ 0 ]) < 2049 ) & ( int ( f [ 1 ]) > 0 & int ( f [ 1 ]) <= 12 ) & ( int ( f [ 2 ]) > 0 & int ( f [ 2 ]) <= 31 ) ): frmt = \"yyyyMMdd\" elif ( ( int ( f [ 0 ]) > 1970 & int ( f [ 0 ]) < 2049 ) & ( int ( f [ 1 ]) > 0 & int ( f [ 1 ]) <= 31 ) & ( int ( f [ 2 ]) > 0 & int ( f [ 2 ]) <= 12 ) ): frmt = \"yyyyddMM\" else : return idf output_df = idf . select ( F . col ( col ) . cast ( \"string\" )) . withColumn ( col + \"_ts\" , F . to_date ( col , frmt ) ) else : return idf else : return idf # if ((output_df.where(F.col(col + \"_ts\").isNull()).count()) / output_df.count()) > 0.9: # return idf # else: # pass if output_mode == \"replace\" : output_df = ( idf . join ( output_df , col , \"left_outer\" ) . drop ( col ) . withColumnRenamed ( col + \"_ts\" , col ) . orderBy ( id_col , col ) ) elif output_mode == \"append\" : output_df = idf . join ( output_df , col , \"left_outer\" ) . orderBy ( id_col , col + \"_ts\" ) else : return \"Incorrect Output Mode Selected\" if save_output : output_df . write . parquet ( save_output , mode = \"overwrite\" ) else : return output_df def ts_loop_cols_pre ( idf, id_col) This function helps to analyze the potential columns which can be passed for tiime series check. The columns are passed on to the auto-detection block. Parameters idf Input dataframe id_col ID Column Returns Three lists Expand source code def ts_loop_cols_pre ( idf , id_col ): \"\"\" This function helps to analyze the potential columns which can be passed for tiime series check. The columns are passed on to the auto-detection block. Parameters ---------- idf Input dataframe id_col ID Column Returns ------- Three lists \"\"\" lc1 , lc2 , lc3 = [], [], [] for i in idf . dtypes : try : col_len = ( idf . select ( F . max ( F . length ( i [ 0 ]))) . rdd . flatMap ( lambda x : x ) . collect ()[ 0 ] ) except : col_len = 0 if idf . select ( i [ 0 ]) . dropna () . distinct () . count () == 0 : lc1 . append ( i [ 0 ]) lc2 . append ( \"NA\" ) lc3 . append ( col_len ) elif ( ( i [ 0 ] != id_col ) & ( idf . select ( F . length ( i [ 0 ])) . distinct () . count () == 1 ) & ( col_len in [ 4 , 6 , 8 , 10 , 13 ]) ): if i [ 1 ] == \"string\" : lc1 . append ( i [ 0 ]) lc2 . append ( \"string_c\" ) lc3 . append ( col_len ) elif i [ 1 ] == \"long\" : lc1 . append ( i [ 0 ]) lc2 . append ( \"long_c\" ) lc3 . append ( col_len ) elif i [ 1 ] == \"bigint\" : lc1 . append ( i [ 0 ]) lc2 . append ( \"bigint_c\" ) lc3 . append ( col_len ) elif i [ 1 ] == \"int\" : lc1 . append ( i [ 0 ]) lc2 . append ( \"int_c\" ) lc3 . append ( col_len ) elif ( i [ 0 ] != id_col ) & ( i [ 1 ] in [ \"string\" , \"object\" ]): lc1 . append ( i [ 0 ]) lc2 . append ( \"string\" ) lc3 . append ( col_len ) elif ( i [ 0 ] != id_col ) & ( i [ 1 ] in [ \"timestamp\" , \"date\" ]): lc1 . append ( i [ 0 ]) lc2 . append ( \"dt\" ) lc3 . append ( col_len ) else : lc1 . append ( i [ 0 ]) lc2 . append ( \"NA\" ) lc3 . append ( col_len ) return lc1 , lc2 , lc3 def ts_preprocess ( spark, idf, id_col, output_path, tz_offset='local', run_type='local') This function helps to read the input spark dataframe as source and do all the necessary processing. All the intermediate data created through this step foro the Time Series Analyzer. Parameters spark Spark session idf Input dataframe id_col ID Column output_path Output path where the data would be saved tz_offset Timezone offset (Option to chose between options like Local, GMT, UTC, etc.). Default option is set as \"Local\". run_type Option to choose between run type \"Local\" or \"EMR\" or \"Azure\" basis the user flexibility. Default option is set as \"Local\". Returns DataFrame,Output[CSV] Expand source code def ts_preprocess ( spark , idf , id_col , output_path , tz_offset = \"local\" , run_type = \"local\" ): \"\"\" This function helps to read the input spark dataframe as source and do all the necessary processing. All the intermediate data created through this step foro the Time Series Analyzer. Parameters ---------- spark Spark session idf Input dataframe id_col ID Column output_path Output path where the data would be saved tz_offset Timezone offset (Option to chose between options like Local, GMT, UTC, etc.). Default option is set as \"Local\". run_type Option to choose between run type \"Local\" or \"EMR\" or \"Azure\" basis the user flexibility. Default option is set as \"Local\". Returns ------- DataFrame,Output[CSV] \"\"\" if run_type == \"local\" : local_path = output_path elif run_type == \"databricks\" : local_path = output_to_local ( output_path ) elif run_type == \"emr\" : local_path = \"report_stats\" else : raise ValueError ( \"Invalid run_type\" ) Path ( local_path ) . mkdir ( parents = True , exist_ok = True ) num_cols , cat_cols , other_cols = attributeType_segregation ( idf ) ts_loop_col_dtls = ts_loop_cols_pre ( idf , id_col ) l1 , l2 = [], [] for index , i in enumerate ( ts_loop_col_dtls [ 1 ]): if i != \"NA\" : l1 . append ( index ) if i == \"dt\" : l2 . append ( index ) ts_loop_cols = [ ts_loop_col_dtls [ 0 ][ i ] for i in l1 ] pre_exist_ts_cols = [ ts_loop_col_dtls [ 0 ][ i ] for i in l2 ] for i in ts_loop_cols : try : idx = ts_loop_col_dtls [ 0 ] . index ( i ) val_unique_cat = ts_loop_col_dtls [ 2 ][ idx ] trans_cat = ts_loop_col_dtls [ 1 ][ idx ] idf = regex_date_time_parser ( spark , idf , id_col , i , tz_offset , val_unique_cat , trans_cat , save_output = None , output_mode = \"replace\" , ) idf . persist ( pyspark . StorageLevel . MEMORY_AND_DISK ) except : pass odf = idf . distinct () ts_loop_cols_post = [ x [ 0 ] for x in idf . dtypes if x [ 1 ] in [ \"timestamp\" , \"date\" ]] num_cols = [ x for x in num_cols if x not in [ id_col ] + ts_loop_cols_post ] cat_cols = [ x for x in cat_cols if x not in [ id_col ] + ts_loop_cols_post ] c1 = ts_loop_cols c2 = list ( set ( ts_loop_cols_post ) - set ( pre_exist_ts_cols )) c3 = pre_exist_ts_cols c4 = ts_loop_cols_post f = pd . DataFrame ( [ [ \",\" . join ( idf . columns )], [ \",\" . join ( c1 )], [ \",\" . join ( c2 )], [ \",\" . join ( c3 )], [ \",\" . join ( c4 )], [ \",\" . join ( num_cols )], [ \",\" . join ( cat_cols )], ], columns = [ \"cols\" ], ) f . to_csv ( ends_with ( local_path ) + \"ts_cols_stats.csv\" , index = False ) if run_type == \"emr\" : bash_cmd = ( \"aws s3 cp --recursive \" + ends_with ( local_path ) + \" \" + ends_with ( output_path ) ) output = subprocess . check_output ([ \"bash\" , \"-c\" , bash_cmd ]) return odf","title":"<code>ts_auto_detection</code>"},{"location":"api/data_ingest/ts_auto_detection.html#ts_auto_detection","text":"This module help produce the output containing a transformation through auto timestamp / date detection by reading the ingested dataframe from source. As a part of generation of the auto detection output, there are various functions created such as - regex_date_time_parser ts_loop_cols_pre ts_preprocess Respective functions have sections containing the detailed definition of the parameters used for computing. Expand source code # coding=utf-8 \"\"\"This module help produce the output containing a transformation through auto timestamp / date detection by reading the ingested dataframe from source. As a part of generation of the auto detection output, there are various functions created such as - - regex_date_time_parser - ts_loop_cols_pre - ts_preprocess Respective functions have sections containing the detailed definition of the parameters used for computing. \"\"\" import pyspark import datetime import csv import io import os import re import warnings import subprocess from pyspark.sql import functions as F from pyspark.sql import types as T from pyspark.sql import Window from loguru import logger import calendar from anovos.shared.utils import attributeType_segregation , ends_with , output_to_local from anovos.data_analyzer.stats_generator import measures_of_percentiles from anovos.data_transformer.datetime import ( timeUnits_extraction , unix_to_timestamp , lagged_ts , ) from pathlib import Path import dateutil.parser import pandas as pd import numpy as np ###regex based ts parser function def regex_date_time_parser ( spark , idf , id_col , col , tz , val_unique_cat , trans_cat , save_output = None , output_mode = \"replace\" , ): \"\"\" This function helps to produce the transformed output (if applicable) based on the auto-detection of timestamp / date type. The output from this function is decoupled as a part of ingestion. Parameters ---------- spark Spark session idf Input dataframe id_col ID Column col Column passed for Auto detection of Timestamp / date type tz Timezone offset (Option to chose between options like Local, GMT, UTC). Default option is set as \"Local\". val_unique_cat Maximum character length of the field. trans_cat Custom data type basis which further processing will be conditioned. save_output Output path where the transformed ddata can be saved output_mode Option to choose between Append or Replace. If the option Append is selected, the column names are Appended by \"_ts\" else it's replaced by the original column name Returns ------- DataFrame \"\"\" REGEX_PARTS = { \"Y\" : r \"(?:19[4-9]\\d|20[0-3]\\d)\" , # 1940 to 2039 \"y\" : r \"(?:\\d\\d)\" , # 00 to 99 \"m\" : r \"(?:1[012]|0?[1-9])\" , # 0?1 to 12 \"mz\" : r \"(?:1[012]|0[1-9])\" , # 01 to 12 \"B\" : r \"(?:\" r \"D?JAN(?:UAR[IY])?|\" r \"[FP]EB(?:RUAR[IY])?|\" r \"MAC|MAR(?:CH|ET)?|MRT|\" r \"APR(?:IL)?|\" r \"M[EA]I|MAY|\" r \"JUNE?|D?JUNI?|\" r \"JUL(?:Y|AI)?|D?JULI?|\" r \"OG(?:OS)?|AUG(?:UST)?|AGT?(?:USTUS)?|\" r \"SEP(?:T(?:EMBER)?)?|\" r \"O[KC]T(?:OBER)?|\" r \"NO[VP](?:EMBER)?|\" r \"D[EI][SC](?:EMBER)?\" r \")\" , \"d\" : r \"(?:3[01]|[12]\\d|0?[1-9])\" , # 0?1 to 31 \"d_range\" : r \"(?:3[01]|[12]\\d|0?[1-9])(?: ?[-] ?(?:3[01]|[12]\\d|0?[1-9]))?\" , # 14-15 \"dz\" : r \"(?:3[01]|[12]\\d|0[1-9])\" , # 01 to 31 \"j\" : r \"(?:36[0-6]|3[0-5]\\d|[12]\\d\\d|0?[1-9]\\d|0?0?[1-9])\" , # 0?0?1 to 366 \"H\" : r \"(?:2[0-4]|[01]?\\d)\" , # 0?0 to 24 \"HZ\" : r \"(?:2[0-4]|[01]\\d)\" , # 0?0 to 24 \"I\" : r \"(?:1[012]|0?[1-9])\" , # 0?1 to 12 \"M\" : r \"(?:[1-5]\\d|0\\d)\" , # 00 to 59 \"S\" : r \"(?:6[01]|[0-5]\\d)\" , # 00 to 61 (leap second) \"p\" : r \"(?:MIDNI(?:GHT|TE)|AFTERNOON|MORNING|NOON|[MN]N|H(?:(?:OU)?RS?)?|[AP]\\.? ?M\\.?)\" , \"p2\" : r \"(?:MIDNI(?:GHT|TE)|NOON|[AP]\\.? ?M\\.?)\" , \"Z\" : r \"(?:A(?:C(?:DT|ST|T|WST)|DT|E(?:DT|ST)|FT|K(?:DT|ST)|M(?:ST|T)|RT|ST|WST\" r \"|Z(?:O(?:ST|T)|T))|B(?:DT|I(?:OT|T)|OT|R(?:ST|T)|ST|TT)|C(?:AT|CT|DT|E(\" r \"?:ST|T)|H(?:A(?:DT|ST)|O(?:ST|T)|ST|UT)|I(?:ST|T)|KT|L(?:ST|T)|O(?:ST|T\" r \")|ST|T|VT|WST|XT)|D(?:AVT|DUT|FT)|E(?:A(?:S(?:ST|T)|T)|CT|DT|E(?:ST|T)|\" r \"G(?:ST|T)|IT|ST)|F(?:ET|JT|K(?:ST|T)|NT)|G(?:A(?:LT|MT)|ET|FT|I(?:LT|T)\" r \"|MT|ST|YT)|H(?:AEC|DT|KT|MT|OV(?:ST|T)|ST)|I(?:CT|D(?:LW|T)|OT|R(?:DT|K\" r \"T|ST)|ST)|JST|K(?:ALT|GT|OST|RAT|ST)|L(?:HST|INT)|M(?:A(?:GT|RT|WT)|DT|\" r \"E(?:ST|T)|HT|I(?:ST|T)|MT|S(?:K|T)|UT|VT|YT)|N(?:CT|DT|FT|PT|ST|T|UT|Z(\" r \"?:DT|ST))|O(?:MST|RAT)|P(?:DT|ET(?:T)?|GT|H(?:OT|T)|KT|M(?:DT|ST)|ONT|S\" r \"T|Y(?:ST|T))|R(?:ET|OTT)|S(?:A(?:KT|MT|ST)|BT|CT|DT|GT|LST|R(?:ET|T)|ST\" r \"|YOT)|T(?:AHT|FT|HA|JT|KT|LT|MT|OT|RT|VT)|U(?:LA(?:ST|T)|TC|Y(?:ST|T)|Z\" r \"T)|V(?:ET|LAT|O(?:LT|ST)|UT)|W(?:A(?:KT|ST|T)|E(?:ST|T)|IT|ST)|Y(?:AKT|\" r \"EKT))\" , # FROM: en.wikipedia.org/wiki/List_of_time_zone_abbreviations \"z\" : r \"(?:[+-](?:0\\d|1[0-4]):?(?:00|15|30|45))\" , # [+-] 00:00 to 14:45 \"A\" : r \"(?:\" r \"MON(?:DAY)?|(?:IS|SE)N(?:[IE]N)?|\" r \"TUE(?:S(?:DAY)?)?|SEL(?:ASA)?|\" r \"WED(?:NESDAY)?|RABU?|\" r \"THU(?:RS(?:DAY)?)?|KH?A(?:M(?:IS)?)?|\" r \"FRI(?:DAY)?|JUM(?:[AM]A?T)?|\" r \"SAT(?:URDAY)?|SAB(?:TU)?|\" r \"SUN(?:DAY)?|AHA?D|MIN(?:GGU)?\" r \")\" , \"th\" : r \"(?:ST|ND|RD|TH|\u00ba)\" , } REGEX_PATTERNS_PARSERS = { # 14/8/1991 \"dd_mm_YYYY_1\" : r \"(?: {d} / {m} / {Y} )\" , \"dd_d2\" : r \"(?: {d} \\\\ {m} \\\\ {Y} )\" , \"dd_mm_YYYY_3\" : r \"(?: {d} [-] {m} [-] {Y} )\" , \"dd_mm_YYYY_4\" : r \"(?: {d} \\. {m} \\. {Y} )\" , # 'dd_mm_YYYY_5': r\"(?:{d}{m}{Y})\", # too many phone numbers \"dd_mm_YYYY_6\" : r \"(?: {d} ? {m} ? {Y} )\" , \"dd_mm_YYYY_7\" : r \"(?: {dz}{mz}{Y} )\" , # 14/8/91 \"dd_mm_yy_1\" : r \"(?: {d} / {m} / {y} )\" , \"dd_mm_yy_2\" : r \"(?: {d} \\\\ {m} \\\\ {y} )\" , \"dd_mm_yy_3\" : r \"(?: {d} [-] {m} [-] {y} )\" , \"dd_mm_yy_4\" : r \"(?: {d} \\. {m} \\. {y} )\" , # 'dd_mm_yy_5': r\"(?:{dz}{mz}{y})\", # too many phone numbers # 14 Aug, 1991 \"dd_mmm_YYYY_1\" : r \"(?: {d}{th} ? ?/ ? {B} ?/ ? {Y} )\" , \"dd_mmm_YYYY_2\" : r \"(?: {d}{th} ? ? \\\\ ? {B} ? \\\\ ? {Y} )\" , \"dd_mmm_YYYY_3\" : r \"(?: {d}{th} ? ?[-] ? {B} ?[ -] ? {Y} )\" , \"dd_mmm_YYYY_4\" : r \"(?: {d}{th} ? ?[ -]? ? {B} ?,? ? {Y} )\" , \"dd_mmm_YYYY_5\" : r \"(?: {d}{th} ? ?\\. ? {B} ?\\. ? {Y} )\" , # 14 Aug '91 \"dd_mmm_yy_1\" : r \"(?: {d}{th} ? ?/ ? {B} ?/ ?'? {y} )\" , \"dd_mmm_yy_2\" : r \"(?: {d}{th} ? ? \\\\ ? {B} ? \\\\ ?'? {y} )\" , \"dd_mmm_yy_3\" : r \"(?: {d}{th} ? ?[-] ? {B} ?[-] ?'? {y} )\" , \"dd_mmm_yy_4\" : r \"(?: {d}{th} ? ?[ -]? ? {B} ?,? ?'? {y} )\" , \"dd_mmm_yy_5\" : r \"(?: {d}{th} ? ?\\. ? {B} ?\\. ?'? {y} )\" , # 14th Aug \"dd_mmm\" : r \"(?: {d}{th} ? ?[/ \\\\ . -] ? {B} )\" , # 08/14/1991 # WARNING! dateutil set to day first \"mm_dd_YYYY_1\" : r \"(?: {m} / {d} / {Y} )\" , \"mm_dd_YYYY_2\" : r \"(?: {m} \\\\ {d} \\\\ {Y} )\" , \"mm_dd_YYYY_3\" : r \"(?: {m} [-] {d} [-] {Y} )\" , \"mm_dd_YYYY_4\" : r \"(?: {m} {d} {Y} )\" , \"mm_dd_YYYY_5\" : r \"(?: {m} \\. {d} \\. {Y} )\" , \"mm_dd_YYYY_6\" : r \"(?: {mz}{dz}{Y} )\" , # 8/14/91 # WARNING! dateutil set to day first \"mm_dd_yy_1\" : r \"(?: {m} / {d} / {y} )\" , \"mm_dd_yy_2\" : r \"(?: {m} \\\\ {d} \\\\ {y} )\" , \"mm_dd_yy_3\" : r \"(?: {m} [-] {d} [-] {y} )\" , \"mm_dd_yy_4\" : r \"(?: {m} \\. {d} \\. {y} )\" , # 'mm_dd_yy_5': r\"(?:{mz}{dz}{y})\", # too many phone numbers # Aug 14th, 1991 \"mmm_dd_YYYY_1\" : r \"(?: {B} ?/ ? {d}{th} ? ?/ ? {Y} )\" , \"mmm_dd_YYYY_2\" : r \"(?: {B} ? \\\\ ? {d}{th} ? ? \\\\ ? {Y} )\" , \"mmm_dd_YYYY_3\" : r \"(?: {B} ?[-] ? {d}{th} ? ?[ -] ? {Y} )\" , \"mmm_dd_YYYY_4\" : r \"(?: {B} ?[ -]? ? {d}{th} ? ?, ? {Y} )\" , \"mmm_dd_YYYY_5\" : r \"(?: {B} ?\\. ? {d}{th} ? ?\\. ? {Y} )\" , # Aug-14 '91 \"mmm_dd_yy_1\" : r \"(?: {B} ?/ ? {d}{th} ? ?/ ?'? {y} )\" , \"mmm_dd_yy_2\" : r \"(?: {B} ? \\\\ ? {d}{th} ? ? \\\\ ?'? {y} )\" , \"mmm_dd_yy_3\" : r \"(?: {B} ?[-] ? {d}{th} ? ?[-] ?'? {y} )\" , \"mmm_dd_yy_4\" : r \"(?: {B} ?[. -]? ? {d}{th} ?, '? {y} )\" , \"mmm_dd_yy_5\" : r \"(?: {B} ?\\. ? {d}{th} ? ?\\. ?'? {y} )\" , # Aug-14 # WARNING! dateutil assumes current year \"mmm_dd\" : r \"(?: {B} ?[/ \\\\ . -] ? {d}{th} ?)\" , # # Aug-91 # 'mmm_yy': r\"(?:{B} ?[/\\\\. -] ?'{y})\", # too many false positives # August 1991 \"mmm_YYYY\" : r \"(?: {B} ?[/ \\\\ . -] ? {Y} )\" , # many non-useful dates # 1991-8-14 \"YYYY_mm_dd_1\" : r \"(?: {Y} / {m} / {d} )\" , \"YYYY_mm_dd_2\" : r \"(?: {Y} \\\\ {m} \\\\ {d} )\" , \"YYYY_mm_dd_3\" : r \"(?: {Y} [-] {m} [-] {d} )\" , \"YYYY_mm_dd_4\" : r \"(?: {Y} {m} {d} )\" , \"YYYY_mm_dd_5\" : r \"(?: {Y} \\. {m} \\. {d} )\" , \"YYYY_mm_dd_6\" : r \"(?: {Y}{mz}{dz} )\" , # 910814 (ISO 8601) # 'yy_mm_dd_1': r\"(?:{y} {m} {d})\", # too many random numbers \"yy_mm_dd_2\" : r \"(?: {y} / {m} / {d} )\" , \"yy_mm_dd_3\" : r \"(?: {y} \\\\ {m} \\\\ {d} )\" , \"yy_mm_dd_4\" : r \"(?: {y} [-] {m} [-] {d} )\" , \"yy_mm_dd_5\" : r \"(?: {y} \\. {m} \\. {d} )\" , # 'yy_mm_dd_6': r\"(?:{y}{mz}{dz})\", # too many phone numbers # 1991-Aug-14 \"YYYY_mmm_dd_1\" : r \"(?: {Y} ?/ ? {B} ?/ ? {d} )\" , \"YYYY_mmm_dd_2\" : r \"(?: {Y} ? \\\\ ? {B} ? \\\\ ? {d} )\" , \"YYYY_mmm_dd_3\" : r \"(?: {Y} ?[-] ? {B} ?[-] ? {d} )\" , \"YYYY_mmm_dd_4\" : r \"(?: {Y} ? {B} ?[ -]? ? {d}{th} ?)\" , # 91-Aug-14 \"yy_mmm_dd_1\" : r \"(?:'? {y} ?/ ? {B} ?/ ? {d} )\" , \"yy_mmm_dd_2\" : r \"(?:'? {y} ? \\\\ ? {B} ? \\\\ ? {d} )\" , \"yy_mmm_dd_3\" : r \"(?:'? {y} ?[-] ? {B} ?[-] ? {d} )\" , \"yy_mmm_dd_4\" : r \"(?:'? {y} ? {B} ?[ -]? ? {d}{th} ?)\" , # # 1991.226 (Aug 14 = day 226 in 1991) # dateutil fails # 'YYYY_ddd_1': r\"(?:{Y}\\.{j})\", # too many random numbers # 'YYYY_ddd_2': r\"(?:{Y}[-]{j})\", # too many random numbers # time \"HH_MM_SS\" : r \"(?: {H} : {M} : {S} (?: ? {p} )?(?: ?(?:Z| {Z} | {z} ))?)\" , \"HH_MZ_pp_1\" : r \"(?: {H} : {M} (?: ? {p} )?(?: ?(?:Z| {Z} | {z} ))?)\" , \"HH_MZ_pp_1b\" : r \"(?: {H} [:. ] {M} (?: ? {p} )(?: ?(?:Z| {Z} | {z} ))?)\" , \"HH_MZ_pp_2\" : r \"(?:(?<!\\.) {HZ} [. ]? {M} (?: ? {p} )(?: ?(?:Z| {Z} | {z} ))?)\" , \"HH_pp\" : r \"(?:(?<!\\.) {H} ? {p2} (?: ?(?:Z| {Z} | {z} ))?)\" , # # 910814094500 (9:45am) # 'yy_mm_dd_HH_MM_SS': r\"(?:{y}{mz}{dz}{H}{M}{S})\", # too many phone numbers # 1991-08-14T09:45:00Z \"YYYY_mm_dd_HH_MM\" : r \"(?: {Y} [-] {m} [-] {d} [T ] {H} : {M} (?: ?(?:Z| {Z} | {z} ))?)\" , \"YYYY_mm_dd_HH_MM_SS_1\" : r \"(?: {Y} [-] {m} [-] {d} [T ] {H} : {M} : {S} (?: ?(?:Z| {Z} | {z} ))?)\" , \"YYYY_mm_dd_HH_MM_SS_2\" : r \"(?: {Y}{mz}{d} T? {H}{M}{S} (?: ?(?:Z| {Z} | {z} ))?)\" , \"YYYY_dd_mm_HH_MM_SS_3\" : r \"(?: {Y} [-] {d} [-] {m} [T ] {H} : {M} : {S} (?: ?(?:Z| {Z} | {z} ))?)\" , \"mm_dd_YYYY_HH_MM_SS_1\" : r \"(?: {m} [-] {d} [-] {Y} [T ] {H} : {M} : {S} (?: ?(?:Z| {Z} | {z} ))?)\" , \"dd_mm_YYYY_HH_MM_SS_1\" : r \"(?: {d} [-] {m} [-] {Y} [T ] {H} : {M} : {S} (?: ?(?:Z| {Z} | {z} ))?)\" , # # standalone # 'day': r\"{A}\", # too many false positives # 'month': r\"{B}\", # too many false positives # 'year': r\"{Y}\", # too many random numbers # 'timezone': r\"(?:Z|{Z}|{z})\", # too many malay words } # unicode fixes REGEX_FORMATTED = { label : \" \\\\ b\" + pattern . format ( ** REGEX_PARTS ) # fill in the chunks . replace ( \"-]\" , \" \\u2009\\u2010\\u2011\\u2012\\u2013\\u2014 -]\" ) # unicode dashes . replace ( \"'?\" , \"[' \\u2018\\u2019 ]?\" ) # unicode quotes + \" \\\\ b\" for label , pattern in REGEX_PATTERNS_PARSERS . items () } # match emails and urls to avoid returning chunks of them REGEX_FORMATTED [ \"eml\" ] = r \"\"\"[a-zA-Z0-9][^\\s`!@%$^= {} \\[\\]/\\\\\"',()<>:;]+(?:@|%40|\\s+at\\s+|\\s*<\\s*at\\s*>\\s*)[a-zA-Z0-9][-_a-zA-Z0-9~.]+\\.[a-zA-Z]{2,15}\"\"\" REGEX_FORMATTED [ \"url\" ] = r \"\\b(?:(?:https?|ftp|file)://|www\\d?\\.|ftp\\.)[-A-Z0-9+&@#/%=~_|$?!:,.]*[A-Z0-9+&@#/%=~_|$]\" REGEX_FORMATTED [ \"dot\" ] = r \"(?:\\d+\\.){3,}\\d+\" # compile all the regex patterns REGEX_COMPILED = { label : re . compile ( pattern , flags = re . I | re . U ) for label , pattern in REGEX_FORMATTED . items () } if trans_cat == \"dt\" : return idf elif ( trans_cat in [ \"long_c\" , \"bigint_c\" , \"int_c\" ]) & ( int ( val_unique_cat ) in [ 10 , 13 ] ): if int ( val_unique_cat ) == 10 : precision = \"s\" elif int ( val_unique_cat ) == 13 : precision = \"ms\" else : precision = \"ms\" output_df = unix_to_timestamp ( spark , idf , col , precision = precision , tz = tz , output_mode = output_mode ) . orderBy ( id_col , col ) if save_output is not None : output_df . write . parquet ( save_output , mode = \"overwrite\" ) else : return output_df elif trans_cat == \"string\" : list_dates = list ( set ( idf . select ( col ) . rdd . flatMap ( lambda x : x ) . collect ())) def regex_text ( text , longest = True , context_max_len = 999 , dayfirst = False ): # join multiple spaces, convert tabs, strip leading/trailing whitespace if isinstance ( text , str ): pass else : raise ValueError ( \"Incompatible Column Type!!\" ) text = \" \" . join ( text . split ()) matches = [] for regex_label , regex_obj in REGEX_COMPILED . items (): for m in regex_obj . finditer ( text ): context_start = max ( 0 , ( m . start () + m . end () - context_max_len ) // 2 ) context_end = min ( len ( text ), context_start + context_max_len ) context_str = text [ context_start : context_end ] if context_start != 0 : context_str = \" \\u2026 \" + context_str [ 1 :] if context_end != len ( text ): context_str = ( context_str [: - 1 ] + \" \\u2026 \" ) # this is the `...` character parsed_date = None try : with warnings . catch_warnings (): warnings . simplefilter ( \"ignore\" , category = dateutil . parser . UnknownTimezoneWarning , ) if \"HH\" in regex_label : if \"dd\" in regex_label or \"YYYY\" in regex_label : matched_text = re . sub ( r \"[ \\\\ ]\" , \"/\" , m . group ()) parsed_date = dateutil . parser . parse ( matched_text , dayfirst = dayfirst ) else : matched_text = re . sub ( r \"H(?:(?:OU)?RS?)?\" , \"\" , m . group (), flags = re . I ) matched_text = re . sub ( r \"MN\" , r \"AM\" , matched_text , flags = re . I ) matched_text = re . sub ( r \"NN\" , r \"PM\" , matched_text , flags = re . I ) matched_text = re . sub ( r \"(\\d)[. ](\\d)\" , r \"\\1:\\2\" , matched_text ) matched_text = f \"1970-01-01 { matched_text } \" parsed_date = dateutil . parser . parse ( matched_text , dayfirst = dayfirst ) elif \"dd\" in regex_label or \"YYYY\" in regex_label : matched_text = re . sub ( r \"[ \\\\ ]\" , \"/\" , m . group ()) parsed_date = dateutil . parser . parse ( matched_text , dayfirst = dayfirst ) except ValueError : pass matches . append ( { \"REGEX_LABEL\" : regex_label , \"MATCH\" : m . group (), \"START\" : m . start (), \"END\" : m . end (), \"MATCH_LEN\" : m . end () - m . start (), \"NORM_TEXT_LEN\" : len ( text ), \"CONTEXT\" : context_str , \"PARSED\" : parsed_date , } ) # narrow to longest match for match in matches : if not longest or all ( ( other [ \"START\" ] >= match [ \"START\" ] and other [ \"END\" ] <= match [ \"END\" ]) or other [ \"START\" ] > match [ \"END\" ] or other [ \"END\" ] < match [ \"START\" ] for other in matches ): # don't return emails or urls if match [ \"REGEX_LABEL\" ] not in { \"eml\" , \"url\" , \"dot\" }: yield match bl = [] file_lines = list_dates for line_num , line in enumerate ( file_lines ): bl_int = [] for match_info in regex_text ( line ): try : ye , mo , da , ho , mi , se = ( match_info [ \"PARSED\" ] . year , match_info [ \"PARSED\" ] . month , match_info [ \"PARSED\" ] . day , match_info [ \"PARSED\" ] . hour , match_info [ \"PARSED\" ] . minute , match_info [ \"PARSED\" ] . second , ) if len ( bl_int ) == 0 : bl_int = [ ye , mo , da , ho , mi , se ] else : if ye == 1970 and mo == 1 and da == 1 : pass if ho + mi + se == 0 : pass if ye > 1970 : bl_int [ 0 ] = ye if mo > 0 and ye != 1970 : bl_int [ 1 ] = mo if da > 0 and ye != 1970 : bl_int [ 2 ] = da if ho > 0 : bl_int [ 3 ] = ho if mi > 0 : bl_int [ 4 ] = mi if se > 0 : bl_int [ 5 ] = se else : pass except : pass bl . append ( [ match_info [ \"CONTEXT\" ], datetime . datetime ( bl_int [ 0 ], bl_int [ 1 ], bl_int [ 2 ], bl_int [ 3 ], bl_int [ 4 ], bl_int [ 5 ], ), ] ) if len ( bl ) >= 1 : columns = [ col , col + \"_ts\" ] # output_df = spark.createDataFrame(spark.parallelize(bl),columns) output_df = spark . createDataFrame ( pd . DataFrame ( bl , columns = columns )) else : return idf elif trans_cat in [ \"string_c\" , \"int_c\" ]: if int ( val_unique_cat ) == 4 : output_df = idf . select ( col ) . withColumn ( col + \"_ts\" , F . col ( col ) . cast ( \"string\" ) . cast ( \"date\" ) ) elif int ( val_unique_cat ) == 6 : output_df = ( idf . select ( col ) . withColumn ( col , F . concat ( col , F . lit ( \"01\" ))) . withColumn ( col + \"_ts\" , F . to_date ( col , \"yyyyMMdd\" )) ) elif int ( val_unique_cat ) == 8 : f = ( idf . select ( F . max ( F . substring ( col , 1 , 4 )), F . max ( F . substring ( col , 5 , 2 )), F . max ( F . substring ( col , 7 , 2 )), ) . rdd . flatMap ( lambda x : x ) . collect () ) if int ( f [ 1 ]) > 12 : frmt = \"yyyyddMM\" elif int ( f [ 2 ]) > 12 : frmt = \"yyyyMMdd\" elif ( ( int ( f [ 0 ]) > 1970 & int ( f [ 0 ]) < 2049 ) & ( int ( f [ 1 ]) > 0 & int ( f [ 1 ]) <= 12 ) & ( int ( f [ 2 ]) > 0 & int ( f [ 2 ]) <= 31 ) ): frmt = \"yyyyMMdd\" elif ( ( int ( f [ 0 ]) > 1970 & int ( f [ 0 ]) < 2049 ) & ( int ( f [ 1 ]) > 0 & int ( f [ 1 ]) <= 31 ) & ( int ( f [ 2 ]) > 0 & int ( f [ 2 ]) <= 12 ) ): frmt = \"yyyyddMM\" else : return idf output_df = idf . select ( F . col ( col ) . cast ( \"string\" )) . withColumn ( col + \"_ts\" , F . to_date ( col , frmt ) ) else : return idf else : return idf # if ((output_df.where(F.col(col + \"_ts\").isNull()).count()) / output_df.count()) > 0.9: # return idf # else: # pass if output_mode == \"replace\" : output_df = ( idf . join ( output_df , col , \"left_outer\" ) . drop ( col ) . withColumnRenamed ( col + \"_ts\" , col ) . orderBy ( id_col , col ) ) elif output_mode == \"append\" : output_df = idf . join ( output_df , col , \"left_outer\" ) . orderBy ( id_col , col + \"_ts\" ) else : return \"Incorrect Output Mode Selected\" if save_output : output_df . write . parquet ( save_output , mode = \"overwrite\" ) else : return output_df def ts_loop_cols_pre ( idf , id_col ): \"\"\" This function helps to analyze the potential columns which can be passed for tiime series check. The columns are passed on to the auto-detection block. Parameters ---------- idf Input dataframe id_col ID Column Returns ------- Three lists \"\"\" lc1 , lc2 , lc3 = [], [], [] for i in idf . dtypes : try : col_len = ( idf . select ( F . max ( F . length ( i [ 0 ]))) . rdd . flatMap ( lambda x : x ) . collect ()[ 0 ] ) except : col_len = 0 if idf . select ( i [ 0 ]) . dropna () . distinct () . count () == 0 : lc1 . append ( i [ 0 ]) lc2 . append ( \"NA\" ) lc3 . append ( col_len ) elif ( ( i [ 0 ] != id_col ) & ( idf . select ( F . length ( i [ 0 ])) . distinct () . count () == 1 ) & ( col_len in [ 4 , 6 , 8 , 10 , 13 ]) ): if i [ 1 ] == \"string\" : lc1 . append ( i [ 0 ]) lc2 . append ( \"string_c\" ) lc3 . append ( col_len ) elif i [ 1 ] == \"long\" : lc1 . append ( i [ 0 ]) lc2 . append ( \"long_c\" ) lc3 . append ( col_len ) elif i [ 1 ] == \"bigint\" : lc1 . append ( i [ 0 ]) lc2 . append ( \"bigint_c\" ) lc3 . append ( col_len ) elif i [ 1 ] == \"int\" : lc1 . append ( i [ 0 ]) lc2 . append ( \"int_c\" ) lc3 . append ( col_len ) elif ( i [ 0 ] != id_col ) & ( i [ 1 ] in [ \"string\" , \"object\" ]): lc1 . append ( i [ 0 ]) lc2 . append ( \"string\" ) lc3 . append ( col_len ) elif ( i [ 0 ] != id_col ) & ( i [ 1 ] in [ \"timestamp\" , \"date\" ]): lc1 . append ( i [ 0 ]) lc2 . append ( \"dt\" ) lc3 . append ( col_len ) else : lc1 . append ( i [ 0 ]) lc2 . append ( \"NA\" ) lc3 . append ( col_len ) return lc1 , lc2 , lc3 def ts_preprocess ( spark , idf , id_col , output_path , tz_offset = \"local\" , run_type = \"local\" ): \"\"\" This function helps to read the input spark dataframe as source and do all the necessary processing. All the intermediate data created through this step foro the Time Series Analyzer. Parameters ---------- spark Spark session idf Input dataframe id_col ID Column output_path Output path where the data would be saved tz_offset Timezone offset (Option to chose between options like Local, GMT, UTC, etc.). Default option is set as \"Local\". run_type Option to choose between run type \"Local\" or \"EMR\" or \"Azure\" basis the user flexibility. Default option is set as \"Local\". Returns ------- DataFrame,Output[CSV] \"\"\" if run_type == \"local\" : local_path = output_path elif run_type == \"databricks\" : local_path = output_to_local ( output_path ) elif run_type == \"emr\" : local_path = \"report_stats\" else : raise ValueError ( \"Invalid run_type\" ) Path ( local_path ) . mkdir ( parents = True , exist_ok = True ) num_cols , cat_cols , other_cols = attributeType_segregation ( idf ) ts_loop_col_dtls = ts_loop_cols_pre ( idf , id_col ) l1 , l2 = [], [] for index , i in enumerate ( ts_loop_col_dtls [ 1 ]): if i != \"NA\" : l1 . append ( index ) if i == \"dt\" : l2 . append ( index ) ts_loop_cols = [ ts_loop_col_dtls [ 0 ][ i ] for i in l1 ] pre_exist_ts_cols = [ ts_loop_col_dtls [ 0 ][ i ] for i in l2 ] for i in ts_loop_cols : try : idx = ts_loop_col_dtls [ 0 ] . index ( i ) val_unique_cat = ts_loop_col_dtls [ 2 ][ idx ] trans_cat = ts_loop_col_dtls [ 1 ][ idx ] idf = regex_date_time_parser ( spark , idf , id_col , i , tz_offset , val_unique_cat , trans_cat , save_output = None , output_mode = \"replace\" , ) idf . persist ( pyspark . StorageLevel . MEMORY_AND_DISK ) except : pass odf = idf . distinct () ts_loop_cols_post = [ x [ 0 ] for x in idf . dtypes if x [ 1 ] in [ \"timestamp\" , \"date\" ]] num_cols = [ x for x in num_cols if x not in [ id_col ] + ts_loop_cols_post ] cat_cols = [ x for x in cat_cols if x not in [ id_col ] + ts_loop_cols_post ] c1 = ts_loop_cols c2 = list ( set ( ts_loop_cols_post ) - set ( pre_exist_ts_cols )) c3 = pre_exist_ts_cols c4 = ts_loop_cols_post f = pd . DataFrame ( [ [ \",\" . join ( idf . columns )], [ \",\" . join ( c1 )], [ \",\" . join ( c2 )], [ \",\" . join ( c3 )], [ \",\" . join ( c4 )], [ \",\" . join ( num_cols )], [ \",\" . join ( cat_cols )], ], columns = [ \"cols\" ], ) f . to_csv ( ends_with ( local_path ) + \"ts_cols_stats.csv\" , index = False ) if run_type == \"emr\" : bash_cmd = ( \"aws s3 cp --recursive \" + ends_with ( local_path ) + \" \" + ends_with ( output_path ) ) output = subprocess . check_output ([ \"bash\" , \"-c\" , bash_cmd ]) return odf","title":"ts_auto_detection"},{"location":"api/data_ingest/ts_auto_detection.html#functions","text":"def regex_date_time_parser ( spark, idf, id_col, col, tz, val_unique_cat, trans_cat, save_output=None, output_mode='replace') This function helps to produce the transformed output (if applicable) based on the auto-detection of timestamp / date type. The output from this function is decoupled as a part of ingestion.","title":"Functions"},{"location":"api/data_report/_index.html","text":"Overview Sub-modules anovos.data_report.basic_report_generation anovos.data_report.report_generation This module generates the final report output specific to the intermediate data generated across each of the modules. The final report, however, can \u2026 anovos.data_report.report_preprocessing","title":"Overview"},{"location":"api/data_report/_index.html#overview","text":"","title":"Overview"},{"location":"api/data_report/_index.html#sub-modules","text":"anovos.data_report.basic_report_generation anovos.data_report.report_generation This module generates the final report output specific to the intermediate data generated across each of the modules. The final report, however, can \u2026 anovos.data_report.report_preprocessing","title":"Sub-modules"},{"location":"api/data_report/basic_report_generation.html","text":"basic_report_generation Expand source code import subprocess from pathlib import Path import datapane as dp import pandas as pd import plotly.express as px from anovos.data_analyzer.association_evaluator import ( correlation_matrix , variable_clustering , IV_calculation , IG_calculation , ) from anovos.data_analyzer.quality_checker import ( duplicate_detection , nullRows_detection , nullColumns_detection , outlier_detection , IDness_detection , biasedness_detection , invalidEntries_detection , ) from anovos.data_analyzer.stats_generator import ( global_summary , measures_of_counts , measures_of_centralTendency , measures_of_cardinality , measures_of_dispersion , measures_of_percentiles , measures_of_shape , ) from anovos.shared.utils import ends_with global_theme = px . colors . sequential . Plasma global_theme_r = px . colors . sequential . Plasma_r global_plot_bg_color = \"rgba(0,0,0,0)\" global_paper_bg_color = \"rgba(0,0,0,0)\" default_template = ( dp . HTML ( \"\"\" <html> <img src=\"https://mobilewalla-anovos.s3.amazonaws.com/anovos.png\" style=\"height:100px;display:flex;margin:auto;float:right\"/> </html> \"\"\" ), dp . Text ( \"# ML-Anovos Report\" ), ) def stats_args ( path , func ): \"\"\" Parameters ---------- path Path to pre-saved statistics func Quality Checker function Returns ------- Dictionary Each key/value is argument (related to pre-saved statistics) to be passed for the quality checker function. \"\"\" output = {} mainfunc_to_args = { \"biasedness_detection\" : [ \"stats_mode\" ], \"IDness_detection\" : [ \"stats_unique\" ], \"outlier_detection\" : [ \"stats_unique\" ], \"correlation_matrix\" : [ \"stats_unique\" ], \"nullColumns_detection\" : [ \"stats_unique\" , \"stats_mode\" , \"stats_missing\" ], \"variable_clustering\" : [ \"stats_unique\" , \"stats_mode\" ], } args_to_statsfunc = { \"stats_unique\" : \"measures_of_cardinality\" , \"stats_mode\" : \"measures_of_centralTendency\" , \"stats_missing\" : \"measures_of_counts\" , } for arg in mainfunc_to_args . get ( func , []): output [ arg ] = { \"file_path\" : ( ends_with ( path ) + args_to_statsfunc [ arg ] + \".csv\" ), \"file_type\" : \"csv\" , \"file_configs\" : { \"header\" : True , \"inferSchema\" : True }, } return output def anovos_basic_report ( spark , idf , id_col = \"\" , label_col = \"\" , event_label = \"\" , output_path = \".\" , run_type = \"local\" , print_impact = True , ): \"\"\" Parameters ---------- spark Spark Session idf Input Dataframe id_col ID column (Default value = \"\") label_col Label/Target column (Default value = \"\") event_label Value of (positive) event (i.e label 1) (Default value = \"\") output_path File Path for saving metrics and basic report (Default value = \".\") run_type \"local\", \"emr\" or \"databricks\" \"emr\" if the files are read from or written in AWS s3 \"databricks\" if the files are read from or written in dbfs in azure databricks (Default value = \"local\") print_impact True, False. This argument is to print out the data analyzer statistics.(Default value = False) \"\"\" global num_cols global cat_cols SG_funcs = [ global_summary , measures_of_counts , measures_of_centralTendency , measures_of_cardinality , measures_of_dispersion , measures_of_percentiles , measures_of_shape , ] QC_rows_funcs = [ duplicate_detection , nullRows_detection ] QC_cols_funcs = [ nullColumns_detection , outlier_detection , IDness_detection , biasedness_detection , invalidEntries_detection , ] AA_funcs = [ correlation_matrix , variable_clustering ] AT_funcs = [ IV_calculation , IG_calculation ] all_funcs = SG_funcs + QC_rows_funcs + QC_cols_funcs + AA_funcs + AT_funcs def output_to_local ( output_path ): punctuations = \":\" for x in output_path : if x in punctuations : local_path = output_path . replace ( x , \"\" ) local_path = \"/\" + local_path return local_path if run_type == \"local\" : local_path = output_path elif run_type == \"databricks\" : local_path = output_to_local ( output_path ) elif run_type == \"emr\" : local_path = \"report_stats\" else : raise ValueError ( \"Invalid run_type\" ) Path ( local_path ) . mkdir ( parents = True , exist_ok = True ) for func in all_funcs : if func in SG_funcs : stats = func ( spark , idf ) elif func in ( QC_rows_funcs + QC_cols_funcs ): extra_args = stats_args ( output_path , func . __name__ ) stats = func ( spark , idf , ** extra_args )[ 1 ] elif func in AA_funcs : extra_args = stats_args ( output_path , func . __name__ ) stats = func ( spark , idf , drop_cols = id_col , ** extra_args ) elif label_col : if func in AT_funcs : stats = func ( spark , idf , label_col = label_col , event_label = event_label ) else : continue stats . toPandas () . to_csv ( ends_with ( local_path ) + func . __name__ + \".csv\" , index = False ) if run_type == \"emr\" : bash_cmd = ( \"aws s3 cp \" + ends_with ( local_path ) + func . __name__ + \".csv \" + ends_with ( output_path ) ) subprocess . check_output ([ \"bash\" , \"-c\" , bash_cmd ]) if print_impact : print ( func . __name__ , \": \\n \" ) stats = spark . read . csv ( ends_with ( output_path ) + func . __name__ + \".csv\" , header = True , inferSchema = True , ) stats . show () def remove_u_score ( col ): col_ = col . split ( \"_\" ) bl = [] for i in col_ : if i == \"nullColumns\" or i == \"nullRows\" : bl . append ( \"Null\" ) else : bl . append ( i [ 0 ] . upper () + i [ 1 :]) return \" \" . join ( bl ) global_summary_df = pd . read_csv ( ends_with ( local_path ) + \"global_summary.csv\" ) rows_count = int ( global_summary_df [ global_summary_df . metric . values == \"rows_count\" ] . value . values [ 0 ] ) catcols_count = int ( global_summary_df [ global_summary_df . metric . values == \"catcols_count\" ] . value . values [ 0 ] ) numcols_count = int ( global_summary_df [ global_summary_df . metric . values == \"numcols_count\" ] . value . values [ 0 ] ) columns_count = int ( global_summary_df [ global_summary_df . metric . values == \"columns_count\" ] . value . values [ 0 ] ) catcols_name = \",\" . join ( list ( global_summary_df [ global_summary_df . metric . values == \"catcols_name\" ] . value . values ) ) numcols_name = \",\" . join ( list ( global_summary_df [ global_summary_df . metric . values == \"numcols_name\" ] . value . values ) ) l1 = dp . Group ( dp . Text ( \"# \" ), dp . Text ( \"*This section summarizes the dataset with key statistical metrics.*\" ), dp . Text ( \"# \" ), dp . Text ( \"# \" ), dp . Text ( \"### Global Summary\" ), dp . Group ( dp . Text ( \" Total Number of Records: **\" + str ( f \" { rows_count : ,d } \" ) + \"**\" ), dp . Text ( \" Total Number of Attributes: **\" + str ( columns_count ) + \"**\" ), dp . Text ( \" Number of Numerical Attributes : **\" + str ( numcols_count ) + \"**\" ), dp . Text ( \" Numerical Attributes Name : **\" + str ( numcols_name ) + \"**\" ), dp . Text ( \" Number of Categorical Attributes : **\" + str ( catcols_count ) + \"**\" ), dp . Text ( \" Categorical Attributes Name : **\" + str ( catcols_name ) + \"**\" ), rows = 6 , ), rows = 8 , ) l2 = dp . Text ( \"### Statistics by Metric Type\" ) SG_content = [] for i in SG_funcs : if i . __name__ != \"global_summary\" : SG_content . append ( dp . DataTable ( pd . read_csv ( ends_with ( local_path ) + str ( i . __name__ ) + \".csv\" ) . round ( 3 ), label = remove_u_score ( i . __name__ ), ) ) l3 = dp . Group ( dp . Select ( blocks = SG_content , type = dp . SelectType . TABS ), dp . Text ( \"# \" )) tab1 = dp . Group ( l1 , dp . Text ( \"# \" ), l2 , l3 , dp . Text ( \"# \" ), dp . Text ( \"# \" ), dp . Text ( \"# \" ), label = \"Descriptive Statistics\" , ) QCcol_content = [] for i in QC_cols_funcs : QCcol_content . append ( [ dp . Text ( \"### \" + str ( remove_u_score ( i . __name__ ))), dp . DataTable ( pd . read_csv ( ends_with ( local_path ) + str ( i . __name__ ) + \".csv\" ) . round ( 3 ) ), dp . Text ( \"#\" ), dp . Text ( \"#\" ), ] ) QCrow_content = [] for i in QC_rows_funcs : if i . __name__ == \"duplicate_detection\" : stats = pd . read_csv ( ends_with ( local_path ) + str ( i . __name__ ) + \".csv\" ) . round ( 3 ) unique_rows_count = ( \" No. Of Unique Rows: **\" + str ( format ( int ( stats [ stats [ \"metric\" ] == \"unique_rows_count\" ] . value . values ), \",\" , ) ) + \"**\" ) total_rows_count = ( \" No. of Rows: **\" + str ( format ( int ( stats [ stats [ \"metric\" ] == \"rows_count\" ] . value . values ), \",\" ) ) + \"**\" ) duplicate_rows_count = ( \" No. of Duplicate Rows: **\" + str ( format ( int ( stats [ stats [ \"metric\" ] == \"duplicate_rows\" ] . value . values ), \",\" , ) ) + \"**\" ) duplicate_rows_pct = ( \" Percentage of Duplicate Rows: **\" + str ( float ( stats [ stats [ \"metric\" ] == \"duplicate_pct\" ] . value . values * 100.0 ) ) + \" %\" + \"**\" ) QCrow_content . append ( [ dp . Text ( \"### \" + str ( remove_u_score ( i . __name__ ))), dp . Group ( dp . Text ( total_rows_count ), dp . Text ( unique_rows_count ), dp . Text ( duplicate_rows_count ), dp . Text ( duplicate_rows_pct ), rows = 4 , ), dp . Text ( \"#\" ), dp . Text ( \"#\" ), ] ) else : QCrow_content . append ( [ dp . Text ( \"### \" + str ( remove_u_score ( i . __name__ ))), dp . DataTable ( pd . read_csv ( ends_with ( local_path ) + str ( i . __name__ ) + \".csv\" ) . round ( 3 ) ), dp . Text ( \"#\" ), dp . Text ( \"#\" ), ] ) QCcol_content = [ item for sublist in QCcol_content for item in sublist ] QCrow_content = [ item for sublist in QCrow_content for item in sublist ] tab2 = dp . Group ( dp . Text ( \"# \" ), dp . Text ( \"*This section identifies the data quality issues at both row and column level.*\" ), dp . Text ( \"# \" ), dp . Text ( \"# \" ), dp . Select ( blocks = [ dp . Group ( dp . Text ( \"# \" ), dp . Group ( * QCcol_content ), rows = 2 , label = \"Column Level\" , ), dp . Group ( dp . Text ( \"# \" ), dp . Group ( * QCrow_content ), rows = 2 , label = \"Row Level\" ), ], type = dp . SelectType . TABS , ), dp . Text ( \"# \" ), dp . Text ( \"# \" ), label = \"Quality Check\" , ) AA_content = [] for i in AA_funcs + AT_funcs : if i . __name__ == \"correlation_matrix\" : stats = pd . read_csv ( ends_with ( local_path ) + str ( i . __name__ ) + \".csv\" ) . round ( 3 ) feats_order = list ( stats [ \"attribute\" ] . values ) stats = stats . round ( 3 ) fig = px . imshow ( stats [ feats_order ], y = feats_order , color_continuous_scale = global_theme , aspect = \"auto\" , ) fig . layout . plot_bgcolor = global_plot_bg_color fig . layout . paper_bgcolor = global_paper_bg_color AA_content . append ( dp . Group ( dp . Text ( \"##\" ), dp . DataTable ( stats [[ \"attribute\" ] + feats_order ]), dp . Plot ( fig ), rows = 3 , label = remove_u_score ( i . __name__ ), ) ) elif i . __name__ == \"variable_clustering\" : stats = ( pd . read_csv ( ends_with ( local_path ) + str ( i . __name__ ) + \".csv\" ) . round ( 3 ) . sort_values ( by = [ \"Cluster\" ], ascending = True ) ) fig = px . sunburst ( stats , path = [ \"Cluster\" , \"Attribute\" ], values = \"RS_Ratio\" , color_discrete_sequence = global_theme , ) fig . layout . plot_bgcolor = global_plot_bg_color fig . layout . paper_bgcolor = global_paper_bg_color fig . layout . autosize = True AA_content . append ( dp . Group ( dp . Text ( \"##\" ), dp . DataTable ( stats ), dp . Plot ( fig ), rows = 3 , label = remove_u_score ( i . __name__ ), ) ) else : if label_col : stats = pd . read_csv ( ends_with ( local_path ) + str ( i . __name__ ) + \".csv\" ) . round ( 3 ) col_nm = [ x for x in list ( stats . columns ) if \"attribute\" not in x ] stats = stats . sort_values ( col_nm [ 0 ], ascending = True ) fig = px . bar ( stats , x = col_nm [ 0 ], y = \"attribute\" , orientation = \"h\" , color_discrete_sequence = global_theme , ) fig . layout . plot_bgcolor = global_plot_bg_color fig . layout . paper_bgcolor = global_paper_bg_color fig . layout . autosize = True AA_content . append ( dp . Group ( dp . Text ( \"##\" ), dp . DataTable ( stats ), dp . Plot ( fig ), label = remove_u_score ( i . __name__ ), rows = 3 , ) ) # @TODO: is there better templating approach such as jinja tab3 = dp . Group ( dp . Text ( \"# \" ), dp . Text ( \"\"\" *This section analyzes the interaction between different attributes and/or the relationship between an attribute & the binary target variable.* \"\"\" ), dp . Text ( \"# \" ), dp . Text ( \"# \" ), dp . Text ( \"### Association Matrix & Plot\" ), dp . Select ( blocks = AA_content , type = dp . SelectType . DROPDOWN ), dp . Text ( \"### \" ), dp . Text ( \"## \" ), dp . Text ( \"## \" ), dp . Text ( \"## \" ), label = \"Attribute Associations\" , ) dp . Report ( default_template [ 0 ], default_template [ 1 ], dp . Select ( blocks = [ tab1 , tab2 , tab3 ], type = dp . SelectType . TABS ), ) . save ( ends_with ( local_path ) + \"basic_report.html\" , open = True ) if run_type == \"emr\" : bash_cmd = ( \"aws s3 cp \" + ends_with ( local_path ) + \"basic_report.html \" + ends_with ( output_path ) ) subprocess . check_output ([ \"bash\" , \"-c\" , bash_cmd ]) Functions def anovos_basic_report ( spark, idf, id_col='', label_col='', event_label='', output_path='.', run_type='local', print_impact=True) Parameters spark Spark Session idf Input Dataframe id_col ID column (Default value = \"\") label_col Label/Target column (Default value = \"\") event_label Value of (positive) event (i.e label 1) (Default value = \"\") output_path File Path for saving metrics and basic report (Default value = \".\") run_type \"local\", \"emr\" or \"databricks\" \"emr\" if the files are read from or written in AWS s3 \"databricks\" if the files are read from or written in dbfs in azure databricks (Default value = \"local\") print_impact True, False. This argument is to print out the data analyzer statistics.(Default value = False) Expand source code def anovos_basic_report ( spark , idf , id_col = \"\" , label_col = \"\" , event_label = \"\" , output_path = \".\" , run_type = \"local\" , print_impact = True , ): \"\"\" Parameters ---------- spark Spark Session idf Input Dataframe id_col ID column (Default value = \"\") label_col Label/Target column (Default value = \"\") event_label Value of (positive) event (i.e label 1) (Default value = \"\") output_path File Path for saving metrics and basic report (Default value = \".\") run_type \"local\", \"emr\" or \"databricks\" \"emr\" if the files are read from or written in AWS s3 \"databricks\" if the files are read from or written in dbfs in azure databricks (Default value = \"local\") print_impact True, False. This argument is to print out the data analyzer statistics.(Default value = False) \"\"\" global num_cols global cat_cols SG_funcs = [ global_summary , measures_of_counts , measures_of_centralTendency , measures_of_cardinality , measures_of_dispersion , measures_of_percentiles , measures_of_shape , ] QC_rows_funcs = [ duplicate_detection , nullRows_detection ] QC_cols_funcs = [ nullColumns_detection , outlier_detection , IDness_detection , biasedness_detection , invalidEntries_detection , ] AA_funcs = [ correlation_matrix , variable_clustering ] AT_funcs = [ IV_calculation , IG_calculation ] all_funcs = SG_funcs + QC_rows_funcs + QC_cols_funcs + AA_funcs + AT_funcs def output_to_local ( output_path ): punctuations = \":\" for x in output_path : if x in punctuations : local_path = output_path . replace ( x , \"\" ) local_path = \"/\" + local_path return local_path if run_type == \"local\" : local_path = output_path elif run_type == \"databricks\" : local_path = output_to_local ( output_path ) elif run_type == \"emr\" : local_path = \"report_stats\" else : raise ValueError ( \"Invalid run_type\" ) Path ( local_path ) . mkdir ( parents = True , exist_ok = True ) for func in all_funcs : if func in SG_funcs : stats = func ( spark , idf ) elif func in ( QC_rows_funcs + QC_cols_funcs ): extra_args = stats_args ( output_path , func . __name__ ) stats = func ( spark , idf , ** extra_args )[ 1 ] elif func in AA_funcs : extra_args = stats_args ( output_path , func . __name__ ) stats = func ( spark , idf , drop_cols = id_col , ** extra_args ) elif label_col : if func in AT_funcs : stats = func ( spark , idf , label_col = label_col , event_label = event_label ) else : continue stats . toPandas () . to_csv ( ends_with ( local_path ) + func . __name__ + \".csv\" , index = False ) if run_type == \"emr\" : bash_cmd = ( \"aws s3 cp \" + ends_with ( local_path ) + func . __name__ + \".csv \" + ends_with ( output_path ) ) subprocess . check_output ([ \"bash\" , \"-c\" , bash_cmd ]) if print_impact : print ( func . __name__ , \": \\n \" ) stats = spark . read . csv ( ends_with ( output_path ) + func . __name__ + \".csv\" , header = True , inferSchema = True , ) stats . show () def remove_u_score ( col ): col_ = col . split ( \"_\" ) bl = [] for i in col_ : if i == \"nullColumns\" or i == \"nullRows\" : bl . append ( \"Null\" ) else : bl . append ( i [ 0 ] . upper () + i [ 1 :]) return \" \" . join ( bl ) global_summary_df = pd . read_csv ( ends_with ( local_path ) + \"global_summary.csv\" ) rows_count = int ( global_summary_df [ global_summary_df . metric . values == \"rows_count\" ] . value . values [ 0 ] ) catcols_count = int ( global_summary_df [ global_summary_df . metric . values == \"catcols_count\" ] . value . values [ 0 ] ) numcols_count = int ( global_summary_df [ global_summary_df . metric . values == \"numcols_count\" ] . value . values [ 0 ] ) columns_count = int ( global_summary_df [ global_summary_df . metric . values == \"columns_count\" ] . value . values [ 0 ] ) catcols_name = \",\" . join ( list ( global_summary_df [ global_summary_df . metric . values == \"catcols_name\" ] . value . values ) ) numcols_name = \",\" . join ( list ( global_summary_df [ global_summary_df . metric . values == \"numcols_name\" ] . value . values ) ) l1 = dp . Group ( dp . Text ( \"# \" ), dp . Text ( \"*This section summarizes the dataset with key statistical metrics.*\" ), dp . Text ( \"# \" ), dp . Text ( \"# \" ), dp . Text ( \"### Global Summary\" ), dp . Group ( dp . Text ( \" Total Number of Records: **\" + str ( f \" { rows_count : ,d } \" ) + \"**\" ), dp . Text ( \" Total Number of Attributes: **\" + str ( columns_count ) + \"**\" ), dp . Text ( \" Number of Numerical Attributes : **\" + str ( numcols_count ) + \"**\" ), dp . Text ( \" Numerical Attributes Name : **\" + str ( numcols_name ) + \"**\" ), dp . Text ( \" Number of Categorical Attributes : **\" + str ( catcols_count ) + \"**\" ), dp . Text ( \" Categorical Attributes Name : **\" + str ( catcols_name ) + \"**\" ), rows = 6 , ), rows = 8 , ) l2 = dp . Text ( \"### Statistics by Metric Type\" ) SG_content = [] for i in SG_funcs : if i . __name__ != \"global_summary\" : SG_content . append ( dp . DataTable ( pd . read_csv ( ends_with ( local_path ) + str ( i . __name__ ) + \".csv\" ) . round ( 3 ), label = remove_u_score ( i . __name__ ), ) ) l3 = dp . Group ( dp . Select ( blocks = SG_content , type = dp . SelectType . TABS ), dp . Text ( \"# \" )) tab1 = dp . Group ( l1 , dp . Text ( \"# \" ), l2 , l3 , dp . Text ( \"# \" ), dp . Text ( \"# \" ), dp . Text ( \"# \" ), label = \"Descriptive Statistics\" , ) QCcol_content = [] for i in QC_cols_funcs : QCcol_content . append ( [ dp . Text ( \"### \" + str ( remove_u_score ( i . __name__ ))), dp . DataTable ( pd . read_csv ( ends_with ( local_path ) + str ( i . __name__ ) + \".csv\" ) . round ( 3 ) ), dp . Text ( \"#\" ), dp . Text ( \"#\" ), ] ) QCrow_content = [] for i in QC_rows_funcs : if i . __name__ == \"duplicate_detection\" : stats = pd . read_csv ( ends_with ( local_path ) + str ( i . __name__ ) + \".csv\" ) . round ( 3 ) unique_rows_count = ( \" No. Of Unique Rows: **\" + str ( format ( int ( stats [ stats [ \"metric\" ] == \"unique_rows_count\" ] . value . values ), \",\" , ) ) + \"**\" ) total_rows_count = ( \" No. of Rows: **\" + str ( format ( int ( stats [ stats [ \"metric\" ] == \"rows_count\" ] . value . values ), \",\" ) ) + \"**\" ) duplicate_rows_count = ( \" No. of Duplicate Rows: **\" + str ( format ( int ( stats [ stats [ \"metric\" ] == \"duplicate_rows\" ] . value . values ), \",\" , ) ) + \"**\" ) duplicate_rows_pct = ( \" Percentage of Duplicate Rows: **\" + str ( float ( stats [ stats [ \"metric\" ] == \"duplicate_pct\" ] . value . values * 100.0 ) ) + \" %\" + \"**\" ) QCrow_content . append ( [ dp . Text ( \"### \" + str ( remove_u_score ( i . __name__ ))), dp . Group ( dp . Text ( total_rows_count ), dp . Text ( unique_rows_count ), dp . Text ( duplicate_rows_count ), dp . Text ( duplicate_rows_pct ), rows = 4 , ), dp . Text ( \"#\" ), dp . Text ( \"#\" ), ] ) else : QCrow_content . append ( [ dp . Text ( \"### \" + str ( remove_u_score ( i . __name__ ))), dp . DataTable ( pd . read_csv ( ends_with ( local_path ) + str ( i . __name__ ) + \".csv\" ) . round ( 3 ) ), dp . Text ( \"#\" ), dp . Text ( \"#\" ), ] ) QCcol_content = [ item for sublist in QCcol_content for item in sublist ] QCrow_content = [ item for sublist in QCrow_content for item in sublist ] tab2 = dp . Group ( dp . Text ( \"# \" ), dp . Text ( \"*This section identifies the data quality issues at both row and column level.*\" ), dp . Text ( \"# \" ), dp . Text ( \"# \" ), dp . Select ( blocks = [ dp . Group ( dp . Text ( \"# \" ), dp . Group ( * QCcol_content ), rows = 2 , label = \"Column Level\" , ), dp . Group ( dp . Text ( \"# \" ), dp . Group ( * QCrow_content ), rows = 2 , label = \"Row Level\" ), ], type = dp . SelectType . TABS , ), dp . Text ( \"# \" ), dp . Text ( \"# \" ), label = \"Quality Check\" , ) AA_content = [] for i in AA_funcs + AT_funcs : if i . __name__ == \"correlation_matrix\" : stats = pd . read_csv ( ends_with ( local_path ) + str ( i . __name__ ) + \".csv\" ) . round ( 3 ) feats_order = list ( stats [ \"attribute\" ] . values ) stats = stats . round ( 3 ) fig = px . imshow ( stats [ feats_order ], y = feats_order , color_continuous_scale = global_theme , aspect = \"auto\" , ) fig . layout . plot_bgcolor = global_plot_bg_color fig . layout . paper_bgcolor = global_paper_bg_color AA_content . append ( dp . Group ( dp . Text ( \"##\" ), dp . DataTable ( stats [[ \"attribute\" ] + feats_order ]), dp . Plot ( fig ), rows = 3 , label = remove_u_score ( i . __name__ ), ) ) elif i . __name__ == \"variable_clustering\" : stats = ( pd . read_csv ( ends_with ( local_path ) + str ( i . __name__ ) + \".csv\" ) . round ( 3 ) . sort_values ( by = [ \"Cluster\" ], ascending = True ) ) fig = px . sunburst ( stats , path = [ \"Cluster\" , \"Attribute\" ], values = \"RS_Ratio\" , color_discrete_sequence = global_theme , ) fig . layout . plot_bgcolor = global_plot_bg_color fig . layout . paper_bgcolor = global_paper_bg_color fig . layout . autosize = True AA_content . append ( dp . Group ( dp . Text ( \"##\" ), dp . DataTable ( stats ), dp . Plot ( fig ), rows = 3 , label = remove_u_score ( i . __name__ ), ) ) else : if label_col : stats = pd . read_csv ( ends_with ( local_path ) + str ( i . __name__ ) + \".csv\" ) . round ( 3 ) col_nm = [ x for x in list ( stats . columns ) if \"attribute\" not in x ] stats = stats . sort_values ( col_nm [ 0 ], ascending = True ) fig = px . bar ( stats , x = col_nm [ 0 ], y = \"attribute\" , orientation = \"h\" , color_discrete_sequence = global_theme , ) fig . layout . plot_bgcolor = global_plot_bg_color fig . layout . paper_bgcolor = global_paper_bg_color fig . layout . autosize = True AA_content . append ( dp . Group ( dp . Text ( \"##\" ), dp . DataTable ( stats ), dp . Plot ( fig ), label = remove_u_score ( i . __name__ ), rows = 3 , ) ) # @TODO: is there better templating approach such as jinja tab3 = dp . Group ( dp . Text ( \"# \" ), dp . Text ( \"\"\" *This section analyzes the interaction between different attributes and/or the relationship between an attribute & the binary target variable.* \"\"\" ), dp . Text ( \"# \" ), dp . Text ( \"# \" ), dp . Text ( \"### Association Matrix & Plot\" ), dp . Select ( blocks = AA_content , type = dp . SelectType . DROPDOWN ), dp . Text ( \"### \" ), dp . Text ( \"## \" ), dp . Text ( \"## \" ), dp . Text ( \"## \" ), label = \"Attribute Associations\" , ) dp . Report ( default_template [ 0 ], default_template [ 1 ], dp . Select ( blocks = [ tab1 , tab2 , tab3 ], type = dp . SelectType . TABS ), ) . save ( ends_with ( local_path ) + \"basic_report.html\" , open = True ) if run_type == \"emr\" : bash_cmd = ( \"aws s3 cp \" + ends_with ( local_path ) + \"basic_report.html \" + ends_with ( output_path ) ) subprocess . check_output ([ \"bash\" , \"-c\" , bash_cmd ]) def stats_args ( path, func) Parameters path Path to pre-saved statistics func Quality Checker function Returns Dictionary Each key/value is argument (related to pre-saved statistics) to be passed for the quality checker function. Expand source code def stats_args ( path , func ): \"\"\" Parameters ---------- path Path to pre-saved statistics func Quality Checker function Returns ------- Dictionary Each key/value is argument (related to pre-saved statistics) to be passed for the quality checker function. \"\"\" output = {} mainfunc_to_args = { \"biasedness_detection\" : [ \"stats_mode\" ], \"IDness_detection\" : [ \"stats_unique\" ], \"outlier_detection\" : [ \"stats_unique\" ], \"correlation_matrix\" : [ \"stats_unique\" ], \"nullColumns_detection\" : [ \"stats_unique\" , \"stats_mode\" , \"stats_missing\" ], \"variable_clustering\" : [ \"stats_unique\" , \"stats_mode\" ], } args_to_statsfunc = { \"stats_unique\" : \"measures_of_cardinality\" , \"stats_mode\" : \"measures_of_centralTendency\" , \"stats_missing\" : \"measures_of_counts\" , } for arg in mainfunc_to_args . get ( func , []): output [ arg ] = { \"file_path\" : ( ends_with ( path ) + args_to_statsfunc [ arg ] + \".csv\" ), \"file_type\" : \"csv\" , \"file_configs\" : { \"header\" : True , \"inferSchema\" : True }, } return output","title":"<code>basic_report_generation</code>"},{"location":"api/data_report/basic_report_generation.html#basic_report_generation","text":"Expand source code import subprocess from pathlib import Path import datapane as dp import pandas as pd import plotly.express as px from anovos.data_analyzer.association_evaluator import ( correlation_matrix , variable_clustering , IV_calculation , IG_calculation , ) from anovos.data_analyzer.quality_checker import ( duplicate_detection , nullRows_detection , nullColumns_detection , outlier_detection , IDness_detection , biasedness_detection , invalidEntries_detection , ) from anovos.data_analyzer.stats_generator import ( global_summary , measures_of_counts , measures_of_centralTendency , measures_of_cardinality , measures_of_dispersion , measures_of_percentiles , measures_of_shape , ) from anovos.shared.utils import ends_with global_theme = px . colors . sequential . Plasma global_theme_r = px . colors . sequential . Plasma_r global_plot_bg_color = \"rgba(0,0,0,0)\" global_paper_bg_color = \"rgba(0,0,0,0)\" default_template = ( dp . HTML ( \"\"\" <html> <img src=\"https://mobilewalla-anovos.s3.amazonaws.com/anovos.png\" style=\"height:100px;display:flex;margin:auto;float:right\"/> </html> \"\"\" ), dp . Text ( \"# ML-Anovos Report\" ), ) def stats_args ( path , func ): \"\"\" Parameters ---------- path Path to pre-saved statistics func Quality Checker function Returns ------- Dictionary Each key/value is argument (related to pre-saved statistics) to be passed for the quality checker function. \"\"\" output = {} mainfunc_to_args = { \"biasedness_detection\" : [ \"stats_mode\" ], \"IDness_detection\" : [ \"stats_unique\" ], \"outlier_detection\" : [ \"stats_unique\" ], \"correlation_matrix\" : [ \"stats_unique\" ], \"nullColumns_detection\" : [ \"stats_unique\" , \"stats_mode\" , \"stats_missing\" ], \"variable_clustering\" : [ \"stats_unique\" , \"stats_mode\" ], } args_to_statsfunc = { \"stats_unique\" : \"measures_of_cardinality\" , \"stats_mode\" : \"measures_of_centralTendency\" , \"stats_missing\" : \"measures_of_counts\" , } for arg in mainfunc_to_args . get ( func , []): output [ arg ] = { \"file_path\" : ( ends_with ( path ) + args_to_statsfunc [ arg ] + \".csv\" ), \"file_type\" : \"csv\" , \"file_configs\" : { \"header\" : True , \"inferSchema\" : True }, } return output def anovos_basic_report ( spark , idf , id_col = \"\" , label_col = \"\" , event_label = \"\" , output_path = \".\" , run_type = \"local\" , print_impact = True , ): \"\"\" Parameters ---------- spark Spark Session idf Input Dataframe id_col ID column (Default value = \"\") label_col Label/Target column (Default value = \"\") event_label Value of (positive) event (i.e label 1) (Default value = \"\") output_path File Path for saving metrics and basic report (Default value = \".\") run_type \"local\", \"emr\" or \"databricks\" \"emr\" if the files are read from or written in AWS s3 \"databricks\" if the files are read from or written in dbfs in azure databricks (Default value = \"local\") print_impact True, False. This argument is to print out the data analyzer statistics.(Default value = False) \"\"\" global num_cols global cat_cols SG_funcs = [ global_summary , measures_of_counts , measures_of_centralTendency , measures_of_cardinality , measures_of_dispersion , measures_of_percentiles , measures_of_shape , ] QC_rows_funcs = [ duplicate_detection , nullRows_detection ] QC_cols_funcs = [ nullColumns_detection , outlier_detection , IDness_detection , biasedness_detection , invalidEntries_detection , ] AA_funcs = [ correlation_matrix , variable_clustering ] AT_funcs = [ IV_calculation , IG_calculation ] all_funcs = SG_funcs + QC_rows_funcs + QC_cols_funcs + AA_funcs + AT_funcs def output_to_local ( output_path ): punctuations = \":\" for x in output_path : if x in punctuations : local_path = output_path . replace ( x , \"\" ) local_path = \"/\" + local_path return local_path if run_type == \"local\" : local_path = output_path elif run_type == \"databricks\" : local_path = output_to_local ( output_path ) elif run_type == \"emr\" : local_path = \"report_stats\" else : raise ValueError ( \"Invalid run_type\" ) Path ( local_path ) . mkdir ( parents = True , exist_ok = True ) for func in all_funcs : if func in SG_funcs : stats = func ( spark , idf ) elif func in ( QC_rows_funcs + QC_cols_funcs ): extra_args = stats_args ( output_path , func . __name__ ) stats = func ( spark , idf , ** extra_args )[ 1 ] elif func in AA_funcs : extra_args = stats_args ( output_path , func . __name__ ) stats = func ( spark , idf , drop_cols = id_col , ** extra_args ) elif label_col : if func in AT_funcs : stats = func ( spark , idf , label_col = label_col , event_label = event_label ) else : continue stats . toPandas () . to_csv ( ends_with ( local_path ) + func . __name__ + \".csv\" , index = False ) if run_type == \"emr\" : bash_cmd = ( \"aws s3 cp \" + ends_with ( local_path ) + func . __name__ + \".csv \" + ends_with ( output_path ) ) subprocess . check_output ([ \"bash\" , \"-c\" , bash_cmd ]) if print_impact : print ( func . __name__ , \": \\n \" ) stats = spark . read . csv ( ends_with ( output_path ) + func . __name__ + \".csv\" , header = True , inferSchema = True , ) stats . show () def remove_u_score ( col ): col_ = col . split ( \"_\" ) bl = [] for i in col_ : if i == \"nullColumns\" or i == \"nullRows\" : bl . append ( \"Null\" ) else : bl . append ( i [ 0 ] . upper () + i [ 1 :]) return \" \" . join ( bl ) global_summary_df = pd . read_csv ( ends_with ( local_path ) + \"global_summary.csv\" ) rows_count = int ( global_summary_df [ global_summary_df . metric . values == \"rows_count\" ] . value . values [ 0 ] ) catcols_count = int ( global_summary_df [ global_summary_df . metric . values == \"catcols_count\" ] . value . values [ 0 ] ) numcols_count = int ( global_summary_df [ global_summary_df . metric . values == \"numcols_count\" ] . value . values [ 0 ] ) columns_count = int ( global_summary_df [ global_summary_df . metric . values == \"columns_count\" ] . value . values [ 0 ] ) catcols_name = \",\" . join ( list ( global_summary_df [ global_summary_df . metric . values == \"catcols_name\" ] . value . values ) ) numcols_name = \",\" . join ( list ( global_summary_df [ global_summary_df . metric . values == \"numcols_name\" ] . value . values ) ) l1 = dp . Group ( dp . Text ( \"# \" ), dp . Text ( \"*This section summarizes the dataset with key statistical metrics.*\" ), dp . Text ( \"# \" ), dp . Text ( \"# \" ), dp . Text ( \"### Global Summary\" ), dp . Group ( dp . Text ( \" Total Number of Records: **\" + str ( f \" { rows_count : ,d } \" ) + \"**\" ), dp . Text ( \" Total Number of Attributes: **\" + str ( columns_count ) + \"**\" ), dp . Text ( \" Number of Numerical Attributes : **\" + str ( numcols_count ) + \"**\" ), dp . Text ( \" Numerical Attributes Name : **\" + str ( numcols_name ) + \"**\" ), dp . Text ( \" Number of Categorical Attributes : **\" + str ( catcols_count ) + \"**\" ), dp . Text ( \" Categorical Attributes Name : **\" + str ( catcols_name ) + \"**\" ), rows = 6 , ), rows = 8 , ) l2 = dp . Text ( \"### Statistics by Metric Type\" ) SG_content = [] for i in SG_funcs : if i . __name__ != \"global_summary\" : SG_content . append ( dp . DataTable ( pd . read_csv ( ends_with ( local_path ) + str ( i . __name__ ) + \".csv\" ) . round ( 3 ), label = remove_u_score ( i . __name__ ), ) ) l3 = dp . Group ( dp . Select ( blocks = SG_content , type = dp . SelectType . TABS ), dp . Text ( \"# \" )) tab1 = dp . Group ( l1 , dp . Text ( \"# \" ), l2 , l3 , dp . Text ( \"# \" ), dp . Text ( \"# \" ), dp . Text ( \"# \" ), label = \"Descriptive Statistics\" , ) QCcol_content = [] for i in QC_cols_funcs : QCcol_content . append ( [ dp . Text ( \"### \" + str ( remove_u_score ( i . __name__ ))), dp . DataTable ( pd . read_csv ( ends_with ( local_path ) + str ( i . __name__ ) + \".csv\" ) . round ( 3 ) ), dp . Text ( \"#\" ), dp . Text ( \"#\" ), ] ) QCrow_content = [] for i in QC_rows_funcs : if i . __name__ == \"duplicate_detection\" : stats = pd . read_csv ( ends_with ( local_path ) + str ( i . __name__ ) + \".csv\" ) . round ( 3 ) unique_rows_count = ( \" No. Of Unique Rows: **\" + str ( format ( int ( stats [ stats [ \"metric\" ] == \"unique_rows_count\" ] . value . values ), \",\" , ) ) + \"**\" ) total_rows_count = ( \" No. of Rows: **\" + str ( format ( int ( stats [ stats [ \"metric\" ] == \"rows_count\" ] . value . values ), \",\" ) ) + \"**\" ) duplicate_rows_count = ( \" No. of Duplicate Rows: **\" + str ( format ( int ( stats [ stats [ \"metric\" ] == \"duplicate_rows\" ] . value . values ), \",\" , ) ) + \"**\" ) duplicate_rows_pct = ( \" Percentage of Duplicate Rows: **\" + str ( float ( stats [ stats [ \"metric\" ] == \"duplicate_pct\" ] . value . values * 100.0 ) ) + \" %\" + \"**\" ) QCrow_content . append ( [ dp . Text ( \"### \" + str ( remove_u_score ( i . __name__ ))), dp . Group ( dp . Text ( total_rows_count ), dp . Text ( unique_rows_count ), dp . Text ( duplicate_rows_count ), dp . Text ( duplicate_rows_pct ), rows = 4 , ), dp . Text ( \"#\" ), dp . Text ( \"#\" ), ] ) else : QCrow_content . append ( [ dp . Text ( \"### \" + str ( remove_u_score ( i . __name__ ))), dp . DataTable ( pd . read_csv ( ends_with ( local_path ) + str ( i . __name__ ) + \".csv\" ) . round ( 3 ) ), dp . Text ( \"#\" ), dp . Text ( \"#\" ), ] ) QCcol_content = [ item for sublist in QCcol_content for item in sublist ] QCrow_content = [ item for sublist in QCrow_content for item in sublist ] tab2 = dp . Group ( dp . Text ( \"# \" ), dp . Text ( \"*This section identifies the data quality issues at both row and column level.*\" ), dp . Text ( \"# \" ), dp . Text ( \"# \" ), dp . Select ( blocks = [ dp . Group ( dp . Text ( \"# \" ), dp . Group ( * QCcol_content ), rows = 2 , label = \"Column Level\" , ), dp . Group ( dp . Text ( \"# \" ), dp . Group ( * QCrow_content ), rows = 2 , label = \"Row Level\" ), ], type = dp . SelectType . TABS , ), dp . Text ( \"# \" ), dp . Text ( \"# \" ), label = \"Quality Check\" , ) AA_content = [] for i in AA_funcs + AT_funcs : if i . __name__ == \"correlation_matrix\" : stats = pd . read_csv ( ends_with ( local_path ) + str ( i . __name__ ) + \".csv\" ) . round ( 3 ) feats_order = list ( stats [ \"attribute\" ] . values ) stats = stats . round ( 3 ) fig = px . imshow ( stats [ feats_order ], y = feats_order , color_continuous_scale = global_theme , aspect = \"auto\" , ) fig . layout . plot_bgcolor = global_plot_bg_color fig . layout . paper_bgcolor = global_paper_bg_color AA_content . append ( dp . Group ( dp . Text ( \"##\" ), dp . DataTable ( stats [[ \"attribute\" ] + feats_order ]), dp . Plot ( fig ), rows = 3 , label = remove_u_score ( i . __name__ ), ) ) elif i . __name__ == \"variable_clustering\" : stats = ( pd . read_csv ( ends_with ( local_path ) + str ( i . __name__ ) + \".csv\" ) . round ( 3 ) . sort_values ( by = [ \"Cluster\" ], ascending = True ) ) fig = px . sunburst ( stats , path = [ \"Cluster\" , \"Attribute\" ], values = \"RS_Ratio\" , color_discrete_sequence = global_theme , ) fig . layout . plot_bgcolor = global_plot_bg_color fig . layout . paper_bgcolor = global_paper_bg_color fig . layout . autosize = True AA_content . append ( dp . Group ( dp . Text ( \"##\" ), dp . DataTable ( stats ), dp . Plot ( fig ), rows = 3 , label = remove_u_score ( i . __name__ ), ) ) else : if label_col : stats = pd . read_csv ( ends_with ( local_path ) + str ( i . __name__ ) + \".csv\" ) . round ( 3 ) col_nm = [ x for x in list ( stats . columns ) if \"attribute\" not in x ] stats = stats . sort_values ( col_nm [ 0 ], ascending = True ) fig = px . bar ( stats , x = col_nm [ 0 ], y = \"attribute\" , orientation = \"h\" , color_discrete_sequence = global_theme , ) fig . layout . plot_bgcolor = global_plot_bg_color fig . layout . paper_bgcolor = global_paper_bg_color fig . layout . autosize = True AA_content . append ( dp . Group ( dp . Text ( \"##\" ), dp . DataTable ( stats ), dp . Plot ( fig ), label = remove_u_score ( i . __name__ ), rows = 3 , ) ) # @TODO: is there better templating approach such as jinja tab3 = dp . Group ( dp . Text ( \"# \" ), dp . Text ( \"\"\" *This section analyzes the interaction between different attributes and/or the relationship between an attribute & the binary target variable.* \"\"\" ), dp . Text ( \"# \" ), dp . Text ( \"# \" ), dp . Text ( \"### Association Matrix & Plot\" ), dp . Select ( blocks = AA_content , type = dp . SelectType . DROPDOWN ), dp . Text ( \"### \" ), dp . Text ( \"## \" ), dp . Text ( \"## \" ), dp . Text ( \"## \" ), label = \"Attribute Associations\" , ) dp . Report ( default_template [ 0 ], default_template [ 1 ], dp . Select ( blocks = [ tab1 , tab2 , tab3 ], type = dp . SelectType . TABS ), ) . save ( ends_with ( local_path ) + \"basic_report.html\" , open = True ) if run_type == \"emr\" : bash_cmd = ( \"aws s3 cp \" + ends_with ( local_path ) + \"basic_report.html \" + ends_with ( output_path ) ) subprocess . check_output ([ \"bash\" , \"-c\" , bash_cmd ])","title":"basic_report_generation"},{"location":"api/data_report/basic_report_generation.html#functions","text":"def anovos_basic_report ( spark, idf, id_col='', label_col='', event_label='', output_path='.', run_type='local', print_impact=True)","title":"Functions"},{"location":"api/data_report/report_generation.html","text":"report_generation This module generates the final report output specific to the intermediate data generated across each of the modules. The final report, however, can be proccessed through the config.yaml file or by generating it through the respective functions. Below are some of the functions used to process the final output. line_chart_gen_stability data_analyzer_output drift_stability_ind chart_gen_list executive_summary_gen wiki_generator descriptive_statistics quality_check attribute_associations data_drift_stability plotSeasonalDecompose gen_time_series_plots list_ts_remove_append ts_viz_1_1 \u2014 ts_viz_1_3 ts_viz_2_1 \u2014 ts_viz_2_3 ts_viz_3_1 \u2014 ts_viz_3_3 ts_landscape ts_stats ts_viz_generate anovos_report However, each of the functions have been detailed in the respective sections across the parameters used. Expand source code # coding=utf-8 \"\"\"This module generates the final report output specific to the intermediate data generated across each of the modules. The final report, however, can be proccessed through the config.yaml file or by generating it through the respective functions. Below are some of the functions used to process the final output. - line_chart_gen_stability - data_analyzer_output - drift_stability_ind - chart_gen_list - executive_summary_gen - wiki_generator - descriptive_statistics - quality_check - attribute_associations - data_drift_stability - plotSeasonalDecompose - gen_time_series_plots - list_ts_remove_append - ts_viz_1_1 \u2014 ts_viz_1_3 - ts_viz_2_1 \u2014 ts_viz_2_3 - ts_viz_3_1 \u2014 ts_viz_3_3 - ts_landscape - ts_stats - ts_viz_generate - anovos_report However, each of the functions have been detailed in the respective sections across the parameters used. \"\"\" import json import os import subprocess import datapane as dp import numpy as np import pandas as pd import plotly.express as px import plotly.graph_objects as go from loguru import logger import dateutil.parser from statsmodels.tsa.seasonal import seasonal_decompose import plotly.tools as tls from plotly.subplots import make_subplots from statsmodels.tsa.stattools import adfuller , kpss from sklearn.preprocessing import PowerTransformer from anovos.shared.utils import ends_with import warnings warnings . filterwarnings ( \"ignore\" ) global_theme = px . colors . sequential . Plasma global_theme_r = px . colors . sequential . Plasma_r global_plot_bg_color = \"rgba(0,0,0,0)\" global_paper_bg_color = \"rgba(0,0,0,0)\" default_template = ( dp . HTML ( \"\"\" <html> <img src=\"https://mobilewalla-anovos.s3.amazonaws.com/anovos.png\" style=\"height:100px;display:flex;margin:auto;float:right\" /> </html>\"\"\" ), dp . Text ( \"# ML-Anovos Report\" ), ) def remove_u_score ( col ): \"\"\" This functions help to remove the \"_\" present in a specific text Parameters ---------- col Analysis column containing \"_\" present gets replaced along with upper case conversion Returns ------- String \"\"\" col_ = col . split ( \"_\" ) bl = [] for i in col_ : if i == \"nullColumns\" or i == \"nullRows\" : bl . append ( \"Null\" ) else : bl . append ( i [ 0 ] . upper () + i [ 1 :]) return \" \" . join ( bl ) def line_chart_gen_stability ( df1 , df2 , col ): \"\"\" This function helps to produce charts which are specific to data stability index. It taken into account the stability input along with the analysis column to produce the desired output. Parameters ---------- df1 Analysis dataframe pertaining to summarized stability metrics df2 Analysis dataframe pertaining to historical data col Analysis column Returns ------- DatapaneObject \"\"\" def val_cat ( val ): \"\"\" Parameters ---------- val Returns ------- String \"\"\" if val >= 3.5 : return \"Very Stable\" elif val >= 3 and val < 3.5 : return \"Stable\" elif val >= 2 and val < 3 : return \"Marginally Stable\" elif val >= 1 and val < 2 : return \"Unstable\" elif val >= 0 and val < 1 : return \"Very Unstable\" else : return \"Out of Range\" val_si = list ( df2 [ df2 [ \"attribute\" ] == col ] . stability_index . values )[ 0 ] f1 = go . Figure () f1 . add_trace ( go . Indicator ( mode = \"gauge+number\" , value = val_si , gauge = { \"axis\" : { \"range\" : [ None , 4 ], \"tickwidth\" : 1 , \"tickcolor\" : \"black\" }, \"bgcolor\" : \"white\" , \"steps\" : [ { \"range\" : [ 0 , 1 ], \"color\" : px . colors . sequential . Reds [ 7 ]}, { \"range\" : [ 1 , 2 ], \"color\" : px . colors . sequential . Reds [ 6 ]}, { \"range\" : [ 2 , 3 ], \"color\" : px . colors . sequential . Oranges [ 4 ]}, { \"range\" : [ 3 , 3.5 ], \"color\" : px . colors . sequential . BuGn [ 7 ]}, { \"range\" : [ 3.5 , 4 ], \"color\" : px . colors . sequential . BuGn [ 8 ]}, ], \"threshold\" : { \"line\" : { \"color\" : \"black\" , \"width\" : 3 }, \"thickness\" : 1 , \"value\" : val_si , }, \"bar\" : { \"color\" : global_plot_bg_color }, }, title = { \"text\" : \"Order of Stability: \" + val_cat ( val_si )}, ) ) f1 . update_layout ( height = 400 , font = { \"color\" : \"black\" , \"family\" : \"Arial\" }) f5 = \"Stability Index for \" + str ( col . upper ()) if len ( df1 . columns ) > 0 : df1 = df1 [ df1 [ \"attribute\" ] == col ] f2 = px . line ( df1 , x = \"idx\" , y = \"mean\" , markers = True , title = \"CV of Mean is \" + str ( list ( df2 [ df2 [ \"attribute\" ] == col ] . mean_cv . values )[ 0 ]), ) f2 . update_traces ( line_color = global_theme [ 2 ], marker = dict ( size = 14 )) f2 . layout . plot_bgcolor = global_plot_bg_color f2 . layout . paper_bgcolor = global_paper_bg_color f3 = px . line ( df1 , x = \"idx\" , y = \"stddev\" , markers = True , title = \"CV of Stddev is \" + str ( list ( df2 [ df2 [ \"attribute\" ] == col ] . stddev_cv . values )[ 0 ]), ) f3 . update_traces ( line_color = global_theme [ 6 ], marker = dict ( size = 14 )) f3 . layout . plot_bgcolor = global_plot_bg_color f3 . layout . paper_bgcolor = global_paper_bg_color f4 = px . line ( df1 , x = \"idx\" , y = \"kurtosis\" , markers = True , title = \"CV of Kurtosis is \" + str ( list ( df2 [ df2 [ \"attribute\" ] == col ] . kurtosis_cv . values )[ 0 ]), ) f4 . update_traces ( line_color = global_theme [ 4 ], marker = dict ( size = 14 )) f4 . layout . plot_bgcolor = global_plot_bg_color f4 . layout . paper_bgcolor = global_paper_bg_color return dp . Group ( dp . Text ( \"#\" ), dp . Text ( f5 ), dp . Plot ( f1 ), dp . Group ( dp . Plot ( f2 ), dp . Plot ( f3 ), dp . Plot ( f4 ), columns = 3 ), rows = 4 , label = col , ) else : return dp . Group ( dp . Text ( \"#\" ), dp . Text ( f5 ), dp . Plot ( f1 ), rows = 3 , label = col ) def data_analyzer_output ( master_path , avl_recs_tab , tab_name ): \"\"\" This section produces output in form of datapane objects which is specific to the different data analyzer modules. It is used by referring to the Master path along with the Available list of metrics & the Tab name. Parameters ---------- master_path Path containing all the output from analyzed data avl_recs_tab Available file names from the analysis tab tab_name Analysis tab from association_evaluator / quality_checker / stats_generator Returns ------- DatapaneObject \"\"\" df_list = [] df_plot_list = [] # @FIXME: unused variables plot_list = [] avl_recs_tab = [ x for x in avl_recs_tab if \"global_summary\" not in x ] for index , i in enumerate ( avl_recs_tab ): data = pd . read_csv ( ends_with ( master_path ) + str ( i ) + \".csv\" ) if len ( data . index ) == 0 : continue if tab_name == \"quality_checker\" : if i == \"duplicate_detection\" : duplicate_recs = pd . read_csv ( ends_with ( master_path ) + str ( i ) + \".csv\" ) . round ( 3 ) _unique_rows_count = int ( duplicate_recs [ duplicate_recs [ \"metric\" ] == \"unique_rows_count\" ] . value . values ) _rows_count = int ( duplicate_recs [ duplicate_recs [ \"metric\" ] == \"rows_count\" ] . value . values ) _duplicate_rows_count = int ( duplicate_recs [ duplicate_recs [ \"metric\" ] == \"duplicate_rows\" ] . value . values ) _duplicate_pct = float ( duplicate_recs [ duplicate_recs [ \"metric\" ] == \"duplicate_pct\" ] . value . values * 100.0 ) unique_rows_count = f \" No. Of Unique Rows: ** { _unique_rows_count } , **\" # @FIXME: variable names exists in outer scope rows_count = f \" No. of Rows: ** { _rows_count } , **\" duplicate_rows = ( f \" No. of Duplicate Rows: ** { _duplicate_rows_count } , **\" ) duplicate_pct = f \" Percentage of Duplicate Rows: ** { _duplicate_pct } %**\" df_list . append ( [ dp . Text ( \"### \" + str ( remove_u_score ( i ))), dp . Group ( dp . Text ( rows_count ), dp . Text ( unique_rows_count ), dp . Text ( duplicate_rows ), dp . Text ( duplicate_pct ), rows = 4 , ), dp . Text ( \"#\" ), dp . Text ( \"#\" ), ] ) elif i == \"outlier_detection\" : df_list . append ( [ dp . Text ( \"### \" + str ( remove_u_score ( i ))), dp . DataTable ( pd . read_csv ( ends_with ( master_path ) + str ( i ) + \".csv\" ) . round ( 3 ) ), \"outlier_charts_placeholder\" , ] ) else : df_list . append ( [ dp . Text ( \"### \" + str ( remove_u_score ( i ))), dp . DataTable ( pd . read_csv ( ends_with ( master_path ) + str ( i ) + \".csv\" ) . round ( 3 ) ), dp . Text ( \"#\" ), dp . Text ( \"#\" ), ] ) elif tab_name == \"association_evaluator\" : for j in avl_recs_tab : if j == \"correlation_matrix\" : df_list_ = pd . read_csv ( ends_with ( master_path ) + str ( j ) + \".csv\" ) . round ( 3 ) feats_order = list ( df_list_ [ \"attribute\" ] . values ) df_list_ = df_list_ . round ( 3 ) fig = px . imshow ( df_list_ [ feats_order ], y = feats_order , color_continuous_scale = global_theme , aspect = \"auto\" , ) fig . layout . plot_bgcolor = global_plot_bg_color fig . layout . paper_bgcolor = global_paper_bg_color # fig.update_layout(title_text=str(\"Correlation Plot \")) df_plot_list . append ( dp . Group ( dp . Text ( \"##\" ), dp . DataTable ( df_list_ [[ \"attribute\" ] + feats_order ]), dp . Plot ( fig ), rows = 3 , label = remove_u_score ( j ), ) ) elif j == \"variable_clustering\" : df_list_ = ( pd . read_csv ( ends_with ( master_path ) + str ( j ) + \".csv\" ) . round ( 3 ) . sort_values ( by = [ \"Cluster\" ], ascending = True ) ) fig = px . sunburst ( df_list_ , path = [ \"Cluster\" , \"Attribute\" ], values = \"RS_Ratio\" , color_discrete_sequence = global_theme , ) # fig.update_layout(title_text=str(\"Distribution of homogenous variable across Clusters\")) fig . layout . plot_bgcolor = global_plot_bg_color fig . layout . paper_bgcolor = global_paper_bg_color # fig.update_layout(title_text=str(\"Variable Clustering Plot \")) fig . layout . autosize = True df_plot_list . append ( dp . Group ( dp . Text ( \"##\" ), dp . DataTable ( df_list_ ), dp . Plot ( fig ), rows = 3 , label = remove_u_score ( j ), ) ) else : try : df_list_ = pd . read_csv ( ends_with ( master_path ) + str ( j ) + \".csv\" ) . round ( 3 ) col_nm = [ x for x in list ( df_list_ . columns ) if \"attribute\" not in x ] df_list_ = df_list_ . sort_values ( col_nm [ 0 ], ascending = True ) fig = px . bar ( df_list_ , x = col_nm [ 0 ], y = \"attribute\" , orientation = \"h\" , color_discrete_sequence = global_theme , ) fig . layout . plot_bgcolor = global_plot_bg_color fig . layout . paper_bgcolor = global_paper_bg_color # fig.update_layout(title_text=str(\"Representation of \" + str(remove_u_score(j)))) fig . layout . autosize = True df_plot_list . append ( dp . Group ( dp . Text ( \"##\" ), dp . DataTable ( df_list_ ), dp . Plot ( fig ), label = remove_u_score ( j ), rows = 3 , ) ) except Exception as e : logger . error ( f \"processing failed, error { e } \" ) pass if len ( avl_recs_tab ) == 1 : df_plot_list . append ( dp . Group ( dp . DataTable ( pd . DataFrame ( columns = [ \" \" ], index = range ( 1 )), label = \" \" ), dp . Plot ( blank_chart , label = \" \" ), label = \" \" , ) ) else : pass return df_plot_list else : df_list . append ( dp . DataTable ( pd . read_csv ( ends_with ( master_path ) + str ( i ) + \".csv\" ) . round ( 3 ), label = remove_u_score ( avl_recs_tab [ index ]), ) ) if tab_name == \"quality_checker\" and len ( avl_recs_tab ) == 1 : return df_list [ 0 ], [ dp . Text ( \"#\" ), dp . Plot ( blank_chart )] elif tab_name == \"stats_generator\" and len ( avl_recs_tab ) == 1 : return [ df_list [ 0 ], dp . DataTable ( pd . DataFrame ( columns = [ \" \" ], index = range ( 1 )), label = \" \" ), ] else : return df_list def drift_stability_ind ( missing_recs_drift , drift_tab , missing_recs_stability , stability_tab ): \"\"\" This function helps to produce the drift & stability indicator for further processing. Ideally a data with both drift & stability should produce a list of [1,1] Parameters ---------- missing_recs_drift Missing files from the drift tab drift_tab \"drift_statistics\" missing_recs_stability Missing files from the stability tab stability_tab \"stability_index, stabilityIndex_metrics\" Returns ------- List \"\"\" if len ( missing_recs_drift ) == len ( drift_tab ): drift_ind = 0 else : drift_ind = 1 if len ( missing_recs_stability ) == len ( stability_tab ): stability_ind = 0 elif ( \"stabilityIndex_metrics\" in missing_recs_stability ) and ( \"stability_index\" not in missing_recs_stability ): stability_ind = 0.5 else : stability_ind = 1 return drift_ind , stability_ind def chart_gen_list ( master_path , chart_type , type_col = None ): \"\"\" This function helps to produce the charts in a list object form nested by a datapane object. Parameters ---------- master_path Path containing all the charts same as the other files from data analyzed output chart_type Files containing only the specific chart names for the specific chart category type_col None. Default value is kept as None Returns ------- DatapaneObject \"\"\" plot_list = [] for i in chart_type : col_name = i [ i . find ( \"_\" ) + 1 :] if type_col == \"numerical\" : if col_name in numcols_name . replace ( \" \" , \"\" ) . split ( \",\" ): plot_list . append ( dp . Plot ( go . Figure ( json . load ( open ( ends_with ( master_path ) + i ))), label = col_name , ) ) else : pass elif type_col == \"categorical\" : if col_name in catcols_name . replace ( \" \" , \"\" ) . split ( \",\" ): plot_list . append ( dp . Plot ( go . Figure ( json . load ( open ( ends_with ( master_path ) + i ))), label = col_name , ) ) else : pass else : plot_list . append ( dp . Plot ( go . Figure ( json . load ( open ( ends_with ( master_path ) + i ))), label = col_name , ) ) return plot_list def executive_summary_gen ( master_path , label_col , ds_ind , id_col , iv_threshold , corr_threshold , print_report = False , ): \"\"\" This function helps to produce output specific to the Executive Summary Tab. Parameters ---------- master_path Path containing the input files. label_col Label column. ds_ind Drift stability indicator in list form. id_col ID column. iv_threshold IV threshold beyond which attributes can be called as significant. corr_threshold Correlation threshold beyond which attributes can be categorized under correlated. print_report Printing option flexibility. Default value is kept as False. Returns ------- DatapaneObject / Output[HTML] \"\"\" try : obj_dtls = json . load ( open ( ends_with ( master_path ) + \"freqDist_\" + str ( label_col )) ) # @FIXME: never used local variable text_val = list ( list ( obj_dtls . values ())[ 0 ][ 0 ] . items ())[ 8 ][ 1 ] x_val = list ( list ( obj_dtls . values ())[ 0 ][ 0 ] . items ())[ 11 ][ 1 ] y_val = list ( list ( obj_dtls . values ())[ 0 ][ 0 ] . items ())[ 13 ][ 1 ] label_fig_ = go . Figure ( data = [ go . Pie ( labels = x_val , values = y_val , textinfo = \"label+percent\" , insidetextorientation = \"radial\" , pull = [ 0 , 0.1 ], marker_colors = global_theme , ) ] ) label_fig_ . update_traces ( textposition = \"inside\" , textinfo = \"percent+label\" ) label_fig_ . update_layout ( legend = dict ( orientation = \"h\" , x = 0.5 , yanchor = \"bottom\" , xanchor = \"center\" ) ) label_fig_ . layout . plot_bgcolor = global_plot_bg_color label_fig_ . layout . paper_bgcolor = global_paper_bg_color except Exception as e : logger . error ( f \"processing failed, error { e } \" ) label_fig_ = None a1 = ( \"The dataset contains **\" + str ( f \" { rows_count : ,d } \" ) + \"** records and **\" + str ( numcols_count + catcols_count ) + \"** attributes (**\" + str ( numcols_count ) + \"** numerical + **\" + str ( catcols_count ) + \"** categorical).\" ) if label_col is None : a2 = dp . Group ( dp . Text ( \"- There is **no** target variable in the dataset\" ), dp . Text ( \"- Data Diagnosis:\" ), rows = 2 , ) else : if label_fig_ is None : a2 = dp . Group ( dp . Text ( \"- Target variable is **\" + str ( label_col ) + \"** \" ), dp . Text ( \"- Data Diagnosis:\" ), rows = 2 , ) else : a2 = dp . Group ( dp . Text ( \"- Target variable is **\" + str ( label_col ) + \"** \" ), dp . Plot ( label_fig_ ), dp . Text ( \"- Data Diagnosis:\" ), rows = 3 , ) try : x1 = list ( pd . read_csv ( ends_with ( master_path ) + \"measures_of_dispersion.csv\" ) . query ( \"`cov`>1\" ) . attribute . values ) if len ( x1 ) > 0 : x1_1 = [ \"High Variance\" , x1 ] else : x1_1 = [ \"High Variance\" , None ] except Exception as e : logger . error ( f \"processing failed, error { e } \" ) x1_1 = [ \"High Variance\" , None ] try : x2 = list ( pd . read_csv ( ends_with ( master_path ) + \"measures_of_shape.csv\" ) . query ( \"`skewness`>0\" ) . attribute . values ) if len ( x2 ) > 0 : x2_1 = [ \"Positive Skewness\" , x2 ] else : x2_1 = [ \"Positive Skewness\" , None ] except Exception as e : logger . error ( f \"processing failed, error { e } \" ) x2_1 = [ \"Positive Skewness\" , None ] try : x3 = list ( pd . read_csv ( ends_with ( master_path ) + \"measures_of_shape.csv\" ) . query ( \"`skewness`<0\" ) . attribute . values ) if len ( x3 ) > 0 : x3_1 = [ \"Negative Skewness\" , x3 ] else : x3_1 = [ \"Negative Skewness\" , None ] except Exception as e : logger . error ( f \"processing failed, error { e } \" ) x3_1 = [ \"Negative Skewness\" , None ] try : x4 = list ( pd . read_csv ( ends_with ( master_path ) + \"measures_of_shape.csv\" ) . query ( \"`kurtosis`>0\" ) . attribute . values ) if len ( x4 ) > 0 : x4_1 = [ \"High Kurtosis\" , x4 ] else : x4_1 = [ \"High Kurtosis\" , None ] except Exception as e : logger . error ( f \"processing failed, error { e } \" ) x4_1 = [ \"High Kurtosis\" , None ] try : x5 = list ( pd . read_csv ( ends_with ( master_path ) + \"measures_of_shape.csv\" ) . query ( \"`kurtosis`<0\" ) . attribute . values ) if len ( x5 ) > 0 : x5_1 = [ \"Low Kurtosis\" , x5 ] else : x5_1 = [ \"Low Kurtosis\" , None ] except Exception as e : logger . error ( f \"processing failed, error { e } \" ) x5_1 = [ \"Low Kurtosis\" , None ] try : x6 = list ( pd . read_csv ( ends_with ( master_path ) + \"measures_of_counts.csv\" ) . query ( \"`fill_pct`<0.7\" ) . attribute . values ) if len ( x6 ) > 0 : x6_1 = [ \"Low Fill Rates\" , x6 ] else : x6_1 = [ \"Low Fill Rates\" , None ] except Exception as e : logger . error ( f \"processing failed, error { e } \" ) x6_1 = [ \"Low Fill Rates\" , None ] try : biasedness_df = pd . read_csv ( ends_with ( master_path ) + \"biasedness_detection.csv\" ) if \"treated\" in biasedness_df : x7 = list ( biasedness_df . query ( \"`treated`>0\" ) . attribute . values ) else : x7 = list ( biasedness_df . query ( \"`flagged`>0\" ) . attribute . values ) if len ( x7 ) > 0 : x7_1 = [ \"High Biasedness\" , x7 ] else : x7_1 = [ \"High Biasedness\" , None ] except Exception as e : logger . error ( f \"processing failed, error { e } \" ) x7_1 = [ \"High Biasedness\" , None ] try : x8 = list ( pd . read_csv ( ends_with ( master_path ) + \"outlier_detection.csv\" ) . attribute . values ) if len ( x8 ) > 0 : x8_1 = [ \"Outliers\" , x8 ] else : x8_1 = [ \"Outliers\" , None ] except Exception as e : logger . error ( f \"processing failed, error { e } \" ) x8_1 = [ \"Outliers\" , None ] try : corr_matrx = pd . read_csv ( ends_with ( master_path ) + \"correlation_matrix.csv\" ) corr_matrx = corr_matrx [ list ( corr_matrx . attribute . values )] corr_matrx = corr_matrx . where ( np . triu ( np . ones ( corr_matrx . shape ), k = 1 ) . astype ( np . bool ) ) to_drop = [ column for column in corr_matrx . columns if any ( corr_matrx [ column ] > corr_threshold ) ] if len ( to_drop ) > 0 : x9_1 = [ \"High Correlation\" , to_drop ] else : x9_1 = [ \"High Correlation\" , None ] except Exception as e : logger . error ( f \"processing failed, error { e } \" ) x9_1 = [ \"High Correlation\" , None ] try : x10 = list ( pd . read_csv ( ends_with ( master_path ) + \"IV_calculation.csv\" ) . query ( \"`iv`>\" + str ( iv_threshold )) . attribute . values ) if len ( x10 ) > 0 : x10_1 = [ \"Significant Attributes\" , x10 ] else : x10_1 = [ \"Significant Attributes\" , None ] except Exception as e : logger . error ( f \"processing failed, error { e } \" ) x10_1 = [ \"Significant Attributes\" , None ] blank_list_df = [] for i in [ x1_1 , x2_1 , x3_1 , x4_1 , x5_1 , x6_1 , x7_1 , x8_1 , x9_1 , x10_1 ]: try : for j in i [ 1 ]: blank_list_df . append ([ i [ 0 ], j ]) except Exception as e : logger . error ( f \"processing failed, error { e } \" ) blank_list_df . append ([ i [ 0 ], \"NA\" ]) list_n = [] x1 = pd . DataFrame ( blank_list_df , columns = [ \"Metric\" , \"Attribute\" ]) x1 [ \"Value\" ] = \"\u2714\" all_cols = ( catcols_name . replace ( \" \" , \"\" ) + \",\" + numcols_name . replace ( \" \" , \"\" ) ) . split ( \",\" ) remainder_cols = list ( set ( all_cols ) - set ( x1 . Attribute . values )) total_metrics = set ( list ( x1 . Metric . values )) for i in remainder_cols : for j in total_metrics : list_n . append ([ j , i ]) x2 = pd . DataFrame ( list_n , columns = [ \"Metric\" , \"Attribute\" ]) x2 [ \"Value\" ] = \"\u2718\" x = x1 . append ( x2 , ignore_index = True ) x = ( x . drop_duplicates () . pivot ( index = \"Attribute\" , columns = \"Metric\" , values = \"Value\" ) . fillna ( \"\u2718\" ) . reset_index ()[ [ \"Attribute\" , \"Outliers\" , \"Significant Attributes\" , \"Positive Skewness\" , \"Negative Skewness\" , \"High Variance\" , \"High Correlation\" , \"High Kurtosis\" , \"Low Kurtosis\" , ] ] ) x = x [ x . Attribute . values != \"NA\" ] if ds_ind [ 0 ] == 1 and ds_ind [ 1 ] >= 0.5 : a5 = \"Data Health based on Drift Metrics & Stability Index : \" report = dp . Group ( dp . Text ( \"# \" ), dp . Text ( \"**Key Report Highlights**\" ), dp . Text ( \"- \" + a1 ), a2 , dp . DataTable ( x ), dp . Text ( \"- \" + a5 ), dp . Group ( dp . BigNumber ( heading = \"# Drifted Attributes\" , value = str ( str ( drifted_feats ) + \" out of \" + str ( len_feats )), ), dp . BigNumber ( heading = \"% Drifted Attributes\" , value = str ( np . round (( 100 * drifted_feats / len_feats ), 2 )) + \"%\" , ), dp . BigNumber ( heading = \"# Unstable Attributes\" , value = str ( len ( unstable_attr )) + \" out of \" + str ( len ( total_unstable_attr )), change = \"numerical\" , is_upward_change = True , ), dp . BigNumber ( heading = \"% Unstable Attributes\" , value = str ( np . round ( 100 * len ( unstable_attr ) / len ( total_unstable_attr ), 2 ) ) + \"%\" , ), columns = 4 , ), dp . Text ( \"# \" ), dp . Text ( \"# \" ), label = \"Executive Summary\" , ) if ds_ind [ 0 ] == 0 and ds_ind [ 1 ] >= 0.5 : a5 = \"Data Health based on Stability Index : \" report = dp . Group ( dp . Text ( \"# \" ), dp . Text ( \"**Key Report Highlights**\" ), dp . Text ( \"# \" ), dp . Text ( \"- \" + a1 ), a2 , dp . DataTable ( x ), dp . Text ( \"- \" + a5 ), dp . Group ( dp . BigNumber ( heading = \"# Unstable Attributes\" , value = str ( len ( unstable_attr )) + \" out of \" + str ( len ( total_unstable_attr )), change = \"numerical\" , is_upward_change = True , ), dp . BigNumber ( heading = \"% Unstable Attributes\" , value = str ( np . round ( 100 * len ( unstable_attr ) / len ( total_unstable_attr ), 2 ) ) + \"%\" , ), columns = 2 , ), dp . Text ( \"# \" ), dp . Text ( \"# \" ), label = \"Executive Summary\" , ) if ds_ind [ 0 ] == 1 and ds_ind [ 1 ] == 0 : a5 = \"Data Health based on Drift Metrics : \" report = dp . Group ( dp . Text ( \"# \" ), dp . Text ( \"**Key Report Highlights**\" ), dp . Text ( \"# \" ), dp . Text ( \"- \" + a1 ), a2 , dp . DataTable ( x ), dp . Text ( \"- \" + a5 ), dp . Group ( dp . BigNumber ( heading = \"# Drifted Attributes\" , value = str ( str ( drifted_feats ) + \" out of \" + str ( len_feats )), ), dp . BigNumber ( heading = \"% Drifted Attributes\" , value = str ( np . round (( 100 * drifted_feats / len_feats ), 2 )) + \"%\" , ), columns = 2 , ), dp . Text ( \"# \" ), dp . Text ( \"# \" ), label = \"Executive Summary\" , ) if ds_ind [ 0 ] == 0 and ds_ind [ 1 ] == 0 : report = dp . Group ( dp . Text ( \"# \" ), dp . Text ( \"**Key Report Highlights**\" ), dp . Text ( \"# \" ), dp . Text ( \"- \" + a1 ), a2 , dp . DataTable ( x ), dp . Text ( \"# \" ), label = \"Executive Summary\" , ) if print_report : dp . Report ( default_template [ 0 ], default_template [ 1 ], report ) . save ( ends_with ( master_path ) + \"executive_summary.html\" , open = True ) return report # @FIXME: rename variables with their corresponding within the config files def wiki_generator ( master_path , dataDict_path = None , metricDict_path = None , print_report = False ): \"\"\" This function helps to produce output specific to the Wiki Tab. Parameters ---------- master_path Path containing the input files. dataDict_path Data dictionary path. Default value is kept as None. metricDict_path Metric dictionary path. Default value is kept as None. print_report Printing option flexibility. Default value is kept as False. Returns ------- DatapaneObject / Output[HTML] \"\"\" try : datatype_df = pd . read_csv ( ends_with ( master_path ) + \"data_type.csv\" ) except FileNotFoundError : logger . error ( f \"file { master_path } /data_type.csv doesn't exist, cannot read datatypes\" ) except Exception : logger . info ( \"generate an empty dataframe with columns attribute and data_type \" ) datatype_df = pd . DataFrame ( columns = [ \"attribute\" , \"data_type\" ], index = range ( 1 )) try : data_dict = pd . read_csv ( dataDict_path ) . merge ( datatype_df , how = \"outer\" , on = \"attribute\" ) except FileNotFoundError : logger . error ( f \"file { dataDict_path } doesn't exist, cannot read data dict\" ) except Exception : data_dict = datatype_df try : metric_dict = pd . read_csv ( metricDict_path ) except FileNotFoundError : logger . error ( f \"file { metricDict_path } doesn't exist, cannot read metrics dict\" ) except Exception : metric_dict = pd . DataFrame ( columns = [ \"Section Category\" , \"Section Name\" , \"Metric Name\" , \"Metric Definitions\" , ], index = range ( 1 ), ) report = dp . Group ( dp . Text ( \"# \" ), dp . Text ( \"\"\" *A quick reference to the attributes from the dataset (Data Dictionary) and the metrics computed in the report (Metric Dictionary).* \"\"\" ), dp . Text ( \"# \" ), dp . Text ( \"# \" ), dp . Select ( blocks = [ dp . Group ( dp . Group ( dp . Text ( \"## \" ), dp . DataTable ( data_dict )), label = \"Data Dictionary\" , ), dp . Group ( dp . Text ( \"##\" ), dp . DataTable ( metric_dict ), label = \"Metric Dictionary\" ), ], type = dp . SelectType . TABS , ), dp . Text ( \"# \" ), dp . Text ( \"# \" ), dp . Text ( \"# \" ), dp . Text ( \"# \" ), label = \"Wiki\" , ) if print_report : dp . Report ( default_template [ 0 ], default_template [ 1 ], report ) . save ( ends_with ( master_path ) + \"wiki_generator.html\" , open = True ) return report def descriptive_statistics ( master_path , SG_tabs , avl_recs_SG , missing_recs_SG , all_charts_num_1_ , all_charts_cat_1_ , print_report = False , ): \"\"\" This function helps to produce output specific to the Descriptive Stats Tab. Parameters ---------- master_path Path containing the input files. SG_tabs measures_of_counts','measures_of_centralTendency','measures_of_cardinality','measures_of_percentiles','measures_of_dispersion','measures_of_shape','global_summary' avl_recs_SG Available files from the SG_tabs (Stats Generator tabs) missing_recs_SG Missing files from the SG_tabs (Stats Generator tabs) all_charts_num_1_ Numerical charts (histogram) all collated in a list format supported as per datapane objects all_charts_cat_1_ Categorical charts (barplot) all collated in a list format supported as per datapane objects print_report Printing option flexibility. Default value is kept as False. Returns ------- DatapaneObject / Output[HTML] \"\"\" if \"global_summary\" in avl_recs_SG : cnt = 0 else : cnt = 1 if len ( missing_recs_SG ) + cnt == len ( SG_tabs ): return \"null_report\" else : if \"global_summary\" in avl_recs_SG : l1 = dp . Group ( dp . Text ( \"# \" ), dp . Text ( \"*This section summarizes the dataset with key statistical metrics and distribution plots.*\" ), dp . Text ( \"# \" ), dp . Text ( \"# \" ), dp . Text ( \"### Global Summary\" ), dp . Group ( dp . Text ( \" Total Number of Records: **\" + f \" { rows_count : , } \" + \"**\" ), dp . Text ( \" Total Number of Attributes: **\" + str ( columns_count ) + \"**\" ), dp . Text ( \" Number of Numerical Attributes : **\" + str ( numcols_count ) + \"**\" ), dp . Text ( \" Numerical Attributes Name : **\" + str ( numcols_name ) + \"**\" ), dp . Text ( \" Number of Categorical Attributes : **\" + str ( catcols_count ) + \"**\" ), dp . Text ( \" Categorical Attributes Name : **\" + str ( catcols_name ) + \"**\" ), rows = 6 , ), rows = 8 , ) else : l1 = dp . Text ( \"# \" ) if len ( data_analyzer_output ( master_path , avl_recs_SG , \"stats_generator\" )) > 0 : l2 = dp . Text ( \"### Statistics by Metric Type\" ) l3 = dp . Group ( dp . Select ( blocks = data_analyzer_output ( master_path , avl_recs_SG , \"stats_generator\" ), type = dp . SelectType . TABS , ), dp . Text ( \"# \" ), ) else : l2 = dp . Text ( \"# \" ) l3 = dp . Text ( \"# \" ) if len ( all_charts_num_1_ ) == 0 and len ( all_charts_cat_1_ ) == 0 : l4 = 1 elif len ( all_charts_num_1_ ) == 0 and len ( all_charts_cat_1_ ) > 0 : l4 = ( dp . Text ( \"# \" ), dp . Text ( \"### Attribute Visualization\" ), dp . Select ( blocks = all_charts_cat_1_ , type = dp . SelectType . DROPDOWN ), dp . Text ( \"# \" ), dp . Text ( \"# \" ), ) elif len ( all_charts_num_1_ ) > 0 and len ( all_charts_cat_1_ ) == 0 : l4 = ( dp . Text ( \"# \" ), dp . Text ( \"### Attribute Visualization\" ), dp . Select ( blocks = all_charts_num_1_ , type = dp . SelectType . DROPDOWN ), dp . Text ( \"# \" ), dp . Text ( \"# \" ), ) else : l4 = ( dp . Text ( \"# \" ), dp . Text ( \"### Attribute Visualization\" ), dp . Group ( dp . Select ( blocks = [ dp . Group ( dp . Select ( blocks = all_charts_num_1_ , type = dp . SelectType . DROPDOWN , ), label = \"Numerical\" , ), dp . Group ( dp . Select ( blocks = all_charts_cat_1_ , type = dp . SelectType . DROPDOWN , ), label = \"Categorical\" , ), ], type = dp . SelectType . TABS , ) ), dp . Text ( \"# \" ), dp . Text ( \"# \" ), ) if l4 == 1 : report = dp . Group ( l1 , dp . Text ( \"# \" ), l2 , l3 , dp . Text ( \"# \" ), dp . Text ( \"# \" ), label = \"Descriptive Statistics\" , ) else : report = dp . Group ( l1 , dp . Text ( \"# \" ), l2 , l3 , * l4 , dp . Text ( \"# \" ), dp . Text ( \"# \" ), label = \"Descriptive Statistics\" , ) if print_report : dp . Report ( default_template [ 0 ], default_template [ 1 ], report ) . save ( ends_with ( master_path ) + \"descriptive_statistics.html\" , open = True ) return report def quality_check ( master_path , QC_tabs , avl_recs_QC , missing_recs_QC , all_charts_num_3_ , print_report = False , ): \"\"\" This function helps to produce output specific to the Quality Checker Tab. Parameters ---------- master_path Path containing the input files. QC_tabs nullColumns_detection','IDness_detection','biasedness_detection','invalidEntries_detection','duplicate_detection','nullRows_detection','outlier_detection' avl_recs_QC Available files from the QC_tabs (Quality Checker tabs) missing_recs_QC Missing files from the QC_tabs (Quality Checker tabs) all_charts_num_3_ Numerical charts (outlier charts) all collated in a list format supported as per datapane objects print_report Printing option flexibility. Default value is kept as False. Returns ------- DatapaneObject / Output[HTML] \"\"\" c_ = [] r_ = [] if len ( missing_recs_QC ) == len ( QC_tabs ): return \"null_report\" else : row_wise = [ \"duplicate_detection\" , \"nullRows_detection\" ] col_wise = [ \"nullColumns_detection\" , \"IDness_detection\" , \"biasedness_detection\" , \"invalidEntries_detection\" , \"outlier_detection\" , ] row_wise_ = [ p for p in row_wise if p in avl_recs_QC ] col_wise_ = [ p for p in col_wise if p in avl_recs_QC ] len_row_wise = len ([ p for p in row_wise if p in avl_recs_QC ]) len_col_wise = len ([ p for p in col_wise if p in avl_recs_QC ]) if len_row_wise == 0 : c = data_analyzer_output ( master_path , col_wise_ , \"quality_checker\" ) for i in c : for j in i : if j == \"outlier_charts_placeholder\" and len ( all_charts_num_3_ ) > 1 : c_ . append ( dp . Select ( blocks = all_charts_num_3_ , type = dp . SelectType . DROPDOWN ) ) elif ( j == \"outlier_charts_placeholder\" and len ( all_charts_num_3_ ) == 0 ): c_ . append ( dp . Plot ( blank_chart )) else : c_ . append ( j ) report = dp . Group ( dp . Text ( \"# \" ), dp . Text ( \"*This section identifies the data quality issues at both row and column level.*\" ), dp . Text ( \"# \" ), dp . Text ( \"# \" ), dp . Group ( * c_ ), dp . Text ( \"# \" ), dp . Text ( \"# \" ), rows = 8 , label = \"Quality Check\" , ) elif len_col_wise == 0 : r = data_analyzer_output ( master_path , row_wise_ , \"quality_checker\" ) for i in r : for j in i : r_ . append ( j ) report = dp . Group ( dp . Text ( \"# \" ), dp . Text ( \"*This section identifies the data quality issues at both row and column level.*\" ), dp . Text ( \"# \" ), dp . Text ( \"# \" ), dp . Group ( * r_ ), dp . Text ( \"# \" ), dp . Text ( \"# \" ), rows = 8 , label = \"Quality Check\" , ) else : c = data_analyzer_output ( master_path , col_wise_ , \"quality_checker\" ) for i in c : for j in i : if j == \"outlier_charts_placeholder\" and len ( all_charts_num_3_ ) > 1 : c_ . append ( dp . Select ( blocks = all_charts_num_3_ , type = dp . SelectType . DROPDOWN ) ) elif ( j == \"outlier_charts_placeholder\" and len ( all_charts_num_3_ ) == 0 ): c_ . append ( dp . Plot ( blank_chart )) else : c_ . append ( j ) r = data_analyzer_output ( master_path , row_wise_ , \"quality_checker\" ) for i in r : for j in i : r_ . append ( j ) report = dp . Group ( dp . Text ( \"# \" ), dp . Text ( \"*This section identifies the data quality issues at both row and column level.*\" ), dp . Text ( \"# \" ), dp . Text ( \"# \" ), dp . Select ( blocks = [ dp . Group ( dp . Text ( \"# \" ), dp . Group ( * c_ ), rows = 2 , label = \"Column Level\" ), dp . Group ( dp . Text ( \"# \" ), dp . Group ( * r_ ), rows = 2 , label = \"Row Level\" ), ], type = dp . SelectType . TABS , ), dp . Text ( \"# \" ), dp . Text ( \"# \" ), label = \"Quality Check\" , ) if print_report : dp . Report ( default_template [ 0 ], default_template [ 1 ], report ) . save ( ends_with ( master_path ) + \"quality_check.html\" , open = True ) return report def attribute_associations ( master_path , AE_tabs , avl_recs_AE , missing_recs_AE , label_col , all_charts_num_2_ , all_charts_cat_2_ , print_report = False , ): \"\"\" This function helps to produce output specific to the Attribute Association Tab. Parameters ---------- master_path Path containing the input files. AE_tabs correlation_matrix','IV_calculation','IG_calculation','variable_clustering' avl_recs_AE Available files from the AE_tabs (Association Evaluator tabs) missing_recs_AE Missing files from the AE_tabs (Association Evaluator tabs) label_col label column all_charts_num_2_ Numerical charts (histogram) all collated in a list format supported as per datapane objects all_charts_cat_2_ Categorical charts (barplot) all collated in a list format supported as per datapane objects print_report Printing option flexibility. Default value is kept as False. Returns ------- DatapaneObject / Output[HTML] \"\"\" if ( len ( missing_recs_AE ) == len ( AE_tabs )) and ( ( len ( all_charts_num_2_ ) + len ( all_charts_cat_2_ )) == 0 ): return \"null_report\" else : if len ( all_charts_num_2_ ) == 0 and len ( all_charts_cat_2_ ) == 0 : target_association_rep = dp . Text ( \"##\" ) else : if len ( all_charts_num_2_ ) > 0 and len ( all_charts_cat_2_ ) == 0 : target_association_rep = dp . Group ( dp . Text ( \"### Attribute to Target Association\" ), dp . Text ( \"\"\" *Bivariate Distribution considering the event captured across different attribute splits (or categories)* \"\"\" ), dp . Select ( blocks = all_charts_num_2_ , type = dp . SelectType . DROPDOWN ), label = \"Numerical\" , ) elif len ( all_charts_num_2_ ) == 0 and len ( all_charts_cat_2_ ) > 0 : target_association_rep = dp . Group ( dp . Text ( \"### Attribute to Target Association\" ), dp . Text ( \"\"\" *Bivariate Distribution considering the event captured across different attribute splits (or categories)* \"\"\" ), dp . Select ( blocks = all_charts_cat_2_ , type = dp . SelectType . DROPDOWN ), label = \"Categorical\" , ) else : target_association_rep = dp . Group ( dp . Text ( \"### Attribute to Target Association\" ), dp . Select ( blocks = [ dp . Group ( dp . Select ( blocks = all_charts_num_2_ , type = dp . SelectType . DROPDOWN , ), label = \"Numerical\" , ), dp . Group ( dp . Select ( blocks = all_charts_cat_2_ , type = dp . SelectType . DROPDOWN , ), label = \"Categorical\" , ), ], type = dp . SelectType . TABS , ), dp . Text ( \"\"\" *Event Rate is defined as % of event label (i.e. label 1) in a bin or a categorical value of an attribute.* \"\"\" ), dp . Text ( \"# \" ), ) if len ( missing_recs_AE ) == len ( AE_tabs ): report = dp . Group ( dp . Text ( \"# \" ), dp . Text ( \"\"\" *This section analyzes the interaction between different attributes and/or the relationship between an attribute & the binary target variable.* \"\"\" ), dp . Text ( \"## \" ), target_association_rep , dp . Text ( \"## \" ), dp . Text ( \"## \" ), label = \"Attribute Associations\" , ) else : report = dp . Group ( dp . Text ( \"# \" ), dp . Text ( \"\"\" *This section analyzes the interaction between different attributes and/or the relationship between an attribute & the binary target variable.* \"\"\" ), dp . Text ( \"# \" ), dp . Text ( \"# \" ), dp . Text ( \"### Association Matrix & Plot\" ), dp . Select ( blocks = data_analyzer_output ( master_path , avl_recs_AE , tab_name = \"association_evaluator\" ), type = dp . SelectType . DROPDOWN , ), dp . Text ( \"### \" ), dp . Text ( \"## \" ), target_association_rep , dp . Text ( \"## \" ), dp . Text ( \"## \" ), label = \"Attribute Associations\" , ) if print_report : dp . Report ( default_template [ 0 ], default_template [ 1 ], report ) . save ( ends_with ( master_path ) + \"attribute_associations.html\" , open = True ) return report def data_drift_stability ( master_path , ds_ind , id_col , drift_threshold_model , all_drift_charts_ , print_report = False , ): \"\"\" This function helps to produce output specific to the Data Drift & Stability Tab. Parameters ---------- master_path Path containing the input files. ds_ind Drift stability indicator in list form. id_col ID column drift_threshold_model threshold which the user is specifying for tagging an attribute to be drifted or not all_drift_charts_ Charts (histogram/barplot) all collated in a list format supported as per datapane objects print_report Printing option flexibility. Default value is kept as False. Returns ------- DatapaneObject / Output[HTML] \"\"\" line_chart_list = [] if ds_ind [ 0 ] > 0 : fig_metric_drift = go . Figure () fig_metric_drift . add_trace ( go . Scatter ( x = list ( drift_df [ drift_df . flagged . values == 1 ][ metric_drift [ 0 ]] . values ), y = list ( drift_df [ drift_df . flagged . values == 1 ] . attribute . values ), marker = dict ( color = global_theme [ 1 ], size = 14 ), mode = \"markers\" , name = metric_drift [ 0 ], ) ) fig_metric_drift . add_trace ( go . Scatter ( x = list ( drift_df [ drift_df . flagged . values == 1 ][ metric_drift [ 1 ]] . values ), y = list ( drift_df [ drift_df . flagged . values == 1 ] . attribute . values ), marker = dict ( color = global_theme [ 3 ], size = 14 ), mode = \"markers\" , name = metric_drift [ 1 ], ) ) fig_metric_drift . add_trace ( go . Scatter ( x = list ( drift_df [ drift_df . flagged . values == 1 ][ metric_drift [ 2 ]] . values ), y = list ( drift_df [ drift_df . flagged . values == 1 ] . attribute . values ), marker = dict ( color = global_theme [ 5 ], size = 14 ), mode = \"markers\" , name = metric_drift [ 2 ], ) ) fig_metric_drift . add_trace ( go . Scatter ( x = list ( drift_df [ drift_df . flagged . values == 1 ][ metric_drift [ 3 ]] . values ), y = list ( drift_df [ drift_df . flagged . values == 1 ] . attribute . values ), marker = dict ( color = global_theme [ 7 ], size = 14 ), mode = \"markers\" , name = metric_drift [ 3 ], ) ) fig_metric_drift . add_vrect ( x0 = 0 , x1 = drift_threshold_model , fillcolor = global_theme [ 7 ], opacity = 0.1 , layer = \"below\" , line_width = 1 , ), fig_metric_drift . update_layout ( legend = dict ( orientation = \"h\" , x = 0.5 , yanchor = \"bottom\" , xanchor = \"center\" ) ) fig_metric_drift . layout . plot_bgcolor = global_plot_bg_color fig_metric_drift . layout . paper_bgcolor = global_paper_bg_color fig_metric_drift . update_xaxes ( showline = True , linewidth = 2 , gridcolor = px . colors . sequential . Greys [ 1 ] ) fig_metric_drift . update_yaxes ( showline = True , linewidth = 2 , gridcolor = px . colors . sequential . Greys [ 2 ] ) # Drift Chart - 2 fig_gauge_drift = go . Figure ( go . Indicator ( domain = { \"x\" : [ 0 , 1 ], \"y\" : [ 0 , 1 ]}, value = drifted_feats , mode = \"gauge+number\" , title = { \"text\" : \"\" }, gauge = { \"axis\" : { \"range\" : [ None , len_feats ]}, \"bar\" : { \"color\" : px . colors . sequential . Reds [ 7 ]}, \"steps\" : [ { \"range\" : [ 0 , drifted_feats ], \"color\" : px . colors . sequential . Reds [ 8 ], }, { \"range\" : [ drifted_feats , len_feats ], \"color\" : px . colors . sequential . Greens [ 8 ], }, ], \"threshold\" : { \"line\" : { \"color\" : \"black\" , \"width\" : 3 }, \"thickness\" : 1 , \"value\" : len_feats , }, }, ) ) fig_gauge_drift . update_layout ( font = { \"color\" : \"black\" , \"family\" : \"Arial\" }) def drift_text_gen ( drifted_feats , len_feats ): \"\"\" Parameters ---------- drifted_feats count of attributes drifted len_feats count of attributes passed for analysis Returns ------- String \"\"\" if drifted_feats == 0 : text = \"\"\" *Drift barometer does not indicate any drift in the underlying data. Please refer to the metric values as displayed in the above table & comparison plot for better understanding* \"\"\" elif drifted_feats == 1 : text = ( \"*Drift barometer indicates that \" + str ( drifted_feats ) + \" out of \" + str ( len_feats ) + \" (\" + str ( np . round (( 100 * drifted_feats / len_feats ), 2 )) + \"%) attributes has been drifted from its source behaviour.*\" ) elif drifted_feats > 1 : text = ( \"*Drift barometer indicates that \" + str ( drifted_feats ) + \" out of \" + str ( len_feats ) + \" (\" + str ( np . round (( 100 * drifted_feats / len_feats ), 2 )) + \"%) attributes have been drifted from its source behaviour.*\" ) else : text = \"\" return text else : pass if ds_ind [ 0 ] == 0 and ds_ind [ 1 ] == 0 : return \"null_report\" elif ds_ind [ 0 ] == 0 and ds_ind [ 1 ] > 0.5 : for i in total_unstable_attr : if len ( total_unstable_attr ) > 1 : line_chart_list . append ( line_chart_gen_stability ( df1 = df_stability , df2 = df_si_ , col = i ) ) else : line_chart_list . append ( line_chart_gen_stability ( df1 = df_stability , df2 = df_si_ , col = i ) ) line_chart_list . append ( dp . Plot ( blank_chart , label = \" \" )) report = dp . Group ( dp . Text ( \"# \" ), dp . Text ( \"\"\" *This section examines the dataset stability wrt the baseline dataset (via computing drift statistics) and/or wrt the historical datasets (via computing stability index).* \"\"\" ), dp . Text ( \"# \" ), dp . Text ( \"# \" ), dp . Text ( \"### Data Stability Analysis\" ), dp . DataTable ( df_si ), dp . Select ( blocks = line_chart_list , type = dp . SelectType . DROPDOWN ), dp . Group ( dp . Text ( \"**Stability Index Interpretation:**\" ), dp . Plot ( plot_index_stability ), rows = 2 , ), label = \"Drift & Stability\" , ) elif ds_ind [ 0 ] == 1 and ds_ind [ 1 ] == 0 : if len ( all_drift_charts_ ) > 0 : report = dp . Group ( dp . Text ( \"# \" ), dp . Text ( \"\"\" *This section examines the dataset stability wrt the baseline dataset (via computing drift statistics) and/or wrt the historical datasets (via computing stability index).* \"\"\" ), dp . Text ( \"# \" ), dp . Text ( \"# \" ), dp . Text ( \"### Data Drift Analysis\" ), dp . DataTable ( drift_df ), dp . Text ( \"*An attribute is flagged as drifted if any drift metric is found to be above the threshold of \" + str ( drift_threshold_model ) + \".*\" ), dp . Text ( \"##\" ), dp . Select ( blocks = all_drift_charts_ , type = dp . SelectType . DROPDOWN ), dp . Text ( \"\"\" *Source & Target datasets were compared to see the % deviation at decile level for numerical attributes and at individual category level for categorical attributes* \"\"\" ), dp . Text ( \"### \" ), dp . Text ( \"### \" ), dp . Text ( \"### Data Health\" ), dp . Group ( dp . Plot ( fig_metric_drift ), dp . Plot ( fig_gauge_drift ), columns = 2 ), dp . Group ( dp . Text ( \"*Representation of attributes across different computed Drift Metrics*\" ), dp . Text ( drift_text_gen ( drifted_feats , len_feats )), columns = 2 , ), label = \"Drift & Stability\" , ) else : report = dp . Group ( dp . Text ( \"# \" ), dp . Text ( \"\"\" *This section examines the dataset stability wrt the baseline dataset (via computing drift statistics) and/or wrt the historical datasets (via computing stability index).* \"\"\" ), dp . Text ( \"# \" ), dp . Text ( \"# \" ), dp . Text ( \"### Data Drift Analysis\" ), dp . DataTable ( drift_df ), dp . Text ( \"*An attribute is flagged as drifted if any drift metric is found to be above the threshold of \" + str ( drift_threshold_model ) + \".*\" ), dp . Text ( \"##\" ), dp . Text ( \"### \" ), dp . Text ( \"### Data Health\" ), dp . Group ( dp . Plot ( fig_metric_drift ), dp . Plot ( fig_gauge_drift ), columns = 2 ), dp . Group ( dp . Text ( \"*Representation of attributes across different computed Drift Metrics*\" ), dp . Text ( drift_text_gen ( drifted_feats , len_feats )), columns = 2 , ), label = \"Drift & Stability\" , ) elif ds_ind [ 0 ] == 1 and ds_ind [ 1 ] >= 0.5 : for i in total_unstable_attr : if len ( total_unstable_attr ) > 1 : line_chart_list . append ( line_chart_gen_stability ( df1 = df_stability , df2 = df_si_ , col = i ) ) else : line_chart_list . append ( line_chart_gen_stability ( df1 = df_stability , df2 = df_si_ , col = i ) ) line_chart_list . append ( dp . Plot ( blank_chart , label = \" \" )) if len ( all_drift_charts_ ) > 0 : report = dp . Group ( dp . Text ( \"# \" ), dp . Text ( \"\"\" *This section examines the dataset stability wrt the baseline dataset (via computing drift statistics) and/or wrt the historical datasets (via computing stability index).* \"\"\" ), dp . Text ( \"# \" ), dp . Text ( \"# \" ), dp . Text ( \"### Data Drift Analysis\" ), dp . DataTable ( drift_df ), dp . Text ( \"*An attribute is flagged as drifted if any drift metric is found to be above the threshold of \" + str ( drift_threshold_model ) + \".*\" ), dp . Text ( \"##\" ), dp . Select ( blocks = all_drift_charts_ , type = dp . SelectType . DROPDOWN ), dp . Text ( \"\"\" *Source & Target datasets were compared to see the % deviation at decile level for numerical attributes and at individual category level for categorical attributes* \"\"\" ), dp . Text ( \"### \" ), dp . Text ( \"### \" ), dp . Text ( \"### Data Health\" ), dp . Group ( dp . Plot ( fig_metric_drift ), dp . Plot ( fig_gauge_drift ), columns = 2 ), dp . Group ( dp . Text ( \"*Representation of attributes across different computed Drift Metrics*\" ), dp . Text ( drift_text_gen ( drifted_feats , len_feats )), columns = 2 , ), dp . Text ( \"## \" ), dp . Text ( \"## \" ), dp . Text ( \"### Data Stability Analysis\" ), dp . DataTable ( df_si ), dp . Select ( blocks = line_chart_list , type = dp . SelectType . DROPDOWN ), dp . Group ( dp . Text ( \"**Stability Index Interpretation:**\" ), dp . Plot ( plot_index_stability ), rows = 2 , ), label = \"Drift & Stability\" , ) else : report = dp . Group ( dp . Text ( \"# \" ), dp . Text ( \"\"\" *This section examines the dataset stability wrt the baseline dataset (via computing drift statistics) and/or wrt the historical datasets (via computing stability index).* \"\"\" ), dp . Text ( \"# \" ), dp . Text ( \"# \" ), dp . Text ( \"### Data Drift Analysis\" ), dp . DataTable ( drift_df ), dp . Text ( \"*An attribute is flagged as drifted if any drift metric is found to be above the threshold of \" + str ( drift_threshold_model ) + \".*\" ), dp . Text ( \"##\" ), dp . Text ( \"### Data Health\" ), dp . Group ( dp . Plot ( fig_metric_drift ), dp . Plot ( fig_gauge_drift ), columns = 2 ), dp . Group ( dp . Text ( \"*Representation of attributes across different computed Drift Metrics*\" ), dp . Text ( drift_text_gen ( drifted_feats , len_feats )), columns = 2 , ), dp . Text ( \"## \" ), dp . Text ( \"## \" ), dp . Text ( \"### Data Stability Analysis\" ), dp . DataTable ( df_si ), dp . Select ( blocks = line_chart_list , type = dp . SelectType . DROPDOWN ), dp . Group ( dp . Text ( \"**Stability Index Interpretation:**\" ), dp . Plot ( plot_index_stability ), rows = 2 , ), label = \"Drift & Stability\" , ) elif ds_ind [ 0 ] == 0 and ds_ind [ 1 ] >= 0.5 : for i in total_unstable_attr : if len ( total_unstable_attr ) > 1 : line_chart_list . append ( line_chart_gen_stability ( df1 = df_stability , df2 = df_si_ , col = i ) ) else : line_chart_list . append ( line_chart_gen_stability ( df1 = df_stability , df2 = df_si_ , col = i ) ) line_chart_list . append ( dp . Plot ( blank_chart , label = \" \" )) report = dp . Group ( dp . Text ( \"# \" ), dp . Text ( \"\"\" *This section examines the dataset stability wrt the baseline dataset (via computing drift statistics) and/or wrt the historical datasets (via computing stability index).* \"\"\" ), dp . Text ( \"# \" ), dp . Text ( \"# \" ), dp . Text ( \"### Data Stability Analysis\" ), dp . DataTable ( df_si ), dp . Select ( blocks = line_chart_list , type = dp . SelectType . DROPDOWN ), dp . Group ( dp . Text ( \"**Stability Index Interpretation:**\" ), dp . Plot ( plot_index_stability ), rows = 2 , ), label = \"Drift & Stability\" , ) else : for i in total_unstable_attr : if len ( total_unstable_attr ) > 1 : line_chart_list . append ( line_chart_gen_stability ( df1 = df_stability , df2 = df_si_ , col = i ) ) else : line_chart_list . append ( line_chart_gen_stability ( df1 = df_stability , df2 = df_si_ , col = i ) ) line_chart_list . append ( dp . Plot ( blank_chart , label = \" \" )) if len ( all_drift_charts_ ) > 0 : report = dp . Group ( dp . Text ( \"# \" ), dp . Text ( \"\"\" *This section examines the dataset stability wrt the baseline dataset (via computing drift statistics) and/or wrt the historical datasets (via computing stability index).* \"\"\" ), dp . Text ( \"# \" ), dp . Text ( \"# \" ), dp . Text ( \"### Data Drift Analysis\" ), dp . DataTable ( drift_df ), dp . Text ( \"*An attribute is flagged as drifted if any drift metric is found to be above the threshold of \" + str ( drift_threshold_model ) + \".*\" ), dp . Text ( \"##\" ), dp . Select ( blocks = all_drift_charts_ , type = dp . SelectType . DROPDOWN ), dp . Text ( \"\"\" *Source & Target datasets were compared to see the % deviation at decile level for numerical attributes and at individual category level for categorical attributes* \"\"\" ), dp . Text ( \"### \" ), dp . Text ( \"### \" ), dp . Text ( \"### Data Health\" ), dp . Group ( dp . Plot ( fig_metric_drift ), dp . Plot ( fig_gauge_drift ), columns = 2 ), dp . Group ( dp . Text ( \"*Representation of attributes across different computed Drift Metrics*\" ), dp . Text ( drift_text_gen ( drifted_feats , len_feats )), columns = 2 , ), dp . Text ( \"## \" ), dp . Text ( \"## \" ), dp . Text ( \"### Data Stability Analysis\" ), dp . DataTable ( df_si ), dp . Select ( blocks = line_chart_list , type = dp . SelectType . DROPDOWN ), dp . Group ( dp . Text ( \"**Stability Index Interpretation:**\" ), dp . Plot ( plot_index_stability ), rows = 2 , ), label = \"Drift & Stability\" , ) else : report = dp . Group ( dp . Text ( \"# \" ), dp . Text ( \"\"\" *This section examines the dataset stability wrt the baseline dataset (via computing drift statistics) and/or wrt the historical datasets (via computing stability index).* \"\"\" ), dp . Text ( \"# \" ), dp . Text ( \"# \" ), dp . Text ( \"### Data Drift Analysis\" ), dp . DataTable ( drift_df ), dp . Text ( \"*An attribute is flagged as drifted if any drift metric is found to be above the threshold of \" + str ( drift_threshold_model ) + \".*\" ), dp . Text ( \"##\" ), dp . Text ( \"### Data Health\" ), dp . Group ( dp . Plot ( fig_metric_drift ), dp . Plot ( fig_gauge_drift ), columns = 2 ), dp . Group ( dp . Text ( \"*Representation of attributes across different computed Drift Metrics*\" ), dp . Text ( drift_text_gen ( drifted_feats , len_feats )), columns = 2 , ), dp . Text ( \"## \" ), dp . Text ( \"## \" ), dp . Text ( \"### Data Stability Analysis\" ), dp . DataTable ( df_si ), dp . Select ( blocks = line_chart_list , type = dp . SelectType . DROPDOWN ), dp . Group ( dp . Text ( \"**Stability Index Interpretation:**\" ), dp . Plot ( plot_index_stability ), rows = 2 , ), label = \"Drift & Stability\" , ) if print_report : dp . Report ( default_template [ 0 ], default_template [ 1 ], report ) . save ( ends_with ( master_path ) + \"data_drift_stability.html\" , open = True ) return report def plotSeasonalDecompose ( base_path , x_col , y_col , metric_col = \"median\" , title = \"Seasonal Decomposition\" ): \"\"\" This function helps to produce output specific to the Seasonal Decomposition of Time Series. Ideally it's expected to source a data containing atleast 2 cycles or 24 months as the most. Parameters ---------- base_path Base path which is the same as Master path where the aggregated data resides. x_col Timestamp / date column name y_col Numerical column names metric_col Metric of aggregation. Options can be between \"Median\", \"Mean\", \"Min\", \"Max\" title \"Title Description\" Returns ------- Plot \"\"\" df = pd . read_csv ( ends_with ( base_path ) + x_col + \"_\" + y_col + \"_daily.csv\" ) . dropna () df [ x_col ] = pd . to_datetime ( df [ x_col ], format = \"%Y-%m- %d %H:%M:%S. %f \" ) df = df . set_index ( x_col ) if len ([ x for x in df . columns if \"min\" in x ]) == 0 : # result = seasonal_decompose(df[metric_col],model=\"additive\") pass else : result = seasonal_decompose ( df [ metric_col ], model = \"additive\" , period = 12 ) fig = make_subplots ( rows = 2 , cols = 2 , subplot_titles = [ \"Observed\" , \"Trend\" , \"Seasonal\" , \"Residuals\" ], ) # fig = go.Figure() fig . add_trace ( go . Scatter ( x = df . index , y = result . observed , name = \"Observed\" , mode = \"lines+markers\" , line = dict ( color = global_theme [ 0 ]), ), row = 1 , col = 1 , ) fig . add_trace ( go . Scatter ( x = df . index , y = result . trend , name = \"Trend\" , mode = \"lines+markers\" , line = dict ( color = global_theme [ 2 ]), ), row = 1 , col = 2 , ) fig . add_trace ( go . Scatter ( x = df . index , y = result . seasonal , name = \"Seasonal\" , mode = \"lines+markers\" , line = dict ( color = global_theme [ 4 ]), ), row = 2 , col = 1 , ) fig . add_trace ( go . Scatter ( x = df . index , y = result . resid , name = \"Residuals\" , mode = \"lines+markers\" , line = dict ( color = global_theme [ 6 ]), ), row = 2 , col = 2 , ) # fig.add_trace(go.Scatter(x=df.index, y=result.observed, name =\"Observed\", mode='lines+markers',line=dict(color=global_theme[0]))) # fig.add_trace(go.Scatter(x=df.index, y=result.trend, name =\"Trend\", mode='lines+markers',line=dict(color=global_theme[2]))) # fig.add_trace(go.Scatter(x=df.index, y=result.seasonal, name =\"Seasonal\", mode='lines+markers',line=dict(color=global_theme[4]))) # fig.add_trace(go.Scatter(x=df.index, y=result.resid, name =\"Residuals\", mode='lines+markers',line=dict(color=global_theme[6]))) fig . layout . plot_bgcolor = global_plot_bg_color fig . layout . paper_bgcolor = global_paper_bg_color fig . update_xaxes ( gridcolor = px . colors . sequential . Greys [ 1 ]) fig . update_yaxes ( gridcolor = px . colors . sequential . Greys [ 1 ]) fig . update_layout ( autosize = True , width = 2000 , height = 800 ) fig . update_layout ( legend = dict ( orientation = \"h\" , x = 0.5 , yanchor = \"bottom\" , xanchor = \"center\" ) ) return fig def gen_time_series_plots ( base_path , x_col , y_col , time_cat ): \"\"\" This function helps to produce Time Series Plots by sourcing the aggregated data as Daily/Hourly/Weekly level. Parameters ---------- base_path Base path which is the same as Master path where the aggregated data resides. x_col Timestamp / date column name y_col Numerical column names time_cat Time category of analysis which can be between \"Daily\", \"Hourly\", \"Weekly\" Returns ------- Plot \"\"\" df = pd . read_csv ( ends_with ( base_path ) + x_col + \"_\" + y_col + \"_\" + time_cat + \".csv\" ) . dropna () if len ([ x for x in df . columns if \"min\" in x ]) == 0 : if time_cat == \"daily\" : # x_col = x_col + \"_ts\" fig = px . line ( df , x = x_col , y = \"count\" , color = y_col , color_discrete_sequence = global_theme , ) fig . update_layout ( xaxis = dict ( rangeselector = dict ( buttons = list ( [ dict ( count = 1 , label = \"1m\" , step = \"month\" , stepmode = \"backward\" , ), dict ( count = 3 , label = \"3m\" , step = \"month\" , stepmode = \"backward\" , ), dict ( count = 6 , label = \"6m\" , step = \"month\" , stepmode = \"backward\" , ), dict ( count = 1 , label = \"YTD\" , step = \"year\" , stepmode = \"todate\" ), dict ( count = 1 , label = \"1y\" , step = \"year\" , stepmode = \"backward\" , ), dict ( step = \"all\" ), ] ) ), rangeslider = dict ( visible = True ), type = \"date\" , ) ) elif time_cat == \"weekly\" : fig = px . bar ( df , x = \"dow\" , y = \"count\" , color = y_col , color_discrete_sequence = global_theme , ) # fig.update_layout(barmode='stack') elif time_cat == \"hourly\" : fig = px . bar ( df , x = \"daypart_cat\" , y = \"count\" , color = y_col , color_discrete_sequence = global_theme , ) # fig.update_layout(barmode='stack') else : pass else : if time_cat == \"daily\" : # x_col = x_col + \"_ts\" f1 = go . Scatter ( x = list ( df [ x_col ]), y = list ( df [ \"min\" ]), name = \"Min\" , line = dict ( color = global_theme [ 6 ]), ) f2 = go . Scatter ( x = list ( df [ x_col ]), y = list ( df [ \"max\" ]), name = \"Max\" , line = dict ( color = global_theme [ 4 ]), ) f3 = go . Scatter ( x = list ( df [ x_col ]), y = list ( df [ \"mean\" ]), name = \"Mean\" , line = dict ( color = global_theme [ 2 ]), ) f4 = go . Scatter ( x = list ( df [ x_col ]), y = list ( df [ \"median\" ]), name = \"Median\" , line = dict ( color = global_theme [ 0 ]), ) fig = go . Figure ( data = [ f1 , f2 , f3 , f4 ]) fig . update_layout ( xaxis = dict ( rangeselector = dict ( buttons = list ( [ dict ( count = 1 , label = \"1m\" , step = \"month\" , stepmode = \"backward\" , ), dict ( count = 3 , label = \"3m\" , step = \"month\" , stepmode = \"backward\" , ), dict ( count = 6 , label = \"6m\" , step = \"month\" , stepmode = \"backward\" , ), dict ( count = 1 , label = \"YTD\" , step = \"year\" , stepmode = \"todate\" ), dict ( count = 1 , label = \"1y\" , step = \"year\" , stepmode = \"backward\" , ), dict ( step = \"all\" ), ] ) ), rangeslider = dict ( visible = True ), type = \"date\" , ) ) elif time_cat == \"weekly\" : f1 = go . Bar ( x = list ( df [ \"dow\" ]), y = list ( df [ \"min\" ]), marker_color = global_theme [ 6 ], name = \"Min\" , ) f2 = go . Bar ( x = list ( df [ \"dow\" ]), y = list ( df [ \"max\" ]), marker_color = global_theme [ 4 ], name = \"Max\" , ) f3 = go . Bar ( x = list ( df [ \"dow\" ]), y = list ( df [ \"mean\" ]), marker_color = global_theme [ 2 ], name = \"Mean\" , ) f4 = go . Bar ( x = list ( df [ \"dow\" ]), y = list ( df [ \"median\" ]), marker_color = global_theme [ 0 ], name = \"Median\" , ) fig = go . Figure ( data = [ f1 , f2 , f3 , f4 ]) fig . update_layout ( barmode = \"group\" ) elif time_cat == \"hourly\" : f1 = go . Bar ( x = list ( df [ \"daypart_cat\" ]), y = list ( df [ \"min\" ]), marker_color = global_theme [ 6 ], name = \"Min\" , ) f2 = go . Bar ( x = list ( df [ \"daypart_cat\" ]), y = list ( df [ \"max\" ]), marker_color = global_theme [ 4 ], name = \"Max\" , ) f3 = go . Bar ( x = list ( df [ \"daypart_cat\" ]), y = list ( df [ \"mean\" ]), marker_color = global_theme [ 2 ], name = \"Mean\" , ) f4 = go . Bar ( x = list ( df [ \"daypart_cat\" ]), y = list ( df [ \"median\" ]), marker_color = global_theme [ 0 ], name = \"Median\" , ) fig = go . Figure ( data = [ f1 , f2 , f3 , f4 ]) fig . update_layout ( barmode = \"group\" ) else : pass fig . layout . plot_bgcolor = global_plot_bg_color fig . layout . paper_bgcolor = global_paper_bg_color fig . update_xaxes ( gridcolor = px . colors . sequential . Greys [ 1 ]) fig . update_yaxes ( gridcolor = px . colors . sequential . Greys [ 1 ]) fig . update_layout ( legend = dict ( orientation = \"h\" , x = 0.5 , yanchor = \"bottom\" , xanchor = \"center\" ) ) return fig def list_ts_remove_append ( l , opt ): \"\"\" This function helps to remove or append \"_ts\" from any list. Parameters ---------- l List containing column name opt Option to choose between 1 & Others to enable the functionality of removing or appending \"_ts\" within the elements of a list Returns ------- List \"\"\" ll = [] if opt == 1 : for i in l : if i [ - 3 :] == \"_ts\" : ll . append ( i [ 0 : - 3 :]) else : ll . append ( i ) return ll else : for i in l : if i [ - 3 :] == \"_ts\" : ll . append ( i ) else : ll . append ( i + \"_ts\" ) return ll def ts_viz_1_1 ( base_path , x_col , y_col , output_type ): \"\"\" Parameters ---------- base_path Base path which is the same as Master path where the aggregated data resides. x_col Timestamp / date column name y_col Numerical column names output_type Time category of analysis which can be between \"Daily\", \"Hourly\", \"Weekly\" Returns ------- Plot \"\"\" ts_fig = gen_time_series_plots ( base_path , x_col , y_col , output_type ) return ts_fig def ts_viz_1_2 ( base_path , ts_col , col_list , output_type ): \"\"\" Parameters ---------- base_path Base path which is the same as Master path where the aggregated data resides. ts_col Timestamp / date column name col_list Numerical / Categorical column names output_type Time category of analysis which can be between \"Daily\", \"Hourly\", \"Weekly\" Returns ------- DatapaneObject \"\"\" bl = [] for i in col_list : if len ( col_list ) > 1 : bl . append ( dp . Group ( ts_viz_1_1 ( base_path , ts_col , i , output_type ), label = i )) else : bl . append ( dp . Group ( ts_viz_1_1 ( base_path , ts_col , i , output_type ), label = i )) bl . append ( dp . Plot ( blank_chart , label = \"_\" )) return dp . Select ( blocks = bl , type = dp . SelectType . DROPDOWN ) def ts_viz_1_3 ( base_path , ts_col , num_cols , cat_cols , output_type ): \"\"\" Parameters ---------- base_path Base path which is the same as Master path where the aggregated data resides. ts_col Timestamp / date column name num_cols Numerical column names cat_cols Categorical column names output_type Time category of analysis which can be between \"Daily\", \"Hourly\", \"Weekly\" Returns ------- DatapaneObject \"\"\" ts_v = [] # print(num_cols) # print(cat_cols) if len ( num_cols ) == 0 : for i in ts_col : if len ( ts_col ) > 1 : ts_v . append ( dp . Group ( ts_viz_1_2 ( base_path , i , cat_cols , output_type ), label = i ) ) else : ts_v . append ( dp . Group ( ts_viz_1_2 ( base_path , i , cat_cols , output_type ), label = i ) ) ts_v . append ( dp . Plot ( blank_chart , label = \"_\" )) elif len ( cat_cols ) == 0 : for i in ts_col : if len ( ts_col ) > 1 : ts_v . append ( dp . Group ( ts_viz_1_2 ( base_path , i , num_cols , output_type ), label = i ) ) else : ts_v . append ( dp . Group ( ts_viz_1_2 ( base_path , i , num_cols , output_type ), label = i ) ) ts_v . append ( dp . Plot ( blank_chart , label = \"_\" )) elif ( len ( num_cols ) >= 1 ) & ( len ( cat_cols ) >= 1 ): for i in ts_col : if len ( ts_col ) > 1 : ts_v . append ( dp . Group ( dp . Select ( blocks = [ dp . Group ( ts_viz_1_2 ( base_path , i , num_cols , output_type ), label = \"Numerical\" , ), dp . Group ( ts_viz_1_2 ( base_path , i , cat_cols , output_type ), label = \"Categorical\" , ), ], type = dp . SelectType . TABS , ), label = i , ) ) else : ts_v . append ( dp . Group ( dp . Select ( blocks = [ dp . Group ( ts_viz_1_2 ( base_path , i , num_cols , output_type ), label = \"Numerical\" , ), dp . Group ( ts_viz_1_2 ( base_path , i , cat_cols , output_type ), label = \"Categorical\" , ), ], type = dp . SelectType . TABS , ), label = i , ) ) ts_v . append ( dp . Plot ( blank_chart , label = \"_\" )) return dp . Select ( blocks = ts_v , type = dp . SelectType . DROPDOWN ) def ts_viz_2_1 ( base_path , x_col , y_col ): \"\"\" Parameters ---------- base_path Base path which is the same as Master path where the aggregated data resides. x_col Timestamp / date column name y_col Numerical column names Returns ------- DatapaneObject \"\"\" ts_fig = [] for i in [ \"mean\" , \"median\" , \"min\" , \"max\" ]: ts_fig . append ( dp . Plot ( plotSeasonalDecompose ( base_path , x_col , y_col , metric_col = i ), label = i . title (), ) ) return dp . Select ( blocks = ts_fig , type = dp . SelectType . TABS ) def ts_viz_2_2 ( base_path , ts_col , col_list ): \"\"\" Parameters ---------- base_path Base path which is the same as Master path where the aggregated data resides. ts_col Timestamp / date column name col_list Numerical column names Returns ------- DatapaneObject \"\"\" bl = [] for i in col_list : if len ( col_list ) > 1 : bl . append ( dp . Group ( ts_viz_2_1 ( base_path , ts_col , i ), label = i )) else : bl . append ( dp . Group ( ts_viz_2_1 ( base_path , ts_col , i ), label = i )) bl . append ( dp . Group ( dp . Plot ( blank_chart , label = \" \" ), label = \" \" )) return dp . Select ( blocks = bl , type = dp . SelectType . DROPDOWN ) def ts_viz_2_3 ( base_path , ts_col , num_cols ): \"\"\" Parameters ---------- base_path Base path which is the same as Master path where the aggregated data resides. ts_col Timestamp / date column name num_cols Numerical column names Returns ------- DatapaneObject \"\"\" ts_v = [] if len ( ts_col ) > 1 : for i in ts_col : f = list ( pd . read_csv ( ends_with ( base_path ) + \"stats_\" + i + \"_2.csv\" ) . count_unique_dates . values )[ 0 ] if f >= 24 : ts_v . append ( dp . Group ( ts_viz_2_2 ( base_path , i , num_cols ), label = i )) else : ts_v . append ( dp . Group ( dp . Text ( \"The plots couldn't be displayed as x must have 2 complete cycles requires 24 observations. x only has \" + str ( f ) + \" observation(s)\" ), label = i , ) ) else : for i in ts_col : f = list ( pd . read_csv ( ends_with ( base_path ) + \"stats_\" + i + \"_2.csv\" ) . count_unique_dates . values )[ 0 ] if f >= 24 : ts_v . append ( dp . Group ( ts_viz_2_2 ( base_path , i , num_cols ), label = i )) ts_v . append ( dp . Plot ( blank_chart , label = \"_\" )) else : ts_v . append ( dp . Group ( dp . Text ( \"The plots couldn't be displayed as x must have 2 complete cycles requires 24 observations. x only has \" + str ( f ) + \" observation(s)\" ), label = i , ) ) ts_v . append ( dp . Plot ( blank_chart , label = \"_\" )) return dp . Select ( blocks = ts_v , type = dp . SelectType . DROPDOWN ) def ts_landscape ( base_path , ts_cols , id_col ): \"\"\" This function helps to produce a basic landscaping view of the data by picking up the base path for reading the aggregated data and specified by the timestamp / date column & the ID column. Parameters ---------- base_path Base path which is the same as Master path where the aggregated data resides. ts_col Timestamp / date column name id_col ID Column Returns ------- DatapaneObject \"\"\" if ts_cols is None : return dp . Text ( \"#\" ) else : df_stats_ts = [] for i in ts_cols : if len ( ts_cols ) > 1 : df_stats_ts . append ( dp . Group ( dp . Group ( dp . Text ( \"# \" ), dp . Text ( \"*ID considered here is : \" + str ( id_col ) + \"*\" ), dp . Text ( \"# \" ), dp . Text ( \"#### Consistency Analysis Of Dates\" ), dp . DataTable ( pd . read_csv ( ends_with ( base_path ) + \"stats_\" + i + \"_1.csv\" ) . set_index ( \"attribute\" ) . T , label = i , ), rows = 5 , ), dp . Group ( dp . Text ( \"*The Percentile distribution across different bins of ID-Date / Date-ID combination should be in a considerable range to determine the regularity of Time series. In an ideal scenario the proportion of dates within each ID should be same. Also, the count of IDs across unique dates should be consistent for a balanced distribution*\" ), dp . Text ( \"# \" ), dp . Text ( \"#### Vital Statistics\" ), dp . DataTable ( pd . read_csv ( ends_with ( base_path ) + \"stats_\" + i + \"_2.csv\" ) . T . rename ( columns = { 0 : \"\" }), label = i , ), rows = 4 , ), label = i , rows = 2 , ) ) else : df_stats_ts . append ( dp . Group ( dp . Group ( dp . Text ( \"# \" ), dp . Text ( \"*ID considered here is : \" + str ( id_col ) + \"*\" ), dp . Text ( \"#### Consistency Analysis Of Dates\" ), dp . Text ( \"# \" ), dp . DataTable ( pd . read_csv ( ends_with ( base_path ) + \"stats_\" + i + \"_1.csv\" ) . set_index ( \"attribute\" ) . T , label = i , ), rows = 5 , ), dp . Group ( dp . Text ( \"# \" ), dp . Text ( \"#### Vital Statistics\" ), dp . DataTable ( pd . read_csv ( ends_with ( base_path ) + \"stats_\" + i + \"_2.csv\" ) . T . rename ( columns = { 0 : \"\" }), label = i , ), rows = 3 , ), label = i , rows = 2 , ) ) df_stats_ts . append ( dp . Plot ( blank_chart , label = \"_\" )) return dp . Group ( dp . Text ( \"### Time Stamp Data Diagnosis\" ), dp . Select ( blocks = df_stats_ts , type = dp . SelectType . DROPDOWN ), rows = 2 , ) def lambda_cat ( val ): \"\"\" Parameters ---------- val Value of Box Cox Test which translates into the transformation to be applied. Returns ------- String \"\"\" if val < - 1 : return \"Reciprocal Square Transform\" elif val >= - 1 and val < - 0.5 : return \"Reciprocal Transform\" elif val >= - 0.5 and val < 0 : return \"Receiprocal Square Root Transform\" elif val >= 0 and val < 0.5 : return \"Log Transform\" elif val >= 0.5 and val < 1 : return \"Square Root Transform\" elif val >= 1 and val < 2 : return \"No Transform\" elif val >= 2 : return \"Square Transform\" else : return \"ValueOutOfRange\" def ts_viz_3_1 ( base_path , x_col , y_col ): \"\"\" Parameters ---------- base_path Base path which is the same as Master path where the aggregated data resides. x_col Timestamp / date column name y_col Numerical column names Returns ------- DatapaneObject \"\"\" ts_fig = [] df = pd . read_csv ( ends_with ( base_path ) + x_col + \"_\" + y_col + \"_daily.csv\" ) . dropna () df [ x_col ] = pd . to_datetime ( df [ x_col ], format = \"%Y-%m- %d %H:%M:%S. %f \" ) df = df . set_index ( x_col ) for metric_col in [ \"mean\" , \"median\" , \"min\" , \"max\" ]: adf_test = round ( adfuller ( df [ metric_col ])[ 0 ], 3 ), round ( adfuller ( df [ metric_col ])[ 1 ], 3 ) kpss_test = round ( kpss ( df [ metric_col ], regression = \"ct\" )[ 0 ], 3 ), round ( kpss ( df [ metric_col ], regression = \"ct\" )[ 1 ], 3 ) if adf_test [ 1 ] < 0.05 : adf_flag = True else : adf_flag = False if kpss_test [ 1 ] < 0.05 : kpss_flag = True else : kpss_flag = False # df[metric_col] = df[metric_col].apply(lambda x: boxcox1p(x,0.25)) # lambda_box_cox = round(boxcox(df[metric_col])[1],5) fit = PowerTransformer ( method = \"yeo-johnson\" ) try : lambda_box_cox = round ( fit . fit ( np . array ( df [ metric_col ]) . reshape ( - 1 , 1 )) . lambdas_ [ 0 ], 3 ) cnt = 0 except : cnt = 1 if cnt == 0 : # df[metric_col+\"_transformed\"] = boxcox(df[metric_col],lmbda=lambda_box_cox) df [ metric_col + \"_transformed\" ] = fit . transform ( np . array ( df [ metric_col ]) . reshape ( - 1 , 1 ) ) fig = make_subplots ( rows = 1 , cols = 2 , subplot_titles = [ \"Pre-Transformation\" , \"Post-Transformation\" ], ) fig . add_trace ( go . Scatter ( x = df . index , y = df [ metric_col ], mode = \"lines+markers\" , name = metric_col , line = dict ( color = global_theme [ 1 ]), ), row = 1 , col = 1 , ) fig . add_trace ( go . Scatter ( x = df . index , y = df [ metric_col + \"_transformed\" ], mode = \"lines+markers\" , name = metric_col + \"_transformed\" , line = dict ( color = global_theme [ 7 ]), ), row = 1 , col = 2 , ) fig . layout . plot_bgcolor = global_plot_bg_color fig . layout . paper_bgcolor = global_paper_bg_color fig . update_xaxes ( gridcolor = px . colors . sequential . Greys [ 1 ]) fig . update_yaxes ( gridcolor = px . colors . sequential . Greys [ 1 ]) fig . update_layout ( autosize = True , width = 2000 , height = 400 ) fig . update_layout ( legend = dict ( orientation = \"h\" , x = 0.5 , yanchor = \"bottom\" , xanchor = \"center\" ) ) ts_fig . append ( dp . Group ( dp . Group ( dp . BigNumber ( heading = \"ADF Test Statistic\" , value = adf_test [ 0 ], change = adf_test [ 1 ], is_upward_change = adf_flag , ), dp . BigNumber ( heading = \"KPSS Test Statistic\" , value = kpss_test [ 0 ], change = kpss_test [ 1 ], is_upward_change = kpss_flag , ), dp . BigNumber ( heading = \"Box-Cox Transformation\" , value = lambda_box_cox , change = str ( lambda_cat ( lambda_box_cox )), is_upward_change = True , ), columns = 3 , ), dp . Text ( \"#### Transformation View\" ), dp . Text ( \"Below Transformation is basis the inferencing from the Box Cox Transformation. The Lambda value of \" + str ( lambda_box_cox ) + \" indicates a \" + str ( lambda_cat ( lambda_box_cox )) + \". A Pre-Post Transformation Visualization is done for better clarity. \" ), dp . Plot ( fig ), dp . Text ( \"**Guidelines :** \" ), dp . Text ( \"**ADF** : *The more negative the statistic, the more likely we are to reject the null hypothesis. If the p-value is less than the significance level of 0.05, we can reject the null hypothesis and take that the series is stationary*\" ), dp . Text ( \"**KPSS** : *If the p-value is high, we cannot reject the null hypothesis. So the series is stationary.*\" ), label = metric_col . title (), ) ) else : ts_fig . append ( dp . Group ( dp . Group ( dp . BigNumber ( heading = \"ADF Test Statistic\" , value = adf_test [ 0 ], change = adf_test [ 1 ], is_upward_change = adf_flag , ), dp . BigNumber ( heading = \"KPSS Test Statistic\" , value = kpss_test [ 0 ], change = kpss_test [ 1 ], is_upward_change = kpss_flag , ), dp . BigNumber ( heading = \"Box-Cox Transformation\" , value = \"ValueOutOfRange\" , change = \"ValueOutOfRange\" , is_upward_change = True , ), columns = 3 , ), dp . Text ( \"**Guidelines :** \" ), dp . Text ( \"**ADF** : *The more negative the statistic, the more likely we are to reject the null hypothesis. If the p-value is less than the significance level of 0.05, we can reject the null hypothesis and take that the series is stationary*\" ), dp . Text ( \"**KPSS** : *If the p-value is high, we cannot reject the null hypothesis. So the series is stationary.*\" ), label = metric_col . title (), ) ) return dp . Select ( blocks = ts_fig , type = dp . SelectType . TABS ) def ts_viz_3_2 ( base_path , ts_col , col_list ): \"\"\" Parameters ---------- base_path Base path which is the same as Master path where the aggregated data resides. ts_col Timestamp / date column name col_list Numerical column names Returns ------- DatapaneObject \"\"\" bl = [] for i in col_list : if len ( num_cols ) > 1 : bl . append ( dp . Group ( ts_viz_3_1 ( base_path , ts_col , i ), label = i )) else : bl . append ( dp . Group ( ts_viz_3_1 ( base_path , ts_col , i ), label = i )) bl . append ( dp . Group ( dp . Plot ( blank_chart , label = \" \" ), label = \" \" )) return dp . Select ( blocks = bl , type = dp . SelectType . DROPDOWN ) def ts_viz_3_3 ( base_path , ts_col , num_cols ): \"\"\" Parameters ---------- base_path Base path which is the same as Master path where the aggregated data resides. ts_col Timestamp / date column name num_cols Numerical column names Returns ------- DatapaneObject \"\"\" # f = list(pd.read_csv(ends_with(base_path) + \"stats_\" + i + \"_2.csv\").count_unique_dates.values)[0] # if f >= 6: if len ( ts_col ) > 1 : ts_v = [] for i in ts_col : f = list ( pd . read_csv ( ends_with ( base_path ) + \"stats_\" + i + \"_2.csv\" ) . count_unique_dates . values )[ 0 ] if f >= 6 : ts_v . append ( dp . Group ( ts_viz_3_2 ( base_path , i , num_cols ), label = i )) else : ts_v . append ( dp . Group ( dp . Text ( \"The data contains insufficient data points for the desired transformation analysis. Please ensure the number of unique dates is sufficient.\" ), label = i , ) ) else : ts_v = [] for i in ts_col : f = list ( pd . read_csv ( ends_with ( base_path ) + \"stats_\" + i + \"_2.csv\" ) . count_unique_dates . values )[ 0 ] if f >= 6 : ts_v . append ( dp . Group ( ts_viz_3_2 ( base_path , i , num_cols ), label = i )) ts_v . append ( dp . Plot ( blank_chart , label = \"_\" )) else : ts_v . append ( dp . Group ( dp . Text ( \"The data contains insufficient data points for the desired transformation analysis. Please ensure the number of unique dates is sufficient.\" ), label = i , ) ) ts_v . append ( dp . Plot ( blank_chart , label = \"_\" )) return dp . Select ( blocks = ts_v , type = dp . SelectType . DROPDOWN ) def ts_stats ( base_path ): \"\"\" This function helps to read the base data containing desired input and produces output specific to the `ts_cols_stats.csv` file Parameters ---------- base_path Base path which is the same as Master path where the aggregated data resides. Returns ------- List \"\"\" df = pd . read_csv ( base_path + \"ts_cols_stats.csv\" ) all_stats = [] for i in range ( 0 , 7 ): try : all_stats . append ( df [ df . index . values == i ] . values [ 0 ][ 0 ] . split ( \",\" )) except : all_stats . append ([]) c0 = pd . DataFrame ( all_stats [ 0 ], columns = [ \"attributes\" ]) c1 = pd . DataFrame ( list_ts_remove_append ( all_stats [ 1 ], 1 ), columns = [ \"attributes\" ]) c1 [ \"Analyzed Attributes\" ] = \"\u2714\" c2 = pd . DataFrame ( list_ts_remove_append ( all_stats [ 2 ], 1 ), columns = [ \"attributes\" ]) c2 [ \"Attributes Identified\" ] = \"\u2714\" c3 = pd . DataFrame ( list_ts_remove_append ( all_stats [ 3 ], 1 ), columns = [ \"attributes\" ]) c3 [ \"Attributes Pre-Existed\" ] = \"\u2714\" c4 = pd . DataFrame ( list_ts_remove_append ( all_stats [ 4 ], 1 ), columns = [ \"attributes\" ]) c4 [ \"Overall TimeStamp Attributes\" ] = \"\u2714\" c5 = list_ts_remove_append ( all_stats [ 5 ], 1 ) c6 = list_ts_remove_append ( all_stats [ 6 ], 1 ) return c0 , c1 , c2 , c3 , c4 , c5 , c6 def ts_viz_generate ( master_path , id_col , print_report = False , output_type = None ): \"\"\" This function helps to produce the output in the nested / recursive function supported by datapane. Eventually this is populated at the final report. Parameters ---------- master_path Master path where the aggregated data resides. id_col ID Column print_report Option to specify whether the Report needs to be saved or not. True / False can be used to specify the needful. output_type Time category of analysis which can be between \"Daily\", \"Hourly\", \"Weekly\" Returns ------- DatapaneObject / Output[HTML] \"\"\" master_path = ends_with ( master_path ) try : c0 , c1 , c2 , c3 , c4 , c5 , c6 = ts_stats ( master_path ) except : return \"null_report\" stats_df = ( c0 . merge ( c1 , on = \"attributes\" , how = \"left\" ) . merge ( c2 , on = \"attributes\" , how = \"left\" ) . merge ( c3 , on = \"attributes\" , how = \"left\" ) . merge ( c4 , on = \"attributes\" , how = \"left\" ) . fillna ( \"\u2718\" ) ) global num_cols global cat_cols num_cols , cat_cols = c5 , c6 final_ts_cols = list ( ts_stats ( master_path )[ 4 ] . attributes . values ) if output_type == \"daily\" : report = dp . Group ( dp . Text ( \"# \" ), dp . Text ( \"*This section summarizes the information about timestamp features and how they are interactive with other attributes. An exhaustive diagnosis is done by looking at different time series components, how they could be useful in deriving insights for further downstream applications*\" ), dp . Text ( \"# \" ), dp . Text ( \"# \" ), dp . Text ( \"### Basic Landscaping\" ), dp . Text ( \"Out of **\" + str ( len ( list ( ts_stats ( master_path )[ 1 ] . attributes . values ))) + \"** potential attributes in the data, the module could locate **\" + str ( len ( final_ts_cols )) + \"** attributes as Timestamp\" ), dp . DataTable ( stats_df ), ts_landscape ( master_path , final_ts_cols , id_col ), dp . Text ( \"*Lower the **CoV** (Coefficient Of Variation), Higher the Consistency between the consecutive dates. Similarly the Mean & Variance should be consistent over time*\" ), dp . Text ( \"### Visualization across the Shortlisted Timestamp Attributes\" ), ts_viz_1_3 ( master_path , final_ts_cols , num_cols , cat_cols , output_type ), dp . Text ( \"### Decomposed View\" ), ts_viz_2_3 ( master_path , final_ts_cols , num_cols ), dp . Text ( \"### Stationarity & Transformations\" ), ts_viz_3_3 ( master_path , final_ts_cols , num_cols ), dp . Text ( \"#\" ), dp . Text ( \"#\" ), label = \"Time Series Analyzer\" , ) else : report = dp . Group ( dp . Text ( \"# \" ), dp . Text ( \"*This section summarizes the information about timestamp features and how they are interactive with other attributes. An exhaustive diagnosis is done by looking at different time series components, how they could be useful in deriving insights for further downstream applications*\" ), dp . Text ( \"# \" ), dp . Text ( \"# \" ), dp . Text ( \"### Basic Landscaping\" ), dp . Text ( \"Out of **\" + str ( len ( list ( ts_stats ( master_path )[ 1 ] . attributes . values ))) + \"** potential attributes in the data, the module could locate **\" + str ( len ( final_ts_cols )) + \"** attributes as Timestamp\" ), dp . DataTable ( stats_df ), ts_landscape ( master_path , final_ts_cols , id_col ), dp . Text ( \"*Lower the **CoV** (Coefficient Of Variation), Higher the Consistency between the consecutive dates. Similarly the Mean & Variance should be consistent over time*\" ), dp . Text ( \"### Visualization across the Shortlisted Timestamp Attributes\" ), ts_viz_1_3 ( master_path , final_ts_cols , num_cols , cat_cols , output_type ), dp . Text ( \"#\" ), dp . Text ( \"#\" ), label = \"Time Series Analyzer\" , ) if print_report : dp . Report ( default_template [ 0 ], default_template [ 1 ], report ) . save ( ends_with ( master_path ) + \"time_series_analyzer.html\" , open = True ) return report def anovos_report ( master_path , id_col = \"\" , label_col = \"\" , corr_threshold = 0.4 , iv_threshold = 0.02 , drift_threshold_model = 0.1 , dataDict_path = \".\" , metricDict_path = \".\" , run_type = \"local\" , final_report_path = \".\" , output_type = None , ): \"\"\" This function actually helps to produce the final report by scanning through the output processed from the data analyzer module. Parameters ---------- master_path Path containing the input files. id_col ID column (Default value = \"\") label_col label column (Default value = \"\") corr_threshold Correlation threshold beyond which attributes can be categorized under correlated. (Default value = 0.4) iv_threshold IV threshold beyond which attributes can be called as significant. (Default value = 0.02) drift_threshold_model threshold which the user is specifying for tagging an attribute to be drifted or not (Default value = 0.1) dataDict_path Data dictionary path. Default value is kept as None. metricDict_path Metric dictionary path. Default value is kept as None. run_type local or emr or databricks option. Default is kept as local final_report_path Path where the report will be saved. (Default value = \".\") output_type Time category of analysis which can be between \"Daily\", \"Hourly\", \"Weekly\" Returns ------- Output[HTML] \"\"\" if run_type == \"emr\" : bash_cmd = ( \"aws s3 cp --recursive \" + ends_with ( master_path ) + \" \" + ends_with ( \"report_stats\" ) ) master_path = \"report_stats\" subprocess . check_output ([ \"bash\" , \"-c\" , bash_cmd ]) if \"global_summary.csv\" not in os . listdir ( master_path ): print ( \"Minimum supporting data is unavailable, hence the Report could not be generated.\" ) return None global global_summary_df global numcols_name global catcols_name global rows_count global columns_count global numcols_count global catcols_count global blank_chart global df_si_ global df_si global unstable_attr global total_unstable_attr global drift_df global metric_drift global drift_df global len_feats global drift_df_stats global drifted_feats global df_stability global n_df_stability global stability_interpretation_table global plot_index_stability SG_tabs = [ \"measures_of_counts\" , \"measures_of_centralTendency\" , \"measures_of_cardinality\" , \"measures_of_percentiles\" , \"measures_of_dispersion\" , \"measures_of_shape\" , \"global_summary\" , ] QC_tabs = [ \"nullColumns_detection\" , \"IDness_detection\" , \"biasedness_detection\" , \"invalidEntries_detection\" , \"duplicate_detection\" , \"nullRows_detection\" , \"outlier_detection\" , ] AE_tabs = [ \"correlation_matrix\" , \"IV_calculation\" , \"IG_calculation\" , \"variable_clustering\" , ] drift_tab = [ \"drift_statistics\" ] stability_tab = [ \"stability_index\" , \"stabilityIndex_metrics\" ] avl_SG , avl_QC , avl_AE = [], [], [] stability_interpretation_table = pd . DataFrame ( [ [ \"0-1\" , \"Very Unstable\" ], [ \"1-2\" , \"Unstable\" ], [ \"2-3\" , \"Marginally Stable\" ], [ \"3-3.5\" , \"Stable\" ], [ \"3.5-4\" , \"Very Stable\" ], ], columns = [ \"StabilityIndex\" , \"StabilityOrder\" ], ) plot_index_stability = go . Figure ( data = [ go . Table ( header = dict ( values = list ( stability_interpretation_table . columns ), fill_color = px . colors . sequential . Greys [ 2 ], align = \"center\" , font = dict ( size = 12 ), ), cells = dict ( values = [ stability_interpretation_table . StabilityIndex , stability_interpretation_table . StabilityOrder , ], line_color = px . colors . sequential . Greys [ 2 ], fill_color = \"white\" , align = \"center\" , height = 25 , ), columnwidth = [ 2 , 10 ], ) ] ) plot_index_stability . update_layout ( margin = dict ( l = 20 , r = 700 , t = 20 , b = 20 )) blank_chart = go . Figure () blank_chart . update_layout ( autosize = False , width = 10 , height = 10 ) blank_chart . layout . plot_bgcolor = global_plot_bg_color blank_chart . layout . paper_bgcolor = global_paper_bg_color blank_chart . update_xaxes ( visible = False ) blank_chart . update_yaxes ( visible = False ) global_summary_df = pd . read_csv ( ends_with ( master_path ) + \"global_summary.csv\" ) rows_count = int ( global_summary_df [ global_summary_df . metric . values == \"rows_count\" ] . value . values [ 0 ] ) catcols_count = int ( global_summary_df [ global_summary_df . metric . values == \"catcols_count\" ] . value . values [ 0 ] ) numcols_count = int ( global_summary_df [ global_summary_df . metric . values == \"numcols_count\" ] . value . values [ 0 ] ) columns_count = int ( global_summary_df [ global_summary_df . metric . values == \"columns_count\" ] . value . values [ 0 ] ) if catcols_count > 0 : catcols_name = \",\" . join ( list ( global_summary_df [ global_summary_df . metric . values == \"catcols_name\" ] . value . values ) ) else : catcols_name = \"\" if numcols_count > 0 : numcols_name = \",\" . join ( list ( global_summary_df [ global_summary_df . metric . values == \"numcols_name\" ] . value . values ) ) else : numcols_name = \"\" all_files = os . listdir ( master_path ) eventDist_charts = [ x for x in all_files if \"eventDist\" in x ] stats_files = [ x for x in all_files if \".csv\" in x ] freq_charts = [ x for x in all_files if \"freqDist\" in x ] outlier_charts = [ x for x in all_files if \"outlier\" in x ] drift_charts = [ x for x in all_files if \"drift\" in x and \".csv\" not in x ] all_charts_num_1_ = chart_gen_list ( master_path , chart_type = freq_charts , type_col = \"numerical\" ) all_charts_num_2_ = chart_gen_list ( master_path , chart_type = eventDist_charts , type_col = \"numerical\" ) all_charts_num_3_ = chart_gen_list ( master_path , chart_type = outlier_charts , type_col = \"numerical\" ) all_charts_cat_1_ = chart_gen_list ( master_path , chart_type = freq_charts , type_col = \"categorical\" ) all_charts_cat_2_ = chart_gen_list ( master_path , chart_type = eventDist_charts , type_col = \"categorical\" ) all_drift_charts_ = chart_gen_list ( master_path , chart_type = drift_charts ) for x in [ all_charts_num_1_ , all_charts_num_2_ , all_charts_num_3_ , all_charts_cat_1_ , all_charts_cat_2_ , all_drift_charts_ , ]: if len ( x ) == 1 : x . append ( dp . Plot ( blank_chart , label = \" \" )) else : x mapping_tab_list = [] for i in stats_files : if i . split ( \".csv\" )[ 0 ] in SG_tabs : mapping_tab_list . append ([ i . split ( \".csv\" )[ 0 ], \"Descriptive Statistics\" ]) elif i . split ( \".csv\" )[ 0 ] in QC_tabs : mapping_tab_list . append ([ i . split ( \".csv\" )[ 0 ], \"Quality Check\" ]) elif i . split ( \".csv\" )[ 0 ] in AE_tabs : mapping_tab_list . append ([ i . split ( \".csv\" )[ 0 ], \"Attribute Associations\" ]) elif i . split ( \".csv\" )[ 0 ] in drift_tab or i . split ( \".csv\" )[ 0 ] in stability_tab : mapping_tab_list . append ([ i . split ( \".csv\" )[ 0 ], \"Data Drift & Data Stability\" ]) else : mapping_tab_list . append ([ i . split ( \".csv\" )[ 0 ], \"null\" ]) xx = pd . DataFrame ( mapping_tab_list , columns = [ \"file_name\" , \"tab_name\" ]) xx_avl = list ( set ( xx . file_name . values )) for i in SG_tabs : if i in xx_avl : avl_SG . append ( i ) for j in QC_tabs : if j in xx_avl : avl_QC . append ( j ) for k in AE_tabs : if k in xx_avl : avl_AE . append ( k ) missing_SG = list ( set ( SG_tabs ) - set ( avl_SG )) missing_QC = list ( set ( QC_tabs ) - set ( avl_QC )) missing_AE = list ( set ( AE_tabs ) - set ( avl_AE )) missing_drift = list ( set ( drift_tab ) - set ( xx [ xx . tab_name . values == \"Data Drift & Data Stability\" ] . file_name . values ) ) missing_stability = list ( set ( stability_tab ) - set ( xx [ xx . tab_name . values == \"Data Drift & Data Stability\" ] . file_name . values ) ) ds_ind = drift_stability_ind ( missing_drift , drift_tab , missing_stability , stability_tab ) if ds_ind [ 0 ] > 0 : drift_df = pd . read_csv ( ends_with ( master_path ) + \"drift_statistics.csv\" ) . sort_values ( by = [ \"flagged\" ], ascending = False ) metric_drift = list ( drift_df . drop ([ \"attribute\" , \"flagged\" ], 1 ) . columns ) drift_df = drift_df [ drift_df . attribute . values != id_col ] len_feats = drift_df . shape [ 0 ] drift_df_stats = ( drift_df [ drift_df . flagged . values == 1 ] . melt ( id_vars = \"attribute\" , value_vars = metric_drift ) . sort_values ( by = [ \"variable\" , \"value\" ], ascending = False ) ) drifted_feats = drift_df [ drift_df . flagged . values == 1 ] . shape [ 0 ] if ds_ind [ 1 ] > 0.5 : df_stability = pd . read_csv ( ends_with ( master_path ) + \"stabilityIndex_metrics.csv\" ) df_stability [ \"idx\" ] = df_stability [ \"idx\" ] . astype ( str ) . apply ( lambda x : \"df\" + x ) n_df_stability = str ( df_stability [ \"idx\" ] . nunique ()) df_si_ = pd . read_csv ( ends_with ( master_path ) + \"stability_index.csv\" ) df_si = df_si_ [ [ \"attribute\" , \"stability_index\" , \"mean_si\" , \"stddev_si\" , \"kurtosis_si\" , \"flagged\" , ] ] unstable_attr = list ( df_si_ [ df_si_ . flagged . values == 1 ] . attribute . values ) total_unstable_attr = list ( df_si_ . attribute . values ) elif ds_ind [ 1 ] == 0.5 : df_si_ = pd . read_csv ( ends_with ( master_path ) + \"stability_index.csv\" ) df_si = df_si_ [ [ \"attribute\" , \"stability_index\" , \"mean_si\" , \"stddev_si\" , \"kurtosis_si\" , \"flagged\" , ] ] unstable_attr = list ( df_si_ [ df_si_ . flagged . values == 1 ] . attribute . values ) total_unstable_attr = list ( df_si_ . attribute . values ) df_stability = pd . DataFrame () n_df_stability = \"the\" else : pass tab1 = executive_summary_gen ( master_path , label_col , ds_ind , id_col , iv_threshold , corr_threshold ) tab2 = wiki_generator ( master_path , dataDict_path = dataDict_path , metricDict_path = metricDict_path ) tab3 = descriptive_statistics ( master_path , SG_tabs , avl_SG , missing_SG , all_charts_num_1_ , all_charts_cat_1_ ) tab4 = quality_check ( master_path , QC_tabs , avl_QC , missing_QC , all_charts_num_3_ ) tab5 = attribute_associations ( master_path , AE_tabs , avl_AE , missing_AE , label_col , all_charts_num_2_ , all_charts_cat_2_ , ) tab6 = data_drift_stability ( master_path , ds_ind , id_col , drift_threshold_model , all_drift_charts_ ) tab7 = ts_viz_generate ( master_path , id_col , False , output_type ) final_tabs_list = [] for i in [ tab1 , tab2 , tab3 , tab4 , tab5 , tab6 , tab7 ]: if i == \"null_report\" : pass else : final_tabs_list . append ( i ) if run_type in ( \"local\" , \"databricks\" ): dp . Report ( default_template [ 0 ], default_template [ 1 ], dp . Select ( blocks = final_tabs_list , type = dp . SelectType . TABS ), ) . save ( ends_with ( final_report_path ) + \"ml_anovos_report.html\" , open = True ) elif run_type == \"emr\" : dp . Report ( default_template [ 0 ], default_template [ 1 ], dp . Select ( blocks = final_tabs_list , type = dp . SelectType . TABS ), ) . save ( \"ml_anovos_report.html\" , open = True ) bash_cmd = \"aws s3 cp ml_anovos_report.html \" + ends_with ( final_report_path ) subprocess . check_output ([ \"bash\" , \"-c\" , bash_cmd ]) else : raise ValueError ( \"Invalid run_type\" ) print ( \"Report generated successfully at the specified location\" ) Functions def anovos_report ( master_path, id_col='', label_col='', corr_threshold=0.4, iv_threshold=0.02, drift_threshold_model=0.1, dataDict_path='.', metricDict_path='.', run_type='local', final_report_path='.', output_type=None) This function actually helps to produce the final report by scanning through the output processed from the data analyzer module. Parameters master_path Path containing the input files. id_col ID column (Default value = \"\") label_col label column (Default value = \"\") corr_threshold Correlation threshold beyond which attributes can be categorized under correlated. (Default value = 0.4) iv_threshold IV threshold beyond which attributes can be called as significant. (Default value = 0.02) drift_threshold_model threshold which the user is specifying for tagging an attribute to be drifted or not (Default value = 0.1) dataDict_path Data dictionary path. Default value is kept as None. metricDict_path Metric dictionary path. Default value is kept as None. run_type local or emr or databricks option. Default is kept as local final_report_path Path where the report will be saved. (Default value = \".\") output_type Time category of analysis which can be between \"Daily\", \"Hourly\", \"Weekly\" Returns Output[HTML] Expand source code def anovos_report ( master_path , id_col = \"\" , label_col = \"\" , corr_threshold = 0.4 , iv_threshold = 0.02 , drift_threshold_model = 0.1 , dataDict_path = \".\" , metricDict_path = \".\" , run_type = \"local\" , final_report_path = \".\" , output_type = None , ): \"\"\" This function actually helps to produce the final report by scanning through the output processed from the data analyzer module. Parameters ---------- master_path Path containing the input files. id_col ID column (Default value = \"\") label_col label column (Default value = \"\") corr_threshold Correlation threshold beyond which attributes can be categorized under correlated. (Default value = 0.4) iv_threshold IV threshold beyond which attributes can be called as significant. (Default value = 0.02) drift_threshold_model threshold which the user is specifying for tagging an attribute to be drifted or not (Default value = 0.1) dataDict_path Data dictionary path. Default value is kept as None. metricDict_path Metric dictionary path. Default value is kept as None. run_type local or emr or databricks option. Default is kept as local final_report_path Path where the report will be saved. (Default value = \".\") output_type Time category of analysis which can be between \"Daily\", \"Hourly\", \"Weekly\" Returns ------- Output[HTML] \"\"\" if run_type == \"emr\" : bash_cmd = ( \"aws s3 cp --recursive \" + ends_with ( master_path ) + \" \" + ends_with ( \"report_stats\" ) ) master_path = \"report_stats\" subprocess . check_output ([ \"bash\" , \"-c\" , bash_cmd ]) if \"global_summary.csv\" not in os . listdir ( master_path ): print ( \"Minimum supporting data is unavailable, hence the Report could not be generated.\" ) return None global global_summary_df global numcols_name global catcols_name global rows_count global columns_count global numcols_count global catcols_count global blank_chart global df_si_ global df_si global unstable_attr global total_unstable_attr global drift_df global metric_drift global drift_df global len_feats global drift_df_stats global drifted_feats global df_stability global n_df_stability global stability_interpretation_table global plot_index_stability SG_tabs = [ \"measures_of_counts\" , \"measures_of_centralTendency\" , \"measures_of_cardinality\" , \"measures_of_percentiles\" , \"measures_of_dispersion\" , \"measures_of_shape\" , \"global_summary\" , ] QC_tabs = [ \"nullColumns_detection\" , \"IDness_detection\" , \"biasedness_detection\" , \"invalidEntries_detection\" , \"duplicate_detection\" , \"nullRows_detection\" , \"outlier_detection\" , ] AE_tabs = [ \"correlation_matrix\" , \"IV_calculation\" , \"IG_calculation\" , \"variable_clustering\" , ] drift_tab = [ \"drift_statistics\" ] stability_tab = [ \"stability_index\" , \"stabilityIndex_metrics\" ] avl_SG , avl_QC , avl_AE = [], [], [] stability_interpretation_table = pd . DataFrame ( [ [ \"0-1\" , \"Very Unstable\" ], [ \"1-2\" , \"Unstable\" ], [ \"2-3\" , \"Marginally Stable\" ], [ \"3-3.5\" , \"Stable\" ], [ \"3.5-4\" , \"Very Stable\" ], ], columns = [ \"StabilityIndex\" , \"StabilityOrder\" ], ) plot_index_stability = go . Figure ( data = [ go . Table ( header = dict ( values = list ( stability_interpretation_table . columns ), fill_color = px . colors . sequential . Greys [ 2 ], align = \"center\" , font = dict ( size = 12 ), ), cells = dict ( values = [ stability_interpretation_table . StabilityIndex , stability_interpretation_table . StabilityOrder , ], line_color = px . colors . sequential . Greys [ 2 ], fill_color = \"white\" , align = \"center\" , height = 25 , ), columnwidth = [ 2 , 10 ], ) ] ) plot_index_stability . update_layout ( margin = dict ( l = 20 , r = 700 , t = 20 , b = 20 )) blank_chart = go . Figure () blank_chart . update_layout ( autosize = False , width = 10 , height = 10 ) blank_chart . layout . plot_bgcolor = global_plot_bg_color blank_chart . layout . paper_bgcolor = global_paper_bg_color blank_chart . update_xaxes ( visible = False ) blank_chart . update_yaxes ( visible = False ) global_summary_df = pd . read_csv ( ends_with ( master_path ) + \"global_summary.csv\" ) rows_count = int ( global_summary_df [ global_summary_df . metric . values == \"rows_count\" ] . value . values [ 0 ] ) catcols_count = int ( global_summary_df [ global_summary_df . metric . values == \"catcols_count\" ] . value . values [ 0 ] ) numcols_count = int ( global_summary_df [ global_summary_df . metric . values == \"numcols_count\" ] . value . values [ 0 ] ) columns_count = int ( global_summary_df [ global_summary_df . metric . values == \"columns_count\" ] . value . values [ 0 ] ) if catcols_count > 0 : catcols_name = \",\" . join ( list ( global_summary_df [ global_summary_df . metric . values == \"catcols_name\" ] . value . values ) ) else : catcols_name = \"\" if numcols_count > 0 : numcols_name = \",\" . join ( list ( global_summary_df [ global_summary_df . metric . values == \"numcols_name\" ] . value . values ) ) else : numcols_name = \"\" all_files = os . listdir ( master_path ) eventDist_charts = [ x for x in all_files if \"eventDist\" in x ] stats_files = [ x for x in all_files if \".csv\" in x ] freq_charts = [ x for x in all_files if \"freqDist\" in x ] outlier_charts = [ x for x in all_files if \"outlier\" in x ] drift_charts = [ x for x in all_files if \"drift\" in x and \".csv\" not in x ] all_charts_num_1_ = chart_gen_list ( master_path , chart_type = freq_charts , type_col = \"numerical\" ) all_charts_num_2_ = chart_gen_list ( master_path , chart_type = eventDist_charts , type_col = \"numerical\" ) all_charts_num_3_ = chart_gen_list ( master_path , chart_type = outlier_charts , type_col = \"numerical\" ) all_charts_cat_1_ = chart_gen_list ( master_path , chart_type = freq_charts , type_col = \"categorical\" ) all_charts_cat_2_ = chart_gen_list ( master_path , chart_type = eventDist_charts , type_col = \"categorical\" ) all_drift_charts_ = chart_gen_list ( master_path , chart_type = drift_charts ) for x in [ all_charts_num_1_ , all_charts_num_2_ , all_charts_num_3_ , all_charts_cat_1_ , all_charts_cat_2_ , all_drift_charts_ , ]: if len ( x ) == 1 : x . append ( dp . Plot ( blank_chart , label = \" \" )) else : x mapping_tab_list = [] for i in stats_files : if i . split ( \".csv\" )[ 0 ] in SG_tabs : mapping_tab_list . append ([ i . split ( \".csv\" )[ 0 ], \"Descriptive Statistics\" ]) elif i . split ( \".csv\" )[ 0 ] in QC_tabs : mapping_tab_list . append ([ i . split ( \".csv\" )[ 0 ], \"Quality Check\" ]) elif i . split ( \".csv\" )[ 0 ] in AE_tabs : mapping_tab_list . append ([ i . split ( \".csv\" )[ 0 ], \"Attribute Associations\" ]) elif i . split ( \".csv\" )[ 0 ] in drift_tab or i . split ( \".csv\" )[ 0 ] in stability_tab : mapping_tab_list . append ([ i . split ( \".csv\" )[ 0 ], \"Data Drift & Data Stability\" ]) else : mapping_tab_list . append ([ i . split ( \".csv\" )[ 0 ], \"null\" ]) xx = pd . DataFrame ( mapping_tab_list , columns = [ \"file_name\" , \"tab_name\" ]) xx_avl = list ( set ( xx . file_name . values )) for i in SG_tabs : if i in xx_avl : avl_SG . append ( i ) for j in QC_tabs : if j in xx_avl : avl_QC . append ( j ) for k in AE_tabs : if k in xx_avl : avl_AE . append ( k ) missing_SG = list ( set ( SG_tabs ) - set ( avl_SG )) missing_QC = list ( set ( QC_tabs ) - set ( avl_QC )) missing_AE = list ( set ( AE_tabs ) - set ( avl_AE )) missing_drift = list ( set ( drift_tab ) - set ( xx [ xx . tab_name . values == \"Data Drift & Data Stability\" ] . file_name . values ) ) missing_stability = list ( set ( stability_tab ) - set ( xx [ xx . tab_name . values == \"Data Drift & Data Stability\" ] . file_name . values ) ) ds_ind = drift_stability_ind ( missing_drift , drift_tab , missing_stability , stability_tab ) if ds_ind [ 0 ] > 0 : drift_df = pd . read_csv ( ends_with ( master_path ) + \"drift_statistics.csv\" ) . sort_values ( by = [ \"flagged\" ], ascending = False ) metric_drift = list ( drift_df . drop ([ \"attribute\" , \"flagged\" ], 1 ) . columns ) drift_df = drift_df [ drift_df . attribute . values != id_col ] len_feats = drift_df . shape [ 0 ] drift_df_stats = ( drift_df [ drift_df . flagged . values == 1 ] . melt ( id_vars = \"attribute\" , value_vars = metric_drift ) . sort_values ( by = [ \"variable\" , \"value\" ], ascending = False ) ) drifted_feats = drift_df [ drift_df . flagged . values == 1 ] . shape [ 0 ] if ds_ind [ 1 ] > 0.5 : df_stability = pd . read_csv ( ends_with ( master_path ) + \"stabilityIndex_metrics.csv\" ) df_stability [ \"idx\" ] = df_stability [ \"idx\" ] . astype ( str ) . apply ( lambda x : \"df\" + x ) n_df_stability = str ( df_stability [ \"idx\" ] . nunique ()) df_si_ = pd . read_csv ( ends_with ( master_path ) + \"stability_index.csv\" ) df_si = df_si_ [ [ \"attribute\" , \"stability_index\" , \"mean_si\" , \"stddev_si\" , \"kurtosis_si\" , \"flagged\" , ] ] unstable_attr = list ( df_si_ [ df_si_ . flagged . values == 1 ] . attribute . values ) total_unstable_attr = list ( df_si_ . attribute . values ) elif ds_ind [ 1 ] == 0.5 : df_si_ = pd . read_csv ( ends_with ( master_path ) + \"stability_index.csv\" ) df_si = df_si_ [ [ \"attribute\" , \"stability_index\" , \"mean_si\" , \"stddev_si\" , \"kurtosis_si\" , \"flagged\" , ] ] unstable_attr = list ( df_si_ [ df_si_ . flagged . values == 1 ] . attribute . values ) total_unstable_attr = list ( df_si_ . attribute . values ) df_stability = pd . DataFrame () n_df_stability = \"the\" else : pass tab1 = executive_summary_gen ( master_path , label_col , ds_ind , id_col , iv_threshold , corr_threshold ) tab2 = wiki_generator ( master_path , dataDict_path = dataDict_path , metricDict_path = metricDict_path ) tab3 = descriptive_statistics ( master_path , SG_tabs , avl_SG , missing_SG , all_charts_num_1_ , all_charts_cat_1_ ) tab4 = quality_check ( master_path , QC_tabs , avl_QC , missing_QC , all_charts_num_3_ ) tab5 = attribute_associations ( master_path , AE_tabs , avl_AE , missing_AE , label_col , all_charts_num_2_ , all_charts_cat_2_ , ) tab6 = data_drift_stability ( master_path , ds_ind , id_col , drift_threshold_model , all_drift_charts_ ) tab7 = ts_viz_generate ( master_path , id_col , False , output_type ) final_tabs_list = [] for i in [ tab1 , tab2 , tab3 , tab4 , tab5 , tab6 , tab7 ]: if i == \"null_report\" : pass else : final_tabs_list . append ( i ) if run_type in ( \"local\" , \"databricks\" ): dp . Report ( default_template [ 0 ], default_template [ 1 ], dp . Select ( blocks = final_tabs_list , type = dp . SelectType . TABS ), ) . save ( ends_with ( final_report_path ) + \"ml_anovos_report.html\" , open = True ) elif run_type == \"emr\" : dp . Report ( default_template [ 0 ], default_template [ 1 ], dp . Select ( blocks = final_tabs_list , type = dp . SelectType . TABS ), ) . save ( \"ml_anovos_report.html\" , open = True ) bash_cmd = \"aws s3 cp ml_anovos_report.html \" + ends_with ( final_report_path ) subprocess . check_output ([ \"bash\" , \"-c\" , bash_cmd ]) else : raise ValueError ( \"Invalid run_type\" ) print ( \"Report generated successfully at the specified location\" ) def attribute_associations ( master_path, AE_tabs, avl_recs_AE, missing_recs_AE, label_col, all_charts_num_2_, all_charts_cat_2_, print_report=False) This function helps to produce output specific to the Attribute Association Tab. Parameters master_path Path containing the input files. AE_tabs correlation_matrix','IV_calculation','IG_calculation','variable_clustering' avl_recs_AE Available files from the AE_tabs (Association Evaluator tabs) missing_recs_AE Missing files from the AE_tabs (Association Evaluator tabs) label_col label column all_charts_num_2_ Numerical charts (histogram) all collated in a list format supported as per datapane objects all_charts_cat_2_ Categorical charts (barplot) all collated in a list format supported as per datapane objects print_report Printing option flexibility. Default value is kept as False. Returns DatapaneObject / Output[HTML] Expand source code def attribute_associations ( master_path , AE_tabs , avl_recs_AE , missing_recs_AE , label_col , all_charts_num_2_ , all_charts_cat_2_ , print_report = False , ): \"\"\" This function helps to produce output specific to the Attribute Association Tab. Parameters ---------- master_path Path containing the input files. AE_tabs correlation_matrix','IV_calculation','IG_calculation','variable_clustering' avl_recs_AE Available files from the AE_tabs (Association Evaluator tabs) missing_recs_AE Missing files from the AE_tabs (Association Evaluator tabs) label_col label column all_charts_num_2_ Numerical charts (histogram) all collated in a list format supported as per datapane objects all_charts_cat_2_ Categorical charts (barplot) all collated in a list format supported as per datapane objects print_report Printing option flexibility. Default value is kept as False. Returns ------- DatapaneObject / Output[HTML] \"\"\" if ( len ( missing_recs_AE ) == len ( AE_tabs )) and ( ( len ( all_charts_num_2_ ) + len ( all_charts_cat_2_ )) == 0 ): return \"null_report\" else : if len ( all_charts_num_2_ ) == 0 and len ( all_charts_cat_2_ ) == 0 : target_association_rep = dp . Text ( \"##\" ) else : if len ( all_charts_num_2_ ) > 0 and len ( all_charts_cat_2_ ) == 0 : target_association_rep = dp . Group ( dp . Text ( \"### Attribute to Target Association\" ), dp . Text ( \"\"\" *Bivariate Distribution considering the event captured across different attribute splits (or categories)* \"\"\" ), dp . Select ( blocks = all_charts_num_2_ , type = dp . SelectType . DROPDOWN ), label = \"Numerical\" , ) elif len ( all_charts_num_2_ ) == 0 and len ( all_charts_cat_2_ ) > 0 : target_association_rep = dp . Group ( dp . Text ( \"### Attribute to Target Association\" ), dp . Text ( \"\"\" *Bivariate Distribution considering the event captured across different attribute splits (or categories)* \"\"\" ), dp . Select ( blocks = all_charts_cat_2_ , type = dp . SelectType . DROPDOWN ), label = \"Categorical\" , ) else : target_association_rep = dp . Group ( dp . Text ( \"### Attribute to Target Association\" ), dp . Select ( blocks = [ dp . Group ( dp . Select ( blocks = all_charts_num_2_ , type = dp . SelectType . DROPDOWN , ), label = \"Numerical\" , ), dp . Group ( dp . Select ( blocks = all_charts_cat_2_ , type = dp . SelectType . DROPDOWN , ), label = \"Categorical\" , ), ], type = dp . SelectType . TABS , ), dp . Text ( \"\"\" *Event Rate is defined as % of event label (i.e. label 1) in a bin or a categorical value of an attribute.* \"\"\" ), dp . Text ( \"# \" ), ) if len ( missing_recs_AE ) == len ( AE_tabs ): report = dp . Group ( dp . Text ( \"# \" ), dp . Text ( \"\"\" *This section analyzes the interaction between different attributes and/or the relationship between an attribute & the binary target variable.* \"\"\" ), dp . Text ( \"## \" ), target_association_rep , dp . Text ( \"## \" ), dp . Text ( \"## \" ), label = \"Attribute Associations\" , ) else : report = dp . Group ( dp . Text ( \"# \" ), dp . Text ( \"\"\" *This section analyzes the interaction between different attributes and/or the relationship between an attribute & the binary target variable.* \"\"\" ), dp . Text ( \"# \" ), dp . Text ( \"# \" ), dp . Text ( \"### Association Matrix & Plot\" ), dp . Select ( blocks = data_analyzer_output ( master_path , avl_recs_AE , tab_name = \"association_evaluator\" ), type = dp . SelectType . DROPDOWN , ), dp . Text ( \"### \" ), dp . Text ( \"## \" ), target_association_rep , dp . Text ( \"## \" ), dp . Text ( \"## \" ), label = \"Attribute Associations\" , ) if print_report : dp . Report ( default_template [ 0 ], default_template [ 1 ], report ) . save ( ends_with ( master_path ) + \"attribute_associations.html\" , open = True ) return report def chart_gen_list ( master_path, chart_type, type_col=None) This function helps to produce the charts in a list object form nested by a datapane object. Parameters master_path Path containing all the charts same as the other files from data analyzed output chart_type Files containing only the specific chart names for the specific chart category type_col None. Default value is kept as None Returns DatapaneObject Expand source code def chart_gen_list ( master_path , chart_type , type_col = None ): \"\"\" This function helps to produce the charts in a list object form nested by a datapane object. Parameters ---------- master_path Path containing all the charts same as the other files from data analyzed output chart_type Files containing only the specific chart names for the specific chart category type_col None. Default value is kept as None Returns ------- DatapaneObject \"\"\" plot_list = [] for i in chart_type : col_name = i [ i . find ( \"_\" ) + 1 :] if type_col == \"numerical\" : if col_name in numcols_name . replace ( \" \" , \"\" ) . split ( \",\" ): plot_list . append ( dp . Plot ( go . Figure ( json . load ( open ( ends_with ( master_path ) + i ))), label = col_name , ) ) else : pass elif type_col == \"categorical\" : if col_name in catcols_name . replace ( \" \" , \"\" ) . split ( \",\" ): plot_list . append ( dp . Plot ( go . Figure ( json . load ( open ( ends_with ( master_path ) + i ))), label = col_name , ) ) else : pass else : plot_list . append ( dp . Plot ( go . Figure ( json . load ( open ( ends_with ( master_path ) + i ))), label = col_name , ) ) return plot_list def data_analyzer_output ( master_path, avl_recs_tab, tab_name) This section produces output in form of datapane objects which is specific to the different data analyzer modules. It is used by referring to the Master path along with the Available list of metrics & the Tab name. Parameters master_path Path containing all the output from analyzed data avl_recs_tab Available file names from the analysis tab tab_name Analysis tab from association_evaluator / quality_checker / stats_generator Returns DatapaneObject Expand source code def data_analyzer_output ( master_path , avl_recs_tab , tab_name ): \"\"\" This section produces output in form of datapane objects which is specific to the different data analyzer modules. It is used by referring to the Master path along with the Available list of metrics & the Tab name. Parameters ---------- master_path Path containing all the output from analyzed data avl_recs_tab Available file names from the analysis tab tab_name Analysis tab from association_evaluator / quality_checker / stats_generator Returns ------- DatapaneObject \"\"\" df_list = [] df_plot_list = [] # @FIXME: unused variables plot_list = [] avl_recs_tab = [ x for x in avl_recs_tab if \"global_summary\" not in x ] for index , i in enumerate ( avl_recs_tab ): data = pd . read_csv ( ends_with ( master_path ) + str ( i ) + \".csv\" ) if len ( data . index ) == 0 : continue if tab_name == \"quality_checker\" : if i == \"duplicate_detection\" : duplicate_recs = pd . read_csv ( ends_with ( master_path ) + str ( i ) + \".csv\" ) . round ( 3 ) _unique_rows_count = int ( duplicate_recs [ duplicate_recs [ \"metric\" ] == \"unique_rows_count\" ] . value . values ) _rows_count = int ( duplicate_recs [ duplicate_recs [ \"metric\" ] == \"rows_count\" ] . value . values ) _duplicate_rows_count = int ( duplicate_recs [ duplicate_recs [ \"metric\" ] == \"duplicate_rows\" ] . value . values ) _duplicate_pct = float ( duplicate_recs [ duplicate_recs [ \"metric\" ] == \"duplicate_pct\" ] . value . values * 100.0 ) unique_rows_count = f \" No. Of Unique Rows: ** { _unique_rows_count } , **\" # @FIXME: variable names exists in outer scope rows_count = f \" No. of Rows: ** { _rows_count } , **\" duplicate_rows = ( f \" No. of Duplicate Rows: ** { _duplicate_rows_count } , **\" ) duplicate_pct = f \" Percentage of Duplicate Rows: ** { _duplicate_pct } %**\" df_list . append ( [ dp . Text ( \"### \" + str ( remove_u_score ( i ))), dp . Group ( dp . Text ( rows_count ), dp . Text ( unique_rows_count ), dp . Text ( duplicate_rows ), dp . Text ( duplicate_pct ), rows = 4 , ), dp . Text ( \"#\" ), dp . Text ( \"#\" ), ] ) elif i == \"outlier_detection\" : df_list . append ( [ dp . Text ( \"### \" + str ( remove_u_score ( i ))), dp . DataTable ( pd . read_csv ( ends_with ( master_path ) + str ( i ) + \".csv\" ) . round ( 3 ) ), \"outlier_charts_placeholder\" , ] ) else : df_list . append ( [ dp . Text ( \"### \" + str ( remove_u_score ( i ))), dp . DataTable ( pd . read_csv ( ends_with ( master_path ) + str ( i ) + \".csv\" ) . round ( 3 ) ), dp . Text ( \"#\" ), dp . Text ( \"#\" ), ] ) elif tab_name == \"association_evaluator\" : for j in avl_recs_tab : if j == \"correlation_matrix\" : df_list_ = pd . read_csv ( ends_with ( master_path ) + str ( j ) + \".csv\" ) . round ( 3 ) feats_order = list ( df_list_ [ \"attribute\" ] . values ) df_list_ = df_list_ . round ( 3 ) fig = px . imshow ( df_list_ [ feats_order ], y = feats_order , color_continuous_scale = global_theme , aspect = \"auto\" , ) fig . layout . plot_bgcolor = global_plot_bg_color fig . layout . paper_bgcolor = global_paper_bg_color # fig.update_layout(title_text=str(\"Correlation Plot \")) df_plot_list . append ( dp . Group ( dp . Text ( \"##\" ), dp . DataTable ( df_list_ [[ \"attribute\" ] + feats_order ]), dp . Plot ( fig ), rows = 3 , label = remove_u_score ( j ), ) ) elif j == \"variable_clustering\" : df_list_ = ( pd . read_csv ( ends_with ( master_path ) + str ( j ) + \".csv\" ) . round ( 3 ) . sort_values ( by = [ \"Cluster\" ], ascending = True ) ) fig = px . sunburst ( df_list_ , path = [ \"Cluster\" , \"Attribute\" ], values = \"RS_Ratio\" , color_discrete_sequence = global_theme , ) # fig.update_layout(title_text=str(\"Distribution of homogenous variable across Clusters\")) fig . layout . plot_bgcolor = global_plot_bg_color fig . layout . paper_bgcolor = global_paper_bg_color # fig.update_layout(title_text=str(\"Variable Clustering Plot \")) fig . layout . autosize = True df_plot_list . append ( dp . Group ( dp . Text ( \"##\" ), dp . DataTable ( df_list_ ), dp . Plot ( fig ), rows = 3 , label = remove_u_score ( j ), ) ) else : try : df_list_ = pd . read_csv ( ends_with ( master_path ) + str ( j ) + \".csv\" ) . round ( 3 ) col_nm = [ x for x in list ( df_list_ . columns ) if \"attribute\" not in x ] df_list_ = df_list_ . sort_values ( col_nm [ 0 ], ascending = True ) fig = px . bar ( df_list_ , x = col_nm [ 0 ], y = \"attribute\" , orientation = \"h\" , color_discrete_sequence = global_theme , ) fig . layout . plot_bgcolor = global_plot_bg_color fig . layout . paper_bgcolor = global_paper_bg_color # fig.update_layout(title_text=str(\"Representation of \" + str(remove_u_score(j)))) fig . layout . autosize = True df_plot_list . append ( dp . Group ( dp . Text ( \"##\" ), dp . DataTable ( df_list_ ), dp . Plot ( fig ), label = remove_u_score ( j ), rows = 3 , ) ) except Exception as e : logger . error ( f \"processing failed, error { e } \" ) pass if len ( avl_recs_tab ) == 1 : df_plot_list . append ( dp . Group ( dp . DataTable ( pd . DataFrame ( columns = [ \" \" ], index = range ( 1 )), label = \" \" ), dp . Plot ( blank_chart , label = \" \" ), label = \" \" , ) ) else : pass return df_plot_list else : df_list . append ( dp . DataTable ( pd . read_csv ( ends_with ( master_path ) + str ( i ) + \".csv\" ) . round ( 3 ), label = remove_u_score ( avl_recs_tab [ index ]), ) ) if tab_name == \"quality_checker\" and len ( avl_recs_tab ) == 1 : return df_list [ 0 ], [ dp . Text ( \"#\" ), dp . Plot ( blank_chart )] elif tab_name == \"stats_generator\" and len ( avl_recs_tab ) == 1 : return [ df_list [ 0 ], dp . DataTable ( pd . DataFrame ( columns = [ \" \" ], index = range ( 1 )), label = \" \" ), ] else : return df_list def data_drift_stability ( master_path, ds_ind, id_col, drift_threshold_model, all_drift_charts_, print_report=False) This function helps to produce output specific to the Data Drift & Stability Tab. Parameters master_path Path containing the input files. ds_ind Drift stability indicator in list form. id_col ID column drift_threshold_model threshold which the user is specifying for tagging an attribute to be drifted or not all_drift_charts_ Charts (histogram/barplot) all collated in a list format supported as per datapane objects print_report Printing option flexibility. Default value is kept as False. Returns DatapaneObject / Output[HTML] Expand source code def data_drift_stability ( master_path , ds_ind , id_col , drift_threshold_model , all_drift_charts_ , print_report = False , ): \"\"\" This function helps to produce output specific to the Data Drift & Stability Tab. Parameters ---------- master_path Path containing the input files. ds_ind Drift stability indicator in list form. id_col ID column drift_threshold_model threshold which the user is specifying for tagging an attribute to be drifted or not all_drift_charts_ Charts (histogram/barplot) all collated in a list format supported as per datapane objects print_report Printing option flexibility. Default value is kept as False. Returns ------- DatapaneObject / Output[HTML] \"\"\" line_chart_list = [] if ds_ind [ 0 ] > 0 : fig_metric_drift = go . Figure () fig_metric_drift . add_trace ( go . Scatter ( x = list ( drift_df [ drift_df . flagged . values == 1 ][ metric_drift [ 0 ]] . values ), y = list ( drift_df [ drift_df . flagged . values == 1 ] . attribute . values ), marker = dict ( color = global_theme [ 1 ], size = 14 ), mode = \"markers\" , name = metric_drift [ 0 ], ) ) fig_metric_drift . add_trace ( go . Scatter ( x = list ( drift_df [ drift_df . flagged . values == 1 ][ metric_drift [ 1 ]] . values ), y = list ( drift_df [ drift_df . flagged . values == 1 ] . attribute . values ), marker = dict ( color = global_theme [ 3 ], size = 14 ), mode = \"markers\" , name = metric_drift [ 1 ], ) ) fig_metric_drift . add_trace ( go . Scatter ( x = list ( drift_df [ drift_df . flagged . values == 1 ][ metric_drift [ 2 ]] . values ), y = list ( drift_df [ drift_df . flagged . values == 1 ] . attribute . values ), marker = dict ( color = global_theme [ 5 ], size = 14 ), mode = \"markers\" , name = metric_drift [ 2 ], ) ) fig_metric_drift . add_trace ( go . Scatter ( x = list ( drift_df [ drift_df . flagged . values == 1 ][ metric_drift [ 3 ]] . values ), y = list ( drift_df [ drift_df . flagged . values == 1 ] . attribute . values ), marker = dict ( color = global_theme [ 7 ], size = 14 ), mode = \"markers\" , name = metric_drift [ 3 ], ) ) fig_metric_drift . add_vrect ( x0 = 0 , x1 = drift_threshold_model , fillcolor = global_theme [ 7 ], opacity = 0.1 , layer = \"below\" , line_width = 1 , ), fig_metric_drift . update_layout ( legend = dict ( orientation = \"h\" , x = 0.5 , yanchor = \"bottom\" , xanchor = \"center\" ) ) fig_metric_drift . layout . plot_bgcolor = global_plot_bg_color fig_metric_drift . layout . paper_bgcolor = global_paper_bg_color fig_metric_drift . update_xaxes ( showline = True , linewidth = 2 , gridcolor = px . colors . sequential . Greys [ 1 ] ) fig_metric_drift . update_yaxes ( showline = True , linewidth = 2 , gridcolor = px . colors . sequential . Greys [ 2 ] ) # Drift Chart - 2 fig_gauge_drift = go . Figure ( go . Indicator ( domain = { \"x\" : [ 0 , 1 ], \"y\" : [ 0 , 1 ]}, value = drifted_feats , mode = \"gauge+number\" , title = { \"text\" : \"\" }, gauge = { \"axis\" : { \"range\" : [ None , len_feats ]}, \"bar\" : { \"color\" : px . colors . sequential . Reds [ 7 ]}, \"steps\" : [ { \"range\" : [ 0 , drifted_feats ], \"color\" : px . colors . sequential . Reds [ 8 ], }, { \"range\" : [ drifted_feats , len_feats ], \"color\" : px . colors . sequential . Greens [ 8 ], }, ], \"threshold\" : { \"line\" : { \"color\" : \"black\" , \"width\" : 3 }, \"thickness\" : 1 , \"value\" : len_feats , }, }, ) ) fig_gauge_drift . update_layout ( font = { \"color\" : \"black\" , \"family\" : \"Arial\" }) def drift_text_gen ( drifted_feats , len_feats ): \"\"\" Parameters ---------- drifted_feats count of attributes drifted len_feats count of attributes passed for analysis Returns ------- String \"\"\" if drifted_feats == 0 : text = \"\"\" *Drift barometer does not indicate any drift in the underlying data. Please refer to the metric values as displayed in the above table & comparison plot for better understanding* \"\"\" elif drifted_feats == 1 : text = ( \"*Drift barometer indicates that \" + str ( drifted_feats ) + \" out of \" + str ( len_feats ) + \" (\" + str ( np . round (( 100 * drifted_feats / len_feats ), 2 )) + \"%) attributes has been drifted from its source behaviour.*\" ) elif drifted_feats > 1 : text = ( \"*Drift barometer indicates that \" + str ( drifted_feats ) + \" out of \" + str ( len_feats ) + \" (\" + str ( np . round (( 100 * drifted_feats / len_feats ), 2 )) + \"%) attributes have been drifted from its source behaviour.*\" ) else : text = \"\" return text else : pass if ds_ind [ 0 ] == 0 and ds_ind [ 1 ] == 0 : return \"null_report\" elif ds_ind [ 0 ] == 0 and ds_ind [ 1 ] > 0.5 : for i in total_unstable_attr : if len ( total_unstable_attr ) > 1 : line_chart_list . append ( line_chart_gen_stability ( df1 = df_stability , df2 = df_si_ , col = i ) ) else : line_chart_list . append ( line_chart_gen_stability ( df1 = df_stability , df2 = df_si_ , col = i ) ) line_chart_list . append ( dp . Plot ( blank_chart , label = \" \" )) report = dp . Group ( dp . Text ( \"# \" ), dp . Text ( \"\"\" *This section examines the dataset stability wrt the baseline dataset (via computing drift statistics) and/or wrt the historical datasets (via computing stability index).* \"\"\" ), dp . Text ( \"# \" ), dp . Text ( \"# \" ), dp . Text ( \"### Data Stability Analysis\" ), dp . DataTable ( df_si ), dp . Select ( blocks = line_chart_list , type = dp . SelectType . DROPDOWN ), dp . Group ( dp . Text ( \"**Stability Index Interpretation:**\" ), dp . Plot ( plot_index_stability ), rows = 2 , ), label = \"Drift & Stability\" , ) elif ds_ind [ 0 ] == 1 and ds_ind [ 1 ] == 0 : if len ( all_drift_charts_ ) > 0 : report = dp . Group ( dp . Text ( \"# \" ), dp . Text ( \"\"\" *This section examines the dataset stability wrt the baseline dataset (via computing drift statistics) and/or wrt the historical datasets (via computing stability index).* \"\"\" ), dp . Text ( \"# \" ), dp . Text ( \"# \" ), dp . Text ( \"### Data Drift Analysis\" ), dp . DataTable ( drift_df ), dp . Text ( \"*An attribute is flagged as drifted if any drift metric is found to be above the threshold of \" + str ( drift_threshold_model ) + \".*\" ), dp . Text ( \"##\" ), dp . Select ( blocks = all_drift_charts_ , type = dp . SelectType . DROPDOWN ), dp . Text ( \"\"\" *Source & Target datasets were compared to see the % deviation at decile level for numerical attributes and at individual category level for categorical attributes* \"\"\" ), dp . Text ( \"### \" ), dp . Text ( \"### \" ), dp . Text ( \"### Data Health\" ), dp . Group ( dp . Plot ( fig_metric_drift ), dp . Plot ( fig_gauge_drift ), columns = 2 ), dp . Group ( dp . Text ( \"*Representation of attributes across different computed Drift Metrics*\" ), dp . Text ( drift_text_gen ( drifted_feats , len_feats )), columns = 2 , ), label = \"Drift & Stability\" , ) else : report = dp . Group ( dp . Text ( \"# \" ), dp . Text ( \"\"\" *This section examines the dataset stability wrt the baseline dataset (via computing drift statistics) and/or wrt the historical datasets (via computing stability index).* \"\"\" ), dp . Text ( \"# \" ), dp . Text ( \"# \" ), dp . Text ( \"### Data Drift Analysis\" ), dp . DataTable ( drift_df ), dp . Text ( \"*An attribute is flagged as drifted if any drift metric is found to be above the threshold of \" + str ( drift_threshold_model ) + \".*\" ), dp . Text ( \"##\" ), dp . Text ( \"### \" ), dp . Text ( \"### Data Health\" ), dp . Group ( dp . Plot ( fig_metric_drift ), dp . Plot ( fig_gauge_drift ), columns = 2 ), dp . Group ( dp . Text ( \"*Representation of attributes across different computed Drift Metrics*\" ), dp . Text ( drift_text_gen ( drifted_feats , len_feats )), columns = 2 , ), label = \"Drift & Stability\" , ) elif ds_ind [ 0 ] == 1 and ds_ind [ 1 ] >= 0.5 : for i in total_unstable_attr : if len ( total_unstable_attr ) > 1 : line_chart_list . append ( line_chart_gen_stability ( df1 = df_stability , df2 = df_si_ , col = i ) ) else : line_chart_list . append ( line_chart_gen_stability ( df1 = df_stability , df2 = df_si_ , col = i ) ) line_chart_list . append ( dp . Plot ( blank_chart , label = \" \" )) if len ( all_drift_charts_ ) > 0 : report = dp . Group ( dp . Text ( \"# \" ), dp . Text ( \"\"\" *This section examines the dataset stability wrt the baseline dataset (via computing drift statistics) and/or wrt the historical datasets (via computing stability index).* \"\"\" ), dp . Text ( \"# \" ), dp . Text ( \"# \" ), dp . Text ( \"### Data Drift Analysis\" ), dp . DataTable ( drift_df ), dp . Text ( \"*An attribute is flagged as drifted if any drift metric is found to be above the threshold of \" + str ( drift_threshold_model ) + \".*\" ), dp . Text ( \"##\" ), dp . Select ( blocks = all_drift_charts_ , type = dp . SelectType . DROPDOWN ), dp . Text ( \"\"\" *Source & Target datasets were compared to see the % deviation at decile level for numerical attributes and at individual category level for categorical attributes* \"\"\" ), dp . Text ( \"### \" ), dp . Text ( \"### \" ), dp . Text ( \"### Data Health\" ), dp . Group ( dp . Plot ( fig_metric_drift ), dp . Plot ( fig_gauge_drift ), columns = 2 ), dp . Group ( dp . Text ( \"*Representation of attributes across different computed Drift Metrics*\" ), dp . Text ( drift_text_gen ( drifted_feats , len_feats )), columns = 2 , ), dp . Text ( \"## \" ), dp . Text ( \"## \" ), dp . Text ( \"### Data Stability Analysis\" ), dp . DataTable ( df_si ), dp . Select ( blocks = line_chart_list , type = dp . SelectType . DROPDOWN ), dp . Group ( dp . Text ( \"**Stability Index Interpretation:**\" ), dp . Plot ( plot_index_stability ), rows = 2 , ), label = \"Drift & Stability\" , ) else : report = dp . Group ( dp . Text ( \"# \" ), dp . Text ( \"\"\" *This section examines the dataset stability wrt the baseline dataset (via computing drift statistics) and/or wrt the historical datasets (via computing stability index).* \"\"\" ), dp . Text ( \"# \" ), dp . Text ( \"# \" ), dp . Text ( \"### Data Drift Analysis\" ), dp . DataTable ( drift_df ), dp . Text ( \"*An attribute is flagged as drifted if any drift metric is found to be above the threshold of \" + str ( drift_threshold_model ) + \".*\" ), dp . Text ( \"##\" ), dp . Text ( \"### Data Health\" ), dp . Group ( dp . Plot ( fig_metric_drift ), dp . Plot ( fig_gauge_drift ), columns = 2 ), dp . Group ( dp . Text ( \"*Representation of attributes across different computed Drift Metrics*\" ), dp . Text ( drift_text_gen ( drifted_feats , len_feats )), columns = 2 , ), dp . Text ( \"## \" ), dp . Text ( \"## \" ), dp . Text ( \"### Data Stability Analysis\" ), dp . DataTable ( df_si ), dp . Select ( blocks = line_chart_list , type = dp . SelectType . DROPDOWN ), dp . Group ( dp . Text ( \"**Stability Index Interpretation:**\" ), dp . Plot ( plot_index_stability ), rows = 2 , ), label = \"Drift & Stability\" , ) elif ds_ind [ 0 ] == 0 and ds_ind [ 1 ] >= 0.5 : for i in total_unstable_attr : if len ( total_unstable_attr ) > 1 : line_chart_list . append ( line_chart_gen_stability ( df1 = df_stability , df2 = df_si_ , col = i ) ) else : line_chart_list . append ( line_chart_gen_stability ( df1 = df_stability , df2 = df_si_ , col = i ) ) line_chart_list . append ( dp . Plot ( blank_chart , label = \" \" )) report = dp . Group ( dp . Text ( \"# \" ), dp . Text ( \"\"\" *This section examines the dataset stability wrt the baseline dataset (via computing drift statistics) and/or wrt the historical datasets (via computing stability index).* \"\"\" ), dp . Text ( \"# \" ), dp . Text ( \"# \" ), dp . Text ( \"### Data Stability Analysis\" ), dp . DataTable ( df_si ), dp . Select ( blocks = line_chart_list , type = dp . SelectType . DROPDOWN ), dp . Group ( dp . Text ( \"**Stability Index Interpretation:**\" ), dp . Plot ( plot_index_stability ), rows = 2 , ), label = \"Drift & Stability\" , ) else : for i in total_unstable_attr : if len ( total_unstable_attr ) > 1 : line_chart_list . append ( line_chart_gen_stability ( df1 = df_stability , df2 = df_si_ , col = i ) ) else : line_chart_list . append ( line_chart_gen_stability ( df1 = df_stability , df2 = df_si_ , col = i ) ) line_chart_list . append ( dp . Plot ( blank_chart , label = \" \" )) if len ( all_drift_charts_ ) > 0 : report = dp . Group ( dp . Text ( \"# \" ), dp . Text ( \"\"\" *This section examines the dataset stability wrt the baseline dataset (via computing drift statistics) and/or wrt the historical datasets (via computing stability index).* \"\"\" ), dp . Text ( \"# \" ), dp . Text ( \"# \" ), dp . Text ( \"### Data Drift Analysis\" ), dp . DataTable ( drift_df ), dp . Text ( \"*An attribute is flagged as drifted if any drift metric is found to be above the threshold of \" + str ( drift_threshold_model ) + \".*\" ), dp . Text ( \"##\" ), dp . Select ( blocks = all_drift_charts_ , type = dp . SelectType . DROPDOWN ), dp . Text ( \"\"\" *Source & Target datasets were compared to see the % deviation at decile level for numerical attributes and at individual category level for categorical attributes* \"\"\" ), dp . Text ( \"### \" ), dp . Text ( \"### \" ), dp . Text ( \"### Data Health\" ), dp . Group ( dp . Plot ( fig_metric_drift ), dp . Plot ( fig_gauge_drift ), columns = 2 ), dp . Group ( dp . Text ( \"*Representation of attributes across different computed Drift Metrics*\" ), dp . Text ( drift_text_gen ( drifted_feats , len_feats )), columns = 2 , ), dp . Text ( \"## \" ), dp . Text ( \"## \" ), dp . Text ( \"### Data Stability Analysis\" ), dp . DataTable ( df_si ), dp . Select ( blocks = line_chart_list , type = dp . SelectType . DROPDOWN ), dp . Group ( dp . Text ( \"**Stability Index Interpretation:**\" ), dp . Plot ( plot_index_stability ), rows = 2 , ), label = \"Drift & Stability\" , ) else : report = dp . Group ( dp . Text ( \"# \" ), dp . Text ( \"\"\" *This section examines the dataset stability wrt the baseline dataset (via computing drift statistics) and/or wrt the historical datasets (via computing stability index).* \"\"\" ), dp . Text ( \"# \" ), dp . Text ( \"# \" ), dp . Text ( \"### Data Drift Analysis\" ), dp . DataTable ( drift_df ), dp . Text ( \"*An attribute is flagged as drifted if any drift metric is found to be above the threshold of \" + str ( drift_threshold_model ) + \".*\" ), dp . Text ( \"##\" ), dp . Text ( \"### Data Health\" ), dp . Group ( dp . Plot ( fig_metric_drift ), dp . Plot ( fig_gauge_drift ), columns = 2 ), dp . Group ( dp . Text ( \"*Representation of attributes across different computed Drift Metrics*\" ), dp . Text ( drift_text_gen ( drifted_feats , len_feats )), columns = 2 , ), dp . Text ( \"## \" ), dp . Text ( \"## \" ), dp . Text ( \"### Data Stability Analysis\" ), dp . DataTable ( df_si ), dp . Select ( blocks = line_chart_list , type = dp . SelectType . DROPDOWN ), dp . Group ( dp . Text ( \"**Stability Index Interpretation:**\" ), dp . Plot ( plot_index_stability ), rows = 2 , ), label = \"Drift & Stability\" , ) if print_report : dp . Report ( default_template [ 0 ], default_template [ 1 ], report ) . save ( ends_with ( master_path ) + \"data_drift_stability.html\" , open = True ) return report def descriptive_statistics ( master_path, SG_tabs, avl_recs_SG, missing_recs_SG, all_charts_num_1_, all_charts_cat_1_, print_report=False) This function helps to produce output specific to the Descriptive Stats Tab. Parameters master_path Path containing the input files. SG_tabs measures_of_counts','measures_of_centralTendency','measures_of_cardinality','measures_of_percentiles','measures_of_dispersion','measures_of_shape','global_summary' avl_recs_SG Available files from the SG_tabs (Stats Generator tabs) missing_recs_SG Missing files from the SG_tabs (Stats Generator tabs) all_charts_num_1_ Numerical charts (histogram) all collated in a list format supported as per datapane objects all_charts_cat_1_ Categorical charts (barplot) all collated in a list format supported as per datapane objects print_report Printing option flexibility. Default value is kept as False. Returns DatapaneObject / Output[HTML] Expand source code def descriptive_statistics ( master_path , SG_tabs , avl_recs_SG , missing_recs_SG , all_charts_num_1_ , all_charts_cat_1_ , print_report = False , ): \"\"\" This function helps to produce output specific to the Descriptive Stats Tab. Parameters ---------- master_path Path containing the input files. SG_tabs measures_of_counts','measures_of_centralTendency','measures_of_cardinality','measures_of_percentiles','measures_of_dispersion','measures_of_shape','global_summary' avl_recs_SG Available files from the SG_tabs (Stats Generator tabs) missing_recs_SG Missing files from the SG_tabs (Stats Generator tabs) all_charts_num_1_ Numerical charts (histogram) all collated in a list format supported as per datapane objects all_charts_cat_1_ Categorical charts (barplot) all collated in a list format supported as per datapane objects print_report Printing option flexibility. Default value is kept as False. Returns ------- DatapaneObject / Output[HTML] \"\"\" if \"global_summary\" in avl_recs_SG : cnt = 0 else : cnt = 1 if len ( missing_recs_SG ) + cnt == len ( SG_tabs ): return \"null_report\" else : if \"global_summary\" in avl_recs_SG : l1 = dp . Group ( dp . Text ( \"# \" ), dp . Text ( \"*This section summarizes the dataset with key statistical metrics and distribution plots.*\" ), dp . Text ( \"# \" ), dp . Text ( \"# \" ), dp . Text ( \"### Global Summary\" ), dp . Group ( dp . Text ( \" Total Number of Records: **\" + f \" { rows_count : , } \" + \"**\" ), dp . Text ( \" Total Number of Attributes: **\" + str ( columns_count ) + \"**\" ), dp . Text ( \" Number of Numerical Attributes : **\" + str ( numcols_count ) + \"**\" ), dp . Text ( \" Numerical Attributes Name : **\" + str ( numcols_name ) + \"**\" ), dp . Text ( \" Number of Categorical Attributes : **\" + str ( catcols_count ) + \"**\" ), dp . Text ( \" Categorical Attributes Name : **\" + str ( catcols_name ) + \"**\" ), rows = 6 , ), rows = 8 , ) else : l1 = dp . Text ( \"# \" ) if len ( data_analyzer_output ( master_path , avl_recs_SG , \"stats_generator\" )) > 0 : l2 = dp . Text ( \"### Statistics by Metric Type\" ) l3 = dp . Group ( dp . Select ( blocks = data_analyzer_output ( master_path , avl_recs_SG , \"stats_generator\" ), type = dp . SelectType . TABS , ), dp . Text ( \"# \" ), ) else : l2 = dp . Text ( \"# \" ) l3 = dp . Text ( \"# \" ) if len ( all_charts_num_1_ ) == 0 and len ( all_charts_cat_1_ ) == 0 : l4 = 1 elif len ( all_charts_num_1_ ) == 0 and len ( all_charts_cat_1_ ) > 0 : l4 = ( dp . Text ( \"# \" ), dp . Text ( \"### Attribute Visualization\" ), dp . Select ( blocks = all_charts_cat_1_ , type = dp . SelectType . DROPDOWN ), dp . Text ( \"# \" ), dp . Text ( \"# \" ), ) elif len ( all_charts_num_1_ ) > 0 and len ( all_charts_cat_1_ ) == 0 : l4 = ( dp . Text ( \"# \" ), dp . Text ( \"### Attribute Visualization\" ), dp . Select ( blocks = all_charts_num_1_ , type = dp . SelectType . DROPDOWN ), dp . Text ( \"# \" ), dp . Text ( \"# \" ), ) else : l4 = ( dp . Text ( \"# \" ), dp . Text ( \"### Attribute Visualization\" ), dp . Group ( dp . Select ( blocks = [ dp . Group ( dp . Select ( blocks = all_charts_num_1_ , type = dp . SelectType . DROPDOWN , ), label = \"Numerical\" , ), dp . Group ( dp . Select ( blocks = all_charts_cat_1_ , type = dp . SelectType . DROPDOWN , ), label = \"Categorical\" , ), ], type = dp . SelectType . TABS , ) ), dp . Text ( \"# \" ), dp . Text ( \"# \" ), ) if l4 == 1 : report = dp . Group ( l1 , dp . Text ( \"# \" ), l2 , l3 , dp . Text ( \"# \" ), dp . Text ( \"# \" ), label = \"Descriptive Statistics\" , ) else : report = dp . Group ( l1 , dp . Text ( \"# \" ), l2 , l3 , * l4 , dp . Text ( \"# \" ), dp . Text ( \"# \" ), label = \"Descriptive Statistics\" , ) if print_report : dp . Report ( default_template [ 0 ], default_template [ 1 ], report ) . save ( ends_with ( master_path ) + \"descriptive_statistics.html\" , open = True ) return report def drift_stability_ind ( missing_recs_drift, drift_tab, missing_recs_stability, stability_tab) This function helps to produce the drift & stability indicator for further processing. Ideally a data with both drift & stability should produce a list of [1,1] Parameters missing_recs_drift Missing files from the drift tab drift_tab \"drift_statistics\" missing_recs_stability Missing files from the stability tab stability_tab \"stability_index, stabilityIndex_metrics\" Returns List Expand source code def drift_stability_ind ( missing_recs_drift , drift_tab , missing_recs_stability , stability_tab ): \"\"\" This function helps to produce the drift & stability indicator for further processing. Ideally a data with both drift & stability should produce a list of [1,1] Parameters ---------- missing_recs_drift Missing files from the drift tab drift_tab \"drift_statistics\" missing_recs_stability Missing files from the stability tab stability_tab \"stability_index, stabilityIndex_metrics\" Returns ------- List \"\"\" if len ( missing_recs_drift ) == len ( drift_tab ): drift_ind = 0 else : drift_ind = 1 if len ( missing_recs_stability ) == len ( stability_tab ): stability_ind = 0 elif ( \"stabilityIndex_metrics\" in missing_recs_stability ) and ( \"stability_index\" not in missing_recs_stability ): stability_ind = 0.5 else : stability_ind = 1 return drift_ind , stability_ind def executive_summary_gen ( master_path, label_col, ds_ind, id_col, iv_threshold, corr_threshold, print_report=False) This function helps to produce output specific to the Executive Summary Tab. Parameters master_path Path containing the input files. label_col Label column. ds_ind Drift stability indicator in list form. id_col ID column. iv_threshold IV threshold beyond which attributes can be called as significant. corr_threshold Correlation threshold beyond which attributes can be categorized under correlated. print_report Printing option flexibility. Default value is kept as False. Returns DatapaneObject / Output[HTML] Expand source code def executive_summary_gen ( master_path , label_col , ds_ind , id_col , iv_threshold , corr_threshold , print_report = False , ): \"\"\" This function helps to produce output specific to the Executive Summary Tab. Parameters ---------- master_path Path containing the input files. label_col Label column. ds_ind Drift stability indicator in list form. id_col ID column. iv_threshold IV threshold beyond which attributes can be called as significant. corr_threshold Correlation threshold beyond which attributes can be categorized under correlated. print_report Printing option flexibility. Default value is kept as False. Returns ------- DatapaneObject / Output[HTML] \"\"\" try : obj_dtls = json . load ( open ( ends_with ( master_path ) + \"freqDist_\" + str ( label_col )) ) # @FIXME: never used local variable text_val = list ( list ( obj_dtls . values ())[ 0 ][ 0 ] . items ())[ 8 ][ 1 ] x_val = list ( list ( obj_dtls . values ())[ 0 ][ 0 ] . items ())[ 11 ][ 1 ] y_val = list ( list ( obj_dtls . values ())[ 0 ][ 0 ] . items ())[ 13 ][ 1 ] label_fig_ = go . Figure ( data = [ go . Pie ( labels = x_val , values = y_val , textinfo = \"label+percent\" , insidetextorientation = \"radial\" , pull = [ 0 , 0.1 ], marker_colors = global_theme , ) ] ) label_fig_ . update_traces ( textposition = \"inside\" , textinfo = \"percent+label\" ) label_fig_ . update_layout ( legend = dict ( orientation = \"h\" , x = 0.5 , yanchor = \"bottom\" , xanchor = \"center\" ) ) label_fig_ . layout . plot_bgcolor = global_plot_bg_color label_fig_ . layout . paper_bgcolor = global_paper_bg_color except Exception as e : logger . error ( f \"processing failed, error { e } \" ) label_fig_ = None a1 = ( \"The dataset contains **\" + str ( f \" { rows_count : ,d } \" ) + \"** records and **\" + str ( numcols_count + catcols_count ) + \"** attributes (**\" + str ( numcols_count ) + \"** numerical + **\" + str ( catcols_count ) + \"** categorical).\" ) if label_col is None : a2 = dp . Group ( dp . Text ( \"- There is **no** target variable in the dataset\" ), dp . Text ( \"- Data Diagnosis:\" ), rows = 2 , ) else : if label_fig_ is None : a2 = dp . Group ( dp . Text ( \"- Target variable is **\" + str ( label_col ) + \"** \" ), dp . Text ( \"- Data Diagnosis:\" ), rows = 2 , ) else : a2 = dp . Group ( dp . Text ( \"- Target variable is **\" + str ( label_col ) + \"** \" ), dp . Plot ( label_fig_ ), dp . Text ( \"- Data Diagnosis:\" ), rows = 3 , ) try : x1 = list ( pd . read_csv ( ends_with ( master_path ) + \"measures_of_dispersion.csv\" ) . query ( \"`cov`>1\" ) . attribute . values ) if len ( x1 ) > 0 : x1_1 = [ \"High Variance\" , x1 ] else : x1_1 = [ \"High Variance\" , None ] except Exception as e : logger . error ( f \"processing failed, error { e } \" ) x1_1 = [ \"High Variance\" , None ] try : x2 = list ( pd . read_csv ( ends_with ( master_path ) + \"measures_of_shape.csv\" ) . query ( \"`skewness`>0\" ) . attribute . values ) if len ( x2 ) > 0 : x2_1 = [ \"Positive Skewness\" , x2 ] else : x2_1 = [ \"Positive Skewness\" , None ] except Exception as e : logger . error ( f \"processing failed, error { e } \" ) x2_1 = [ \"Positive Skewness\" , None ] try : x3 = list ( pd . read_csv ( ends_with ( master_path ) + \"measures_of_shape.csv\" ) . query ( \"`skewness`<0\" ) . attribute . values ) if len ( x3 ) > 0 : x3_1 = [ \"Negative Skewness\" , x3 ] else : x3_1 = [ \"Negative Skewness\" , None ] except Exception as e : logger . error ( f \"processing failed, error { e } \" ) x3_1 = [ \"Negative Skewness\" , None ] try : x4 = list ( pd . read_csv ( ends_with ( master_path ) + \"measures_of_shape.csv\" ) . query ( \"`kurtosis`>0\" ) . attribute . values ) if len ( x4 ) > 0 : x4_1 = [ \"High Kurtosis\" , x4 ] else : x4_1 = [ \"High Kurtosis\" , None ] except Exception as e : logger . error ( f \"processing failed, error { e } \" ) x4_1 = [ \"High Kurtosis\" , None ] try : x5 = list ( pd . read_csv ( ends_with ( master_path ) + \"measures_of_shape.csv\" ) . query ( \"`kurtosis`<0\" ) . attribute . values ) if len ( x5 ) > 0 : x5_1 = [ \"Low Kurtosis\" , x5 ] else : x5_1 = [ \"Low Kurtosis\" , None ] except Exception as e : logger . error ( f \"processing failed, error { e } \" ) x5_1 = [ \"Low Kurtosis\" , None ] try : x6 = list ( pd . read_csv ( ends_with ( master_path ) + \"measures_of_counts.csv\" ) . query ( \"`fill_pct`<0.7\" ) . attribute . values ) if len ( x6 ) > 0 : x6_1 = [ \"Low Fill Rates\" , x6 ] else : x6_1 = [ \"Low Fill Rates\" , None ] except Exception as e : logger . error ( f \"processing failed, error { e } \" ) x6_1 = [ \"Low Fill Rates\" , None ] try : biasedness_df = pd . read_csv ( ends_with ( master_path ) + \"biasedness_detection.csv\" ) if \"treated\" in biasedness_df : x7 = list ( biasedness_df . query ( \"`treated`>0\" ) . attribute . values ) else : x7 = list ( biasedness_df . query ( \"`flagged`>0\" ) . attribute . values ) if len ( x7 ) > 0 : x7_1 = [ \"High Biasedness\" , x7 ] else : x7_1 = [ \"High Biasedness\" , None ] except Exception as e : logger . error ( f \"processing failed, error { e } \" ) x7_1 = [ \"High Biasedness\" , None ] try : x8 = list ( pd . read_csv ( ends_with ( master_path ) + \"outlier_detection.csv\" ) . attribute . values ) if len ( x8 ) > 0 : x8_1 = [ \"Outliers\" , x8 ] else : x8_1 = [ \"Outliers\" , None ] except Exception as e : logger . error ( f \"processing failed, error { e } \" ) x8_1 = [ \"Outliers\" , None ] try : corr_matrx = pd . read_csv ( ends_with ( master_path ) + \"correlation_matrix.csv\" ) corr_matrx = corr_matrx [ list ( corr_matrx . attribute . values )] corr_matrx = corr_matrx . where ( np . triu ( np . ones ( corr_matrx . shape ), k = 1 ) . astype ( np . bool ) ) to_drop = [ column for column in corr_matrx . columns if any ( corr_matrx [ column ] > corr_threshold ) ] if len ( to_drop ) > 0 : x9_1 = [ \"High Correlation\" , to_drop ] else : x9_1 = [ \"High Correlation\" , None ] except Exception as e : logger . error ( f \"processing failed, error { e } \" ) x9_1 = [ \"High Correlation\" , None ] try : x10 = list ( pd . read_csv ( ends_with ( master_path ) + \"IV_calculation.csv\" ) . query ( \"`iv`>\" + str ( iv_threshold )) . attribute . values ) if len ( x10 ) > 0 : x10_1 = [ \"Significant Attributes\" , x10 ] else : x10_1 = [ \"Significant Attributes\" , None ] except Exception as e : logger . error ( f \"processing failed, error { e } \" ) x10_1 = [ \"Significant Attributes\" , None ] blank_list_df = [] for i in [ x1_1 , x2_1 , x3_1 , x4_1 , x5_1 , x6_1 , x7_1 , x8_1 , x9_1 , x10_1 ]: try : for j in i [ 1 ]: blank_list_df . append ([ i [ 0 ], j ]) except Exception as e : logger . error ( f \"processing failed, error { e } \" ) blank_list_df . append ([ i [ 0 ], \"NA\" ]) list_n = [] x1 = pd . DataFrame ( blank_list_df , columns = [ \"Metric\" , \"Attribute\" ]) x1 [ \"Value\" ] = \"\u2714\" all_cols = ( catcols_name . replace ( \" \" , \"\" ) + \",\" + numcols_name . replace ( \" \" , \"\" ) ) . split ( \",\" ) remainder_cols = list ( set ( all_cols ) - set ( x1 . Attribute . values )) total_metrics = set ( list ( x1 . Metric . values )) for i in remainder_cols : for j in total_metrics : list_n . append ([ j , i ]) x2 = pd . DataFrame ( list_n , columns = [ \"Metric\" , \"Attribute\" ]) x2 [ \"Value\" ] = \"\u2718\" x = x1 . append ( x2 , ignore_index = True ) x = ( x . drop_duplicates () . pivot ( index = \"Attribute\" , columns = \"Metric\" , values = \"Value\" ) . fillna ( \"\u2718\" ) . reset_index ()[ [ \"Attribute\" , \"Outliers\" , \"Significant Attributes\" , \"Positive Skewness\" , \"Negative Skewness\" , \"High Variance\" , \"High Correlation\" , \"High Kurtosis\" , \"Low Kurtosis\" , ] ] ) x = x [ x . Attribute . values != \"NA\" ] if ds_ind [ 0 ] == 1 and ds_ind [ 1 ] >= 0.5 : a5 = \"Data Health based on Drift Metrics & Stability Index : \" report = dp . Group ( dp . Text ( \"# \" ), dp . Text ( \"**Key Report Highlights**\" ), dp . Text ( \"- \" + a1 ), a2 , dp . DataTable ( x ), dp . Text ( \"- \" + a5 ), dp . Group ( dp . BigNumber ( heading = \"# Drifted Attributes\" , value = str ( str ( drifted_feats ) + \" out of \" + str ( len_feats )), ), dp . BigNumber ( heading = \"% Drifted Attributes\" , value = str ( np . round (( 100 * drifted_feats / len_feats ), 2 )) + \"%\" , ), dp . BigNumber ( heading = \"# Unstable Attributes\" , value = str ( len ( unstable_attr )) + \" out of \" + str ( len ( total_unstable_attr )), change = \"numerical\" , is_upward_change = True , ), dp . BigNumber ( heading = \"% Unstable Attributes\" , value = str ( np . round ( 100 * len ( unstable_attr ) / len ( total_unstable_attr ), 2 ) ) + \"%\" , ), columns = 4 , ), dp . Text ( \"# \" ), dp . Text ( \"# \" ), label = \"Executive Summary\" , ) if ds_ind [ 0 ] == 0 and ds_ind [ 1 ] >= 0.5 : a5 = \"Data Health based on Stability Index : \" report = dp . Group ( dp . Text ( \"# \" ), dp . Text ( \"**Key Report Highlights**\" ), dp . Text ( \"# \" ), dp . Text ( \"- \" + a1 ), a2 , dp . DataTable ( x ), dp . Text ( \"- \" + a5 ), dp . Group ( dp . BigNumber ( heading = \"# Unstable Attributes\" , value = str ( len ( unstable_attr )) + \" out of \" + str ( len ( total_unstable_attr )), change = \"numerical\" , is_upward_change = True , ), dp . BigNumber ( heading = \"% Unstable Attributes\" , value = str ( np . round ( 100 * len ( unstable_attr ) / len ( total_unstable_attr ), 2 ) ) + \"%\" , ), columns = 2 , ), dp . Text ( \"# \" ), dp . Text ( \"# \" ), label = \"Executive Summary\" , ) if ds_ind [ 0 ] == 1 and ds_ind [ 1 ] == 0 : a5 = \"Data Health based on Drift Metrics : \" report = dp . Group ( dp . Text ( \"# \" ), dp . Text ( \"**Key Report Highlights**\" ), dp . Text ( \"# \" ), dp . Text ( \"- \" + a1 ), a2 , dp . DataTable ( x ), dp . Text ( \"- \" + a5 ), dp . Group ( dp . BigNumber ( heading = \"# Drifted Attributes\" , value = str ( str ( drifted_feats ) + \" out of \" + str ( len_feats )), ), dp . BigNumber ( heading = \"% Drifted Attributes\" , value = str ( np . round (( 100 * drifted_feats / len_feats ), 2 )) + \"%\" , ), columns = 2 , ), dp . Text ( \"# \" ), dp . Text ( \"# \" ), label = \"Executive Summary\" , ) if ds_ind [ 0 ] == 0 and ds_ind [ 1 ] == 0 : report = dp . Group ( dp . Text ( \"# \" ), dp . Text ( \"**Key Report Highlights**\" ), dp . Text ( \"# \" ), dp . Text ( \"- \" + a1 ), a2 , dp . DataTable ( x ), dp . Text ( \"# \" ), label = \"Executive Summary\" , ) if print_report : dp . Report ( default_template [ 0 ], default_template [ 1 ], report ) . save ( ends_with ( master_path ) + \"executive_summary.html\" , open = True ) return report def gen_time_series_plots ( base_path, x_col, y_col, time_cat) This function helps to produce Time Series Plots by sourcing the aggregated data as Daily/Hourly/Weekly level. Parameters base_path Base path which is the same as Master path where the aggregated data resides. x_col Timestamp / date column name y_col Numerical column names time_cat Time category of analysis which can be between \"Daily\", \"Hourly\", \"Weekly\" Returns Plot Expand source code def gen_time_series_plots ( base_path , x_col , y_col , time_cat ): \"\"\" This function helps to produce Time Series Plots by sourcing the aggregated data as Daily/Hourly/Weekly level. Parameters ---------- base_path Base path which is the same as Master path where the aggregated data resides. x_col Timestamp / date column name y_col Numerical column names time_cat Time category of analysis which can be between \"Daily\", \"Hourly\", \"Weekly\" Returns ------- Plot \"\"\" df = pd . read_csv ( ends_with ( base_path ) + x_col + \"_\" + y_col + \"_\" + time_cat + \".csv\" ) . dropna () if len ([ x for x in df . columns if \"min\" in x ]) == 0 : if time_cat == \"daily\" : # x_col = x_col + \"_ts\" fig = px . line ( df , x = x_col , y = \"count\" , color = y_col , color_discrete_sequence = global_theme , ) fig . update_layout ( xaxis = dict ( rangeselector = dict ( buttons = list ( [ dict ( count = 1 , label = \"1m\" , step = \"month\" , stepmode = \"backward\" , ), dict ( count = 3 , label = \"3m\" , step = \"month\" , stepmode = \"backward\" , ), dict ( count = 6 , label = \"6m\" , step = \"month\" , stepmode = \"backward\" , ), dict ( count = 1 , label = \"YTD\" , step = \"year\" , stepmode = \"todate\" ), dict ( count = 1 , label = \"1y\" , step = \"year\" , stepmode = \"backward\" , ), dict ( step = \"all\" ), ] ) ), rangeslider = dict ( visible = True ), type = \"date\" , ) ) elif time_cat == \"weekly\" : fig = px . bar ( df , x = \"dow\" , y = \"count\" , color = y_col , color_discrete_sequence = global_theme , ) # fig.update_layout(barmode='stack') elif time_cat == \"hourly\" : fig = px . bar ( df , x = \"daypart_cat\" , y = \"count\" , color = y_col , color_discrete_sequence = global_theme , ) # fig.update_layout(barmode='stack') else : pass else : if time_cat == \"daily\" : # x_col = x_col + \"_ts\" f1 = go . Scatter ( x = list ( df [ x_col ]), y = list ( df [ \"min\" ]), name = \"Min\" , line = dict ( color = global_theme [ 6 ]), ) f2 = go . Scatter ( x = list ( df [ x_col ]), y = list ( df [ \"max\" ]), name = \"Max\" , line = dict ( color = global_theme [ 4 ]), ) f3 = go . Scatter ( x = list ( df [ x_col ]), y = list ( df [ \"mean\" ]), name = \"Mean\" , line = dict ( color = global_theme [ 2 ]), ) f4 = go . Scatter ( x = list ( df [ x_col ]), y = list ( df [ \"median\" ]), name = \"Median\" , line = dict ( color = global_theme [ 0 ]), ) fig = go . Figure ( data = [ f1 , f2 , f3 , f4 ]) fig . update_layout ( xaxis = dict ( rangeselector = dict ( buttons = list ( [ dict ( count = 1 , label = \"1m\" , step = \"month\" , stepmode = \"backward\" , ), dict ( count = 3 , label = \"3m\" , step = \"month\" , stepmode = \"backward\" , ), dict ( count = 6 , label = \"6m\" , step = \"month\" , stepmode = \"backward\" , ), dict ( count = 1 , label = \"YTD\" , step = \"year\" , stepmode = \"todate\" ), dict ( count = 1 , label = \"1y\" , step = \"year\" , stepmode = \"backward\" , ), dict ( step = \"all\" ), ] ) ), rangeslider = dict ( visible = True ), type = \"date\" , ) ) elif time_cat == \"weekly\" : f1 = go . Bar ( x = list ( df [ \"dow\" ]), y = list ( df [ \"min\" ]), marker_color = global_theme [ 6 ], name = \"Min\" , ) f2 = go . Bar ( x = list ( df [ \"dow\" ]), y = list ( df [ \"max\" ]), marker_color = global_theme [ 4 ], name = \"Max\" , ) f3 = go . Bar ( x = list ( df [ \"dow\" ]), y = list ( df [ \"mean\" ]), marker_color = global_theme [ 2 ], name = \"Mean\" , ) f4 = go . Bar ( x = list ( df [ \"dow\" ]), y = list ( df [ \"median\" ]), marker_color = global_theme [ 0 ], name = \"Median\" , ) fig = go . Figure ( data = [ f1 , f2 , f3 , f4 ]) fig . update_layout ( barmode = \"group\" ) elif time_cat == \"hourly\" : f1 = go . Bar ( x = list ( df [ \"daypart_cat\" ]), y = list ( df [ \"min\" ]), marker_color = global_theme [ 6 ], name = \"Min\" , ) f2 = go . Bar ( x = list ( df [ \"daypart_cat\" ]), y = list ( df [ \"max\" ]), marker_color = global_theme [ 4 ], name = \"Max\" , ) f3 = go . Bar ( x = list ( df [ \"daypart_cat\" ]), y = list ( df [ \"mean\" ]), marker_color = global_theme [ 2 ], name = \"Mean\" , ) f4 = go . Bar ( x = list ( df [ \"daypart_cat\" ]), y = list ( df [ \"median\" ]), marker_color = global_theme [ 0 ], name = \"Median\" , ) fig = go . Figure ( data = [ f1 , f2 , f3 , f4 ]) fig . update_layout ( barmode = \"group\" ) else : pass fig . layout . plot_bgcolor = global_plot_bg_color fig . layout . paper_bgcolor = global_paper_bg_color fig . update_xaxes ( gridcolor = px . colors . sequential . Greys [ 1 ]) fig . update_yaxes ( gridcolor = px . colors . sequential . Greys [ 1 ]) fig . update_layout ( legend = dict ( orientation = \"h\" , x = 0.5 , yanchor = \"bottom\" , xanchor = \"center\" ) ) return fig def lambda_cat ( val) Parameters val Value of Box Cox Test which translates into the transformation to be applied. Returns String Expand source code def lambda_cat ( val ): \"\"\" Parameters ---------- val Value of Box Cox Test which translates into the transformation to be applied. Returns ------- String \"\"\" if val < - 1 : return \"Reciprocal Square Transform\" elif val >= - 1 and val < - 0.5 : return \"Reciprocal Transform\" elif val >= - 0.5 and val < 0 : return \"Receiprocal Square Root Transform\" elif val >= 0 and val < 0.5 : return \"Log Transform\" elif val >= 0.5 and val < 1 : return \"Square Root Transform\" elif val >= 1 and val < 2 : return \"No Transform\" elif val >= 2 : return \"Square Transform\" else : return \"ValueOutOfRange\" def line_chart_gen_stability ( df1, df2, col) This function helps to produce charts which are specific to data stability index. It taken into account the stability input along with the analysis column to produce the desired output. Parameters df1 Analysis dataframe pertaining to summarized stability metrics df2 Analysis dataframe pertaining to historical data col Analysis column Returns DatapaneObject Expand source code def line_chart_gen_stability ( df1 , df2 , col ): \"\"\" This function helps to produce charts which are specific to data stability index. It taken into account the stability input along with the analysis column to produce the desired output. Parameters ---------- df1 Analysis dataframe pertaining to summarized stability metrics df2 Analysis dataframe pertaining to historical data col Analysis column Returns ------- DatapaneObject \"\"\" def val_cat ( val ): \"\"\" Parameters ---------- val Returns ------- String \"\"\" if val >= 3.5 : return \"Very Stable\" elif val >= 3 and val < 3.5 : return \"Stable\" elif val >= 2 and val < 3 : return \"Marginally Stable\" elif val >= 1 and val < 2 : return \"Unstable\" elif val >= 0 and val < 1 : return \"Very Unstable\" else : return \"Out of Range\" val_si = list ( df2 [ df2 [ \"attribute\" ] == col ] . stability_index . values )[ 0 ] f1 = go . Figure () f1 . add_trace ( go . Indicator ( mode = \"gauge+number\" , value = val_si , gauge = { \"axis\" : { \"range\" : [ None , 4 ], \"tickwidth\" : 1 , \"tickcolor\" : \"black\" }, \"bgcolor\" : \"white\" , \"steps\" : [ { \"range\" : [ 0 , 1 ], \"color\" : px . colors . sequential . Reds [ 7 ]}, { \"range\" : [ 1 , 2 ], \"color\" : px . colors . sequential . Reds [ 6 ]}, { \"range\" : [ 2 , 3 ], \"color\" : px . colors . sequential . Oranges [ 4 ]}, { \"range\" : [ 3 , 3.5 ], \"color\" : px . colors . sequential . BuGn [ 7 ]}, { \"range\" : [ 3.5 , 4 ], \"color\" : px . colors . sequential . BuGn [ 8 ]}, ], \"threshold\" : { \"line\" : { \"color\" : \"black\" , \"width\" : 3 }, \"thickness\" : 1 , \"value\" : val_si , }, \"bar\" : { \"color\" : global_plot_bg_color }, }, title = { \"text\" : \"Order of Stability: \" + val_cat ( val_si )}, ) ) f1 . update_layout ( height = 400 , font = { \"color\" : \"black\" , \"family\" : \"Arial\" }) f5 = \"Stability Index for \" + str ( col . upper ()) if len ( df1 . columns ) > 0 : df1 = df1 [ df1 [ \"attribute\" ] == col ] f2 = px . line ( df1 , x = \"idx\" , y = \"mean\" , markers = True , title = \"CV of Mean is \" + str ( list ( df2 [ df2 [ \"attribute\" ] == col ] . mean_cv . values )[ 0 ]), ) f2 . update_traces ( line_color = global_theme [ 2 ], marker = dict ( size = 14 )) f2 . layout . plot_bgcolor = global_plot_bg_color f2 . layout . paper_bgcolor = global_paper_bg_color f3 = px . line ( df1 , x = \"idx\" , y = \"stddev\" , markers = True , title = \"CV of Stddev is \" + str ( list ( df2 [ df2 [ \"attribute\" ] == col ] . stddev_cv . values )[ 0 ]), ) f3 . update_traces ( line_color = global_theme [ 6 ], marker = dict ( size = 14 )) f3 . layout . plot_bgcolor = global_plot_bg_color f3 . layout . paper_bgcolor = global_paper_bg_color f4 = px . line ( df1 , x = \"idx\" , y = \"kurtosis\" , markers = True , title = \"CV of Kurtosis is \" + str ( list ( df2 [ df2 [ \"attribute\" ] == col ] . kurtosis_cv . values )[ 0 ]), ) f4 . update_traces ( line_color = global_theme [ 4 ], marker = dict ( size = 14 )) f4 . layout . plot_bgcolor = global_plot_bg_color f4 . layout . paper_bgcolor = global_paper_bg_color return dp . Group ( dp . Text ( \"#\" ), dp . Text ( f5 ), dp . Plot ( f1 ), dp . Group ( dp . Plot ( f2 ), dp . Plot ( f3 ), dp . Plot ( f4 ), columns = 3 ), rows = 4 , label = col , ) else : return dp . Group ( dp . Text ( \"#\" ), dp . Text ( f5 ), dp . Plot ( f1 ), rows = 3 , label = col ) def list_ts_remove_append ( l, opt) This function helps to remove or append \"_ts\" from any list. Parameters l List containing column name opt Option to choose between 1 & Others to enable the functionality of removing or appending \"_ts\" within the elements of a list Returns List Expand source code def list_ts_remove_append ( l , opt ): \"\"\" This function helps to remove or append \"_ts\" from any list. Parameters ---------- l List containing column name opt Option to choose between 1 & Others to enable the functionality of removing or appending \"_ts\" within the elements of a list Returns ------- List \"\"\" ll = [] if opt == 1 : for i in l : if i [ - 3 :] == \"_ts\" : ll . append ( i [ 0 : - 3 :]) else : ll . append ( i ) return ll else : for i in l : if i [ - 3 :] == \"_ts\" : ll . append ( i ) else : ll . append ( i + \"_ts\" ) return ll def plotSeasonalDecompose ( base_path, x_col, y_col, metric_col='median', title='Seasonal Decomposition') This function helps to produce output specific to the Seasonal Decomposition of Time Series. Ideally it's expected to source a data containing atleast 2 cycles or 24 months as the most. Parameters base_path Base path which is the same as Master path where the aggregated data resides. x_col Timestamp / date column name y_col Numerical column names metric_col Metric of aggregation. Options can be between \"Median\", \"Mean\", \"Min\", \"Max\" title \"Title Description\" Returns Plot Expand source code def plotSeasonalDecompose ( base_path , x_col , y_col , metric_col = \"median\" , title = \"Seasonal Decomposition\" ): \"\"\" This function helps to produce output specific to the Seasonal Decomposition of Time Series. Ideally it's expected to source a data containing atleast 2 cycles or 24 months as the most. Parameters ---------- base_path Base path which is the same as Master path where the aggregated data resides. x_col Timestamp / date column name y_col Numerical column names metric_col Metric of aggregation. Options can be between \"Median\", \"Mean\", \"Min\", \"Max\" title \"Title Description\" Returns ------- Plot \"\"\" df = pd . read_csv ( ends_with ( base_path ) + x_col + \"_\" + y_col + \"_daily.csv\" ) . dropna () df [ x_col ] = pd . to_datetime ( df [ x_col ], format = \"%Y-%m- %d %H:%M:%S. %f \" ) df = df . set_index ( x_col ) if len ([ x for x in df . columns if \"min\" in x ]) == 0 : # result = seasonal_decompose(df[metric_col],model=\"additive\") pass else : result = seasonal_decompose ( df [ metric_col ], model = \"additive\" , period = 12 ) fig = make_subplots ( rows = 2 , cols = 2 , subplot_titles = [ \"Observed\" , \"Trend\" , \"Seasonal\" , \"Residuals\" ], ) # fig = go.Figure() fig . add_trace ( go . Scatter ( x = df . index , y = result . observed , name = \"Observed\" , mode = \"lines+markers\" , line = dict ( color = global_theme [ 0 ]), ), row = 1 , col = 1 , ) fig . add_trace ( go . Scatter ( x = df . index , y = result . trend , name = \"Trend\" , mode = \"lines+markers\" , line = dict ( color = global_theme [ 2 ]), ), row = 1 , col = 2 , ) fig . add_trace ( go . Scatter ( x = df . index , y = result . seasonal , name = \"Seasonal\" , mode = \"lines+markers\" , line = dict ( color = global_theme [ 4 ]), ), row = 2 , col = 1 , ) fig . add_trace ( go . Scatter ( x = df . index , y = result . resid , name = \"Residuals\" , mode = \"lines+markers\" , line = dict ( color = global_theme [ 6 ]), ), row = 2 , col = 2 , ) # fig.add_trace(go.Scatter(x=df.index, y=result.observed, name =\"Observed\", mode='lines+markers',line=dict(color=global_theme[0]))) # fig.add_trace(go.Scatter(x=df.index, y=result.trend, name =\"Trend\", mode='lines+markers',line=dict(color=global_theme[2]))) # fig.add_trace(go.Scatter(x=df.index, y=result.seasonal, name =\"Seasonal\", mode='lines+markers',line=dict(color=global_theme[4]))) # fig.add_trace(go.Scatter(x=df.index, y=result.resid, name =\"Residuals\", mode='lines+markers',line=dict(color=global_theme[6]))) fig . layout . plot_bgcolor = global_plot_bg_color fig . layout . paper_bgcolor = global_paper_bg_color fig . update_xaxes ( gridcolor = px . colors . sequential . Greys [ 1 ]) fig . update_yaxes ( gridcolor = px . colors . sequential . Greys [ 1 ]) fig . update_layout ( autosize = True , width = 2000 , height = 800 ) fig . update_layout ( legend = dict ( orientation = \"h\" , x = 0.5 , yanchor = \"bottom\" , xanchor = \"center\" ) ) return fig def quality_check ( master_path, QC_tabs, avl_recs_QC, missing_recs_QC, all_charts_num_3_, print_report=False) This function helps to produce output specific to the Quality Checker Tab. Parameters master_path Path containing the input files. QC_tabs nullColumns_detection','IDness_detection','biasedness_detection','invalidEntries_detection','duplicate_detection','nullRows_detection','outlier_detection' avl_recs_QC Available files from the QC_tabs (Quality Checker tabs) missing_recs_QC Missing files from the QC_tabs (Quality Checker tabs) all_charts_num_3_ Numerical charts (outlier charts) all collated in a list format supported as per datapane objects print_report Printing option flexibility. Default value is kept as False. Returns DatapaneObject / Output[HTML] Expand source code def quality_check ( master_path , QC_tabs , avl_recs_QC , missing_recs_QC , all_charts_num_3_ , print_report = False , ): \"\"\" This function helps to produce output specific to the Quality Checker Tab. Parameters ---------- master_path Path containing the input files. QC_tabs nullColumns_detection','IDness_detection','biasedness_detection','invalidEntries_detection','duplicate_detection','nullRows_detection','outlier_detection' avl_recs_QC Available files from the QC_tabs (Quality Checker tabs) missing_recs_QC Missing files from the QC_tabs (Quality Checker tabs) all_charts_num_3_ Numerical charts (outlier charts) all collated in a list format supported as per datapane objects print_report Printing option flexibility. Default value is kept as False. Returns ------- DatapaneObject / Output[HTML] \"\"\" c_ = [] r_ = [] if len ( missing_recs_QC ) == len ( QC_tabs ): return \"null_report\" else : row_wise = [ \"duplicate_detection\" , \"nullRows_detection\" ] col_wise = [ \"nullColumns_detection\" , \"IDness_detection\" , \"biasedness_detection\" , \"invalidEntries_detection\" , \"outlier_detection\" , ] row_wise_ = [ p for p in row_wise if p in avl_recs_QC ] col_wise_ = [ p for p in col_wise if p in avl_recs_QC ] len_row_wise = len ([ p for p in row_wise if p in avl_recs_QC ]) len_col_wise = len ([ p for p in col_wise if p in avl_recs_QC ]) if len_row_wise == 0 : c = data_analyzer_output ( master_path , col_wise_ , \"quality_checker\" ) for i in c : for j in i : if j == \"outlier_charts_placeholder\" and len ( all_charts_num_3_ ) > 1 : c_ . append ( dp . Select ( blocks = all_charts_num_3_ , type = dp . SelectType . DROPDOWN ) ) elif ( j == \"outlier_charts_placeholder\" and len ( all_charts_num_3_ ) == 0 ): c_ . append ( dp . Plot ( blank_chart )) else : c_ . append ( j ) report = dp . Group ( dp . Text ( \"# \" ), dp . Text ( \"*This section identifies the data quality issues at both row and column level.*\" ), dp . Text ( \"# \" ), dp . Text ( \"# \" ), dp . Group ( * c_ ), dp . Text ( \"# \" ), dp . Text ( \"# \" ), rows = 8 , label = \"Quality Check\" , ) elif len_col_wise == 0 : r = data_analyzer_output ( master_path , row_wise_ , \"quality_checker\" ) for i in r : for j in i : r_ . append ( j ) report = dp . Group ( dp . Text ( \"# \" ), dp . Text ( \"*This section identifies the data quality issues at both row and column level.*\" ), dp . Text ( \"# \" ), dp . Text ( \"# \" ), dp . Group ( * r_ ), dp . Text ( \"# \" ), dp . Text ( \"# \" ), rows = 8 , label = \"Quality Check\" , ) else : c = data_analyzer_output ( master_path , col_wise_ , \"quality_checker\" ) for i in c : for j in i : if j == \"outlier_charts_placeholder\" and len ( all_charts_num_3_ ) > 1 : c_ . append ( dp . Select ( blocks = all_charts_num_3_ , type = dp . SelectType . DROPDOWN ) ) elif ( j == \"outlier_charts_placeholder\" and len ( all_charts_num_3_ ) == 0 ): c_ . append ( dp . Plot ( blank_chart )) else : c_ . append ( j ) r = data_analyzer_output ( master_path , row_wise_ , \"quality_checker\" ) for i in r : for j in i : r_ . append ( j ) report = dp . Group ( dp . Text ( \"# \" ), dp . Text ( \"*This section identifies the data quality issues at both row and column level.*\" ), dp . Text ( \"# \" ), dp . Text ( \"# \" ), dp . Select ( blocks = [ dp . Group ( dp . Text ( \"# \" ), dp . Group ( * c_ ), rows = 2 , label = \"Column Level\" ), dp . Group ( dp . Text ( \"# \" ), dp . Group ( * r_ ), rows = 2 , label = \"Row Level\" ), ], type = dp . SelectType . TABS , ), dp . Text ( \"# \" ), dp . Text ( \"# \" ), label = \"Quality Check\" , ) if print_report : dp . Report ( default_template [ 0 ], default_template [ 1 ], report ) . save ( ends_with ( master_path ) + \"quality_check.html\" , open = True ) return report def remove_u_score ( col) This functions help to remove the \"_\" present in a specific text Parameters col Analysis column containing \"_\" present gets replaced along with upper case conversion Returns String Expand source code def remove_u_score ( col ): \"\"\" This functions help to remove the \"_\" present in a specific text Parameters ---------- col Analysis column containing \"_\" present gets replaced along with upper case conversion Returns ------- String \"\"\" col_ = col . split ( \"_\" ) bl = [] for i in col_ : if i == \"nullColumns\" or i == \"nullRows\" : bl . append ( \"Null\" ) else : bl . append ( i [ 0 ] . upper () + i [ 1 :]) return \" \" . join ( bl ) def ts_landscape ( base_path, ts_cols, id_col) This function helps to produce a basic landscaping view of the data by picking up the base path for reading the aggregated data and specified by the timestamp / date column & the ID column. Parameters base_path Base path which is the same as Master path where the aggregated data resides. ts_col Timestamp / date column name id_col ID Column Returns DatapaneObject Expand source code def ts_landscape ( base_path , ts_cols , id_col ): \"\"\" This function helps to produce a basic landscaping view of the data by picking up the base path for reading the aggregated data and specified by the timestamp / date column & the ID column. Parameters ---------- base_path Base path which is the same as Master path where the aggregated data resides. ts_col Timestamp / date column name id_col ID Column Returns ------- DatapaneObject \"\"\" if ts_cols is None : return dp . Text ( \"#\" ) else : df_stats_ts = [] for i in ts_cols : if len ( ts_cols ) > 1 : df_stats_ts . append ( dp . Group ( dp . Group ( dp . Text ( \"# \" ), dp . Text ( \"*ID considered here is : \" + str ( id_col ) + \"*\" ), dp . Text ( \"# \" ), dp . Text ( \"#### Consistency Analysis Of Dates\" ), dp . DataTable ( pd . read_csv ( ends_with ( base_path ) + \"stats_\" + i + \"_1.csv\" ) . set_index ( \"attribute\" ) . T , label = i , ), rows = 5 , ), dp . Group ( dp . Text ( \"*The Percentile distribution across different bins of ID-Date / Date-ID combination should be in a considerable range to determine the regularity of Time series. In an ideal scenario the proportion of dates within each ID should be same. Also, the count of IDs across unique dates should be consistent for a balanced distribution*\" ), dp . Text ( \"# \" ), dp . Text ( \"#### Vital Statistics\" ), dp . DataTable ( pd . read_csv ( ends_with ( base_path ) + \"stats_\" + i + \"_2.csv\" ) . T . rename ( columns = { 0 : \"\" }), label = i , ), rows = 4 , ), label = i , rows = 2 , ) ) else : df_stats_ts . append ( dp . Group ( dp . Group ( dp . Text ( \"# \" ), dp . Text ( \"*ID considered here is : \" + str ( id_col ) + \"*\" ), dp . Text ( \"#### Consistency Analysis Of Dates\" ), dp . Text ( \"# \" ), dp . DataTable ( pd . read_csv ( ends_with ( base_path ) + \"stats_\" + i + \"_1.csv\" ) . set_index ( \"attribute\" ) . T , label = i , ), rows = 5 , ), dp . Group ( dp . Text ( \"# \" ), dp . Text ( \"#### Vital Statistics\" ), dp . DataTable ( pd . read_csv ( ends_with ( base_path ) + \"stats_\" + i + \"_2.csv\" ) . T . rename ( columns = { 0 : \"\" }), label = i , ), rows = 3 , ), label = i , rows = 2 , ) ) df_stats_ts . append ( dp . Plot ( blank_chart , label = \"_\" )) return dp . Group ( dp . Text ( \"### Time Stamp Data Diagnosis\" ), dp . Select ( blocks = df_stats_ts , type = dp . SelectType . DROPDOWN ), rows = 2 , ) def ts_stats ( base_path) This function helps to read the base data containing desired input and produces output specific to the ts_cols_stats.csv file Parameters base_path Base path which is the same as Master path where the aggregated data resides. Returns List Expand source code def ts_stats ( base_path ): \"\"\" This function helps to read the base data containing desired input and produces output specific to the `ts_cols_stats.csv` file Parameters ---------- base_path Base path which is the same as Master path where the aggregated data resides. Returns ------- List \"\"\" df = pd . read_csv ( base_path + \"ts_cols_stats.csv\" ) all_stats = [] for i in range ( 0 , 7 ): try : all_stats . append ( df [ df . index . values == i ] . values [ 0 ][ 0 ] . split ( \",\" )) except : all_stats . append ([]) c0 = pd . DataFrame ( all_stats [ 0 ], columns = [ \"attributes\" ]) c1 = pd . DataFrame ( list_ts_remove_append ( all_stats [ 1 ], 1 ), columns = [ \"attributes\" ]) c1 [ \"Analyzed Attributes\" ] = \"\u2714\" c2 = pd . DataFrame ( list_ts_remove_append ( all_stats [ 2 ], 1 ), columns = [ \"attributes\" ]) c2 [ \"Attributes Identified\" ] = \"\u2714\" c3 = pd . DataFrame ( list_ts_remove_append ( all_stats [ 3 ], 1 ), columns = [ \"attributes\" ]) c3 [ \"Attributes Pre-Existed\" ] = \"\u2714\" c4 = pd . DataFrame ( list_ts_remove_append ( all_stats [ 4 ], 1 ), columns = [ \"attributes\" ]) c4 [ \"Overall TimeStamp Attributes\" ] = \"\u2714\" c5 = list_ts_remove_append ( all_stats [ 5 ], 1 ) c6 = list_ts_remove_append ( all_stats [ 6 ], 1 ) return c0 , c1 , c2 , c3 , c4 , c5 , c6 def ts_viz_1_1 ( base_path, x_col, y_col, output_type) Parameters base_path Base path which is the same as Master path where the aggregated data resides. x_col Timestamp / date column name y_col Numerical column names output_type Time category of analysis which can be between \"Daily\", \"Hourly\", \"Weekly\" Returns Plot Expand source code def ts_viz_1_1 ( base_path , x_col , y_col , output_type ): \"\"\" Parameters ---------- base_path Base path which is the same as Master path where the aggregated data resides. x_col Timestamp / date column name y_col Numerical column names output_type Time category of analysis which can be between \"Daily\", \"Hourly\", \"Weekly\" Returns ------- Plot \"\"\" ts_fig = gen_time_series_plots ( base_path , x_col , y_col , output_type ) return ts_fig def ts_viz_1_2 ( base_path, ts_col, col_list, output_type) Parameters base_path Base path which is the same as Master path where the aggregated data resides. ts_col Timestamp / date column name col_list Numerical / Categorical column names output_type Time category of analysis which can be between \"Daily\", \"Hourly\", \"Weekly\" Returns DatapaneObject Expand source code def ts_viz_1_2 ( base_path , ts_col , col_list , output_type ): \"\"\" Parameters ---------- base_path Base path which is the same as Master path where the aggregated data resides. ts_col Timestamp / date column name col_list Numerical / Categorical column names output_type Time category of analysis which can be between \"Daily\", \"Hourly\", \"Weekly\" Returns ------- DatapaneObject \"\"\" bl = [] for i in col_list : if len ( col_list ) > 1 : bl . append ( dp . Group ( ts_viz_1_1 ( base_path , ts_col , i , output_type ), label = i )) else : bl . append ( dp . Group ( ts_viz_1_1 ( base_path , ts_col , i , output_type ), label = i )) bl . append ( dp . Plot ( blank_chart , label = \"_\" )) return dp . Select ( blocks = bl , type = dp . SelectType . DROPDOWN ) def ts_viz_1_3 ( base_path, ts_col, num_cols, cat_cols, output_type) Parameters base_path Base path which is the same as Master path where the aggregated data resides. ts_col Timestamp / date column name num_cols Numerical column names cat_cols Categorical column names output_type Time category of analysis which can be between \"Daily\", \"Hourly\", \"Weekly\" Returns DatapaneObject Expand source code def ts_viz_1_3 ( base_path , ts_col , num_cols , cat_cols , output_type ): \"\"\" Parameters ---------- base_path Base path which is the same as Master path where the aggregated data resides. ts_col Timestamp / date column name num_cols Numerical column names cat_cols Categorical column names output_type Time category of analysis which can be between \"Daily\", \"Hourly\", \"Weekly\" Returns ------- DatapaneObject \"\"\" ts_v = [] # print(num_cols) # print(cat_cols) if len ( num_cols ) == 0 : for i in ts_col : if len ( ts_col ) > 1 : ts_v . append ( dp . Group ( ts_viz_1_2 ( base_path , i , cat_cols , output_type ), label = i ) ) else : ts_v . append ( dp . Group ( ts_viz_1_2 ( base_path , i , cat_cols , output_type ), label = i ) ) ts_v . append ( dp . Plot ( blank_chart , label = \"_\" )) elif len ( cat_cols ) == 0 : for i in ts_col : if len ( ts_col ) > 1 : ts_v . append ( dp . Group ( ts_viz_1_2 ( base_path , i , num_cols , output_type ), label = i ) ) else : ts_v . append ( dp . Group ( ts_viz_1_2 ( base_path , i , num_cols , output_type ), label = i ) ) ts_v . append ( dp . Plot ( blank_chart , label = \"_\" )) elif ( len ( num_cols ) >= 1 ) & ( len ( cat_cols ) >= 1 ): for i in ts_col : if len ( ts_col ) > 1 : ts_v . append ( dp . Group ( dp . Select ( blocks = [ dp . Group ( ts_viz_1_2 ( base_path , i , num_cols , output_type ), label = \"Numerical\" , ), dp . Group ( ts_viz_1_2 ( base_path , i , cat_cols , output_type ), label = \"Categorical\" , ), ], type = dp . SelectType . TABS , ), label = i , ) ) else : ts_v . append ( dp . Group ( dp . Select ( blocks = [ dp . Group ( ts_viz_1_2 ( base_path , i , num_cols , output_type ), label = \"Numerical\" , ), dp . Group ( ts_viz_1_2 ( base_path , i , cat_cols , output_type ), label = \"Categorical\" , ), ], type = dp . SelectType . TABS , ), label = i , ) ) ts_v . append ( dp . Plot ( blank_chart , label = \"_\" )) return dp . Select ( blocks = ts_v , type = dp . SelectType . DROPDOWN ) def ts_viz_2_1 ( base_path, x_col, y_col) Parameters base_path Base path which is the same as Master path where the aggregated data resides. x_col Timestamp / date column name y_col Numerical column names Returns DatapaneObject Expand source code def ts_viz_2_1 ( base_path , x_col , y_col ): \"\"\" Parameters ---------- base_path Base path which is the same as Master path where the aggregated data resides. x_col Timestamp / date column name y_col Numerical column names Returns ------- DatapaneObject \"\"\" ts_fig = [] for i in [ \"mean\" , \"median\" , \"min\" , \"max\" ]: ts_fig . append ( dp . Plot ( plotSeasonalDecompose ( base_path , x_col , y_col , metric_col = i ), label = i . title (), ) ) return dp . Select ( blocks = ts_fig , type = dp . SelectType . TABS ) def ts_viz_2_2 ( base_path, ts_col, col_list) Parameters base_path Base path which is the same as Master path where the aggregated data resides. ts_col Timestamp / date column name col_list Numerical column names Returns DatapaneObject Expand source code def ts_viz_2_2 ( base_path , ts_col , col_list ): \"\"\" Parameters ---------- base_path Base path which is the same as Master path where the aggregated data resides. ts_col Timestamp / date column name col_list Numerical column names Returns ------- DatapaneObject \"\"\" bl = [] for i in col_list : if len ( col_list ) > 1 : bl . append ( dp . Group ( ts_viz_2_1 ( base_path , ts_col , i ), label = i )) else : bl . append ( dp . Group ( ts_viz_2_1 ( base_path , ts_col , i ), label = i )) bl . append ( dp . Group ( dp . Plot ( blank_chart , label = \" \" ), label = \" \" )) return dp . Select ( blocks = bl , type = dp . SelectType . DROPDOWN ) def ts_viz_2_3 ( base_path, ts_col, num_cols) Parameters base_path Base path which is the same as Master path where the aggregated data resides. ts_col Timestamp / date column name num_cols Numerical column names Returns DatapaneObject Expand source code def ts_viz_2_3 ( base_path , ts_col , num_cols ): \"\"\" Parameters ---------- base_path Base path which is the same as Master path where the aggregated data resides. ts_col Timestamp / date column name num_cols Numerical column names Returns ------- DatapaneObject \"\"\" ts_v = [] if len ( ts_col ) > 1 : for i in ts_col : f = list ( pd . read_csv ( ends_with ( base_path ) + \"stats_\" + i + \"_2.csv\" ) . count_unique_dates . values )[ 0 ] if f >= 24 : ts_v . append ( dp . Group ( ts_viz_2_2 ( base_path , i , num_cols ), label = i )) else : ts_v . append ( dp . Group ( dp . Text ( \"The plots couldn't be displayed as x must have 2 complete cycles requires 24 observations. x only has \" + str ( f ) + \" observation(s)\" ), label = i , ) ) else : for i in ts_col : f = list ( pd . read_csv ( ends_with ( base_path ) + \"stats_\" + i + \"_2.csv\" ) . count_unique_dates . values )[ 0 ] if f >= 24 : ts_v . append ( dp . Group ( ts_viz_2_2 ( base_path , i , num_cols ), label = i )) ts_v . append ( dp . Plot ( blank_chart , label = \"_\" )) else : ts_v . append ( dp . Group ( dp . Text ( \"The plots couldn't be displayed as x must have 2 complete cycles requires 24 observations. x only has \" + str ( f ) + \" observation(s)\" ), label = i , ) ) ts_v . append ( dp . Plot ( blank_chart , label = \"_\" )) return dp . Select ( blocks = ts_v , type = dp . SelectType . DROPDOWN ) def ts_viz_3_1 ( base_path, x_col, y_col) Parameters base_path Base path which is the same as Master path where the aggregated data resides. x_col Timestamp / date column name y_col Numerical column names Returns DatapaneObject Expand source code def ts_viz_3_1 ( base_path , x_col , y_col ): \"\"\" Parameters ---------- base_path Base path which is the same as Master path where the aggregated data resides. x_col Timestamp / date column name y_col Numerical column names Returns ------- DatapaneObject \"\"\" ts_fig = [] df = pd . read_csv ( ends_with ( base_path ) + x_col + \"_\" + y_col + \"_daily.csv\" ) . dropna () df [ x_col ] = pd . to_datetime ( df [ x_col ], format = \"%Y-%m- %d %H:%M:%S. %f \" ) df = df . set_index ( x_col ) for metric_col in [ \"mean\" , \"median\" , \"min\" , \"max\" ]: adf_test = round ( adfuller ( df [ metric_col ])[ 0 ], 3 ), round ( adfuller ( df [ metric_col ])[ 1 ], 3 ) kpss_test = round ( kpss ( df [ metric_col ], regression = \"ct\" )[ 0 ], 3 ), round ( kpss ( df [ metric_col ], regression = \"ct\" )[ 1 ], 3 ) if adf_test [ 1 ] < 0.05 : adf_flag = True else : adf_flag = False if kpss_test [ 1 ] < 0.05 : kpss_flag = True else : kpss_flag = False # df[metric_col] = df[metric_col].apply(lambda x: boxcox1p(x,0.25)) # lambda_box_cox = round(boxcox(df[metric_col])[1],5) fit = PowerTransformer ( method = \"yeo-johnson\" ) try : lambda_box_cox = round ( fit . fit ( np . array ( df [ metric_col ]) . reshape ( - 1 , 1 )) . lambdas_ [ 0 ], 3 ) cnt = 0 except : cnt = 1 if cnt == 0 : # df[metric_col+\"_transformed\"] = boxcox(df[metric_col],lmbda=lambda_box_cox) df [ metric_col + \"_transformed\" ] = fit . transform ( np . array ( df [ metric_col ]) . reshape ( - 1 , 1 ) ) fig = make_subplots ( rows = 1 , cols = 2 , subplot_titles = [ \"Pre-Transformation\" , \"Post-Transformation\" ], ) fig . add_trace ( go . Scatter ( x = df . index , y = df [ metric_col ], mode = \"lines+markers\" , name = metric_col , line = dict ( color = global_theme [ 1 ]), ), row = 1 , col = 1 , ) fig . add_trace ( go . Scatter ( x = df . index , y = df [ metric_col + \"_transformed\" ], mode = \"lines+markers\" , name = metric_col + \"_transformed\" , line = dict ( color = global_theme [ 7 ]), ), row = 1 , col = 2 , ) fig . layout . plot_bgcolor = global_plot_bg_color fig . layout . paper_bgcolor = global_paper_bg_color fig . update_xaxes ( gridcolor = px . colors . sequential . Greys [ 1 ]) fig . update_yaxes ( gridcolor = px . colors . sequential . Greys [ 1 ]) fig . update_layout ( autosize = True , width = 2000 , height = 400 ) fig . update_layout ( legend = dict ( orientation = \"h\" , x = 0.5 , yanchor = \"bottom\" , xanchor = \"center\" ) ) ts_fig . append ( dp . Group ( dp . Group ( dp . BigNumber ( heading = \"ADF Test Statistic\" , value = adf_test [ 0 ], change = adf_test [ 1 ], is_upward_change = adf_flag , ), dp . BigNumber ( heading = \"KPSS Test Statistic\" , value = kpss_test [ 0 ], change = kpss_test [ 1 ], is_upward_change = kpss_flag , ), dp . BigNumber ( heading = \"Box-Cox Transformation\" , value = lambda_box_cox , change = str ( lambda_cat ( lambda_box_cox )), is_upward_change = True , ), columns = 3 , ), dp . Text ( \"#### Transformation View\" ), dp . Text ( \"Below Transformation is basis the inferencing from the Box Cox Transformation. The Lambda value of \" + str ( lambda_box_cox ) + \" indicates a \" + str ( lambda_cat ( lambda_box_cox )) + \". A Pre-Post Transformation Visualization is done for better clarity. \" ), dp . Plot ( fig ), dp . Text ( \"**Guidelines :** \" ), dp . Text ( \"**ADF** : *The more negative the statistic, the more likely we are to reject the null hypothesis. If the p-value is less than the significance level of 0.05, we can reject the null hypothesis and take that the series is stationary*\" ), dp . Text ( \"**KPSS** : *If the p-value is high, we cannot reject the null hypothesis. So the series is stationary.*\" ), label = metric_col . title (), ) ) else : ts_fig . append ( dp . Group ( dp . Group ( dp . BigNumber ( heading = \"ADF Test Statistic\" , value = adf_test [ 0 ], change = adf_test [ 1 ], is_upward_change = adf_flag , ), dp . BigNumber ( heading = \"KPSS Test Statistic\" , value = kpss_test [ 0 ], change = kpss_test [ 1 ], is_upward_change = kpss_flag , ), dp . BigNumber ( heading = \"Box-Cox Transformation\" , value = \"ValueOutOfRange\" , change = \"ValueOutOfRange\" , is_upward_change = True , ), columns = 3 , ), dp . Text ( \"**Guidelines :** \" ), dp . Text ( \"**ADF** : *The more negative the statistic, the more likely we are to reject the null hypothesis. If the p-value is less than the significance level of 0.05, we can reject the null hypothesis and take that the series is stationary*\" ), dp . Text ( \"**KPSS** : *If the p-value is high, we cannot reject the null hypothesis. So the series is stationary.*\" ), label = metric_col . title (), ) ) return dp . Select ( blocks = ts_fig , type = dp . SelectType . TABS ) def ts_viz_3_2 ( base_path, ts_col, col_list) Parameters base_path Base path which is the same as Master path where the aggregated data resides. ts_col Timestamp / date column name col_list Numerical column names Returns DatapaneObject Expand source code def ts_viz_3_2 ( base_path , ts_col , col_list ): \"\"\" Parameters ---------- base_path Base path which is the same as Master path where the aggregated data resides. ts_col Timestamp / date column name col_list Numerical column names Returns ------- DatapaneObject \"\"\" bl = [] for i in col_list : if len ( num_cols ) > 1 : bl . append ( dp . Group ( ts_viz_3_1 ( base_path , ts_col , i ), label = i )) else : bl . append ( dp . Group ( ts_viz_3_1 ( base_path , ts_col , i ), label = i )) bl . append ( dp . Group ( dp . Plot ( blank_chart , label = \" \" ), label = \" \" )) return dp . Select ( blocks = bl , type = dp . SelectType . DROPDOWN ) def ts_viz_3_3 ( base_path, ts_col, num_cols) Parameters base_path Base path which is the same as Master path where the aggregated data resides. ts_col Timestamp / date column name num_cols Numerical column names Returns DatapaneObject Expand source code def ts_viz_3_3 ( base_path , ts_col , num_cols ): \"\"\" Parameters ---------- base_path Base path which is the same as Master path where the aggregated data resides. ts_col Timestamp / date column name num_cols Numerical column names Returns ------- DatapaneObject \"\"\" # f = list(pd.read_csv(ends_with(base_path) + \"stats_\" + i + \"_2.csv\").count_unique_dates.values)[0] # if f >= 6: if len ( ts_col ) > 1 : ts_v = [] for i in ts_col : f = list ( pd . read_csv ( ends_with ( base_path ) + \"stats_\" + i + \"_2.csv\" ) . count_unique_dates . values )[ 0 ] if f >= 6 : ts_v . append ( dp . Group ( ts_viz_3_2 ( base_path , i , num_cols ), label = i )) else : ts_v . append ( dp . Group ( dp . Text ( \"The data contains insufficient data points for the desired transformation analysis. Please ensure the number of unique dates is sufficient.\" ), label = i , ) ) else : ts_v = [] for i in ts_col : f = list ( pd . read_csv ( ends_with ( base_path ) + \"stats_\" + i + \"_2.csv\" ) . count_unique_dates . values )[ 0 ] if f >= 6 : ts_v . append ( dp . Group ( ts_viz_3_2 ( base_path , i , num_cols ), label = i )) ts_v . append ( dp . Plot ( blank_chart , label = \"_\" )) else : ts_v . append ( dp . Group ( dp . Text ( \"The data contains insufficient data points for the desired transformation analysis. Please ensure the number of unique dates is sufficient.\" ), label = i , ) ) ts_v . append ( dp . Plot ( blank_chart , label = \"_\" )) return dp . Select ( blocks = ts_v , type = dp . SelectType . DROPDOWN ) def ts_viz_generate ( master_path, id_col, print_report=False, output_type=None) This function helps to produce the output in the nested / recursive function supported by datapane. Eventually this is populated at the final report. Parameters master_path Master path where the aggregated data resides. id_col ID Column print_report Option to specify whether the Report needs to be saved or not. True / False can be used to specify the needful. output_type Time category of analysis which can be between \"Daily\", \"Hourly\", \"Weekly\" Returns DatapaneObject / Output[HTML] Expand source code def ts_viz_generate ( master_path , id_col , print_report = False , output_type = None ): \"\"\" This function helps to produce the output in the nested / recursive function supported by datapane. Eventually this is populated at the final report. Parameters ---------- master_path Master path where the aggregated data resides. id_col ID Column print_report Option to specify whether the Report needs to be saved or not. True / False can be used to specify the needful. output_type Time category of analysis which can be between \"Daily\", \"Hourly\", \"Weekly\" Returns ------- DatapaneObject / Output[HTML] \"\"\" master_path = ends_with ( master_path ) try : c0 , c1 , c2 , c3 , c4 , c5 , c6 = ts_stats ( master_path ) except : return \"null_report\" stats_df = ( c0 . merge ( c1 , on = \"attributes\" , how = \"left\" ) . merge ( c2 , on = \"attributes\" , how = \"left\" ) . merge ( c3 , on = \"attributes\" , how = \"left\" ) . merge ( c4 , on = \"attributes\" , how = \"left\" ) . fillna ( \"\u2718\" ) ) global num_cols global cat_cols num_cols , cat_cols = c5 , c6 final_ts_cols = list ( ts_stats ( master_path )[ 4 ] . attributes . values ) if output_type == \"daily\" : report = dp . Group ( dp . Text ( \"# \" ), dp . Text ( \"*This section summarizes the information about timestamp features and how they are interactive with other attributes. An exhaustive diagnosis is done by looking at different time series components, how they could be useful in deriving insights for further downstream applications*\" ), dp . Text ( \"# \" ), dp . Text ( \"# \" ), dp . Text ( \"### Basic Landscaping\" ), dp . Text ( \"Out of **\" + str ( len ( list ( ts_stats ( master_path )[ 1 ] . attributes . values ))) + \"** potential attributes in the data, the module could locate **\" + str ( len ( final_ts_cols )) + \"** attributes as Timestamp\" ), dp . DataTable ( stats_df ), ts_landscape ( master_path , final_ts_cols , id_col ), dp . Text ( \"*Lower the **CoV** (Coefficient Of Variation), Higher the Consistency between the consecutive dates. Similarly the Mean & Variance should be consistent over time*\" ), dp . Text ( \"### Visualization across the Shortlisted Timestamp Attributes\" ), ts_viz_1_3 ( master_path , final_ts_cols , num_cols , cat_cols , output_type ), dp . Text ( \"### Decomposed View\" ), ts_viz_2_3 ( master_path , final_ts_cols , num_cols ), dp . Text ( \"### Stationarity & Transformations\" ), ts_viz_3_3 ( master_path , final_ts_cols , num_cols ), dp . Text ( \"#\" ), dp . Text ( \"#\" ), label = \"Time Series Analyzer\" , ) else : report = dp . Group ( dp . Text ( \"# \" ), dp . Text ( \"*This section summarizes the information about timestamp features and how they are interactive with other attributes. An exhaustive diagnosis is done by looking at different time series components, how they could be useful in deriving insights for further downstream applications*\" ), dp . Text ( \"# \" ), dp . Text ( \"# \" ), dp . Text ( \"### Basic Landscaping\" ), dp . Text ( \"Out of **\" + str ( len ( list ( ts_stats ( master_path )[ 1 ] . attributes . values ))) + \"** potential attributes in the data, the module could locate **\" + str ( len ( final_ts_cols )) + \"** attributes as Timestamp\" ), dp . DataTable ( stats_df ), ts_landscape ( master_path , final_ts_cols , id_col ), dp . Text ( \"*Lower the **CoV** (Coefficient Of Variation), Higher the Consistency between the consecutive dates. Similarly the Mean & Variance should be consistent over time*\" ), dp . Text ( \"### Visualization across the Shortlisted Timestamp Attributes\" ), ts_viz_1_3 ( master_path , final_ts_cols , num_cols , cat_cols , output_type ), dp . Text ( \"#\" ), dp . Text ( \"#\" ), label = \"Time Series Analyzer\" , ) if print_report : dp . Report ( default_template [ 0 ], default_template [ 1 ], report ) . save ( ends_with ( master_path ) + \"time_series_analyzer.html\" , open = True ) return report def wiki_generator ( master_path, dataDict_path=None, metricDict_path=None, print_report=False) This function helps to produce output specific to the Wiki Tab. Parameters master_path Path containing the input files. dataDict_path Data dictionary path. Default value is kept as None. metricDict_path Metric dictionary path. Default value is kept as None. print_report Printing option flexibility. Default value is kept as False. Returns DatapaneObject / Output[HTML] Expand source code def wiki_generator ( master_path , dataDict_path = None , metricDict_path = None , print_report = False ): \"\"\" This function helps to produce output specific to the Wiki Tab. Parameters ---------- master_path Path containing the input files. dataDict_path Data dictionary path. Default value is kept as None. metricDict_path Metric dictionary path. Default value is kept as None. print_report Printing option flexibility. Default value is kept as False. Returns ------- DatapaneObject / Output[HTML] \"\"\" try : datatype_df = pd . read_csv ( ends_with ( master_path ) + \"data_type.csv\" ) except FileNotFoundError : logger . error ( f \"file { master_path } /data_type.csv doesn't exist, cannot read datatypes\" ) except Exception : logger . info ( \"generate an empty dataframe with columns attribute and data_type \" ) datatype_df = pd . DataFrame ( columns = [ \"attribute\" , \"data_type\" ], index = range ( 1 )) try : data_dict = pd . read_csv ( dataDict_path ) . merge ( datatype_df , how = \"outer\" , on = \"attribute\" ) except FileNotFoundError : logger . error ( f \"file { dataDict_path } doesn't exist, cannot read data dict\" ) except Exception : data_dict = datatype_df try : metric_dict = pd . read_csv ( metricDict_path ) except FileNotFoundError : logger . error ( f \"file { metricDict_path } doesn't exist, cannot read metrics dict\" ) except Exception : metric_dict = pd . DataFrame ( columns = [ \"Section Category\" , \"Section Name\" , \"Metric Name\" , \"Metric Definitions\" , ], index = range ( 1 ), ) report = dp . Group ( dp . Text ( \"# \" ), dp . Text ( \"\"\" *A quick reference to the attributes from the dataset (Data Dictionary) and the metrics computed in the report (Metric Dictionary).* \"\"\" ), dp . Text ( \"# \" ), dp . Text ( \"# \" ), dp . Select ( blocks = [ dp . Group ( dp . Group ( dp . Text ( \"## \" ), dp . DataTable ( data_dict )), label = \"Data Dictionary\" , ), dp . Group ( dp . Text ( \"##\" ), dp . DataTable ( metric_dict ), label = \"Metric Dictionary\" ), ], type = dp . SelectType . TABS , ), dp . Text ( \"# \" ), dp . Text ( \"# \" ), dp . Text ( \"# \" ), dp . Text ( \"# \" ), label = \"Wiki\" , ) if print_report : dp . Report ( default_template [ 0 ], default_template [ 1 ], report ) . save ( ends_with ( master_path ) + \"wiki_generator.html\" , open = True ) return report","title":"<code>report_generation</code>"},{"location":"api/data_report/report_generation.html#report_generation","text":"This module generates the final report output specific to the intermediate data generated across each of the modules. The final report, however, can be proccessed through the config.yaml file or by generating it through the respective functions. Below are some of the functions used to process the final output. line_chart_gen_stability data_analyzer_output drift_stability_ind chart_gen_list executive_summary_gen wiki_generator descriptive_statistics quality_check attribute_associations data_drift_stability plotSeasonalDecompose gen_time_series_plots list_ts_remove_append ts_viz_1_1 \u2014 ts_viz_1_3 ts_viz_2_1 \u2014 ts_viz_2_3 ts_viz_3_1 \u2014 ts_viz_3_3 ts_landscape ts_stats ts_viz_generate anovos_report However, each of the functions have been detailed in the respective sections across the parameters used. Expand source code # coding=utf-8 \"\"\"This module generates the final report output specific to the intermediate data generated across each of the modules. The final report, however, can be proccessed through the config.yaml file or by generating it through the respective functions. Below are some of the functions used to process the final output. - line_chart_gen_stability - data_analyzer_output - drift_stability_ind - chart_gen_list - executive_summary_gen - wiki_generator - descriptive_statistics - quality_check - attribute_associations - data_drift_stability - plotSeasonalDecompose - gen_time_series_plots - list_ts_remove_append - ts_viz_1_1 \u2014 ts_viz_1_3 - ts_viz_2_1 \u2014 ts_viz_2_3 - ts_viz_3_1 \u2014 ts_viz_3_3 - ts_landscape - ts_stats - ts_viz_generate - anovos_report However, each of the functions have been detailed in the respective sections across the parameters used. \"\"\" import json import os import subprocess import datapane as dp import numpy as np import pandas as pd import plotly.express as px import plotly.graph_objects as go from loguru import logger import dateutil.parser from statsmodels.tsa.seasonal import seasonal_decompose import plotly.tools as tls from plotly.subplots import make_subplots from statsmodels.tsa.stattools import adfuller , kpss from sklearn.preprocessing import PowerTransformer from anovos.shared.utils import ends_with import warnings warnings . filterwarnings ( \"ignore\" ) global_theme = px . colors . sequential . Plasma global_theme_r = px . colors . sequential . Plasma_r global_plot_bg_color = \"rgba(0,0,0,0)\" global_paper_bg_color = \"rgba(0,0,0,0)\" default_template = ( dp . HTML ( \"\"\" <html> <img src=\"https://mobilewalla-anovos.s3.amazonaws.com/anovos.png\" style=\"height:100px;display:flex;margin:auto;float:right\" /> </html>\"\"\" ), dp . Text ( \"# ML-Anovos Report\" ), ) def remove_u_score ( col ): \"\"\" This functions help to remove the \"_\" present in a specific text Parameters ---------- col Analysis column containing \"_\" present gets replaced along with upper case conversion Returns ------- String \"\"\" col_ = col . split ( \"_\" ) bl = [] for i in col_ : if i == \"nullColumns\" or i == \"nullRows\" : bl . append ( \"Null\" ) else : bl . append ( i [ 0 ] . upper () + i [ 1 :]) return \" \" . join ( bl ) def line_chart_gen_stability ( df1 , df2 , col ): \"\"\" This function helps to produce charts which are specific to data stability index. It taken into account the stability input along with the analysis column to produce the desired output. Parameters ---------- df1 Analysis dataframe pertaining to summarized stability metrics df2 Analysis dataframe pertaining to historical data col Analysis column Returns ------- DatapaneObject \"\"\" def val_cat ( val ): \"\"\" Parameters ---------- val Returns ------- String \"\"\" if val >= 3.5 : return \"Very Stable\" elif val >= 3 and val < 3.5 : return \"Stable\" elif val >= 2 and val < 3 : return \"Marginally Stable\" elif val >= 1 and val < 2 : return \"Unstable\" elif val >= 0 and val < 1 : return \"Very Unstable\" else : return \"Out of Range\" val_si = list ( df2 [ df2 [ \"attribute\" ] == col ] . stability_index . values )[ 0 ] f1 = go . Figure () f1 . add_trace ( go . Indicator ( mode = \"gauge+number\" , value = val_si , gauge = { \"axis\" : { \"range\" : [ None , 4 ], \"tickwidth\" : 1 , \"tickcolor\" : \"black\" }, \"bgcolor\" : \"white\" , \"steps\" : [ { \"range\" : [ 0 , 1 ], \"color\" : px . colors . sequential . Reds [ 7 ]}, { \"range\" : [ 1 , 2 ], \"color\" : px . colors . sequential . Reds [ 6 ]}, { \"range\" : [ 2 , 3 ], \"color\" : px . colors . sequential . Oranges [ 4 ]}, { \"range\" : [ 3 , 3.5 ], \"color\" : px . colors . sequential . BuGn [ 7 ]}, { \"range\" : [ 3.5 , 4 ], \"color\" : px . colors . sequential . BuGn [ 8 ]}, ], \"threshold\" : { \"line\" : { \"color\" : \"black\" , \"width\" : 3 }, \"thickness\" : 1 , \"value\" : val_si , }, \"bar\" : { \"color\" : global_plot_bg_color }, }, title = { \"text\" : \"Order of Stability: \" + val_cat ( val_si )}, ) ) f1 . update_layout ( height = 400 , font = { \"color\" : \"black\" , \"family\" : \"Arial\" }) f5 = \"Stability Index for \" + str ( col . upper ()) if len ( df1 . columns ) > 0 : df1 = df1 [ df1 [ \"attribute\" ] == col ] f2 = px . line ( df1 , x = \"idx\" , y = \"mean\" , markers = True , title = \"CV of Mean is \" + str ( list ( df2 [ df2 [ \"attribute\" ] == col ] . mean_cv . values )[ 0 ]), ) f2 . update_traces ( line_color = global_theme [ 2 ], marker = dict ( size = 14 )) f2 . layout . plot_bgcolor = global_plot_bg_color f2 . layout . paper_bgcolor = global_paper_bg_color f3 = px . line ( df1 , x = \"idx\" , y = \"stddev\" , markers = True , title = \"CV of Stddev is \" + str ( list ( df2 [ df2 [ \"attribute\" ] == col ] . stddev_cv . values )[ 0 ]), ) f3 . update_traces ( line_color = global_theme [ 6 ], marker = dict ( size = 14 )) f3 . layout . plot_bgcolor = global_plot_bg_color f3 . layout . paper_bgcolor = global_paper_bg_color f4 = px . line ( df1 , x = \"idx\" , y = \"kurtosis\" , markers = True , title = \"CV of Kurtosis is \" + str ( list ( df2 [ df2 [ \"attribute\" ] == col ] . kurtosis_cv . values )[ 0 ]), ) f4 . update_traces ( line_color = global_theme [ 4 ], marker = dict ( size = 14 )) f4 . layout . plot_bgcolor = global_plot_bg_color f4 . layout . paper_bgcolor = global_paper_bg_color return dp . Group ( dp . Text ( \"#\" ), dp . Text ( f5 ), dp . Plot ( f1 ), dp . Group ( dp . Plot ( f2 ), dp . Plot ( f3 ), dp . Plot ( f4 ), columns = 3 ), rows = 4 , label = col , ) else : return dp . Group ( dp . Text ( \"#\" ), dp . Text ( f5 ), dp . Plot ( f1 ), rows = 3 , label = col ) def data_analyzer_output ( master_path , avl_recs_tab , tab_name ): \"\"\" This section produces output in form of datapane objects which is specific to the different data analyzer modules. It is used by referring to the Master path along with the Available list of metrics & the Tab name. Parameters ---------- master_path Path containing all the output from analyzed data avl_recs_tab Available file names from the analysis tab tab_name Analysis tab from association_evaluator / quality_checker / stats_generator Returns ------- DatapaneObject \"\"\" df_list = [] df_plot_list = [] # @FIXME: unused variables plot_list = [] avl_recs_tab = [ x for x in avl_recs_tab if \"global_summary\" not in x ] for index , i in enumerate ( avl_recs_tab ): data = pd . read_csv ( ends_with ( master_path ) + str ( i ) + \".csv\" ) if len ( data . index ) == 0 : continue if tab_name == \"quality_checker\" : if i == \"duplicate_detection\" : duplicate_recs = pd . read_csv ( ends_with ( master_path ) + str ( i ) + \".csv\" ) . round ( 3 ) _unique_rows_count = int ( duplicate_recs [ duplicate_recs [ \"metric\" ] == \"unique_rows_count\" ] . value . values ) _rows_count = int ( duplicate_recs [ duplicate_recs [ \"metric\" ] == \"rows_count\" ] . value . values ) _duplicate_rows_count = int ( duplicate_recs [ duplicate_recs [ \"metric\" ] == \"duplicate_rows\" ] . value . values ) _duplicate_pct = float ( duplicate_recs [ duplicate_recs [ \"metric\" ] == \"duplicate_pct\" ] . value . values * 100.0 ) unique_rows_count = f \" No. Of Unique Rows: ** { _unique_rows_count } , **\" # @FIXME: variable names exists in outer scope rows_count = f \" No. of Rows: ** { _rows_count } , **\" duplicate_rows = ( f \" No. of Duplicate Rows: ** { _duplicate_rows_count } , **\" ) duplicate_pct = f \" Percentage of Duplicate Rows: ** { _duplicate_pct } %**\" df_list . append ( [ dp . Text ( \"### \" + str ( remove_u_score ( i ))), dp . Group ( dp . Text ( rows_count ), dp . Text ( unique_rows_count ), dp . Text ( duplicate_rows ), dp . Text ( duplicate_pct ), rows = 4 , ), dp . Text ( \"#\" ), dp . Text ( \"#\" ), ] ) elif i == \"outlier_detection\" : df_list . append ( [ dp . Text ( \"### \" + str ( remove_u_score ( i ))), dp . DataTable ( pd . read_csv ( ends_with ( master_path ) + str ( i ) + \".csv\" ) . round ( 3 ) ), \"outlier_charts_placeholder\" , ] ) else : df_list . append ( [ dp . Text ( \"### \" + str ( remove_u_score ( i ))), dp . DataTable ( pd . read_csv ( ends_with ( master_path ) + str ( i ) + \".csv\" ) . round ( 3 ) ), dp . Text ( \"#\" ), dp . Text ( \"#\" ), ] ) elif tab_name == \"association_evaluator\" : for j in avl_recs_tab : if j == \"correlation_matrix\" : df_list_ = pd . read_csv ( ends_with ( master_path ) + str ( j ) + \".csv\" ) . round ( 3 ) feats_order = list ( df_list_ [ \"attribute\" ] . values ) df_list_ = df_list_ . round ( 3 ) fig = px . imshow ( df_list_ [ feats_order ], y = feats_order , color_continuous_scale = global_theme , aspect = \"auto\" , ) fig . layout . plot_bgcolor = global_plot_bg_color fig . layout . paper_bgcolor = global_paper_bg_color # fig.update_layout(title_text=str(\"Correlation Plot \")) df_plot_list . append ( dp . Group ( dp . Text ( \"##\" ), dp . DataTable ( df_list_ [[ \"attribute\" ] + feats_order ]), dp . Plot ( fig ), rows = 3 , label = remove_u_score ( j ), ) ) elif j == \"variable_clustering\" : df_list_ = ( pd . read_csv ( ends_with ( master_path ) + str ( j ) + \".csv\" ) . round ( 3 ) . sort_values ( by = [ \"Cluster\" ], ascending = True ) ) fig = px . sunburst ( df_list_ , path = [ \"Cluster\" , \"Attribute\" ], values = \"RS_Ratio\" , color_discrete_sequence = global_theme , ) # fig.update_layout(title_text=str(\"Distribution of homogenous variable across Clusters\")) fig . layout . plot_bgcolor = global_plot_bg_color fig . layout . paper_bgcolor = global_paper_bg_color # fig.update_layout(title_text=str(\"Variable Clustering Plot \")) fig . layout . autosize = True df_plot_list . append ( dp . Group ( dp . Text ( \"##\" ), dp . DataTable ( df_list_ ), dp . Plot ( fig ), rows = 3 , label = remove_u_score ( j ), ) ) else : try : df_list_ = pd . read_csv ( ends_with ( master_path ) + str ( j ) + \".csv\" ) . round ( 3 ) col_nm = [ x for x in list ( df_list_ . columns ) if \"attribute\" not in x ] df_list_ = df_list_ . sort_values ( col_nm [ 0 ], ascending = True ) fig = px . bar ( df_list_ , x = col_nm [ 0 ], y = \"attribute\" , orientation = \"h\" , color_discrete_sequence = global_theme , ) fig . layout . plot_bgcolor = global_plot_bg_color fig . layout . paper_bgcolor = global_paper_bg_color # fig.update_layout(title_text=str(\"Representation of \" + str(remove_u_score(j)))) fig . layout . autosize = True df_plot_list . append ( dp . Group ( dp . Text ( \"##\" ), dp . DataTable ( df_list_ ), dp . Plot ( fig ), label = remove_u_score ( j ), rows = 3 , ) ) except Exception as e : logger . error ( f \"processing failed, error { e } \" ) pass if len ( avl_recs_tab ) == 1 : df_plot_list . append ( dp . Group ( dp . DataTable ( pd . DataFrame ( columns = [ \" \" ], index = range ( 1 )), label = \" \" ), dp . Plot ( blank_chart , label = \" \" ), label = \" \" , ) ) else : pass return df_plot_list else : df_list . append ( dp . DataTable ( pd . read_csv ( ends_with ( master_path ) + str ( i ) + \".csv\" ) . round ( 3 ), label = remove_u_score ( avl_recs_tab [ index ]), ) ) if tab_name == \"quality_checker\" and len ( avl_recs_tab ) == 1 : return df_list [ 0 ], [ dp . Text ( \"#\" ), dp . Plot ( blank_chart )] elif tab_name == \"stats_generator\" and len ( avl_recs_tab ) == 1 : return [ df_list [ 0 ], dp . DataTable ( pd . DataFrame ( columns = [ \" \" ], index = range ( 1 )), label = \" \" ), ] else : return df_list def drift_stability_ind ( missing_recs_drift , drift_tab , missing_recs_stability , stability_tab ): \"\"\" This function helps to produce the drift & stability indicator for further processing. Ideally a data with both drift & stability should produce a list of [1,1] Parameters ---------- missing_recs_drift Missing files from the drift tab drift_tab \"drift_statistics\" missing_recs_stability Missing files from the stability tab stability_tab \"stability_index, stabilityIndex_metrics\" Returns ------- List \"\"\" if len ( missing_recs_drift ) == len ( drift_tab ): drift_ind = 0 else : drift_ind = 1 if len ( missing_recs_stability ) == len ( stability_tab ): stability_ind = 0 elif ( \"stabilityIndex_metrics\" in missing_recs_stability ) and ( \"stability_index\" not in missing_recs_stability ): stability_ind = 0.5 else : stability_ind = 1 return drift_ind , stability_ind def chart_gen_list ( master_path , chart_type , type_col = None ): \"\"\" This function helps to produce the charts in a list object form nested by a datapane object. Parameters ---------- master_path Path containing all the charts same as the other files from data analyzed output chart_type Files containing only the specific chart names for the specific chart category type_col None. Default value is kept as None Returns ------- DatapaneObject \"\"\" plot_list = [] for i in chart_type : col_name = i [ i . find ( \"_\" ) + 1 :] if type_col == \"numerical\" : if col_name in numcols_name . replace ( \" \" , \"\" ) . split ( \",\" ): plot_list . append ( dp . Plot ( go . Figure ( json . load ( open ( ends_with ( master_path ) + i ))), label = col_name , ) ) else : pass elif type_col == \"categorical\" : if col_name in catcols_name . replace ( \" \" , \"\" ) . split ( \",\" ): plot_list . append ( dp . Plot ( go . Figure ( json . load ( open ( ends_with ( master_path ) + i ))), label = col_name , ) ) else : pass else : plot_list . append ( dp . Plot ( go . Figure ( json . load ( open ( ends_with ( master_path ) + i ))), label = col_name , ) ) return plot_list def executive_summary_gen ( master_path , label_col , ds_ind , id_col , iv_threshold , corr_threshold , print_report = False , ): \"\"\" This function helps to produce output specific to the Executive Summary Tab. Parameters ---------- master_path Path containing the input files. label_col Label column. ds_ind Drift stability indicator in list form. id_col ID column. iv_threshold IV threshold beyond which attributes can be called as significant. corr_threshold Correlation threshold beyond which attributes can be categorized under correlated. print_report Printing option flexibility. Default value is kept as False. Returns ------- DatapaneObject / Output[HTML] \"\"\" try : obj_dtls = json . load ( open ( ends_with ( master_path ) + \"freqDist_\" + str ( label_col )) ) # @FIXME: never used local variable text_val = list ( list ( obj_dtls . values ())[ 0 ][ 0 ] . items ())[ 8 ][ 1 ] x_val = list ( list ( obj_dtls . values ())[ 0 ][ 0 ] . items ())[ 11 ][ 1 ] y_val = list ( list ( obj_dtls . values ())[ 0 ][ 0 ] . items ())[ 13 ][ 1 ] label_fig_ = go . Figure ( data = [ go . Pie ( labels = x_val , values = y_val , textinfo = \"label+percent\" , insidetextorientation = \"radial\" , pull = [ 0 , 0.1 ], marker_colors = global_theme , ) ] ) label_fig_ . update_traces ( textposition = \"inside\" , textinfo = \"percent+label\" ) label_fig_ . update_layout ( legend = dict ( orientation = \"h\" , x = 0.5 , yanchor = \"bottom\" , xanchor = \"center\" ) ) label_fig_ . layout . plot_bgcolor = global_plot_bg_color label_fig_ . layout . paper_bgcolor = global_paper_bg_color except Exception as e : logger . error ( f \"processing failed, error { e } \" ) label_fig_ = None a1 = ( \"The dataset contains **\" + str ( f \" { rows_count : ,d } \" ) + \"** records and **\" + str ( numcols_count + catcols_count ) + \"** attributes (**\" + str ( numcols_count ) + \"** numerical + **\" + str ( catcols_count ) + \"** categorical).\" ) if label_col is None : a2 = dp . Group ( dp . Text ( \"- There is **no** target variable in the dataset\" ), dp . Text ( \"- Data Diagnosis:\" ), rows = 2 , ) else : if label_fig_ is None : a2 = dp . Group ( dp . Text ( \"- Target variable is **\" + str ( label_col ) + \"** \" ), dp . Text ( \"- Data Diagnosis:\" ), rows = 2 , ) else : a2 = dp . Group ( dp . Text ( \"- Target variable is **\" + str ( label_col ) + \"** \" ), dp . Plot ( label_fig_ ), dp . Text ( \"- Data Diagnosis:\" ), rows = 3 , ) try : x1 = list ( pd . read_csv ( ends_with ( master_path ) + \"measures_of_dispersion.csv\" ) . query ( \"`cov`>1\" ) . attribute . values ) if len ( x1 ) > 0 : x1_1 = [ \"High Variance\" , x1 ] else : x1_1 = [ \"High Variance\" , None ] except Exception as e : logger . error ( f \"processing failed, error { e } \" ) x1_1 = [ \"High Variance\" , None ] try : x2 = list ( pd . read_csv ( ends_with ( master_path ) + \"measures_of_shape.csv\" ) . query ( \"`skewness`>0\" ) . attribute . values ) if len ( x2 ) > 0 : x2_1 = [ \"Positive Skewness\" , x2 ] else : x2_1 = [ \"Positive Skewness\" , None ] except Exception as e : logger . error ( f \"processing failed, error { e } \" ) x2_1 = [ \"Positive Skewness\" , None ] try : x3 = list ( pd . read_csv ( ends_with ( master_path ) + \"measures_of_shape.csv\" ) . query ( \"`skewness`<0\" ) . attribute . values ) if len ( x3 ) > 0 : x3_1 = [ \"Negative Skewness\" , x3 ] else : x3_1 = [ \"Negative Skewness\" , None ] except Exception as e : logger . error ( f \"processing failed, error { e } \" ) x3_1 = [ \"Negative Skewness\" , None ] try : x4 = list ( pd . read_csv ( ends_with ( master_path ) + \"measures_of_shape.csv\" ) . query ( \"`kurtosis`>0\" ) . attribute . values ) if len ( x4 ) > 0 : x4_1 = [ \"High Kurtosis\" , x4 ] else : x4_1 = [ \"High Kurtosis\" , None ] except Exception as e : logger . error ( f \"processing failed, error { e } \" ) x4_1 = [ \"High Kurtosis\" , None ] try : x5 = list ( pd . read_csv ( ends_with ( master_path ) + \"measures_of_shape.csv\" ) . query ( \"`kurtosis`<0\" ) . attribute . values ) if len ( x5 ) > 0 : x5_1 = [ \"Low Kurtosis\" , x5 ] else : x5_1 = [ \"Low Kurtosis\" , None ] except Exception as e : logger . error ( f \"processing failed, error { e } \" ) x5_1 = [ \"Low Kurtosis\" , None ] try : x6 = list ( pd . read_csv ( ends_with ( master_path ) + \"measures_of_counts.csv\" ) . query ( \"`fill_pct`<0.7\" ) . attribute . values ) if len ( x6 ) > 0 : x6_1 = [ \"Low Fill Rates\" , x6 ] else : x6_1 = [ \"Low Fill Rates\" , None ] except Exception as e : logger . error ( f \"processing failed, error { e } \" ) x6_1 = [ \"Low Fill Rates\" , None ] try : biasedness_df = pd . read_csv ( ends_with ( master_path ) + \"biasedness_detection.csv\" ) if \"treated\" in biasedness_df : x7 = list ( biasedness_df . query ( \"`treated`>0\" ) . attribute . values ) else : x7 = list ( biasedness_df . query ( \"`flagged`>0\" ) . attribute . values ) if len ( x7 ) > 0 : x7_1 = [ \"High Biasedness\" , x7 ] else : x7_1 = [ \"High Biasedness\" , None ] except Exception as e : logger . error ( f \"processing failed, error { e } \" ) x7_1 = [ \"High Biasedness\" , None ] try : x8 = list ( pd . read_csv ( ends_with ( master_path ) + \"outlier_detection.csv\" ) . attribute . values ) if len ( x8 ) > 0 : x8_1 = [ \"Outliers\" , x8 ] else : x8_1 = [ \"Outliers\" , None ] except Exception as e : logger . error ( f \"processing failed, error { e } \" ) x8_1 = [ \"Outliers\" , None ] try : corr_matrx = pd . read_csv ( ends_with ( master_path ) + \"correlation_matrix.csv\" ) corr_matrx = corr_matrx [ list ( corr_matrx . attribute . values )] corr_matrx = corr_matrx . where ( np . triu ( np . ones ( corr_matrx . shape ), k = 1 ) . astype ( np . bool ) ) to_drop = [ column for column in corr_matrx . columns if any ( corr_matrx [ column ] > corr_threshold ) ] if len ( to_drop ) > 0 : x9_1 = [ \"High Correlation\" , to_drop ] else : x9_1 = [ \"High Correlation\" , None ] except Exception as e : logger . error ( f \"processing failed, error { e } \" ) x9_1 = [ \"High Correlation\" , None ] try : x10 = list ( pd . read_csv ( ends_with ( master_path ) + \"IV_calculation.csv\" ) . query ( \"`iv`>\" + str ( iv_threshold )) . attribute . values ) if len ( x10 ) > 0 : x10_1 = [ \"Significant Attributes\" , x10 ] else : x10_1 = [ \"Significant Attributes\" , None ] except Exception as e : logger . error ( f \"processing failed, error { e } \" ) x10_1 = [ \"Significant Attributes\" , None ] blank_list_df = [] for i in [ x1_1 , x2_1 , x3_1 , x4_1 , x5_1 , x6_1 , x7_1 , x8_1 , x9_1 , x10_1 ]: try : for j in i [ 1 ]: blank_list_df . append ([ i [ 0 ], j ]) except Exception as e : logger . error ( f \"processing failed, error { e } \" ) blank_list_df . append ([ i [ 0 ], \"NA\" ]) list_n = [] x1 = pd . DataFrame ( blank_list_df , columns = [ \"Metric\" , \"Attribute\" ]) x1 [ \"Value\" ] = \"\u2714\" all_cols = ( catcols_name . replace ( \" \" , \"\" ) + \",\" + numcols_name . replace ( \" \" , \"\" ) ) . split ( \",\" ) remainder_cols = list ( set ( all_cols ) - set ( x1 . Attribute . values )) total_metrics = set ( list ( x1 . Metric . values )) for i in remainder_cols : for j in total_metrics : list_n . append ([ j , i ]) x2 = pd . DataFrame ( list_n , columns = [ \"Metric\" , \"Attribute\" ]) x2 [ \"Value\" ] = \"\u2718\" x = x1 . append ( x2 , ignore_index = True ) x = ( x . drop_duplicates () . pivot ( index = \"Attribute\" , columns = \"Metric\" , values = \"Value\" ) . fillna ( \"\u2718\" ) . reset_index ()[ [ \"Attribute\" , \"Outliers\" , \"Significant Attributes\" , \"Positive Skewness\" , \"Negative Skewness\" , \"High Variance\" , \"High Correlation\" , \"High Kurtosis\" , \"Low Kurtosis\" , ] ] ) x = x [ x . Attribute . values != \"NA\" ] if ds_ind [ 0 ] == 1 and ds_ind [ 1 ] >= 0.5 : a5 = \"Data Health based on Drift Metrics & Stability Index : \" report = dp . Group ( dp . Text ( \"# \" ), dp . Text ( \"**Key Report Highlights**\" ), dp . Text ( \"- \" + a1 ), a2 , dp . DataTable ( x ), dp . Text ( \"- \" + a5 ), dp . Group ( dp . BigNumber ( heading = \"# Drifted Attributes\" , value = str ( str ( drifted_feats ) + \" out of \" + str ( len_feats )), ), dp . BigNumber ( heading = \"% Drifted Attributes\" , value = str ( np . round (( 100 * drifted_feats / len_feats ), 2 )) + \"%\" , ), dp . BigNumber ( heading = \"# Unstable Attributes\" , value = str ( len ( unstable_attr )) + \" out of \" + str ( len ( total_unstable_attr )), change = \"numerical\" , is_upward_change = True , ), dp . BigNumber ( heading = \"% Unstable Attributes\" , value = str ( np . round ( 100 * len ( unstable_attr ) / len ( total_unstable_attr ), 2 ) ) + \"%\" , ), columns = 4 , ), dp . Text ( \"# \" ), dp . Text ( \"# \" ), label = \"Executive Summary\" , ) if ds_ind [ 0 ] == 0 and ds_ind [ 1 ] >= 0.5 : a5 = \"Data Health based on Stability Index : \" report = dp . Group ( dp . Text ( \"# \" ), dp . Text ( \"**Key Report Highlights**\" ), dp . Text ( \"# \" ), dp . Text ( \"- \" + a1 ), a2 , dp . DataTable ( x ), dp . Text ( \"- \" + a5 ), dp . Group ( dp . BigNumber ( heading = \"# Unstable Attributes\" , value = str ( len ( unstable_attr )) + \" out of \" + str ( len ( total_unstable_attr )), change = \"numerical\" , is_upward_change = True , ), dp . BigNumber ( heading = \"% Unstable Attributes\" , value = str ( np . round ( 100 * len ( unstable_attr ) / len ( total_unstable_attr ), 2 ) ) + \"%\" , ), columns = 2 , ), dp . Text ( \"# \" ), dp . Text ( \"# \" ), label = \"Executive Summary\" , ) if ds_ind [ 0 ] == 1 and ds_ind [ 1 ] == 0 : a5 = \"Data Health based on Drift Metrics : \" report = dp . Group ( dp . Text ( \"# \" ), dp . Text ( \"**Key Report Highlights**\" ), dp . Text ( \"# \" ), dp . Text ( \"- \" + a1 ), a2 , dp . DataTable ( x ), dp . Text ( \"- \" + a5 ), dp . Group ( dp . BigNumber ( heading = \"# Drifted Attributes\" , value = str ( str ( drifted_feats ) + \" out of \" + str ( len_feats )), ), dp . BigNumber ( heading = \"% Drifted Attributes\" , value = str ( np . round (( 100 * drifted_feats / len_feats ), 2 )) + \"%\" , ), columns = 2 , ), dp . Text ( \"# \" ), dp . Text ( \"# \" ), label = \"Executive Summary\" , ) if ds_ind [ 0 ] == 0 and ds_ind [ 1 ] == 0 : report = dp . Group ( dp . Text ( \"# \" ), dp . Text ( \"**Key Report Highlights**\" ), dp . Text ( \"# \" ), dp . Text ( \"- \" + a1 ), a2 , dp . DataTable ( x ), dp . Text ( \"# \" ), label = \"Executive Summary\" , ) if print_report : dp . Report ( default_template [ 0 ], default_template [ 1 ], report ) . save ( ends_with ( master_path ) + \"executive_summary.html\" , open = True ) return report # @FIXME: rename variables with their corresponding within the config files def wiki_generator ( master_path , dataDict_path = None , metricDict_path = None , print_report = False ): \"\"\" This function helps to produce output specific to the Wiki Tab. Parameters ---------- master_path Path containing the input files. dataDict_path Data dictionary path. Default value is kept as None. metricDict_path Metric dictionary path. Default value is kept as None. print_report Printing option flexibility. Default value is kept as False. Returns ------- DatapaneObject / Output[HTML] \"\"\" try : datatype_df = pd . read_csv ( ends_with ( master_path ) + \"data_type.csv\" ) except FileNotFoundError : logger . error ( f \"file { master_path } /data_type.csv doesn't exist, cannot read datatypes\" ) except Exception : logger . info ( \"generate an empty dataframe with columns attribute and data_type \" ) datatype_df = pd . DataFrame ( columns = [ \"attribute\" , \"data_type\" ], index = range ( 1 )) try : data_dict = pd . read_csv ( dataDict_path ) . merge ( datatype_df , how = \"outer\" , on = \"attribute\" ) except FileNotFoundError : logger . error ( f \"file { dataDict_path } doesn't exist, cannot read data dict\" ) except Exception : data_dict = datatype_df try : metric_dict = pd . read_csv ( metricDict_path ) except FileNotFoundError : logger . error ( f \"file { metricDict_path } doesn't exist, cannot read metrics dict\" ) except Exception : metric_dict = pd . DataFrame ( columns = [ \"Section Category\" , \"Section Name\" , \"Metric Name\" , \"Metric Definitions\" , ], index = range ( 1 ), ) report = dp . Group ( dp . Text ( \"# \" ), dp . Text ( \"\"\" *A quick reference to the attributes from the dataset (Data Dictionary) and the metrics computed in the report (Metric Dictionary).* \"\"\" ), dp . Text ( \"# \" ), dp . Text ( \"# \" ), dp . Select ( blocks = [ dp . Group ( dp . Group ( dp . Text ( \"## \" ), dp . DataTable ( data_dict )), label = \"Data Dictionary\" , ), dp . Group ( dp . Text ( \"##\" ), dp . DataTable ( metric_dict ), label = \"Metric Dictionary\" ), ], type = dp . SelectType . TABS , ), dp . Text ( \"# \" ), dp . Text ( \"# \" ), dp . Text ( \"# \" ), dp . Text ( \"# \" ), label = \"Wiki\" , ) if print_report : dp . Report ( default_template [ 0 ], default_template [ 1 ], report ) . save ( ends_with ( master_path ) + \"wiki_generator.html\" , open = True ) return report def descriptive_statistics ( master_path , SG_tabs , avl_recs_SG , missing_recs_SG , all_charts_num_1_ , all_charts_cat_1_ , print_report = False , ): \"\"\" This function helps to produce output specific to the Descriptive Stats Tab. Parameters ---------- master_path Path containing the input files. SG_tabs measures_of_counts','measures_of_centralTendency','measures_of_cardinality','measures_of_percentiles','measures_of_dispersion','measures_of_shape','global_summary' avl_recs_SG Available files from the SG_tabs (Stats Generator tabs) missing_recs_SG Missing files from the SG_tabs (Stats Generator tabs) all_charts_num_1_ Numerical charts (histogram) all collated in a list format supported as per datapane objects all_charts_cat_1_ Categorical charts (barplot) all collated in a list format supported as per datapane objects print_report Printing option flexibility. Default value is kept as False. Returns ------- DatapaneObject / Output[HTML] \"\"\" if \"global_summary\" in avl_recs_SG : cnt = 0 else : cnt = 1 if len ( missing_recs_SG ) + cnt == len ( SG_tabs ): return \"null_report\" else : if \"global_summary\" in avl_recs_SG : l1 = dp . Group ( dp . Text ( \"# \" ), dp . Text ( \"*This section summarizes the dataset with key statistical metrics and distribution plots.*\" ), dp . Text ( \"# \" ), dp . Text ( \"# \" ), dp . Text ( \"### Global Summary\" ), dp . Group ( dp . Text ( \" Total Number of Records: **\" + f \" { rows_count : , } \" + \"**\" ), dp . Text ( \" Total Number of Attributes: **\" + str ( columns_count ) + \"**\" ), dp . Text ( \" Number of Numerical Attributes : **\" + str ( numcols_count ) + \"**\" ), dp . Text ( \" Numerical Attributes Name : **\" + str ( numcols_name ) + \"**\" ), dp . Text ( \" Number of Categorical Attributes : **\" + str ( catcols_count ) + \"**\" ), dp . Text ( \" Categorical Attributes Name : **\" + str ( catcols_name ) + \"**\" ), rows = 6 , ), rows = 8 , ) else : l1 = dp . Text ( \"# \" ) if len ( data_analyzer_output ( master_path , avl_recs_SG , \"stats_generator\" )) > 0 : l2 = dp . Text ( \"### Statistics by Metric Type\" ) l3 = dp . Group ( dp . Select ( blocks = data_analyzer_output ( master_path , avl_recs_SG , \"stats_generator\" ), type = dp . SelectType . TABS , ), dp . Text ( \"# \" ), ) else : l2 = dp . Text ( \"# \" ) l3 = dp . Text ( \"# \" ) if len ( all_charts_num_1_ ) == 0 and len ( all_charts_cat_1_ ) == 0 : l4 = 1 elif len ( all_charts_num_1_ ) == 0 and len ( all_charts_cat_1_ ) > 0 : l4 = ( dp . Text ( \"# \" ), dp . Text ( \"### Attribute Visualization\" ), dp . Select ( blocks = all_charts_cat_1_ , type = dp . SelectType . DROPDOWN ), dp . Text ( \"# \" ), dp . Text ( \"# \" ), ) elif len ( all_charts_num_1_ ) > 0 and len ( all_charts_cat_1_ ) == 0 : l4 = ( dp . Text ( \"# \" ), dp . Text ( \"### Attribute Visualization\" ), dp . Select ( blocks = all_charts_num_1_ , type = dp . SelectType . DROPDOWN ), dp . Text ( \"# \" ), dp . Text ( \"# \" ), ) else : l4 = ( dp . Text ( \"# \" ), dp . Text ( \"### Attribute Visualization\" ), dp . Group ( dp . Select ( blocks = [ dp . Group ( dp . Select ( blocks = all_charts_num_1_ , type = dp . SelectType . DROPDOWN , ), label = \"Numerical\" , ), dp . Group ( dp . Select ( blocks = all_charts_cat_1_ , type = dp . SelectType . DROPDOWN , ), label = \"Categorical\" , ), ], type = dp . SelectType . TABS , ) ), dp . Text ( \"# \" ), dp . Text ( \"# \" ), ) if l4 == 1 : report = dp . Group ( l1 , dp . Text ( \"# \" ), l2 , l3 , dp . Text ( \"# \" ), dp . Text ( \"# \" ), label = \"Descriptive Statistics\" , ) else : report = dp . Group ( l1 , dp . Text ( \"# \" ), l2 , l3 , * l4 , dp . Text ( \"# \" ), dp . Text ( \"# \" ), label = \"Descriptive Statistics\" , ) if print_report : dp . Report ( default_template [ 0 ], default_template [ 1 ], report ) . save ( ends_with ( master_path ) + \"descriptive_statistics.html\" , open = True ) return report def quality_check ( master_path , QC_tabs , avl_recs_QC , missing_recs_QC , all_charts_num_3_ , print_report = False , ): \"\"\" This function helps to produce output specific to the Quality Checker Tab. Parameters ---------- master_path Path containing the input files. QC_tabs nullColumns_detection','IDness_detection','biasedness_detection','invalidEntries_detection','duplicate_detection','nullRows_detection','outlier_detection' avl_recs_QC Available files from the QC_tabs (Quality Checker tabs) missing_recs_QC Missing files from the QC_tabs (Quality Checker tabs) all_charts_num_3_ Numerical charts (outlier charts) all collated in a list format supported as per datapane objects print_report Printing option flexibility. Default value is kept as False. Returns ------- DatapaneObject / Output[HTML] \"\"\" c_ = [] r_ = [] if len ( missing_recs_QC ) == len ( QC_tabs ): return \"null_report\" else : row_wise = [ \"duplicate_detection\" , \"nullRows_detection\" ] col_wise = [ \"nullColumns_detection\" , \"IDness_detection\" , \"biasedness_detection\" , \"invalidEntries_detection\" , \"outlier_detection\" , ] row_wise_ = [ p for p in row_wise if p in avl_recs_QC ] col_wise_ = [ p for p in col_wise if p in avl_recs_QC ] len_row_wise = len ([ p for p in row_wise if p in avl_recs_QC ]) len_col_wise = len ([ p for p in col_wise if p in avl_recs_QC ]) if len_row_wise == 0 : c = data_analyzer_output ( master_path , col_wise_ , \"quality_checker\" ) for i in c : for j in i : if j == \"outlier_charts_placeholder\" and len ( all_charts_num_3_ ) > 1 : c_ . append ( dp . Select ( blocks = all_charts_num_3_ , type = dp . SelectType . DROPDOWN ) ) elif ( j == \"outlier_charts_placeholder\" and len ( all_charts_num_3_ ) == 0 ): c_ . append ( dp . Plot ( blank_chart )) else : c_ . append ( j ) report = dp . Group ( dp . Text ( \"# \" ), dp . Text ( \"*This section identifies the data quality issues at both row and column level.*\" ), dp . Text ( \"# \" ), dp . Text ( \"# \" ), dp . Group ( * c_ ), dp . Text ( \"# \" ), dp . Text ( \"# \" ), rows = 8 , label = \"Quality Check\" , ) elif len_col_wise == 0 : r = data_analyzer_output ( master_path , row_wise_ , \"quality_checker\" ) for i in r : for j in i : r_ . append ( j ) report = dp . Group ( dp . Text ( \"# \" ), dp . Text ( \"*This section identifies the data quality issues at both row and column level.*\" ), dp . Text ( \"# \" ), dp . Text ( \"# \" ), dp . Group ( * r_ ), dp . Text ( \"# \" ), dp . Text ( \"# \" ), rows = 8 , label = \"Quality Check\" , ) else : c = data_analyzer_output ( master_path , col_wise_ , \"quality_checker\" ) for i in c : for j in i : if j == \"outlier_charts_placeholder\" and len ( all_charts_num_3_ ) > 1 : c_ . append ( dp . Select ( blocks = all_charts_num_3_ , type = dp . SelectType . DROPDOWN ) ) elif ( j == \"outlier_charts_placeholder\" and len ( all_charts_num_3_ ) == 0 ): c_ . append ( dp . Plot ( blank_chart )) else : c_ . append ( j ) r = data_analyzer_output ( master_path , row_wise_ , \"quality_checker\" ) for i in r : for j in i : r_ . append ( j ) report = dp . Group ( dp . Text ( \"# \" ), dp . Text ( \"*This section identifies the data quality issues at both row and column level.*\" ), dp . Text ( \"# \" ), dp . Text ( \"# \" ), dp . Select ( blocks = [ dp . Group ( dp . Text ( \"# \" ), dp . Group ( * c_ ), rows = 2 , label = \"Column Level\" ), dp . Group ( dp . Text ( \"# \" ), dp . Group ( * r_ ), rows = 2 , label = \"Row Level\" ), ], type = dp . SelectType . TABS , ), dp . Text ( \"# \" ), dp . Text ( \"# \" ), label = \"Quality Check\" , ) if print_report : dp . Report ( default_template [ 0 ], default_template [ 1 ], report ) . save ( ends_with ( master_path ) + \"quality_check.html\" , open = True ) return report def attribute_associations ( master_path , AE_tabs , avl_recs_AE , missing_recs_AE , label_col , all_charts_num_2_ , all_charts_cat_2_ , print_report = False , ): \"\"\" This function helps to produce output specific to the Attribute Association Tab. Parameters ---------- master_path Path containing the input files. AE_tabs correlation_matrix','IV_calculation','IG_calculation','variable_clustering' avl_recs_AE Available files from the AE_tabs (Association Evaluator tabs) missing_recs_AE Missing files from the AE_tabs (Association Evaluator tabs) label_col label column all_charts_num_2_ Numerical charts (histogram) all collated in a list format supported as per datapane objects all_charts_cat_2_ Categorical charts (barplot) all collated in a list format supported as per datapane objects print_report Printing option flexibility. Default value is kept as False. Returns ------- DatapaneObject / Output[HTML] \"\"\" if ( len ( missing_recs_AE ) == len ( AE_tabs )) and ( ( len ( all_charts_num_2_ ) + len ( all_charts_cat_2_ )) == 0 ): return \"null_report\" else : if len ( all_charts_num_2_ ) == 0 and len ( all_charts_cat_2_ ) == 0 : target_association_rep = dp . Text ( \"##\" ) else : if len ( all_charts_num_2_ ) > 0 and len ( all_charts_cat_2_ ) == 0 : target_association_rep = dp . Group ( dp . Text ( \"### Attribute to Target Association\" ), dp . Text ( \"\"\" *Bivariate Distribution considering the event captured across different attribute splits (or categories)* \"\"\" ), dp . Select ( blocks = all_charts_num_2_ , type = dp . SelectType . DROPDOWN ), label = \"Numerical\" , ) elif len ( all_charts_num_2_ ) == 0 and len ( all_charts_cat_2_ ) > 0 : target_association_rep = dp . Group ( dp . Text ( \"### Attribute to Target Association\" ), dp . Text ( \"\"\" *Bivariate Distribution considering the event captured across different attribute splits (or categories)* \"\"\" ), dp . Select ( blocks = all_charts_cat_2_ , type = dp . SelectType . DROPDOWN ), label = \"Categorical\" , ) else : target_association_rep = dp . Group ( dp . Text ( \"### Attribute to Target Association\" ), dp . Select ( blocks = [ dp . Group ( dp . Select ( blocks = all_charts_num_2_ , type = dp . SelectType . DROPDOWN , ), label = \"Numerical\" , ), dp . Group ( dp . Select ( blocks = all_charts_cat_2_ , type = dp . SelectType . DROPDOWN , ), label = \"Categorical\" , ), ], type = dp . SelectType . TABS , ), dp . Text ( \"\"\" *Event Rate is defined as % of event label (i.e. label 1) in a bin or a categorical value of an attribute.* \"\"\" ), dp . Text ( \"# \" ), ) if len ( missing_recs_AE ) == len ( AE_tabs ): report = dp . Group ( dp . Text ( \"# \" ), dp . Text ( \"\"\" *This section analyzes the interaction between different attributes and/or the relationship between an attribute & the binary target variable.* \"\"\" ), dp . Text ( \"## \" ), target_association_rep , dp . Text ( \"## \" ), dp . Text ( \"## \" ), label = \"Attribute Associations\" , ) else : report = dp . Group ( dp . Text ( \"# \" ), dp . Text ( \"\"\" *This section analyzes the interaction between different attributes and/or the relationship between an attribute & the binary target variable.* \"\"\" ), dp . Text ( \"# \" ), dp . Text ( \"# \" ), dp . Text ( \"### Association Matrix & Plot\" ), dp . Select ( blocks = data_analyzer_output ( master_path , avl_recs_AE , tab_name = \"association_evaluator\" ), type = dp . SelectType . DROPDOWN , ), dp . Text ( \"### \" ), dp . Text ( \"## \" ), target_association_rep , dp . Text ( \"## \" ), dp . Text ( \"## \" ), label = \"Attribute Associations\" , ) if print_report : dp . Report ( default_template [ 0 ], default_template [ 1 ], report ) . save ( ends_with ( master_path ) + \"attribute_associations.html\" , open = True ) return report def data_drift_stability ( master_path , ds_ind , id_col , drift_threshold_model , all_drift_charts_ , print_report = False , ): \"\"\" This function helps to produce output specific to the Data Drift & Stability Tab. Parameters ---------- master_path Path containing the input files. ds_ind Drift stability indicator in list form. id_col ID column drift_threshold_model threshold which the user is specifying for tagging an attribute to be drifted or not all_drift_charts_ Charts (histogram/barplot) all collated in a list format supported as per datapane objects print_report Printing option flexibility. Default value is kept as False. Returns ------- DatapaneObject / Output[HTML] \"\"\" line_chart_list = [] if ds_ind [ 0 ] > 0 : fig_metric_drift = go . Figure () fig_metric_drift . add_trace ( go . Scatter ( x = list ( drift_df [ drift_df . flagged . values == 1 ][ metric_drift [ 0 ]] . values ), y = list ( drift_df [ drift_df . flagged . values == 1 ] . attribute . values ), marker = dict ( color = global_theme [ 1 ], size = 14 ), mode = \"markers\" , name = metric_drift [ 0 ], ) ) fig_metric_drift . add_trace ( go . Scatter ( x = list ( drift_df [ drift_df . flagged . values == 1 ][ metric_drift [ 1 ]] . values ), y = list ( drift_df [ drift_df . flagged . values == 1 ] . attribute . values ), marker = dict ( color = global_theme [ 3 ], size = 14 ), mode = \"markers\" , name = metric_drift [ 1 ], ) ) fig_metric_drift . add_trace ( go . Scatter ( x = list ( drift_df [ drift_df . flagged . values == 1 ][ metric_drift [ 2 ]] . values ), y = list ( drift_df [ drift_df . flagged . values == 1 ] . attribute . values ), marker = dict ( color = global_theme [ 5 ], size = 14 ), mode = \"markers\" , name = metric_drift [ 2 ], ) ) fig_metric_drift . add_trace ( go . Scatter ( x = list ( drift_df [ drift_df . flagged . values == 1 ][ metric_drift [ 3 ]] . values ), y = list ( drift_df [ drift_df . flagged . values == 1 ] . attribute . values ), marker = dict ( color = global_theme [ 7 ], size = 14 ), mode = \"markers\" , name = metric_drift [ 3 ], ) ) fig_metric_drift . add_vrect ( x0 = 0 , x1 = drift_threshold_model , fillcolor = global_theme [ 7 ], opacity = 0.1 , layer = \"below\" , line_width = 1 , ), fig_metric_drift . update_layout ( legend = dict ( orientation = \"h\" , x = 0.5 , yanchor = \"bottom\" , xanchor = \"center\" ) ) fig_metric_drift . layout . plot_bgcolor = global_plot_bg_color fig_metric_drift . layout . paper_bgcolor = global_paper_bg_color fig_metric_drift . update_xaxes ( showline = True , linewidth = 2 , gridcolor = px . colors . sequential . Greys [ 1 ] ) fig_metric_drift . update_yaxes ( showline = True , linewidth = 2 , gridcolor = px . colors . sequential . Greys [ 2 ] ) # Drift Chart - 2 fig_gauge_drift = go . Figure ( go . Indicator ( domain = { \"x\" : [ 0 , 1 ], \"y\" : [ 0 , 1 ]}, value = drifted_feats , mode = \"gauge+number\" , title = { \"text\" : \"\" }, gauge = { \"axis\" : { \"range\" : [ None , len_feats ]}, \"bar\" : { \"color\" : px . colors . sequential . Reds [ 7 ]}, \"steps\" : [ { \"range\" : [ 0 , drifted_feats ], \"color\" : px . colors . sequential . Reds [ 8 ], }, { \"range\" : [ drifted_feats , len_feats ], \"color\" : px . colors . sequential . Greens [ 8 ], }, ], \"threshold\" : { \"line\" : { \"color\" : \"black\" , \"width\" : 3 }, \"thickness\" : 1 , \"value\" : len_feats , }, }, ) ) fig_gauge_drift . update_layout ( font = { \"color\" : \"black\" , \"family\" : \"Arial\" }) def drift_text_gen ( drifted_feats , len_feats ): \"\"\" Parameters ---------- drifted_feats count of attributes drifted len_feats count of attributes passed for analysis Returns ------- String \"\"\" if drifted_feats == 0 : text = \"\"\" *Drift barometer does not indicate any drift in the underlying data. Please refer to the metric values as displayed in the above table & comparison plot for better understanding* \"\"\" elif drifted_feats == 1 : text = ( \"*Drift barometer indicates that \" + str ( drifted_feats ) + \" out of \" + str ( len_feats ) + \" (\" + str ( np . round (( 100 * drifted_feats / len_feats ), 2 )) + \"%) attributes has been drifted from its source behaviour.*\" ) elif drifted_feats > 1 : text = ( \"*Drift barometer indicates that \" + str ( drifted_feats ) + \" out of \" + str ( len_feats ) + \" (\" + str ( np . round (( 100 * drifted_feats / len_feats ), 2 )) + \"%) attributes have been drifted from its source behaviour.*\" ) else : text = \"\" return text else : pass if ds_ind [ 0 ] == 0 and ds_ind [ 1 ] == 0 : return \"null_report\" elif ds_ind [ 0 ] == 0 and ds_ind [ 1 ] > 0.5 : for i in total_unstable_attr : if len ( total_unstable_attr ) > 1 : line_chart_list . append ( line_chart_gen_stability ( df1 = df_stability , df2 = df_si_ , col = i ) ) else : line_chart_list . append ( line_chart_gen_stability ( df1 = df_stability , df2 = df_si_ , col = i ) ) line_chart_list . append ( dp . Plot ( blank_chart , label = \" \" )) report = dp . Group ( dp . Text ( \"# \" ), dp . Text ( \"\"\" *This section examines the dataset stability wrt the baseline dataset (via computing drift statistics) and/or wrt the historical datasets (via computing stability index).* \"\"\" ), dp . Text ( \"# \" ), dp . Text ( \"# \" ), dp . Text ( \"### Data Stability Analysis\" ), dp . DataTable ( df_si ), dp . Select ( blocks = line_chart_list , type = dp . SelectType . DROPDOWN ), dp . Group ( dp . Text ( \"**Stability Index Interpretation:**\" ), dp . Plot ( plot_index_stability ), rows = 2 , ), label = \"Drift & Stability\" , ) elif ds_ind [ 0 ] == 1 and ds_ind [ 1 ] == 0 : if len ( all_drift_charts_ ) > 0 : report = dp . Group ( dp . Text ( \"# \" ), dp . Text ( \"\"\" *This section examines the dataset stability wrt the baseline dataset (via computing drift statistics) and/or wrt the historical datasets (via computing stability index).* \"\"\" ), dp . Text ( \"# \" ), dp . Text ( \"# \" ), dp . Text ( \"### Data Drift Analysis\" ), dp . DataTable ( drift_df ), dp . Text ( \"*An attribute is flagged as drifted if any drift metric is found to be above the threshold of \" + str ( drift_threshold_model ) + \".*\" ), dp . Text ( \"##\" ), dp . Select ( blocks = all_drift_charts_ , type = dp . SelectType . DROPDOWN ), dp . Text ( \"\"\" *Source & Target datasets were compared to see the % deviation at decile level for numerical attributes and at individual category level for categorical attributes* \"\"\" ), dp . Text ( \"### \" ), dp . Text ( \"### \" ), dp . Text ( \"### Data Health\" ), dp . Group ( dp . Plot ( fig_metric_drift ), dp . Plot ( fig_gauge_drift ), columns = 2 ), dp . Group ( dp . Text ( \"*Representation of attributes across different computed Drift Metrics*\" ), dp . Text ( drift_text_gen ( drifted_feats , len_feats )), columns = 2 , ), label = \"Drift & Stability\" , ) else : report = dp . Group ( dp . Text ( \"# \" ), dp . Text ( \"\"\" *This section examines the dataset stability wrt the baseline dataset (via computing drift statistics) and/or wrt the historical datasets (via computing stability index).* \"\"\" ), dp . Text ( \"# \" ), dp . Text ( \"# \" ), dp . Text ( \"### Data Drift Analysis\" ), dp . DataTable ( drift_df ), dp . Text ( \"*An attribute is flagged as drifted if any drift metric is found to be above the threshold of \" + str ( drift_threshold_model ) + \".*\" ), dp . Text ( \"##\" ), dp . Text ( \"### \" ), dp . Text ( \"### Data Health\" ), dp . Group ( dp . Plot ( fig_metric_drift ), dp . Plot ( fig_gauge_drift ), columns = 2 ), dp . Group ( dp . Text ( \"*Representation of attributes across different computed Drift Metrics*\" ), dp . Text ( drift_text_gen ( drifted_feats , len_feats )), columns = 2 , ), label = \"Drift & Stability\" , ) elif ds_ind [ 0 ] == 1 and ds_ind [ 1 ] >= 0.5 : for i in total_unstable_attr : if len ( total_unstable_attr ) > 1 : line_chart_list . append ( line_chart_gen_stability ( df1 = df_stability , df2 = df_si_ , col = i ) ) else : line_chart_list . append ( line_chart_gen_stability ( df1 = df_stability , df2 = df_si_ , col = i ) ) line_chart_list . append ( dp . Plot ( blank_chart , label = \" \" )) if len ( all_drift_charts_ ) > 0 : report = dp . Group ( dp . Text ( \"# \" ), dp . Text ( \"\"\" *This section examines the dataset stability wrt the baseline dataset (via computing drift statistics) and/or wrt the historical datasets (via computing stability index).* \"\"\" ), dp . Text ( \"# \" ), dp . Text ( \"# \" ), dp . Text ( \"### Data Drift Analysis\" ), dp . DataTable ( drift_df ), dp . Text ( \"*An attribute is flagged as drifted if any drift metric is found to be above the threshold of \" + str ( drift_threshold_model ) + \".*\" ), dp . Text ( \"##\" ), dp . Select ( blocks = all_drift_charts_ , type = dp . SelectType . DROPDOWN ), dp . Text ( \"\"\" *Source & Target datasets were compared to see the % deviation at decile level for numerical attributes and at individual category level for categorical attributes* \"\"\" ), dp . Text ( \"### \" ), dp . Text ( \"### \" ), dp . Text ( \"### Data Health\" ), dp . Group ( dp . Plot ( fig_metric_drift ), dp . Plot ( fig_gauge_drift ), columns = 2 ), dp . Group ( dp . Text ( \"*Representation of attributes across different computed Drift Metrics*\" ), dp . Text ( drift_text_gen ( drifted_feats , len_feats )), columns = 2 , ), dp . Text ( \"## \" ), dp . Text ( \"## \" ), dp . Text ( \"### Data Stability Analysis\" ), dp . DataTable ( df_si ), dp . Select ( blocks = line_chart_list , type = dp . SelectType . DROPDOWN ), dp . Group ( dp . Text ( \"**Stability Index Interpretation:**\" ), dp . Plot ( plot_index_stability ), rows = 2 , ), label = \"Drift & Stability\" , ) else : report = dp . Group ( dp . Text ( \"# \" ), dp . Text ( \"\"\" *This section examines the dataset stability wrt the baseline dataset (via computing drift statistics) and/or wrt the historical datasets (via computing stability index).* \"\"\" ), dp . Text ( \"# \" ), dp . Text ( \"# \" ), dp . Text ( \"### Data Drift Analysis\" ), dp . DataTable ( drift_df ), dp . Text ( \"*An attribute is flagged as drifted if any drift metric is found to be above the threshold of \" + str ( drift_threshold_model ) + \".*\" ), dp . Text ( \"##\" ), dp . Text ( \"### Data Health\" ), dp . Group ( dp . Plot ( fig_metric_drift ), dp . Plot ( fig_gauge_drift ), columns = 2 ), dp . Group ( dp . Text ( \"*Representation of attributes across different computed Drift Metrics*\" ), dp . Text ( drift_text_gen ( drifted_feats , len_feats )), columns = 2 , ), dp . Text ( \"## \" ), dp . Text ( \"## \" ), dp . Text ( \"### Data Stability Analysis\" ), dp . DataTable ( df_si ), dp . Select ( blocks = line_chart_list , type = dp . SelectType . DROPDOWN ), dp . Group ( dp . Text ( \"**Stability Index Interpretation:**\" ), dp . Plot ( plot_index_stability ), rows = 2 , ), label = \"Drift & Stability\" , ) elif ds_ind [ 0 ] == 0 and ds_ind [ 1 ] >= 0.5 : for i in total_unstable_attr : if len ( total_unstable_attr ) > 1 : line_chart_list . append ( line_chart_gen_stability ( df1 = df_stability , df2 = df_si_ , col = i ) ) else : line_chart_list . append ( line_chart_gen_stability ( df1 = df_stability , df2 = df_si_ , col = i ) ) line_chart_list . append ( dp . Plot ( blank_chart , label = \" \" )) report = dp . Group ( dp . Text ( \"# \" ), dp . Text ( \"\"\" *This section examines the dataset stability wrt the baseline dataset (via computing drift statistics) and/or wrt the historical datasets (via computing stability index).* \"\"\" ), dp . Text ( \"# \" ), dp . Text ( \"# \" ), dp . Text ( \"### Data Stability Analysis\" ), dp . DataTable ( df_si ), dp . Select ( blocks = line_chart_list , type = dp . SelectType . DROPDOWN ), dp . Group ( dp . Text ( \"**Stability Index Interpretation:**\" ), dp . Plot ( plot_index_stability ), rows = 2 , ), label = \"Drift & Stability\" , ) else : for i in total_unstable_attr : if len ( total_unstable_attr ) > 1 : line_chart_list . append ( line_chart_gen_stability ( df1 = df_stability , df2 = df_si_ , col = i ) ) else : line_chart_list . append ( line_chart_gen_stability ( df1 = df_stability , df2 = df_si_ , col = i ) ) line_chart_list . append ( dp . Plot ( blank_chart , label = \" \" )) if len ( all_drift_charts_ ) > 0 : report = dp . Group ( dp . Text ( \"# \" ), dp . Text ( \"\"\" *This section examines the dataset stability wrt the baseline dataset (via computing drift statistics) and/or wrt the historical datasets (via computing stability index).* \"\"\" ), dp . Text ( \"# \" ), dp . Text ( \"# \" ), dp . Text ( \"### Data Drift Analysis\" ), dp . DataTable ( drift_df ), dp . Text ( \"*An attribute is flagged as drifted if any drift metric is found to be above the threshold of \" + str ( drift_threshold_model ) + \".*\" ), dp . Text ( \"##\" ), dp . Select ( blocks = all_drift_charts_ , type = dp . SelectType . DROPDOWN ), dp . Text ( \"\"\" *Source & Target datasets were compared to see the % deviation at decile level for numerical attributes and at individual category level for categorical attributes* \"\"\" ), dp . Text ( \"### \" ), dp . Text ( \"### \" ), dp . Text ( \"### Data Health\" ), dp . Group ( dp . Plot ( fig_metric_drift ), dp . Plot ( fig_gauge_drift ), columns = 2 ), dp . Group ( dp . Text ( \"*Representation of attributes across different computed Drift Metrics*\" ), dp . Text ( drift_text_gen ( drifted_feats , len_feats )), columns = 2 , ), dp . Text ( \"## \" ), dp . Text ( \"## \" ), dp . Text ( \"### Data Stability Analysis\" ), dp . DataTable ( df_si ), dp . Select ( blocks = line_chart_list , type = dp . SelectType . DROPDOWN ), dp . Group ( dp . Text ( \"**Stability Index Interpretation:**\" ), dp . Plot ( plot_index_stability ), rows = 2 , ), label = \"Drift & Stability\" , ) else : report = dp . Group ( dp . Text ( \"# \" ), dp . Text ( \"\"\" *This section examines the dataset stability wrt the baseline dataset (via computing drift statistics) and/or wrt the historical datasets (via computing stability index).* \"\"\" ), dp . Text ( \"# \" ), dp . Text ( \"# \" ), dp . Text ( \"### Data Drift Analysis\" ), dp . DataTable ( drift_df ), dp . Text ( \"*An attribute is flagged as drifted if any drift metric is found to be above the threshold of \" + str ( drift_threshold_model ) + \".*\" ), dp . Text ( \"##\" ), dp . Text ( \"### Data Health\" ), dp . Group ( dp . Plot ( fig_metric_drift ), dp . Plot ( fig_gauge_drift ), columns = 2 ), dp . Group ( dp . Text ( \"*Representation of attributes across different computed Drift Metrics*\" ), dp . Text ( drift_text_gen ( drifted_feats , len_feats )), columns = 2 , ), dp . Text ( \"## \" ), dp . Text ( \"## \" ), dp . Text ( \"### Data Stability Analysis\" ), dp . DataTable ( df_si ), dp . Select ( blocks = line_chart_list , type = dp . SelectType . DROPDOWN ), dp . Group ( dp . Text ( \"**Stability Index Interpretation:**\" ), dp . Plot ( plot_index_stability ), rows = 2 , ), label = \"Drift & Stability\" , ) if print_report : dp . Report ( default_template [ 0 ], default_template [ 1 ], report ) . save ( ends_with ( master_path ) + \"data_drift_stability.html\" , open = True ) return report def plotSeasonalDecompose ( base_path , x_col , y_col , metric_col = \"median\" , title = \"Seasonal Decomposition\" ): \"\"\" This function helps to produce output specific to the Seasonal Decomposition of Time Series. Ideally it's expected to source a data containing atleast 2 cycles or 24 months as the most. Parameters ---------- base_path Base path which is the same as Master path where the aggregated data resides. x_col Timestamp / date column name y_col Numerical column names metric_col Metric of aggregation. Options can be between \"Median\", \"Mean\", \"Min\", \"Max\" title \"Title Description\" Returns ------- Plot \"\"\" df = pd . read_csv ( ends_with ( base_path ) + x_col + \"_\" + y_col + \"_daily.csv\" ) . dropna () df [ x_col ] = pd . to_datetime ( df [ x_col ], format = \"%Y-%m- %d %H:%M:%S. %f \" ) df = df . set_index ( x_col ) if len ([ x for x in df . columns if \"min\" in x ]) == 0 : # result = seasonal_decompose(df[metric_col],model=\"additive\") pass else : result = seasonal_decompose ( df [ metric_col ], model = \"additive\" , period = 12 ) fig = make_subplots ( rows = 2 , cols = 2 , subplot_titles = [ \"Observed\" , \"Trend\" , \"Seasonal\" , \"Residuals\" ], ) # fig = go.Figure() fig . add_trace ( go . Scatter ( x = df . index , y = result . observed , name = \"Observed\" , mode = \"lines+markers\" , line = dict ( color = global_theme [ 0 ]), ), row = 1 , col = 1 , ) fig . add_trace ( go . Scatter ( x = df . index , y = result . trend , name = \"Trend\" , mode = \"lines+markers\" , line = dict ( color = global_theme [ 2 ]), ), row = 1 , col = 2 , ) fig . add_trace ( go . Scatter ( x = df . index , y = result . seasonal , name = \"Seasonal\" , mode = \"lines+markers\" , line = dict ( color = global_theme [ 4 ]), ), row = 2 , col = 1 , ) fig . add_trace ( go . Scatter ( x = df . index , y = result . resid , name = \"Residuals\" , mode = \"lines+markers\" , line = dict ( color = global_theme [ 6 ]), ), row = 2 , col = 2 , ) # fig.add_trace(go.Scatter(x=df.index, y=result.observed, name =\"Observed\", mode='lines+markers',line=dict(color=global_theme[0]))) # fig.add_trace(go.Scatter(x=df.index, y=result.trend, name =\"Trend\", mode='lines+markers',line=dict(color=global_theme[2]))) # fig.add_trace(go.Scatter(x=df.index, y=result.seasonal, name =\"Seasonal\", mode='lines+markers',line=dict(color=global_theme[4]))) # fig.add_trace(go.Scatter(x=df.index, y=result.resid, name =\"Residuals\", mode='lines+markers',line=dict(color=global_theme[6]))) fig . layout . plot_bgcolor = global_plot_bg_color fig . layout . paper_bgcolor = global_paper_bg_color fig . update_xaxes ( gridcolor = px . colors . sequential . Greys [ 1 ]) fig . update_yaxes ( gridcolor = px . colors . sequential . Greys [ 1 ]) fig . update_layout ( autosize = True , width = 2000 , height = 800 ) fig . update_layout ( legend = dict ( orientation = \"h\" , x = 0.5 , yanchor = \"bottom\" , xanchor = \"center\" ) ) return fig def gen_time_series_plots ( base_path , x_col , y_col , time_cat ): \"\"\" This function helps to produce Time Series Plots by sourcing the aggregated data as Daily/Hourly/Weekly level. Parameters ---------- base_path Base path which is the same as Master path where the aggregated data resides. x_col Timestamp / date column name y_col Numerical column names time_cat Time category of analysis which can be between \"Daily\", \"Hourly\", \"Weekly\" Returns ------- Plot \"\"\" df = pd . read_csv ( ends_with ( base_path ) + x_col + \"_\" + y_col + \"_\" + time_cat + \".csv\" ) . dropna () if len ([ x for x in df . columns if \"min\" in x ]) == 0 : if time_cat == \"daily\" : # x_col = x_col + \"_ts\" fig = px . line ( df , x = x_col , y = \"count\" , color = y_col , color_discrete_sequence = global_theme , ) fig . update_layout ( xaxis = dict ( rangeselector = dict ( buttons = list ( [ dict ( count = 1 , label = \"1m\" , step = \"month\" , stepmode = \"backward\" , ), dict ( count = 3 , label = \"3m\" , step = \"month\" , stepmode = \"backward\" , ), dict ( count = 6 , label = \"6m\" , step = \"month\" , stepmode = \"backward\" , ), dict ( count = 1 , label = \"YTD\" , step = \"year\" , stepmode = \"todate\" ), dict ( count = 1 , label = \"1y\" , step = \"year\" , stepmode = \"backward\" , ), dict ( step = \"all\" ), ] ) ), rangeslider = dict ( visible = True ), type = \"date\" , ) ) elif time_cat == \"weekly\" : fig = px . bar ( df , x = \"dow\" , y = \"count\" , color = y_col , color_discrete_sequence = global_theme , ) # fig.update_layout(barmode='stack') elif time_cat == \"hourly\" : fig = px . bar ( df , x = \"daypart_cat\" , y = \"count\" , color = y_col , color_discrete_sequence = global_theme , ) # fig.update_layout(barmode='stack') else : pass else : if time_cat == \"daily\" : # x_col = x_col + \"_ts\" f1 = go . Scatter ( x = list ( df [ x_col ]), y = list ( df [ \"min\" ]), name = \"Min\" , line = dict ( color = global_theme [ 6 ]), ) f2 = go . Scatter ( x = list ( df [ x_col ]), y = list ( df [ \"max\" ]), name = \"Max\" , line = dict ( color = global_theme [ 4 ]), ) f3 = go . Scatter ( x = list ( df [ x_col ]), y = list ( df [ \"mean\" ]), name = \"Mean\" , line = dict ( color = global_theme [ 2 ]), ) f4 = go . Scatter ( x = list ( df [ x_col ]), y = list ( df [ \"median\" ]), name = \"Median\" , line = dict ( color = global_theme [ 0 ]), ) fig = go . Figure ( data = [ f1 , f2 , f3 , f4 ]) fig . update_layout ( xaxis = dict ( rangeselector = dict ( buttons = list ( [ dict ( count = 1 , label = \"1m\" , step = \"month\" , stepmode = \"backward\" , ), dict ( count = 3 , label = \"3m\" , step = \"month\" , stepmode = \"backward\" , ), dict ( count = 6 , label = \"6m\" , step = \"month\" , stepmode = \"backward\" , ), dict ( count = 1 , label = \"YTD\" , step = \"year\" , stepmode = \"todate\" ), dict ( count = 1 , label = \"1y\" , step = \"year\" , stepmode = \"backward\" , ), dict ( step = \"all\" ), ] ) ), rangeslider = dict ( visible = True ), type = \"date\" , ) ) elif time_cat == \"weekly\" : f1 = go . Bar ( x = list ( df [ \"dow\" ]), y = list ( df [ \"min\" ]), marker_color = global_theme [ 6 ], name = \"Min\" , ) f2 = go . Bar ( x = list ( df [ \"dow\" ]), y = list ( df [ \"max\" ]), marker_color = global_theme [ 4 ], name = \"Max\" , ) f3 = go . Bar ( x = list ( df [ \"dow\" ]), y = list ( df [ \"mean\" ]), marker_color = global_theme [ 2 ], name = \"Mean\" , ) f4 = go . Bar ( x = list ( df [ \"dow\" ]), y = list ( df [ \"median\" ]), marker_color = global_theme [ 0 ], name = \"Median\" , ) fig = go . Figure ( data = [ f1 , f2 , f3 , f4 ]) fig . update_layout ( barmode = \"group\" ) elif time_cat == \"hourly\" : f1 = go . Bar ( x = list ( df [ \"daypart_cat\" ]), y = list ( df [ \"min\" ]), marker_color = global_theme [ 6 ], name = \"Min\" , ) f2 = go . Bar ( x = list ( df [ \"daypart_cat\" ]), y = list ( df [ \"max\" ]), marker_color = global_theme [ 4 ], name = \"Max\" , ) f3 = go . Bar ( x = list ( df [ \"daypart_cat\" ]), y = list ( df [ \"mean\" ]), marker_color = global_theme [ 2 ], name = \"Mean\" , ) f4 = go . Bar ( x = list ( df [ \"daypart_cat\" ]), y = list ( df [ \"median\" ]), marker_color = global_theme [ 0 ], name = \"Median\" , ) fig = go . Figure ( data = [ f1 , f2 , f3 , f4 ]) fig . update_layout ( barmode = \"group\" ) else : pass fig . layout . plot_bgcolor = global_plot_bg_color fig . layout . paper_bgcolor = global_paper_bg_color fig . update_xaxes ( gridcolor = px . colors . sequential . Greys [ 1 ]) fig . update_yaxes ( gridcolor = px . colors . sequential . Greys [ 1 ]) fig . update_layout ( legend = dict ( orientation = \"h\" , x = 0.5 , yanchor = \"bottom\" , xanchor = \"center\" ) ) return fig def list_ts_remove_append ( l , opt ): \"\"\" This function helps to remove or append \"_ts\" from any list. Parameters ---------- l List containing column name opt Option to choose between 1 & Others to enable the functionality of removing or appending \"_ts\" within the elements of a list Returns ------- List \"\"\" ll = [] if opt == 1 : for i in l : if i [ - 3 :] == \"_ts\" : ll . append ( i [ 0 : - 3 :]) else : ll . append ( i ) return ll else : for i in l : if i [ - 3 :] == \"_ts\" : ll . append ( i ) else : ll . append ( i + \"_ts\" ) return ll def ts_viz_1_1 ( base_path , x_col , y_col , output_type ): \"\"\" Parameters ---------- base_path Base path which is the same as Master path where the aggregated data resides. x_col Timestamp / date column name y_col Numerical column names output_type Time category of analysis which can be between \"Daily\", \"Hourly\", \"Weekly\" Returns ------- Plot \"\"\" ts_fig = gen_time_series_plots ( base_path , x_col , y_col , output_type ) return ts_fig def ts_viz_1_2 ( base_path , ts_col , col_list , output_type ): \"\"\" Parameters ---------- base_path Base path which is the same as Master path where the aggregated data resides. ts_col Timestamp / date column name col_list Numerical / Categorical column names output_type Time category of analysis which can be between \"Daily\", \"Hourly\", \"Weekly\" Returns ------- DatapaneObject \"\"\" bl = [] for i in col_list : if len ( col_list ) > 1 : bl . append ( dp . Group ( ts_viz_1_1 ( base_path , ts_col , i , output_type ), label = i )) else : bl . append ( dp . Group ( ts_viz_1_1 ( base_path , ts_col , i , output_type ), label = i )) bl . append ( dp . Plot ( blank_chart , label = \"_\" )) return dp . Select ( blocks = bl , type = dp . SelectType . DROPDOWN ) def ts_viz_1_3 ( base_path , ts_col , num_cols , cat_cols , output_type ): \"\"\" Parameters ---------- base_path Base path which is the same as Master path where the aggregated data resides. ts_col Timestamp / date column name num_cols Numerical column names cat_cols Categorical column names output_type Time category of analysis which can be between \"Daily\", \"Hourly\", \"Weekly\" Returns ------- DatapaneObject \"\"\" ts_v = [] # print(num_cols) # print(cat_cols) if len ( num_cols ) == 0 : for i in ts_col : if len ( ts_col ) > 1 : ts_v . append ( dp . Group ( ts_viz_1_2 ( base_path , i , cat_cols , output_type ), label = i ) ) else : ts_v . append ( dp . Group ( ts_viz_1_2 ( base_path , i , cat_cols , output_type ), label = i ) ) ts_v . append ( dp . Plot ( blank_chart , label = \"_\" )) elif len ( cat_cols ) == 0 : for i in ts_col : if len ( ts_col ) > 1 : ts_v . append ( dp . Group ( ts_viz_1_2 ( base_path , i , num_cols , output_type ), label = i ) ) else : ts_v . append ( dp . Group ( ts_viz_1_2 ( base_path , i , num_cols , output_type ), label = i ) ) ts_v . append ( dp . Plot ( blank_chart , label = \"_\" )) elif ( len ( num_cols ) >= 1 ) & ( len ( cat_cols ) >= 1 ): for i in ts_col : if len ( ts_col ) > 1 : ts_v . append ( dp . Group ( dp . Select ( blocks = [ dp . Group ( ts_viz_1_2 ( base_path , i , num_cols , output_type ), label = \"Numerical\" , ), dp . Group ( ts_viz_1_2 ( base_path , i , cat_cols , output_type ), label = \"Categorical\" , ), ], type = dp . SelectType . TABS , ), label = i , ) ) else : ts_v . append ( dp . Group ( dp . Select ( blocks = [ dp . Group ( ts_viz_1_2 ( base_path , i , num_cols , output_type ), label = \"Numerical\" , ), dp . Group ( ts_viz_1_2 ( base_path , i , cat_cols , output_type ), label = \"Categorical\" , ), ], type = dp . SelectType . TABS , ), label = i , ) ) ts_v . append ( dp . Plot ( blank_chart , label = \"_\" )) return dp . Select ( blocks = ts_v , type = dp . SelectType . DROPDOWN ) def ts_viz_2_1 ( base_path , x_col , y_col ): \"\"\" Parameters ---------- base_path Base path which is the same as Master path where the aggregated data resides. x_col Timestamp / date column name y_col Numerical column names Returns ------- DatapaneObject \"\"\" ts_fig = [] for i in [ \"mean\" , \"median\" , \"min\" , \"max\" ]: ts_fig . append ( dp . Plot ( plotSeasonalDecompose ( base_path , x_col , y_col , metric_col = i ), label = i . title (), ) ) return dp . Select ( blocks = ts_fig , type = dp . SelectType . TABS ) def ts_viz_2_2 ( base_path , ts_col , col_list ): \"\"\" Parameters ---------- base_path Base path which is the same as Master path where the aggregated data resides. ts_col Timestamp / date column name col_list Numerical column names Returns ------- DatapaneObject \"\"\" bl = [] for i in col_list : if len ( col_list ) > 1 : bl . append ( dp . Group ( ts_viz_2_1 ( base_path , ts_col , i ), label = i )) else : bl . append ( dp . Group ( ts_viz_2_1 ( base_path , ts_col , i ), label = i )) bl . append ( dp . Group ( dp . Plot ( blank_chart , label = \" \" ), label = \" \" )) return dp . Select ( blocks = bl , type = dp . SelectType . DROPDOWN ) def ts_viz_2_3 ( base_path , ts_col , num_cols ): \"\"\" Parameters ---------- base_path Base path which is the same as Master path where the aggregated data resides. ts_col Timestamp / date column name num_cols Numerical column names Returns ------- DatapaneObject \"\"\" ts_v = [] if len ( ts_col ) > 1 : for i in ts_col : f = list ( pd . read_csv ( ends_with ( base_path ) + \"stats_\" + i + \"_2.csv\" ) . count_unique_dates . values )[ 0 ] if f >= 24 : ts_v . append ( dp . Group ( ts_viz_2_2 ( base_path , i , num_cols ), label = i )) else : ts_v . append ( dp . Group ( dp . Text ( \"The plots couldn't be displayed as x must have 2 complete cycles requires 24 observations. x only has \" + str ( f ) + \" observation(s)\" ), label = i , ) ) else : for i in ts_col : f = list ( pd . read_csv ( ends_with ( base_path ) + \"stats_\" + i + \"_2.csv\" ) . count_unique_dates . values )[ 0 ] if f >= 24 : ts_v . append ( dp . Group ( ts_viz_2_2 ( base_path , i , num_cols ), label = i )) ts_v . append ( dp . Plot ( blank_chart , label = \"_\" )) else : ts_v . append ( dp . Group ( dp . Text ( \"The plots couldn't be displayed as x must have 2 complete cycles requires 24 observations. x only has \" + str ( f ) + \" observation(s)\" ), label = i , ) ) ts_v . append ( dp . Plot ( blank_chart , label = \"_\" )) return dp . Select ( blocks = ts_v , type = dp . SelectType . DROPDOWN ) def ts_landscape ( base_path , ts_cols , id_col ): \"\"\" This function helps to produce a basic landscaping view of the data by picking up the base path for reading the aggregated data and specified by the timestamp / date column & the ID column. Parameters ---------- base_path Base path which is the same as Master path where the aggregated data resides. ts_col Timestamp / date column name id_col ID Column Returns ------- DatapaneObject \"\"\" if ts_cols is None : return dp . Text ( \"#\" ) else : df_stats_ts = [] for i in ts_cols : if len ( ts_cols ) > 1 : df_stats_ts . append ( dp . Group ( dp . Group ( dp . Text ( \"# \" ), dp . Text ( \"*ID considered here is : \" + str ( id_col ) + \"*\" ), dp . Text ( \"# \" ), dp . Text ( \"#### Consistency Analysis Of Dates\" ), dp . DataTable ( pd . read_csv ( ends_with ( base_path ) + \"stats_\" + i + \"_1.csv\" ) . set_index ( \"attribute\" ) . T , label = i , ), rows = 5 , ), dp . Group ( dp . Text ( \"*The Percentile distribution across different bins of ID-Date / Date-ID combination should be in a considerable range to determine the regularity of Time series. In an ideal scenario the proportion of dates within each ID should be same. Also, the count of IDs across unique dates should be consistent for a balanced distribution*\" ), dp . Text ( \"# \" ), dp . Text ( \"#### Vital Statistics\" ), dp . DataTable ( pd . read_csv ( ends_with ( base_path ) + \"stats_\" + i + \"_2.csv\" ) . T . rename ( columns = { 0 : \"\" }), label = i , ), rows = 4 , ), label = i , rows = 2 , ) ) else : df_stats_ts . append ( dp . Group ( dp . Group ( dp . Text ( \"# \" ), dp . Text ( \"*ID considered here is : \" + str ( id_col ) + \"*\" ), dp . Text ( \"#### Consistency Analysis Of Dates\" ), dp . Text ( \"# \" ), dp . DataTable ( pd . read_csv ( ends_with ( base_path ) + \"stats_\" + i + \"_1.csv\" ) . set_index ( \"attribute\" ) . T , label = i , ), rows = 5 , ), dp . Group ( dp . Text ( \"# \" ), dp . Text ( \"#### Vital Statistics\" ), dp . DataTable ( pd . read_csv ( ends_with ( base_path ) + \"stats_\" + i + \"_2.csv\" ) . T . rename ( columns = { 0 : \"\" }), label = i , ), rows = 3 , ), label = i , rows = 2 , ) ) df_stats_ts . append ( dp . Plot ( blank_chart , label = \"_\" )) return dp . Group ( dp . Text ( \"### Time Stamp Data Diagnosis\" ), dp . Select ( blocks = df_stats_ts , type = dp . SelectType . DROPDOWN ), rows = 2 , ) def lambda_cat ( val ): \"\"\" Parameters ---------- val Value of Box Cox Test which translates into the transformation to be applied. Returns ------- String \"\"\" if val < - 1 : return \"Reciprocal Square Transform\" elif val >= - 1 and val < - 0.5 : return \"Reciprocal Transform\" elif val >= - 0.5 and val < 0 : return \"Receiprocal Square Root Transform\" elif val >= 0 and val < 0.5 : return \"Log Transform\" elif val >= 0.5 and val < 1 : return \"Square Root Transform\" elif val >= 1 and val < 2 : return \"No Transform\" elif val >= 2 : return \"Square Transform\" else : return \"ValueOutOfRange\" def ts_viz_3_1 ( base_path , x_col , y_col ): \"\"\" Parameters ---------- base_path Base path which is the same as Master path where the aggregated data resides. x_col Timestamp / date column name y_col Numerical column names Returns ------- DatapaneObject \"\"\" ts_fig = [] df = pd . read_csv ( ends_with ( base_path ) + x_col + \"_\" + y_col + \"_daily.csv\" ) . dropna () df [ x_col ] = pd . to_datetime ( df [ x_col ], format = \"%Y-%m- %d %H:%M:%S. %f \" ) df = df . set_index ( x_col ) for metric_col in [ \"mean\" , \"median\" , \"min\" , \"max\" ]: adf_test = round ( adfuller ( df [ metric_col ])[ 0 ], 3 ), round ( adfuller ( df [ metric_col ])[ 1 ], 3 ) kpss_test = round ( kpss ( df [ metric_col ], regression = \"ct\" )[ 0 ], 3 ), round ( kpss ( df [ metric_col ], regression = \"ct\" )[ 1 ], 3 ) if adf_test [ 1 ] < 0.05 : adf_flag = True else : adf_flag = False if kpss_test [ 1 ] < 0.05 : kpss_flag = True else : kpss_flag = False # df[metric_col] = df[metric_col].apply(lambda x: boxcox1p(x,0.25)) # lambda_box_cox = round(boxcox(df[metric_col])[1],5) fit = PowerTransformer ( method = \"yeo-johnson\" ) try : lambda_box_cox = round ( fit . fit ( np . array ( df [ metric_col ]) . reshape ( - 1 , 1 )) . lambdas_ [ 0 ], 3 ) cnt = 0 except : cnt = 1 if cnt == 0 : # df[metric_col+\"_transformed\"] = boxcox(df[metric_col],lmbda=lambda_box_cox) df [ metric_col + \"_transformed\" ] = fit . transform ( np . array ( df [ metric_col ]) . reshape ( - 1 , 1 ) ) fig = make_subplots ( rows = 1 , cols = 2 , subplot_titles = [ \"Pre-Transformation\" , \"Post-Transformation\" ], ) fig . add_trace ( go . Scatter ( x = df . index , y = df [ metric_col ], mode = \"lines+markers\" , name = metric_col , line = dict ( color = global_theme [ 1 ]), ), row = 1 , col = 1 , ) fig . add_trace ( go . Scatter ( x = df . index , y = df [ metric_col + \"_transformed\" ], mode = \"lines+markers\" , name = metric_col + \"_transformed\" , line = dict ( color = global_theme [ 7 ]), ), row = 1 , col = 2 , ) fig . layout . plot_bgcolor = global_plot_bg_color fig . layout . paper_bgcolor = global_paper_bg_color fig . update_xaxes ( gridcolor = px . colors . sequential . Greys [ 1 ]) fig . update_yaxes ( gridcolor = px . colors . sequential . Greys [ 1 ]) fig . update_layout ( autosize = True , width = 2000 , height = 400 ) fig . update_layout ( legend = dict ( orientation = \"h\" , x = 0.5 , yanchor = \"bottom\" , xanchor = \"center\" ) ) ts_fig . append ( dp . Group ( dp . Group ( dp . BigNumber ( heading = \"ADF Test Statistic\" , value = adf_test [ 0 ], change = adf_test [ 1 ], is_upward_change = adf_flag , ), dp . BigNumber ( heading = \"KPSS Test Statistic\" , value = kpss_test [ 0 ], change = kpss_test [ 1 ], is_upward_change = kpss_flag , ), dp . BigNumber ( heading = \"Box-Cox Transformation\" , value = lambda_box_cox , change = str ( lambda_cat ( lambda_box_cox )), is_upward_change = True , ), columns = 3 , ), dp . Text ( \"#### Transformation View\" ), dp . Text ( \"Below Transformation is basis the inferencing from the Box Cox Transformation. The Lambda value of \" + str ( lambda_box_cox ) + \" indicates a \" + str ( lambda_cat ( lambda_box_cox )) + \". A Pre-Post Transformation Visualization is done for better clarity. \" ), dp . Plot ( fig ), dp . Text ( \"**Guidelines :** \" ), dp . Text ( \"**ADF** : *The more negative the statistic, the more likely we are to reject the null hypothesis. If the p-value is less than the significance level of 0.05, we can reject the null hypothesis and take that the series is stationary*\" ), dp . Text ( \"**KPSS** : *If the p-value is high, we cannot reject the null hypothesis. So the series is stationary.*\" ), label = metric_col . title (), ) ) else : ts_fig . append ( dp . Group ( dp . Group ( dp . BigNumber ( heading = \"ADF Test Statistic\" , value = adf_test [ 0 ], change = adf_test [ 1 ], is_upward_change = adf_flag , ), dp . BigNumber ( heading = \"KPSS Test Statistic\" , value = kpss_test [ 0 ], change = kpss_test [ 1 ], is_upward_change = kpss_flag , ), dp . BigNumber ( heading = \"Box-Cox Transformation\" , value = \"ValueOutOfRange\" , change = \"ValueOutOfRange\" , is_upward_change = True , ), columns = 3 , ), dp . Text ( \"**Guidelines :** \" ), dp . Text ( \"**ADF** : *The more negative the statistic, the more likely we are to reject the null hypothesis. If the p-value is less than the significance level of 0.05, we can reject the null hypothesis and take that the series is stationary*\" ), dp . Text ( \"**KPSS** : *If the p-value is high, we cannot reject the null hypothesis. So the series is stationary.*\" ), label = metric_col . title (), ) ) return dp . Select ( blocks = ts_fig , type = dp . SelectType . TABS ) def ts_viz_3_2 ( base_path , ts_col , col_list ): \"\"\" Parameters ---------- base_path Base path which is the same as Master path where the aggregated data resides. ts_col Timestamp / date column name col_list Numerical column names Returns ------- DatapaneObject \"\"\" bl = [] for i in col_list : if len ( num_cols ) > 1 : bl . append ( dp . Group ( ts_viz_3_1 ( base_path , ts_col , i ), label = i )) else : bl . append ( dp . Group ( ts_viz_3_1 ( base_path , ts_col , i ), label = i )) bl . append ( dp . Group ( dp . Plot ( blank_chart , label = \" \" ), label = \" \" )) return dp . Select ( blocks = bl , type = dp . SelectType . DROPDOWN ) def ts_viz_3_3 ( base_path , ts_col , num_cols ): \"\"\" Parameters ---------- base_path Base path which is the same as Master path where the aggregated data resides. ts_col Timestamp / date column name num_cols Numerical column names Returns ------- DatapaneObject \"\"\" # f = list(pd.read_csv(ends_with(base_path) + \"stats_\" + i + \"_2.csv\").count_unique_dates.values)[0] # if f >= 6: if len ( ts_col ) > 1 : ts_v = [] for i in ts_col : f = list ( pd . read_csv ( ends_with ( base_path ) + \"stats_\" + i + \"_2.csv\" ) . count_unique_dates . values )[ 0 ] if f >= 6 : ts_v . append ( dp . Group ( ts_viz_3_2 ( base_path , i , num_cols ), label = i )) else : ts_v . append ( dp . Group ( dp . Text ( \"The data contains insufficient data points for the desired transformation analysis. Please ensure the number of unique dates is sufficient.\" ), label = i , ) ) else : ts_v = [] for i in ts_col : f = list ( pd . read_csv ( ends_with ( base_path ) + \"stats_\" + i + \"_2.csv\" ) . count_unique_dates . values )[ 0 ] if f >= 6 : ts_v . append ( dp . Group ( ts_viz_3_2 ( base_path , i , num_cols ), label = i )) ts_v . append ( dp . Plot ( blank_chart , label = \"_\" )) else : ts_v . append ( dp . Group ( dp . Text ( \"The data contains insufficient data points for the desired transformation analysis. Please ensure the number of unique dates is sufficient.\" ), label = i , ) ) ts_v . append ( dp . Plot ( blank_chart , label = \"_\" )) return dp . Select ( blocks = ts_v , type = dp . SelectType . DROPDOWN ) def ts_stats ( base_path ): \"\"\" This function helps to read the base data containing desired input and produces output specific to the `ts_cols_stats.csv` file Parameters ---------- base_path Base path which is the same as Master path where the aggregated data resides. Returns ------- List \"\"\" df = pd . read_csv ( base_path + \"ts_cols_stats.csv\" ) all_stats = [] for i in range ( 0 , 7 ): try : all_stats . append ( df [ df . index . values == i ] . values [ 0 ][ 0 ] . split ( \",\" )) except : all_stats . append ([]) c0 = pd . DataFrame ( all_stats [ 0 ], columns = [ \"attributes\" ]) c1 = pd . DataFrame ( list_ts_remove_append ( all_stats [ 1 ], 1 ), columns = [ \"attributes\" ]) c1 [ \"Analyzed Attributes\" ] = \"\u2714\" c2 = pd . DataFrame ( list_ts_remove_append ( all_stats [ 2 ], 1 ), columns = [ \"attributes\" ]) c2 [ \"Attributes Identified\" ] = \"\u2714\" c3 = pd . DataFrame ( list_ts_remove_append ( all_stats [ 3 ], 1 ), columns = [ \"attributes\" ]) c3 [ \"Attributes Pre-Existed\" ] = \"\u2714\" c4 = pd . DataFrame ( list_ts_remove_append ( all_stats [ 4 ], 1 ), columns = [ \"attributes\" ]) c4 [ \"Overall TimeStamp Attributes\" ] = \"\u2714\" c5 = list_ts_remove_append ( all_stats [ 5 ], 1 ) c6 = list_ts_remove_append ( all_stats [ 6 ], 1 ) return c0 , c1 , c2 , c3 , c4 , c5 , c6 def ts_viz_generate ( master_path , id_col , print_report = False , output_type = None ): \"\"\" This function helps to produce the output in the nested / recursive function supported by datapane. Eventually this is populated at the final report. Parameters ---------- master_path Master path where the aggregated data resides. id_col ID Column print_report Option to specify whether the Report needs to be saved or not. True / False can be used to specify the needful. output_type Time category of analysis which can be between \"Daily\", \"Hourly\", \"Weekly\" Returns ------- DatapaneObject / Output[HTML] \"\"\" master_path = ends_with ( master_path ) try : c0 , c1 , c2 , c3 , c4 , c5 , c6 = ts_stats ( master_path ) except : return \"null_report\" stats_df = ( c0 . merge ( c1 , on = \"attributes\" , how = \"left\" ) . merge ( c2 , on = \"attributes\" , how = \"left\" ) . merge ( c3 , on = \"attributes\" , how = \"left\" ) . merge ( c4 , on = \"attributes\" , how = \"left\" ) . fillna ( \"\u2718\" ) ) global num_cols global cat_cols num_cols , cat_cols = c5 , c6 final_ts_cols = list ( ts_stats ( master_path )[ 4 ] . attributes . values ) if output_type == \"daily\" : report = dp . Group ( dp . Text ( \"# \" ), dp . Text ( \"*This section summarizes the information about timestamp features and how they are interactive with other attributes. An exhaustive diagnosis is done by looking at different time series components, how they could be useful in deriving insights for further downstream applications*\" ), dp . Text ( \"# \" ), dp . Text ( \"# \" ), dp . Text ( \"### Basic Landscaping\" ), dp . Text ( \"Out of **\" + str ( len ( list ( ts_stats ( master_path )[ 1 ] . attributes . values ))) + \"** potential attributes in the data, the module could locate **\" + str ( len ( final_ts_cols )) + \"** attributes as Timestamp\" ), dp . DataTable ( stats_df ), ts_landscape ( master_path , final_ts_cols , id_col ), dp . Text ( \"*Lower the **CoV** (Coefficient Of Variation), Higher the Consistency between the consecutive dates. Similarly the Mean & Variance should be consistent over time*\" ), dp . Text ( \"### Visualization across the Shortlisted Timestamp Attributes\" ), ts_viz_1_3 ( master_path , final_ts_cols , num_cols , cat_cols , output_type ), dp . Text ( \"### Decomposed View\" ), ts_viz_2_3 ( master_path , final_ts_cols , num_cols ), dp . Text ( \"### Stationarity & Transformations\" ), ts_viz_3_3 ( master_path , final_ts_cols , num_cols ), dp . Text ( \"#\" ), dp . Text ( \"#\" ), label = \"Time Series Analyzer\" , ) else : report = dp . Group ( dp . Text ( \"# \" ), dp . Text ( \"*This section summarizes the information about timestamp features and how they are interactive with other attributes. An exhaustive diagnosis is done by looking at different time series components, how they could be useful in deriving insights for further downstream applications*\" ), dp . Text ( \"# \" ), dp . Text ( \"# \" ), dp . Text ( \"### Basic Landscaping\" ), dp . Text ( \"Out of **\" + str ( len ( list ( ts_stats ( master_path )[ 1 ] . attributes . values ))) + \"** potential attributes in the data, the module could locate **\" + str ( len ( final_ts_cols )) + \"** attributes as Timestamp\" ), dp . DataTable ( stats_df ), ts_landscape ( master_path , final_ts_cols , id_col ), dp . Text ( \"*Lower the **CoV** (Coefficient Of Variation), Higher the Consistency between the consecutive dates. Similarly the Mean & Variance should be consistent over time*\" ), dp . Text ( \"### Visualization across the Shortlisted Timestamp Attributes\" ), ts_viz_1_3 ( master_path , final_ts_cols , num_cols , cat_cols , output_type ), dp . Text ( \"#\" ), dp . Text ( \"#\" ), label = \"Time Series Analyzer\" , ) if print_report : dp . Report ( default_template [ 0 ], default_template [ 1 ], report ) . save ( ends_with ( master_path ) + \"time_series_analyzer.html\" , open = True ) return report def anovos_report ( master_path , id_col = \"\" , label_col = \"\" , corr_threshold = 0.4 , iv_threshold = 0.02 , drift_threshold_model = 0.1 , dataDict_path = \".\" , metricDict_path = \".\" , run_type = \"local\" , final_report_path = \".\" , output_type = None , ): \"\"\" This function actually helps to produce the final report by scanning through the output processed from the data analyzer module. Parameters ---------- master_path Path containing the input files. id_col ID column (Default value = \"\") label_col label column (Default value = \"\") corr_threshold Correlation threshold beyond which attributes can be categorized under correlated. (Default value = 0.4) iv_threshold IV threshold beyond which attributes can be called as significant. (Default value = 0.02) drift_threshold_model threshold which the user is specifying for tagging an attribute to be drifted or not (Default value = 0.1) dataDict_path Data dictionary path. Default value is kept as None. metricDict_path Metric dictionary path. Default value is kept as None. run_type local or emr or databricks option. Default is kept as local final_report_path Path where the report will be saved. (Default value = \".\") output_type Time category of analysis which can be between \"Daily\", \"Hourly\", \"Weekly\" Returns ------- Output[HTML] \"\"\" if run_type == \"emr\" : bash_cmd = ( \"aws s3 cp --recursive \" + ends_with ( master_path ) + \" \" + ends_with ( \"report_stats\" ) ) master_path = \"report_stats\" subprocess . check_output ([ \"bash\" , \"-c\" , bash_cmd ]) if \"global_summary.csv\" not in os . listdir ( master_path ): print ( \"Minimum supporting data is unavailable, hence the Report could not be generated.\" ) return None global global_summary_df global numcols_name global catcols_name global rows_count global columns_count global numcols_count global catcols_count global blank_chart global df_si_ global df_si global unstable_attr global total_unstable_attr global drift_df global metric_drift global drift_df global len_feats global drift_df_stats global drifted_feats global df_stability global n_df_stability global stability_interpretation_table global plot_index_stability SG_tabs = [ \"measures_of_counts\" , \"measures_of_centralTendency\" , \"measures_of_cardinality\" , \"measures_of_percentiles\" , \"measures_of_dispersion\" , \"measures_of_shape\" , \"global_summary\" , ] QC_tabs = [ \"nullColumns_detection\" , \"IDness_detection\" , \"biasedness_detection\" , \"invalidEntries_detection\" , \"duplicate_detection\" , \"nullRows_detection\" , \"outlier_detection\" , ] AE_tabs = [ \"correlation_matrix\" , \"IV_calculation\" , \"IG_calculation\" , \"variable_clustering\" , ] drift_tab = [ \"drift_statistics\" ] stability_tab = [ \"stability_index\" , \"stabilityIndex_metrics\" ] avl_SG , avl_QC , avl_AE = [], [], [] stability_interpretation_table = pd . DataFrame ( [ [ \"0-1\" , \"Very Unstable\" ], [ \"1-2\" , \"Unstable\" ], [ \"2-3\" , \"Marginally Stable\" ], [ \"3-3.5\" , \"Stable\" ], [ \"3.5-4\" , \"Very Stable\" ], ], columns = [ \"StabilityIndex\" , \"StabilityOrder\" ], ) plot_index_stability = go . Figure ( data = [ go . Table ( header = dict ( values = list ( stability_interpretation_table . columns ), fill_color = px . colors . sequential . Greys [ 2 ], align = \"center\" , font = dict ( size = 12 ), ), cells = dict ( values = [ stability_interpretation_table . StabilityIndex , stability_interpretation_table . StabilityOrder , ], line_color = px . colors . sequential . Greys [ 2 ], fill_color = \"white\" , align = \"center\" , height = 25 , ), columnwidth = [ 2 , 10 ], ) ] ) plot_index_stability . update_layout ( margin = dict ( l = 20 , r = 700 , t = 20 , b = 20 )) blank_chart = go . Figure () blank_chart . update_layout ( autosize = False , width = 10 , height = 10 ) blank_chart . layout . plot_bgcolor = global_plot_bg_color blank_chart . layout . paper_bgcolor = global_paper_bg_color blank_chart . update_xaxes ( visible = False ) blank_chart . update_yaxes ( visible = False ) global_summary_df = pd . read_csv ( ends_with ( master_path ) + \"global_summary.csv\" ) rows_count = int ( global_summary_df [ global_summary_df . metric . values == \"rows_count\" ] . value . values [ 0 ] ) catcols_count = int ( global_summary_df [ global_summary_df . metric . values == \"catcols_count\" ] . value . values [ 0 ] ) numcols_count = int ( global_summary_df [ global_summary_df . metric . values == \"numcols_count\" ] . value . values [ 0 ] ) columns_count = int ( global_summary_df [ global_summary_df . metric . values == \"columns_count\" ] . value . values [ 0 ] ) if catcols_count > 0 : catcols_name = \",\" . join ( list ( global_summary_df [ global_summary_df . metric . values == \"catcols_name\" ] . value . values ) ) else : catcols_name = \"\" if numcols_count > 0 : numcols_name = \",\" . join ( list ( global_summary_df [ global_summary_df . metric . values == \"numcols_name\" ] . value . values ) ) else : numcols_name = \"\" all_files = os . listdir ( master_path ) eventDist_charts = [ x for x in all_files if \"eventDist\" in x ] stats_files = [ x for x in all_files if \".csv\" in x ] freq_charts = [ x for x in all_files if \"freqDist\" in x ] outlier_charts = [ x for x in all_files if \"outlier\" in x ] drift_charts = [ x for x in all_files if \"drift\" in x and \".csv\" not in x ] all_charts_num_1_ = chart_gen_list ( master_path , chart_type = freq_charts , type_col = \"numerical\" ) all_charts_num_2_ = chart_gen_list ( master_path , chart_type = eventDist_charts , type_col = \"numerical\" ) all_charts_num_3_ = chart_gen_list ( master_path , chart_type = outlier_charts , type_col = \"numerical\" ) all_charts_cat_1_ = chart_gen_list ( master_path , chart_type = freq_charts , type_col = \"categorical\" ) all_charts_cat_2_ = chart_gen_list ( master_path , chart_type = eventDist_charts , type_col = \"categorical\" ) all_drift_charts_ = chart_gen_list ( master_path , chart_type = drift_charts ) for x in [ all_charts_num_1_ , all_charts_num_2_ , all_charts_num_3_ , all_charts_cat_1_ , all_charts_cat_2_ , all_drift_charts_ , ]: if len ( x ) == 1 : x . append ( dp . Plot ( blank_chart , label = \" \" )) else : x mapping_tab_list = [] for i in stats_files : if i . split ( \".csv\" )[ 0 ] in SG_tabs : mapping_tab_list . append ([ i . split ( \".csv\" )[ 0 ], \"Descriptive Statistics\" ]) elif i . split ( \".csv\" )[ 0 ] in QC_tabs : mapping_tab_list . append ([ i . split ( \".csv\" )[ 0 ], \"Quality Check\" ]) elif i . split ( \".csv\" )[ 0 ] in AE_tabs : mapping_tab_list . append ([ i . split ( \".csv\" )[ 0 ], \"Attribute Associations\" ]) elif i . split ( \".csv\" )[ 0 ] in drift_tab or i . split ( \".csv\" )[ 0 ] in stability_tab : mapping_tab_list . append ([ i . split ( \".csv\" )[ 0 ], \"Data Drift & Data Stability\" ]) else : mapping_tab_list . append ([ i . split ( \".csv\" )[ 0 ], \"null\" ]) xx = pd . DataFrame ( mapping_tab_list , columns = [ \"file_name\" , \"tab_name\" ]) xx_avl = list ( set ( xx . file_name . values )) for i in SG_tabs : if i in xx_avl : avl_SG . append ( i ) for j in QC_tabs : if j in xx_avl : avl_QC . append ( j ) for k in AE_tabs : if k in xx_avl : avl_AE . append ( k ) missing_SG = list ( set ( SG_tabs ) - set ( avl_SG )) missing_QC = list ( set ( QC_tabs ) - set ( avl_QC )) missing_AE = list ( set ( AE_tabs ) - set ( avl_AE )) missing_drift = list ( set ( drift_tab ) - set ( xx [ xx . tab_name . values == \"Data Drift & Data Stability\" ] . file_name . values ) ) missing_stability = list ( set ( stability_tab ) - set ( xx [ xx . tab_name . values == \"Data Drift & Data Stability\" ] . file_name . values ) ) ds_ind = drift_stability_ind ( missing_drift , drift_tab , missing_stability , stability_tab ) if ds_ind [ 0 ] > 0 : drift_df = pd . read_csv ( ends_with ( master_path ) + \"drift_statistics.csv\" ) . sort_values ( by = [ \"flagged\" ], ascending = False ) metric_drift = list ( drift_df . drop ([ \"attribute\" , \"flagged\" ], 1 ) . columns ) drift_df = drift_df [ drift_df . attribute . values != id_col ] len_feats = drift_df . shape [ 0 ] drift_df_stats = ( drift_df [ drift_df . flagged . values == 1 ] . melt ( id_vars = \"attribute\" , value_vars = metric_drift ) . sort_values ( by = [ \"variable\" , \"value\" ], ascending = False ) ) drifted_feats = drift_df [ drift_df . flagged . values == 1 ] . shape [ 0 ] if ds_ind [ 1 ] > 0.5 : df_stability = pd . read_csv ( ends_with ( master_path ) + \"stabilityIndex_metrics.csv\" ) df_stability [ \"idx\" ] = df_stability [ \"idx\" ] . astype ( str ) . apply ( lambda x : \"df\" + x ) n_df_stability = str ( df_stability [ \"idx\" ] . nunique ()) df_si_ = pd . read_csv ( ends_with ( master_path ) + \"stability_index.csv\" ) df_si = df_si_ [ [ \"attribute\" , \"stability_index\" , \"mean_si\" , \"stddev_si\" , \"kurtosis_si\" , \"flagged\" , ] ] unstable_attr = list ( df_si_ [ df_si_ . flagged . values == 1 ] . attribute . values ) total_unstable_attr = list ( df_si_ . attribute . values ) elif ds_ind [ 1 ] == 0.5 : df_si_ = pd . read_csv ( ends_with ( master_path ) + \"stability_index.csv\" ) df_si = df_si_ [ [ \"attribute\" , \"stability_index\" , \"mean_si\" , \"stddev_si\" , \"kurtosis_si\" , \"flagged\" , ] ] unstable_attr = list ( df_si_ [ df_si_ . flagged . values == 1 ] . attribute . values ) total_unstable_attr = list ( df_si_ . attribute . values ) df_stability = pd . DataFrame () n_df_stability = \"the\" else : pass tab1 = executive_summary_gen ( master_path , label_col , ds_ind , id_col , iv_threshold , corr_threshold ) tab2 = wiki_generator ( master_path , dataDict_path = dataDict_path , metricDict_path = metricDict_path ) tab3 = descriptive_statistics ( master_path , SG_tabs , avl_SG , missing_SG , all_charts_num_1_ , all_charts_cat_1_ ) tab4 = quality_check ( master_path , QC_tabs , avl_QC , missing_QC , all_charts_num_3_ ) tab5 = attribute_associations ( master_path , AE_tabs , avl_AE , missing_AE , label_col , all_charts_num_2_ , all_charts_cat_2_ , ) tab6 = data_drift_stability ( master_path , ds_ind , id_col , drift_threshold_model , all_drift_charts_ ) tab7 = ts_viz_generate ( master_path , id_col , False , output_type ) final_tabs_list = [] for i in [ tab1 , tab2 , tab3 , tab4 , tab5 , tab6 , tab7 ]: if i == \"null_report\" : pass else : final_tabs_list . append ( i ) if run_type in ( \"local\" , \"databricks\" ): dp . Report ( default_template [ 0 ], default_template [ 1 ], dp . Select ( blocks = final_tabs_list , type = dp . SelectType . TABS ), ) . save ( ends_with ( final_report_path ) + \"ml_anovos_report.html\" , open = True ) elif run_type == \"emr\" : dp . Report ( default_template [ 0 ], default_template [ 1 ], dp . Select ( blocks = final_tabs_list , type = dp . SelectType . TABS ), ) . save ( \"ml_anovos_report.html\" , open = True ) bash_cmd = \"aws s3 cp ml_anovos_report.html \" + ends_with ( final_report_path ) subprocess . check_output ([ \"bash\" , \"-c\" , bash_cmd ]) else : raise ValueError ( \"Invalid run_type\" ) print ( \"Report generated successfully at the specified location\" )","title":"report_generation"},{"location":"api/data_report/report_generation.html#functions","text":"def anovos_report ( master_path, id_col='', label_col='', corr_threshold=0.4, iv_threshold=0.02, drift_threshold_model=0.1, dataDict_path='.', metricDict_path='.', run_type='local', final_report_path='.', output_type=None) This function actually helps to produce the final report by scanning through the output processed from the data analyzer module.","title":"Functions"},{"location":"api/data_report/report_preprocessing.html","text":"report_preprocessing Expand source code import subprocess from pathlib import Path import numpy as np import pandas as pd import plotly.express as px import plotly.graph_objects as go import pyspark from loguru import logger from pyspark.sql import functions as F from pyspark.sql import types as T from pyspark.sql.window import Window from anovos.data_analyzer.stats_generator import uniqueCount_computation from anovos.data_ingest.data_ingest import read_dataset from anovos.data_transformer.transformers import ( outlier_categories , imputation_MMM , attribute_binning , ) from anovos.shared.utils import attributeType_segregation , ends_with from ..shared.utils import platform_root_path import warnings warnings . filterwarnings ( \"ignore\" ) global_theme = px . colors . sequential . Plasma global_theme_r = px . colors . sequential . Plasma_r global_plot_bg_color = \"rgba(0,0,0,0)\" global_paper_bg_color = \"rgba(0,0,0,0)\" num_cols = [] cat_cols = [] def master_to_local ( master_path ): punctuations = \":\" for x in master_path : if x in punctuations : local_path = master_path . replace ( x , \"\" ) local_path = \"/\" + local_path return local_path def save_stats ( spark , idf , master_path , function_name , reread = False , run_type = \"local\" ): \"\"\" Parameters ---------- spark Spark Session idf input dataframe master_path Path to master folder under which all statistics will be saved in a csv file format. function_name Function Name for which statistics need to be saved. file name will be saved as csv reread option to reread. Default value is kept as False run_type local or emr or databricks based on the mode of execution. Default value is kept as local Returns ------- \"\"\" if run_type == \"local\" : local_path = master_path elif run_type == \"databricks\" : local_path = master_to_local ( master_path ) elif run_type == \"emr\" : local_path = \"report_stats\" else : raise ValueError ( \"Invalid run_type\" ) Path ( local_path ) . mkdir ( parents = True , exist_ok = True ) idf . toPandas () . to_csv ( ends_with ( local_path ) + function_name + \".csv\" , index = False ) if run_type == \"emr\" : bash_cmd = ( \"aws s3 cp \" + ends_with ( local_path ) + function_name + \".csv \" + ends_with ( master_path ) ) subprocess . check_output ([ \"bash\" , \"-c\" , bash_cmd ]) if reread : odf = spark . read . csv ( ends_with ( master_path ) + function_name + \".csv\" , header = True , inferSchema = True , ) return odf def edit_binRange ( col ): \"\"\" Parameters ---------- col The column which is passed as input and needs to be treated. The generated output will not contain any range whose value at either side is the same. Returns ------- \"\"\" try : list_col = col . split ( \"-\" ) deduped_col = list ( set ( list_col )) if len ( list_col ) != len ( deduped_col ): return deduped_col [ 0 ] else : return col except Exception as e : logger . error ( f \"processing failed during edit_binRange, error { e } \" ) pass f_edit_binRange = F . udf ( edit_binRange , T . StringType ()) def binRange_to_binIdx ( spark , col , cutoffs_path ): \"\"\" Parameters ---------- spark Spark Session col The input column which is needed to by mapped with respective index cutoffs_path paths containing the range cutoffs applicable for each index Returns ------- \"\"\" bin_cutoffs = ( spark . read . parquet ( cutoffs_path ) . where ( F . col ( \"attribute\" ) == col ) . select ( \"parameters\" ) . rdd . flatMap ( lambda x : x ) . collect ()[ 0 ] ) bin_ranges = [] max_cat = len ( bin_cutoffs ) + 1 for idx in range ( 0 , max_cat ): if idx == 0 : bin_ranges . append ( \"<= \" + str ( round ( bin_cutoffs [ idx ], 4 ))) elif idx < ( max_cat - 1 ): bin_ranges . append ( str ( round ( bin_cutoffs [ idx - 1 ], 4 )) + \"-\" + str ( round ( bin_cutoffs [ idx ], 4 )) ) else : bin_ranges . append ( \"> \" + str ( round ( bin_cutoffs [ idx - 1 ], 4 ))) mapping = spark . createDataFrame ( zip ( range ( 1 , max_cat + 1 ), bin_ranges ), schema = [ \"bin_idx\" , col ] ) return mapping def plot_frequency ( spark , idf , col , cutoffs_path ): \"\"\" Parameters ---------- spark Spark Session idf Input dataframe which would be referred for producing the frequency charts in form of bar plots / histograms col Analysis column cutoffs_path Path containing the range cut offs details for the analysis column Returns ------- \"\"\" odf = ( idf . groupBy ( col ) . count () . withColumn ( \"count_%\" , 100 * ( F . col ( \"count\" ) / F . sum ( \"count\" ) . over ( Window . partitionBy ())), ) . withColumn ( col , f_edit_binRange ( col )) ) if col in cat_cols : odf_pd = odf . orderBy ( \"count\" , ascending = False ) . toPandas () . fillna ( \"Missing\" ) odf_pd . loc [ odf_pd [ col ] == \"others\" , col ] = \"others*\" if col in num_cols : mapping = binRange_to_binIdx ( spark , col , cutoffs_path ) odf_pd = ( odf . join ( mapping , col , \"left_outer\" ) . orderBy ( \"bin_idx\" ) . toPandas () . fillna ( \"Missing\" ) ) fig = px . bar ( odf_pd , x = col , y = \"count\" , text = odf_pd [ \"count_%\" ] . apply ( lambda x : \" {0:1.2f} %\" . format ( x )), color_discrete_sequence = global_theme , ) fig . update_traces ( textposition = \"outside\" ) fig . update_layout ( title_text = str ( \"Frequency Distribution for \" + str ( col . upper ()))) fig . update_xaxes ( type = \"category\" ) # fig.update_layout(barmode='stack', xaxis={'categoryorder':'total descending'}) fig . layout . plot_bgcolor = global_plot_bg_color fig . layout . paper_bgcolor = global_paper_bg_color # plotly.offline.plot(fig, auto_open=False, validate=False, filename=f\"{base_loc}/{file_name_}bar_graph.html\") return fig def plot_outlier ( spark , idf , col , split_var = None , sample_size = 500000 ): \"\"\" Parameters ---------- spark Spark Session idf Input dataframe which would be referred for capturing the outliers in form of violin charts col Analysis column split_var Column which is needed. Default value is kept as None sample_size Maximum Sample size. Default value is kept as 500000 Returns ------- \"\"\" idf_sample = idf . select ( col ) . sample ( False , min ( 1.0 , float ( sample_size ) / idf . count ()), 0 ) idf_sample . persist ( pyspark . StorageLevel . MEMORY_AND_DISK ) . count () idf_imputed = imputation_MMM ( spark , idf_sample ) idf_pd = idf_imputed . toPandas () fig = px . violin ( idf_pd , y = col , color = split_var , box = True , points = \"outliers\" , color_discrete_sequence = [ global_theme_r [ 8 ], global_theme_r [ 4 ]], ) fig . layout . plot_bgcolor = global_plot_bg_color fig . layout . paper_bgcolor = global_paper_bg_color fig . update_layout ( legend = dict ( orientation = \"h\" , x = 0.5 , yanchor = \"bottom\" , xanchor = \"center\" ) ) return fig def plot_eventRate ( spark , idf , col , label_col , event_label , cutoffs_path ): \"\"\" Parameters ---------- spark Spark Session idf Input dataframe which would be referred for producing the frequency charts in form of bar plots / histogram col Analysis column label_col Label column event_label Event label cutoffs_path Path containing the range cut offs details for the analysis column Returns ------- \"\"\" odf = ( idf . withColumn ( label_col , F . when ( F . col ( label_col ) == event_label , 1 ) . otherwise ( 0 ) ) . groupBy ( col ) . pivot ( label_col ) . count () . fillna ( 0 , subset = [ \"0\" , \"1\" ]) . withColumn ( \"event_rate\" , 100 * ( F . col ( \"1\" ) / ( F . col ( \"0\" ) + F . col ( \"1\" )))) . withColumn ( \"attribute_name\" , F . lit ( col )) . withColumn ( col , f_edit_binRange ( col )) ) if col in cat_cols : odf_pd = odf . orderBy ( \"event_rate\" , ascending = False ) . toPandas () odf_pd . loc [ odf_pd [ col ] == \"others\" , col ] = \"others*\" if col in num_cols : mapping = binRange_to_binIdx ( spark , col , cutoffs_path ) odf_pd = odf . join ( mapping , col , \"left_outer\" ) . orderBy ( \"bin_idx\" ) . toPandas () fig = px . bar ( odf_pd , x = col , y = \"event_rate\" , text = odf_pd [ \"event_rate\" ] . apply ( lambda x : \" {0:1.2f} %\" . format ( x )), color_discrete_sequence = global_theme , ) fig . update_traces ( textposition = \"outside\" ) fig . update_layout ( title_text = str ( \"Event Rate Distribution for \" + str ( col . upper ()) + str ( \" [Target Variable : \" + str ( event_label ) + str ( \"]\" )) ) ) fig . update_xaxes ( type = \"category\" ) fig . layout . plot_bgcolor = global_plot_bg_color fig . layout . paper_bgcolor = global_paper_bg_color # plotly.offline.plot(fig, auto_open=False, validate=False, filename=f\"{base_loc}/{file_name_}feat_analysis_label.html\") return fig def plot_comparative_drift ( spark , idf , source , col , cutoffs_path ): \"\"\" Parameters ---------- spark Spark Session idf Target dataframe which would be referred for producing the frequency charts in form of bar plots / histogram source Source dataframe of comparison col Analysis column cutoffs_path Path containing the range cut offs details for the analysis column Returns ------- \"\"\" odf = ( idf . groupBy ( col ) . agg (( F . count ( col ) / idf . count ()) . alias ( \"countpct_target\" )) . fillna ( np . nan , subset = [ col ]) ) if col in cat_cols : odf_pd = ( odf . join ( source . withColumnRenamed ( \"p\" , \"countpct_source\" ) . fillna ( np . nan , subset = [ col ] ), col , \"full_outer\" , ) . orderBy ( \"countpct_target\" , ascending = False ) . toPandas () ) if col in num_cols : mapping = binRange_to_binIdx ( spark , col , cutoffs_path ) odf_pd = ( odf . join ( mapping , col , \"left_outer\" ) . fillna ( np . nan , subset = [ \"bin_idx\" ]) . join ( source . fillna ( np . nan , subset = [ col ]) . select ( F . col ( col ) . alias ( \"bin_idx\" ), F . col ( \"p\" ) . alias ( \"countpct_source\" ) ), \"bin_idx\" , \"full_outer\" , ) . orderBy ( \"bin_idx\" ) . toPandas () ) odf_pd . fillna ( { col : \"Missing\" , \"countpct_source\" : 0 , \"countpct_target\" : 0 }, inplace = True ) odf_pd [ \"%_diff\" ] = ( ( odf_pd [ \"countpct_target\" ] / odf_pd [ \"countpct_source\" ]) - 1 ) * 100 fig = go . Figure () fig . add_bar ( y = list ( odf_pd . countpct_source . values ), x = odf_pd [ col ], name = \"source\" , marker = dict ( color = global_theme ), ) fig . update_traces ( overwrite = True , marker = { \"opacity\" : 0.7 }) fig . add_bar ( y = list ( odf_pd . countpct_target . values ), x = odf_pd [ col ], name = \"target\" , text = odf_pd [ \"%_diff\" ] . apply ( lambda x : \" {0:0.2f} %\" . format ( x )), marker = dict ( color = global_theme ), ) fig . update_traces ( textposition = \"outside\" ) fig . update_layout ( paper_bgcolor = global_paper_bg_color , plot_bgcolor = global_plot_bg_color , showlegend = False , ) fig . update_layout ( title_text = str ( \"Drift Comparison for \" + col + \"<br><sup>(L->R : Source->Target)</sup>\" ) ) fig . update_traces ( marker = dict ( color = global_theme )) fig . update_xaxes ( type = \"category\" ) # fig.add_trace(go.Scatter(x=odf_pd[col], y=odf_pd.countpct_target.values, mode='lines+markers', # line=dict(color=px.colors.qualitative.Antique[10], width=3, dash='dot'))) fig . update_layout ( xaxis_tickfont_size = 14 , yaxis = dict ( title = \"frequency\" , titlefont_size = 16 , tickfont_size = 14 ), ) return fig def charts_to_objects ( spark , idf , list_of_cols = \"all\" , drop_cols = [], label_col = None , event_label = 1 , bin_method = \"equal_range\" , bin_size = 10 , coverage = 1.0 , drift_detector = False , outlier_charts = False , source_path = \"NA\" , master_path = \".\" , stats_unique = {}, run_type = \"local\" , ): \"\"\" Parameters ---------- spark Spark Session idf Input dataframe list_of_cols List of columns passed for analysis (Default value = \"all\") drop_cols List of columns dropped from analysis (Default value = []) label_col Label column (Default value = None) event_label Event label (Default value = 1) bin_method Binning method equal_range or equal_frequency (Default value = \"equal_range\") bin_size Maximum bin size categories. Default value is kept as 10 coverage Maximum coverage of categories. Default value is kept as 1.0 (which is 100%) drift_detector True or False as per the availability. Default value is kept as False source_path Source data path. Default value is kept as \"NA\" to save intermediate data in \"intermediate_data/\" folder. master_path Path where the output needs to be saved, ideally the same path where the analyzed data output is also saved (Default value = \".\") stats_unique Takes arguments for read_dataset (data_ingest module) function in a dictionary format to read pre-saved statistics on unique value count i.e. if measures_of_cardinality or uniqueCount_computation (data_analyzer.stats_generator module) has been computed & saved before. (Default value = {}) run_type local or emr or databricks run type. Default value is kept as local Returns ------- \"\"\" global num_cols global cat_cols if list_of_cols == \"all\" : num_cols , cat_cols , other_cols = attributeType_segregation ( idf ) list_of_cols = num_cols + cat_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] if stats_unique == {}: remove_cols = ( uniqueCount_computation ( spark , idf , list_of_cols ) . where ( F . col ( \"unique_values\" ) < 2 ) . select ( \"attribute\" ) . rdd . flatMap ( lambda x : x ) . collect () ) else : remove_cols = ( read_dataset ( spark , ** stats_unique ) . where ( F . col ( \"unique_values\" ) < 2 ) . select ( \"attribute\" ) . rdd . flatMap ( lambda x : x ) . collect () ) list_of_cols = list ( set ([ e for e in list_of_cols if e not in ( drop_cols + remove_cols )]) ) if any ( x not in idf . columns for x in list_of_cols ) | ( len ( list_of_cols ) == 0 ): raise TypeError ( \"Invalid input for Column(s)\" ) num_cols , cat_cols , other_cols = attributeType_segregation ( idf . select ( list_of_cols )) if cat_cols : idf_cleaned = outlier_categories ( spark , idf , list_of_cols = cat_cols , coverage = coverage , max_category = bin_size ) else : idf_cleaned = idf if run_type in list ( platform_root_path . keys ()): root_path = platform_root_path [ run_type ] else : root_path = \"\" if source_path == \"NA\" : source_path = root_path + \"intermediate_data\" if drift_detector : encoding_model_exists = True binned_cols = ( spark . read . parquet ( source_path + \"/drift_statistics/attribute_binning\" ) . select ( \"attribute\" ) . rdd . flatMap ( lambda x : x ) . collect () ) to_be_binned = [ e for e in num_cols if e not in binned_cols ] else : encoding_model_exists = False binned_cols = [] to_be_binned = num_cols if to_be_binned : idf_encoded = attribute_binning ( spark , idf_cleaned , list_of_cols = to_be_binned , method_type = bin_method , bin_size = bin_size , bin_dtype = \"categorical\" , pre_existing_model = False , model_path = source_path + \"/charts_to_objects\" , output_mode = \"append\" , ) else : idf_encoded = idf_cleaned if binned_cols : idf_encoded = attribute_binning ( spark , idf_encoded , list_of_cols = binned_cols , method_type = bin_method , bin_size = bin_size , bin_dtype = \"categorical\" , pre_existing_model = True , model_path = source_path + \"/drift_statistics\" , output_mode = \"append\" , ) cutoffs_path1 = source_path + \"/charts_to_objects/attribute_binning\" cutoffs_path2 = source_path + \"/drift_statistics/attribute_binning\" idf_encoded . persist ( pyspark . StorageLevel . MEMORY_AND_DISK ) if run_type == \"local\" : local_path = master_path elif run_type == \"databricks\" : local_path = master_to_local ( master_path ) elif run_type == \"emr\" : local_path = \"report_stats\" else : raise ValueError ( \"Invalid run_type\" ) Path ( local_path ) . mkdir ( parents = True , exist_ok = True ) for idx , col in enumerate ( list_of_cols ): if col in binned_cols : cutoffs_path = cutoffs_path2 else : cutoffs_path = cutoffs_path1 if col in cat_cols : f = plot_frequency ( spark , idf_encoded , col , cutoffs_path ) f . write_json ( ends_with ( local_path ) + \"freqDist_\" + col ) if label_col : if col != label_col : f = plot_eventRate ( spark , idf_encoded , col , label_col , event_label , cutoffs_path ) f . write_json ( ends_with ( local_path ) + \"eventDist_\" + col ) if drift_detector : try : frequency_path = ( source_path + \"/drift_statistics/frequency_counts/\" + col ) idf_source = spark . read . csv ( frequency_path , header = True , inferSchema = True ) f = plot_comparative_drift ( spark , idf_encoded , idf_source , col , cutoffs_path ) f . write_json ( ends_with ( local_path ) + \"drift_\" + col ) except Exception as e : logger . error ( f \"processing failed during drift detection, error { e } \" ) pass if col in num_cols : if outlier_charts : f = plot_outlier ( spark , idf , col , split_var = None ) f . write_json ( ends_with ( local_path ) + \"outlier_\" + col ) f = plot_frequency ( spark , idf_encoded . drop ( col ) . withColumnRenamed ( col + \"_binned\" , col ), col , cutoffs_path , ) f . write_json ( ends_with ( local_path ) + \"freqDist_\" + col ) if label_col : if col != label_col : f = plot_eventRate ( spark , idf_encoded . drop ( col ) . withColumnRenamed ( col + \"_binned\" , col ), col , label_col , event_label , cutoffs_path , ) f . write_json ( ends_with ( local_path ) + \"eventDist_\" + col ) if drift_detector : try : frequency_path = ( source_path + \"/drift_statistics/frequency_counts/\" + col ) idf_source = spark . read . csv ( frequency_path , header = True , inferSchema = True ) f = plot_comparative_drift ( spark , idf_encoded . drop ( col ) . withColumnRenamed ( col + \"_binned\" , col ), idf_source , col , cutoffs_path , ) f . write_json ( ends_with ( local_path ) + \"drift_\" + col ) except Exception as e : logger . error ( f \"processing failed during drift detection, error { e } \" ) pass pd . DataFrame ( idf . dtypes , columns = [ \"attribute\" , \"data_type\" ]) . to_csv ( ends_with ( local_path ) + \"data_type.csv\" , index = False ) if run_type == \"emr\" : bash_cmd = ( \"aws s3 cp --recursive \" + ends_with ( local_path ) + \" \" + ends_with ( master_path ) ) subprocess . check_output ([ \"bash\" , \"-c\" , bash_cmd ]) Functions def binRange_to_binIdx ( spark, col, cutoffs_path) Parameters spark Spark Session col The input column which is needed to by mapped with respective index cutoffs_path paths containing the range cutoffs applicable for each index Returns Expand source code def binRange_to_binIdx ( spark , col , cutoffs_path ): \"\"\" Parameters ---------- spark Spark Session col The input column which is needed to by mapped with respective index cutoffs_path paths containing the range cutoffs applicable for each index Returns ------- \"\"\" bin_cutoffs = ( spark . read . parquet ( cutoffs_path ) . where ( F . col ( \"attribute\" ) == col ) . select ( \"parameters\" ) . rdd . flatMap ( lambda x : x ) . collect ()[ 0 ] ) bin_ranges = [] max_cat = len ( bin_cutoffs ) + 1 for idx in range ( 0 , max_cat ): if idx == 0 : bin_ranges . append ( \"<= \" + str ( round ( bin_cutoffs [ idx ], 4 ))) elif idx < ( max_cat - 1 ): bin_ranges . append ( str ( round ( bin_cutoffs [ idx - 1 ], 4 )) + \"-\" + str ( round ( bin_cutoffs [ idx ], 4 )) ) else : bin_ranges . append ( \"> \" + str ( round ( bin_cutoffs [ idx - 1 ], 4 ))) mapping = spark . createDataFrame ( zip ( range ( 1 , max_cat + 1 ), bin_ranges ), schema = [ \"bin_idx\" , col ] ) return mapping def charts_to_objects ( spark, idf, list_of_cols='all', drop_cols=[], label_col=None, event_label=1, bin_method='equal_range', bin_size=10, coverage=1.0, drift_detector=False, outlier_charts=False, source_path='NA', master_path='.', stats_unique={}, run_type='local') Parameters spark Spark Session idf Input dataframe list_of_cols List of columns passed for analysis (Default value = \"all\") drop_cols List of columns dropped from analysis (Default value = []) label_col Label column (Default value = None) event_label Event label (Default value = 1) bin_method Binning method equal_range or equal_frequency (Default value = \"equal_range\") bin_size Maximum bin size categories. Default value is kept as 10 coverage Maximum coverage of categories. Default value is kept as 1.0 (which is 100%) drift_detector True or False as per the availability. Default value is kept as False source_path Source data path. Default value is kept as \"NA\" to save intermediate data in \"intermediate_data/\" folder. master_path Path where the output needs to be saved, ideally the same path where the analyzed data output is also saved (Default value = \".\") stats_unique Takes arguments for read_dataset (data_ingest module) function in a dictionary format to read pre-saved statistics on unique value count i.e. if measures_of_cardinality or uniqueCount_computation (data_analyzer.stats_generator module) has been computed & saved before. (Default value = {}) run_type local or emr or databricks run type. Default value is kept as local Returns Expand source code def charts_to_objects ( spark , idf , list_of_cols = \"all\" , drop_cols = [], label_col = None , event_label = 1 , bin_method = \"equal_range\" , bin_size = 10 , coverage = 1.0 , drift_detector = False , outlier_charts = False , source_path = \"NA\" , master_path = \".\" , stats_unique = {}, run_type = \"local\" , ): \"\"\" Parameters ---------- spark Spark Session idf Input dataframe list_of_cols List of columns passed for analysis (Default value = \"all\") drop_cols List of columns dropped from analysis (Default value = []) label_col Label column (Default value = None) event_label Event label (Default value = 1) bin_method Binning method equal_range or equal_frequency (Default value = \"equal_range\") bin_size Maximum bin size categories. Default value is kept as 10 coverage Maximum coverage of categories. Default value is kept as 1.0 (which is 100%) drift_detector True or False as per the availability. Default value is kept as False source_path Source data path. Default value is kept as \"NA\" to save intermediate data in \"intermediate_data/\" folder. master_path Path where the output needs to be saved, ideally the same path where the analyzed data output is also saved (Default value = \".\") stats_unique Takes arguments for read_dataset (data_ingest module) function in a dictionary format to read pre-saved statistics on unique value count i.e. if measures_of_cardinality or uniqueCount_computation (data_analyzer.stats_generator module) has been computed & saved before. (Default value = {}) run_type local or emr or databricks run type. Default value is kept as local Returns ------- \"\"\" global num_cols global cat_cols if list_of_cols == \"all\" : num_cols , cat_cols , other_cols = attributeType_segregation ( idf ) list_of_cols = num_cols + cat_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] if stats_unique == {}: remove_cols = ( uniqueCount_computation ( spark , idf , list_of_cols ) . where ( F . col ( \"unique_values\" ) < 2 ) . select ( \"attribute\" ) . rdd . flatMap ( lambda x : x ) . collect () ) else : remove_cols = ( read_dataset ( spark , ** stats_unique ) . where ( F . col ( \"unique_values\" ) < 2 ) . select ( \"attribute\" ) . rdd . flatMap ( lambda x : x ) . collect () ) list_of_cols = list ( set ([ e for e in list_of_cols if e not in ( drop_cols + remove_cols )]) ) if any ( x not in idf . columns for x in list_of_cols ) | ( len ( list_of_cols ) == 0 ): raise TypeError ( \"Invalid input for Column(s)\" ) num_cols , cat_cols , other_cols = attributeType_segregation ( idf . select ( list_of_cols )) if cat_cols : idf_cleaned = outlier_categories ( spark , idf , list_of_cols = cat_cols , coverage = coverage , max_category = bin_size ) else : idf_cleaned = idf if run_type in list ( platform_root_path . keys ()): root_path = platform_root_path [ run_type ] else : root_path = \"\" if source_path == \"NA\" : source_path = root_path + \"intermediate_data\" if drift_detector : encoding_model_exists = True binned_cols = ( spark . read . parquet ( source_path + \"/drift_statistics/attribute_binning\" ) . select ( \"attribute\" ) . rdd . flatMap ( lambda x : x ) . collect () ) to_be_binned = [ e for e in num_cols if e not in binned_cols ] else : encoding_model_exists = False binned_cols = [] to_be_binned = num_cols if to_be_binned : idf_encoded = attribute_binning ( spark , idf_cleaned , list_of_cols = to_be_binned , method_type = bin_method , bin_size = bin_size , bin_dtype = \"categorical\" , pre_existing_model = False , model_path = source_path + \"/charts_to_objects\" , output_mode = \"append\" , ) else : idf_encoded = idf_cleaned if binned_cols : idf_encoded = attribute_binning ( spark , idf_encoded , list_of_cols = binned_cols , method_type = bin_method , bin_size = bin_size , bin_dtype = \"categorical\" , pre_existing_model = True , model_path = source_path + \"/drift_statistics\" , output_mode = \"append\" , ) cutoffs_path1 = source_path + \"/charts_to_objects/attribute_binning\" cutoffs_path2 = source_path + \"/drift_statistics/attribute_binning\" idf_encoded . persist ( pyspark . StorageLevel . MEMORY_AND_DISK ) if run_type == \"local\" : local_path = master_path elif run_type == \"databricks\" : local_path = master_to_local ( master_path ) elif run_type == \"emr\" : local_path = \"report_stats\" else : raise ValueError ( \"Invalid run_type\" ) Path ( local_path ) . mkdir ( parents = True , exist_ok = True ) for idx , col in enumerate ( list_of_cols ): if col in binned_cols : cutoffs_path = cutoffs_path2 else : cutoffs_path = cutoffs_path1 if col in cat_cols : f = plot_frequency ( spark , idf_encoded , col , cutoffs_path ) f . write_json ( ends_with ( local_path ) + \"freqDist_\" + col ) if label_col : if col != label_col : f = plot_eventRate ( spark , idf_encoded , col , label_col , event_label , cutoffs_path ) f . write_json ( ends_with ( local_path ) + \"eventDist_\" + col ) if drift_detector : try : frequency_path = ( source_path + \"/drift_statistics/frequency_counts/\" + col ) idf_source = spark . read . csv ( frequency_path , header = True , inferSchema = True ) f = plot_comparative_drift ( spark , idf_encoded , idf_source , col , cutoffs_path ) f . write_json ( ends_with ( local_path ) + \"drift_\" + col ) except Exception as e : logger . error ( f \"processing failed during drift detection, error { e } \" ) pass if col in num_cols : if outlier_charts : f = plot_outlier ( spark , idf , col , split_var = None ) f . write_json ( ends_with ( local_path ) + \"outlier_\" + col ) f = plot_frequency ( spark , idf_encoded . drop ( col ) . withColumnRenamed ( col + \"_binned\" , col ), col , cutoffs_path , ) f . write_json ( ends_with ( local_path ) + \"freqDist_\" + col ) if label_col : if col != label_col : f = plot_eventRate ( spark , idf_encoded . drop ( col ) . withColumnRenamed ( col + \"_binned\" , col ), col , label_col , event_label , cutoffs_path , ) f . write_json ( ends_with ( local_path ) + \"eventDist_\" + col ) if drift_detector : try : frequency_path = ( source_path + \"/drift_statistics/frequency_counts/\" + col ) idf_source = spark . read . csv ( frequency_path , header = True , inferSchema = True ) f = plot_comparative_drift ( spark , idf_encoded . drop ( col ) . withColumnRenamed ( col + \"_binned\" , col ), idf_source , col , cutoffs_path , ) f . write_json ( ends_with ( local_path ) + \"drift_\" + col ) except Exception as e : logger . error ( f \"processing failed during drift detection, error { e } \" ) pass pd . DataFrame ( idf . dtypes , columns = [ \"attribute\" , \"data_type\" ]) . to_csv ( ends_with ( local_path ) + \"data_type.csv\" , index = False ) if run_type == \"emr\" : bash_cmd = ( \"aws s3 cp --recursive \" + ends_with ( local_path ) + \" \" + ends_with ( master_path ) ) subprocess . check_output ([ \"bash\" , \"-c\" , bash_cmd ]) def edit_binRange ( col) Parameters col The column which is passed as input and needs to be treated. The generated output will not contain any range whose value at either side is the same. Returns Expand source code def edit_binRange ( col ): \"\"\" Parameters ---------- col The column which is passed as input and needs to be treated. The generated output will not contain any range whose value at either side is the same. Returns ------- \"\"\" try : list_col = col . split ( \"-\" ) deduped_col = list ( set ( list_col )) if len ( list_col ) != len ( deduped_col ): return deduped_col [ 0 ] else : return col except Exception as e : logger . error ( f \"processing failed during edit_binRange, error { e } \" ) pass def f_edit_binRange ( col) Parameters col The column which is passed as input and needs to be treated. The generated output will not contain any range whose value at either side is the same. Returns Expand source code def edit_binRange ( col ): \"\"\" Parameters ---------- col The column which is passed as input and needs to be treated. The generated output will not contain any range whose value at either side is the same. Returns ------- \"\"\" try : list_col = col . split ( \"-\" ) deduped_col = list ( set ( list_col )) if len ( list_col ) != len ( deduped_col ): return deduped_col [ 0 ] else : return col except Exception as e : logger . error ( f \"processing failed during edit_binRange, error { e } \" ) pass def master_to_local ( master_path) Expand source code def master_to_local ( master_path ): punctuations = \":\" for x in master_path : if x in punctuations : local_path = master_path . replace ( x , \"\" ) local_path = \"/\" + local_path return local_path def plot_comparative_drift ( spark, idf, source, col, cutoffs_path) Parameters spark Spark Session idf Target dataframe which would be referred for producing the frequency charts in form of bar plots / histogram source Source dataframe of comparison col Analysis column cutoffs_path Path containing the range cut offs details for the analysis column Returns Expand source code def plot_comparative_drift ( spark , idf , source , col , cutoffs_path ): \"\"\" Parameters ---------- spark Spark Session idf Target dataframe which would be referred for producing the frequency charts in form of bar plots / histogram source Source dataframe of comparison col Analysis column cutoffs_path Path containing the range cut offs details for the analysis column Returns ------- \"\"\" odf = ( idf . groupBy ( col ) . agg (( F . count ( col ) / idf . count ()) . alias ( \"countpct_target\" )) . fillna ( np . nan , subset = [ col ]) ) if col in cat_cols : odf_pd = ( odf . join ( source . withColumnRenamed ( \"p\" , \"countpct_source\" ) . fillna ( np . nan , subset = [ col ] ), col , \"full_outer\" , ) . orderBy ( \"countpct_target\" , ascending = False ) . toPandas () ) if col in num_cols : mapping = binRange_to_binIdx ( spark , col , cutoffs_path ) odf_pd = ( odf . join ( mapping , col , \"left_outer\" ) . fillna ( np . nan , subset = [ \"bin_idx\" ]) . join ( source . fillna ( np . nan , subset = [ col ]) . select ( F . col ( col ) . alias ( \"bin_idx\" ), F . col ( \"p\" ) . alias ( \"countpct_source\" ) ), \"bin_idx\" , \"full_outer\" , ) . orderBy ( \"bin_idx\" ) . toPandas () ) odf_pd . fillna ( { col : \"Missing\" , \"countpct_source\" : 0 , \"countpct_target\" : 0 }, inplace = True ) odf_pd [ \"%_diff\" ] = ( ( odf_pd [ \"countpct_target\" ] / odf_pd [ \"countpct_source\" ]) - 1 ) * 100 fig = go . Figure () fig . add_bar ( y = list ( odf_pd . countpct_source . values ), x = odf_pd [ col ], name = \"source\" , marker = dict ( color = global_theme ), ) fig . update_traces ( overwrite = True , marker = { \"opacity\" : 0.7 }) fig . add_bar ( y = list ( odf_pd . countpct_target . values ), x = odf_pd [ col ], name = \"target\" , text = odf_pd [ \"%_diff\" ] . apply ( lambda x : \" {0:0.2f} %\" . format ( x )), marker = dict ( color = global_theme ), ) fig . update_traces ( textposition = \"outside\" ) fig . update_layout ( paper_bgcolor = global_paper_bg_color , plot_bgcolor = global_plot_bg_color , showlegend = False , ) fig . update_layout ( title_text = str ( \"Drift Comparison for \" + col + \"<br><sup>(L->R : Source->Target)</sup>\" ) ) fig . update_traces ( marker = dict ( color = global_theme )) fig . update_xaxes ( type = \"category\" ) # fig.add_trace(go.Scatter(x=odf_pd[col], y=odf_pd.countpct_target.values, mode='lines+markers', # line=dict(color=px.colors.qualitative.Antique[10], width=3, dash='dot'))) fig . update_layout ( xaxis_tickfont_size = 14 , yaxis = dict ( title = \"frequency\" , titlefont_size = 16 , tickfont_size = 14 ), ) return fig def plot_eventRate ( spark, idf, col, label_col, event_label, cutoffs_path) Parameters spark Spark Session idf Input dataframe which would be referred for producing the frequency charts in form of bar plots / histogram col Analysis column label_col Label column event_label Event label cutoffs_path Path containing the range cut offs details for the analysis column Returns Expand source code def plot_eventRate ( spark , idf , col , label_col , event_label , cutoffs_path ): \"\"\" Parameters ---------- spark Spark Session idf Input dataframe which would be referred for producing the frequency charts in form of bar plots / histogram col Analysis column label_col Label column event_label Event label cutoffs_path Path containing the range cut offs details for the analysis column Returns ------- \"\"\" odf = ( idf . withColumn ( label_col , F . when ( F . col ( label_col ) == event_label , 1 ) . otherwise ( 0 ) ) . groupBy ( col ) . pivot ( label_col ) . count () . fillna ( 0 , subset = [ \"0\" , \"1\" ]) . withColumn ( \"event_rate\" , 100 * ( F . col ( \"1\" ) / ( F . col ( \"0\" ) + F . col ( \"1\" )))) . withColumn ( \"attribute_name\" , F . lit ( col )) . withColumn ( col , f_edit_binRange ( col )) ) if col in cat_cols : odf_pd = odf . orderBy ( \"event_rate\" , ascending = False ) . toPandas () odf_pd . loc [ odf_pd [ col ] == \"others\" , col ] = \"others*\" if col in num_cols : mapping = binRange_to_binIdx ( spark , col , cutoffs_path ) odf_pd = odf . join ( mapping , col , \"left_outer\" ) . orderBy ( \"bin_idx\" ) . toPandas () fig = px . bar ( odf_pd , x = col , y = \"event_rate\" , text = odf_pd [ \"event_rate\" ] . apply ( lambda x : \" {0:1.2f} %\" . format ( x )), color_discrete_sequence = global_theme , ) fig . update_traces ( textposition = \"outside\" ) fig . update_layout ( title_text = str ( \"Event Rate Distribution for \" + str ( col . upper ()) + str ( \" [Target Variable : \" + str ( event_label ) + str ( \"]\" )) ) ) fig . update_xaxes ( type = \"category\" ) fig . layout . plot_bgcolor = global_plot_bg_color fig . layout . paper_bgcolor = global_paper_bg_color # plotly.offline.plot(fig, auto_open=False, validate=False, filename=f\"{base_loc}/{file_name_}feat_analysis_label.html\") return fig def plot_frequency ( spark, idf, col, cutoffs_path) Parameters spark Spark Session idf Input dataframe which would be referred for producing the frequency charts in form of bar plots / histograms col Analysis column cutoffs_path Path containing the range cut offs details for the analysis column Returns Expand source code def plot_frequency ( spark , idf , col , cutoffs_path ): \"\"\" Parameters ---------- spark Spark Session idf Input dataframe which would be referred for producing the frequency charts in form of bar plots / histograms col Analysis column cutoffs_path Path containing the range cut offs details for the analysis column Returns ------- \"\"\" odf = ( idf . groupBy ( col ) . count () . withColumn ( \"count_%\" , 100 * ( F . col ( \"count\" ) / F . sum ( \"count\" ) . over ( Window . partitionBy ())), ) . withColumn ( col , f_edit_binRange ( col )) ) if col in cat_cols : odf_pd = odf . orderBy ( \"count\" , ascending = False ) . toPandas () . fillna ( \"Missing\" ) odf_pd . loc [ odf_pd [ col ] == \"others\" , col ] = \"others*\" if col in num_cols : mapping = binRange_to_binIdx ( spark , col , cutoffs_path ) odf_pd = ( odf . join ( mapping , col , \"left_outer\" ) . orderBy ( \"bin_idx\" ) . toPandas () . fillna ( \"Missing\" ) ) fig = px . bar ( odf_pd , x = col , y = \"count\" , text = odf_pd [ \"count_%\" ] . apply ( lambda x : \" {0:1.2f} %\" . format ( x )), color_discrete_sequence = global_theme , ) fig . update_traces ( textposition = \"outside\" ) fig . update_layout ( title_text = str ( \"Frequency Distribution for \" + str ( col . upper ()))) fig . update_xaxes ( type = \"category\" ) # fig.update_layout(barmode='stack', xaxis={'categoryorder':'total descending'}) fig . layout . plot_bgcolor = global_plot_bg_color fig . layout . paper_bgcolor = global_paper_bg_color # plotly.offline.plot(fig, auto_open=False, validate=False, filename=f\"{base_loc}/{file_name_}bar_graph.html\") return fig def plot_outlier ( spark, idf, col, split_var=None, sample_size=500000) Parameters spark Spark Session idf Input dataframe which would be referred for capturing the outliers in form of violin charts col Analysis column split_var Column which is needed. Default value is kept as None sample_size Maximum Sample size. Default value is kept as 500000 Returns Expand source code def plot_outlier ( spark , idf , col , split_var = None , sample_size = 500000 ): \"\"\" Parameters ---------- spark Spark Session idf Input dataframe which would be referred for capturing the outliers in form of violin charts col Analysis column split_var Column which is needed. Default value is kept as None sample_size Maximum Sample size. Default value is kept as 500000 Returns ------- \"\"\" idf_sample = idf . select ( col ) . sample ( False , min ( 1.0 , float ( sample_size ) / idf . count ()), 0 ) idf_sample . persist ( pyspark . StorageLevel . MEMORY_AND_DISK ) . count () idf_imputed = imputation_MMM ( spark , idf_sample ) idf_pd = idf_imputed . toPandas () fig = px . violin ( idf_pd , y = col , color = split_var , box = True , points = \"outliers\" , color_discrete_sequence = [ global_theme_r [ 8 ], global_theme_r [ 4 ]], ) fig . layout . plot_bgcolor = global_plot_bg_color fig . layout . paper_bgcolor = global_paper_bg_color fig . update_layout ( legend = dict ( orientation = \"h\" , x = 0.5 , yanchor = \"bottom\" , xanchor = \"center\" ) ) return fig def save_stats ( spark, idf, master_path, function_name, reread=False, run_type='local') Parameters spark Spark Session idf input dataframe master_path Path to master folder under which all statistics will be saved in a csv file format. function_name Function Name for which statistics need to be saved. file name will be saved as csv reread option to reread. Default value is kept as False run_type local or emr or databricks based on the mode of execution. Default value is kept as local Returns Expand source code def save_stats ( spark , idf , master_path , function_name , reread = False , run_type = \"local\" ): \"\"\" Parameters ---------- spark Spark Session idf input dataframe master_path Path to master folder under which all statistics will be saved in a csv file format. function_name Function Name for which statistics need to be saved. file name will be saved as csv reread option to reread. Default value is kept as False run_type local or emr or databricks based on the mode of execution. Default value is kept as local Returns ------- \"\"\" if run_type == \"local\" : local_path = master_path elif run_type == \"databricks\" : local_path = master_to_local ( master_path ) elif run_type == \"emr\" : local_path = \"report_stats\" else : raise ValueError ( \"Invalid run_type\" ) Path ( local_path ) . mkdir ( parents = True , exist_ok = True ) idf . toPandas () . to_csv ( ends_with ( local_path ) + function_name + \".csv\" , index = False ) if run_type == \"emr\" : bash_cmd = ( \"aws s3 cp \" + ends_with ( local_path ) + function_name + \".csv \" + ends_with ( master_path ) ) subprocess . check_output ([ \"bash\" , \"-c\" , bash_cmd ]) if reread : odf = spark . read . csv ( ends_with ( master_path ) + function_name + \".csv\" , header = True , inferSchema = True , ) return odf","title":"<code>report_preprocessing</code>"},{"location":"api/data_report/report_preprocessing.html#report_preprocessing","text":"Expand source code import subprocess from pathlib import Path import numpy as np import pandas as pd import plotly.express as px import plotly.graph_objects as go import pyspark from loguru import logger from pyspark.sql import functions as F from pyspark.sql import types as T from pyspark.sql.window import Window from anovos.data_analyzer.stats_generator import uniqueCount_computation from anovos.data_ingest.data_ingest import read_dataset from anovos.data_transformer.transformers import ( outlier_categories , imputation_MMM , attribute_binning , ) from anovos.shared.utils import attributeType_segregation , ends_with from ..shared.utils import platform_root_path import warnings warnings . filterwarnings ( \"ignore\" ) global_theme = px . colors . sequential . Plasma global_theme_r = px . colors . sequential . Plasma_r global_plot_bg_color = \"rgba(0,0,0,0)\" global_paper_bg_color = \"rgba(0,0,0,0)\" num_cols = [] cat_cols = [] def master_to_local ( master_path ): punctuations = \":\" for x in master_path : if x in punctuations : local_path = master_path . replace ( x , \"\" ) local_path = \"/\" + local_path return local_path def save_stats ( spark , idf , master_path , function_name , reread = False , run_type = \"local\" ): \"\"\" Parameters ---------- spark Spark Session idf input dataframe master_path Path to master folder under which all statistics will be saved in a csv file format. function_name Function Name for which statistics need to be saved. file name will be saved as csv reread option to reread. Default value is kept as False run_type local or emr or databricks based on the mode of execution. Default value is kept as local Returns ------- \"\"\" if run_type == \"local\" : local_path = master_path elif run_type == \"databricks\" : local_path = master_to_local ( master_path ) elif run_type == \"emr\" : local_path = \"report_stats\" else : raise ValueError ( \"Invalid run_type\" ) Path ( local_path ) . mkdir ( parents = True , exist_ok = True ) idf . toPandas () . to_csv ( ends_with ( local_path ) + function_name + \".csv\" , index = False ) if run_type == \"emr\" : bash_cmd = ( \"aws s3 cp \" + ends_with ( local_path ) + function_name + \".csv \" + ends_with ( master_path ) ) subprocess . check_output ([ \"bash\" , \"-c\" , bash_cmd ]) if reread : odf = spark . read . csv ( ends_with ( master_path ) + function_name + \".csv\" , header = True , inferSchema = True , ) return odf def edit_binRange ( col ): \"\"\" Parameters ---------- col The column which is passed as input and needs to be treated. The generated output will not contain any range whose value at either side is the same. Returns ------- \"\"\" try : list_col = col . split ( \"-\" ) deduped_col = list ( set ( list_col )) if len ( list_col ) != len ( deduped_col ): return deduped_col [ 0 ] else : return col except Exception as e : logger . error ( f \"processing failed during edit_binRange, error { e } \" ) pass f_edit_binRange = F . udf ( edit_binRange , T . StringType ()) def binRange_to_binIdx ( spark , col , cutoffs_path ): \"\"\" Parameters ---------- spark Spark Session col The input column which is needed to by mapped with respective index cutoffs_path paths containing the range cutoffs applicable for each index Returns ------- \"\"\" bin_cutoffs = ( spark . read . parquet ( cutoffs_path ) . where ( F . col ( \"attribute\" ) == col ) . select ( \"parameters\" ) . rdd . flatMap ( lambda x : x ) . collect ()[ 0 ] ) bin_ranges = [] max_cat = len ( bin_cutoffs ) + 1 for idx in range ( 0 , max_cat ): if idx == 0 : bin_ranges . append ( \"<= \" + str ( round ( bin_cutoffs [ idx ], 4 ))) elif idx < ( max_cat - 1 ): bin_ranges . append ( str ( round ( bin_cutoffs [ idx - 1 ], 4 )) + \"-\" + str ( round ( bin_cutoffs [ idx ], 4 )) ) else : bin_ranges . append ( \"> \" + str ( round ( bin_cutoffs [ idx - 1 ], 4 ))) mapping = spark . createDataFrame ( zip ( range ( 1 , max_cat + 1 ), bin_ranges ), schema = [ \"bin_idx\" , col ] ) return mapping def plot_frequency ( spark , idf , col , cutoffs_path ): \"\"\" Parameters ---------- spark Spark Session idf Input dataframe which would be referred for producing the frequency charts in form of bar plots / histograms col Analysis column cutoffs_path Path containing the range cut offs details for the analysis column Returns ------- \"\"\" odf = ( idf . groupBy ( col ) . count () . withColumn ( \"count_%\" , 100 * ( F . col ( \"count\" ) / F . sum ( \"count\" ) . over ( Window . partitionBy ())), ) . withColumn ( col , f_edit_binRange ( col )) ) if col in cat_cols : odf_pd = odf . orderBy ( \"count\" , ascending = False ) . toPandas () . fillna ( \"Missing\" ) odf_pd . loc [ odf_pd [ col ] == \"others\" , col ] = \"others*\" if col in num_cols : mapping = binRange_to_binIdx ( spark , col , cutoffs_path ) odf_pd = ( odf . join ( mapping , col , \"left_outer\" ) . orderBy ( \"bin_idx\" ) . toPandas () . fillna ( \"Missing\" ) ) fig = px . bar ( odf_pd , x = col , y = \"count\" , text = odf_pd [ \"count_%\" ] . apply ( lambda x : \" {0:1.2f} %\" . format ( x )), color_discrete_sequence = global_theme , ) fig . update_traces ( textposition = \"outside\" ) fig . update_layout ( title_text = str ( \"Frequency Distribution for \" + str ( col . upper ()))) fig . update_xaxes ( type = \"category\" ) # fig.update_layout(barmode='stack', xaxis={'categoryorder':'total descending'}) fig . layout . plot_bgcolor = global_plot_bg_color fig . layout . paper_bgcolor = global_paper_bg_color # plotly.offline.plot(fig, auto_open=False, validate=False, filename=f\"{base_loc}/{file_name_}bar_graph.html\") return fig def plot_outlier ( spark , idf , col , split_var = None , sample_size = 500000 ): \"\"\" Parameters ---------- spark Spark Session idf Input dataframe which would be referred for capturing the outliers in form of violin charts col Analysis column split_var Column which is needed. Default value is kept as None sample_size Maximum Sample size. Default value is kept as 500000 Returns ------- \"\"\" idf_sample = idf . select ( col ) . sample ( False , min ( 1.0 , float ( sample_size ) / idf . count ()), 0 ) idf_sample . persist ( pyspark . StorageLevel . MEMORY_AND_DISK ) . count () idf_imputed = imputation_MMM ( spark , idf_sample ) idf_pd = idf_imputed . toPandas () fig = px . violin ( idf_pd , y = col , color = split_var , box = True , points = \"outliers\" , color_discrete_sequence = [ global_theme_r [ 8 ], global_theme_r [ 4 ]], ) fig . layout . plot_bgcolor = global_plot_bg_color fig . layout . paper_bgcolor = global_paper_bg_color fig . update_layout ( legend = dict ( orientation = \"h\" , x = 0.5 , yanchor = \"bottom\" , xanchor = \"center\" ) ) return fig def plot_eventRate ( spark , idf , col , label_col , event_label , cutoffs_path ): \"\"\" Parameters ---------- spark Spark Session idf Input dataframe which would be referred for producing the frequency charts in form of bar plots / histogram col Analysis column label_col Label column event_label Event label cutoffs_path Path containing the range cut offs details for the analysis column Returns ------- \"\"\" odf = ( idf . withColumn ( label_col , F . when ( F . col ( label_col ) == event_label , 1 ) . otherwise ( 0 ) ) . groupBy ( col ) . pivot ( label_col ) . count () . fillna ( 0 , subset = [ \"0\" , \"1\" ]) . withColumn ( \"event_rate\" , 100 * ( F . col ( \"1\" ) / ( F . col ( \"0\" ) + F . col ( \"1\" )))) . withColumn ( \"attribute_name\" , F . lit ( col )) . withColumn ( col , f_edit_binRange ( col )) ) if col in cat_cols : odf_pd = odf . orderBy ( \"event_rate\" , ascending = False ) . toPandas () odf_pd . loc [ odf_pd [ col ] == \"others\" , col ] = \"others*\" if col in num_cols : mapping = binRange_to_binIdx ( spark , col , cutoffs_path ) odf_pd = odf . join ( mapping , col , \"left_outer\" ) . orderBy ( \"bin_idx\" ) . toPandas () fig = px . bar ( odf_pd , x = col , y = \"event_rate\" , text = odf_pd [ \"event_rate\" ] . apply ( lambda x : \" {0:1.2f} %\" . format ( x )), color_discrete_sequence = global_theme , ) fig . update_traces ( textposition = \"outside\" ) fig . update_layout ( title_text = str ( \"Event Rate Distribution for \" + str ( col . upper ()) + str ( \" [Target Variable : \" + str ( event_label ) + str ( \"]\" )) ) ) fig . update_xaxes ( type = \"category\" ) fig . layout . plot_bgcolor = global_plot_bg_color fig . layout . paper_bgcolor = global_paper_bg_color # plotly.offline.plot(fig, auto_open=False, validate=False, filename=f\"{base_loc}/{file_name_}feat_analysis_label.html\") return fig def plot_comparative_drift ( spark , idf , source , col , cutoffs_path ): \"\"\" Parameters ---------- spark Spark Session idf Target dataframe which would be referred for producing the frequency charts in form of bar plots / histogram source Source dataframe of comparison col Analysis column cutoffs_path Path containing the range cut offs details for the analysis column Returns ------- \"\"\" odf = ( idf . groupBy ( col ) . agg (( F . count ( col ) / idf . count ()) . alias ( \"countpct_target\" )) . fillna ( np . nan , subset = [ col ]) ) if col in cat_cols : odf_pd = ( odf . join ( source . withColumnRenamed ( \"p\" , \"countpct_source\" ) . fillna ( np . nan , subset = [ col ] ), col , \"full_outer\" , ) . orderBy ( \"countpct_target\" , ascending = False ) . toPandas () ) if col in num_cols : mapping = binRange_to_binIdx ( spark , col , cutoffs_path ) odf_pd = ( odf . join ( mapping , col , \"left_outer\" ) . fillna ( np . nan , subset = [ \"bin_idx\" ]) . join ( source . fillna ( np . nan , subset = [ col ]) . select ( F . col ( col ) . alias ( \"bin_idx\" ), F . col ( \"p\" ) . alias ( \"countpct_source\" ) ), \"bin_idx\" , \"full_outer\" , ) . orderBy ( \"bin_idx\" ) . toPandas () ) odf_pd . fillna ( { col : \"Missing\" , \"countpct_source\" : 0 , \"countpct_target\" : 0 }, inplace = True ) odf_pd [ \"%_diff\" ] = ( ( odf_pd [ \"countpct_target\" ] / odf_pd [ \"countpct_source\" ]) - 1 ) * 100 fig = go . Figure () fig . add_bar ( y = list ( odf_pd . countpct_source . values ), x = odf_pd [ col ], name = \"source\" , marker = dict ( color = global_theme ), ) fig . update_traces ( overwrite = True , marker = { \"opacity\" : 0.7 }) fig . add_bar ( y = list ( odf_pd . countpct_target . values ), x = odf_pd [ col ], name = \"target\" , text = odf_pd [ \"%_diff\" ] . apply ( lambda x : \" {0:0.2f} %\" . format ( x )), marker = dict ( color = global_theme ), ) fig . update_traces ( textposition = \"outside\" ) fig . update_layout ( paper_bgcolor = global_paper_bg_color , plot_bgcolor = global_plot_bg_color , showlegend = False , ) fig . update_layout ( title_text = str ( \"Drift Comparison for \" + col + \"<br><sup>(L->R : Source->Target)</sup>\" ) ) fig . update_traces ( marker = dict ( color = global_theme )) fig . update_xaxes ( type = \"category\" ) # fig.add_trace(go.Scatter(x=odf_pd[col], y=odf_pd.countpct_target.values, mode='lines+markers', # line=dict(color=px.colors.qualitative.Antique[10], width=3, dash='dot'))) fig . update_layout ( xaxis_tickfont_size = 14 , yaxis = dict ( title = \"frequency\" , titlefont_size = 16 , tickfont_size = 14 ), ) return fig def charts_to_objects ( spark , idf , list_of_cols = \"all\" , drop_cols = [], label_col = None , event_label = 1 , bin_method = \"equal_range\" , bin_size = 10 , coverage = 1.0 , drift_detector = False , outlier_charts = False , source_path = \"NA\" , master_path = \".\" , stats_unique = {}, run_type = \"local\" , ): \"\"\" Parameters ---------- spark Spark Session idf Input dataframe list_of_cols List of columns passed for analysis (Default value = \"all\") drop_cols List of columns dropped from analysis (Default value = []) label_col Label column (Default value = None) event_label Event label (Default value = 1) bin_method Binning method equal_range or equal_frequency (Default value = \"equal_range\") bin_size Maximum bin size categories. Default value is kept as 10 coverage Maximum coverage of categories. Default value is kept as 1.0 (which is 100%) drift_detector True or False as per the availability. Default value is kept as False source_path Source data path. Default value is kept as \"NA\" to save intermediate data in \"intermediate_data/\" folder. master_path Path where the output needs to be saved, ideally the same path where the analyzed data output is also saved (Default value = \".\") stats_unique Takes arguments for read_dataset (data_ingest module) function in a dictionary format to read pre-saved statistics on unique value count i.e. if measures_of_cardinality or uniqueCount_computation (data_analyzer.stats_generator module) has been computed & saved before. (Default value = {}) run_type local or emr or databricks run type. Default value is kept as local Returns ------- \"\"\" global num_cols global cat_cols if list_of_cols == \"all\" : num_cols , cat_cols , other_cols = attributeType_segregation ( idf ) list_of_cols = num_cols + cat_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] if stats_unique == {}: remove_cols = ( uniqueCount_computation ( spark , idf , list_of_cols ) . where ( F . col ( \"unique_values\" ) < 2 ) . select ( \"attribute\" ) . rdd . flatMap ( lambda x : x ) . collect () ) else : remove_cols = ( read_dataset ( spark , ** stats_unique ) . where ( F . col ( \"unique_values\" ) < 2 ) . select ( \"attribute\" ) . rdd . flatMap ( lambda x : x ) . collect () ) list_of_cols = list ( set ([ e for e in list_of_cols if e not in ( drop_cols + remove_cols )]) ) if any ( x not in idf . columns for x in list_of_cols ) | ( len ( list_of_cols ) == 0 ): raise TypeError ( \"Invalid input for Column(s)\" ) num_cols , cat_cols , other_cols = attributeType_segregation ( idf . select ( list_of_cols )) if cat_cols : idf_cleaned = outlier_categories ( spark , idf , list_of_cols = cat_cols , coverage = coverage , max_category = bin_size ) else : idf_cleaned = idf if run_type in list ( platform_root_path . keys ()): root_path = platform_root_path [ run_type ] else : root_path = \"\" if source_path == \"NA\" : source_path = root_path + \"intermediate_data\" if drift_detector : encoding_model_exists = True binned_cols = ( spark . read . parquet ( source_path + \"/drift_statistics/attribute_binning\" ) . select ( \"attribute\" ) . rdd . flatMap ( lambda x : x ) . collect () ) to_be_binned = [ e for e in num_cols if e not in binned_cols ] else : encoding_model_exists = False binned_cols = [] to_be_binned = num_cols if to_be_binned : idf_encoded = attribute_binning ( spark , idf_cleaned , list_of_cols = to_be_binned , method_type = bin_method , bin_size = bin_size , bin_dtype = \"categorical\" , pre_existing_model = False , model_path = source_path + \"/charts_to_objects\" , output_mode = \"append\" , ) else : idf_encoded = idf_cleaned if binned_cols : idf_encoded = attribute_binning ( spark , idf_encoded , list_of_cols = binned_cols , method_type = bin_method , bin_size = bin_size , bin_dtype = \"categorical\" , pre_existing_model = True , model_path = source_path + \"/drift_statistics\" , output_mode = \"append\" , ) cutoffs_path1 = source_path + \"/charts_to_objects/attribute_binning\" cutoffs_path2 = source_path + \"/drift_statistics/attribute_binning\" idf_encoded . persist ( pyspark . StorageLevel . MEMORY_AND_DISK ) if run_type == \"local\" : local_path = master_path elif run_type == \"databricks\" : local_path = master_to_local ( master_path ) elif run_type == \"emr\" : local_path = \"report_stats\" else : raise ValueError ( \"Invalid run_type\" ) Path ( local_path ) . mkdir ( parents = True , exist_ok = True ) for idx , col in enumerate ( list_of_cols ): if col in binned_cols : cutoffs_path = cutoffs_path2 else : cutoffs_path = cutoffs_path1 if col in cat_cols : f = plot_frequency ( spark , idf_encoded , col , cutoffs_path ) f . write_json ( ends_with ( local_path ) + \"freqDist_\" + col ) if label_col : if col != label_col : f = plot_eventRate ( spark , idf_encoded , col , label_col , event_label , cutoffs_path ) f . write_json ( ends_with ( local_path ) + \"eventDist_\" + col ) if drift_detector : try : frequency_path = ( source_path + \"/drift_statistics/frequency_counts/\" + col ) idf_source = spark . read . csv ( frequency_path , header = True , inferSchema = True ) f = plot_comparative_drift ( spark , idf_encoded , idf_source , col , cutoffs_path ) f . write_json ( ends_with ( local_path ) + \"drift_\" + col ) except Exception as e : logger . error ( f \"processing failed during drift detection, error { e } \" ) pass if col in num_cols : if outlier_charts : f = plot_outlier ( spark , idf , col , split_var = None ) f . write_json ( ends_with ( local_path ) + \"outlier_\" + col ) f = plot_frequency ( spark , idf_encoded . drop ( col ) . withColumnRenamed ( col + \"_binned\" , col ), col , cutoffs_path , ) f . write_json ( ends_with ( local_path ) + \"freqDist_\" + col ) if label_col : if col != label_col : f = plot_eventRate ( spark , idf_encoded . drop ( col ) . withColumnRenamed ( col + \"_binned\" , col ), col , label_col , event_label , cutoffs_path , ) f . write_json ( ends_with ( local_path ) + \"eventDist_\" + col ) if drift_detector : try : frequency_path = ( source_path + \"/drift_statistics/frequency_counts/\" + col ) idf_source = spark . read . csv ( frequency_path , header = True , inferSchema = True ) f = plot_comparative_drift ( spark , idf_encoded . drop ( col ) . withColumnRenamed ( col + \"_binned\" , col ), idf_source , col , cutoffs_path , ) f . write_json ( ends_with ( local_path ) + \"drift_\" + col ) except Exception as e : logger . error ( f \"processing failed during drift detection, error { e } \" ) pass pd . DataFrame ( idf . dtypes , columns = [ \"attribute\" , \"data_type\" ]) . to_csv ( ends_with ( local_path ) + \"data_type.csv\" , index = False ) if run_type == \"emr\" : bash_cmd = ( \"aws s3 cp --recursive \" + ends_with ( local_path ) + \" \" + ends_with ( master_path ) ) subprocess . check_output ([ \"bash\" , \"-c\" , bash_cmd ])","title":"report_preprocessing"},{"location":"api/data_report/report_preprocessing.html#functions","text":"def binRange_to_binIdx ( spark, col, cutoffs_path)","title":"Functions"},{"location":"api/data_transformer/_index.html","text":"Overview Sub-modules anovos.data_transformer.datetime Datetime module supports various transformations related to date and timestamp datatype columns. All available functions in this release can be \u2026 anovos.data_transformer.transformers The data transformer module supports selected pre-processing & transformation functions, such as binning, encoding, scaling, imputation, to name a \u2026","title":"Overview"},{"location":"api/data_transformer/_index.html#overview","text":"","title":"Overview"},{"location":"api/data_transformer/_index.html#sub-modules","text":"anovos.data_transformer.datetime Datetime module supports various transformations related to date and timestamp datatype columns. All available functions in this release can be \u2026 anovos.data_transformer.transformers The data transformer module supports selected pre-processing & transformation functions, such as binning, encoding, scaling, imputation, to name a \u2026","title":"Sub-modules"},{"location":"api/data_transformer/datetime.html","text":"datetime Datetime module supports various transformations related to date and timestamp datatype columns. All available functions in this release can be classified into the following 4 categories: Conversion: - Between Timestamp and Epoch (timestamp_to_unix and unix_to_timestamp) - Between Timestamp and String (timestamp_to_string and string_to_timestamp) - Between Date Formats (dateformat_conversion) - Between Time Zones (timezone_conversion) Calculation: - Time difference - [Timestamp 1 - Timestamp 2] (time_diff) - Time elapsed - [Current - Given Timestamp] (time_elapsed) - Adding/subtracting time units (adding_timeUnits) - Aggregate features at X granularity level (aggregator) - Aggregate features with window frame (window_aggregator) - Lagged features - lagged date and time diff from the lagged date (lagged_ts) Extraction: - Time component extraction (timeUnits_extraction) - Start/end of month/year/quarter (start_of_month, end_of_month, start_of_year, end_of_year, start_of_quarter and end_of_quarter) Binary features: - Timestamp comparison (timestamp_comparison) - Is start/end of month/year/quarter nor not (is_monthStart, is_monthEnd, is_yearStart, is_yearEnd, is_quarterStart, is_quarterEnd) - Is first half of the year/selected hours/leap year/weekend or not (is_yearFirstHalf, is_selectedHour, is_leapYear and is_weekend) Expand source code \"\"\" Datetime module supports various transformations related to date and timestamp datatype columns. All available functions in this release can be classified into the following 4 categories: Conversion: - Between Timestamp and Epoch (timestamp_to_unix and unix_to_timestamp) - Between Timestamp and String (timestamp_to_string and string_to_timestamp) - Between Date Formats (dateformat_conversion) - Between Time Zones (timezone_conversion) Calculation: - Time difference - [Timestamp 1 - Timestamp 2] (time_diff) - Time elapsed - [Current - Given Timestamp] (time_elapsed) - Adding/subtracting time units (adding_timeUnits) - Aggregate features at X granularity level (aggregator) - Aggregate features with window frame (window_aggregator) - Lagged features - lagged date and time diff from the lagged date (lagged_ts) Extraction: - Time component extraction (timeUnits_extraction) - Start/end of month/year/quarter (start_of_month, end_of_month, start_of_year, end_of_year, start_of_quarter and end_of_quarter) Binary features: - Timestamp comparison (timestamp_comparison) - Is start/end of month/year/quarter nor not (is_monthStart, is_monthEnd, is_yearStart, is_yearEnd, is_quarterStart, is_quarterEnd) - Is first half of the year/selected hours/leap year/weekend or not (is_yearFirstHalf, is_selectedHour, is_leapYear and is_weekend) \"\"\" import calendar import warnings import pytz from pyspark.sql import Window from pyspark.sql import functions as F from pyspark.sql import types as T from datetime import datetime as dt def argument_checker ( func_name , args ): \"\"\" Parameters ---------- func_name function name for which argument needs to be check args arguments to check in dictionary format Returns ------- List list of columns to analyze \"\"\" list_of_cols = args [ \"list_of_cols\" ] all_columns = args [ \"all_columns\" ] if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if any ( x not in all_columns for x in list_of_cols ): raise TypeError ( \"Invalid input for Column(s)\" ) if len ( list_of_cols ) == 0 : warnings . warn ( \"No timestamp conversion - No column(s) to convert\" ) return [] if func_name not in [ \"aggregator\" ]: if args [ \"output_mode\" ] not in ( \"replace\" , \"append\" ): raise TypeError ( \"Invalid input for output_mode\" ) if func_name in [ \"timestamp_to_unix\" , \"unix_to_timestamp\" ]: if args [ \"precision\" ] not in ( \"ms\" , \"s\" ): raise TypeError ( \"Invalid input for precision\" ) if args [ \"tz\" ] not in ( \"local\" , \"gmt\" , \"utc\" ): raise TypeError ( \"Invalid input for timezone\" ) if func_name in [ \"string_to_timestamp\" ]: if args [ \"output_type\" ] not in ( \"ts\" , \"dt\" ): raise TypeError ( \"Invalid input for output_type\" ) if func_name in [ \"timeUnits_extraction\" ]: if any ( x not in args [ \"all_units\" ] for x in args [ \"units\" ]): raise TypeError ( \"Invalid input for Unit(s)\" ) if func_name in [ \"adding_timeUnits\" ]: if args [ \"unit\" ] not in ( args [ \"all_units\" ] + [( e + \"s\" ) for e in args [ \"all_units\" ]] ): raise TypeError ( \"Invalid input for Unit\" ) if func_name in [ \"timestamp_comparison\" ]: if args [ \"comparison_type\" ] not in args [ \"all_types\" ]: raise TypeError ( \"Invalid input for comparison_type\" ) if func_name in [ \"is_selectedHour\" ]: hours = list ( range ( 0 , 24 )) if args [ \"start_hour\" ] not in hours : raise TypeError ( \"Invalid input for start_hour\" ) if args [ \"end_hour\" ] not in hours : raise TypeError ( \"Invalid input for end_hour\" ) if func_name in [ \"window_aggregator\" ]: if any ( x not in args [ \"all_aggs\" ] for x in args [ \"list_of_aggs\" ]): raise TypeError ( \"Invalid input for Aggregate Function(s)\" ) if args [ \"window_type\" ] not in ( \"expanding\" , \"rolling\" ): raise TypeError ( \"Invalid input for Window Type\" ) if ( args [ \"window_type\" ] == \"rolling\" ) & ( not str ( args [ \"window_size\" ]) . isnumeric () ): raise TypeError ( \"Invalid input for Window Size\" ) if func_name in [ \"aggregator\" ]: if any ( x not in args [ \"all_aggs\" ] for x in args [ \"list_of_aggs\" ]): raise TypeError ( \"Invalid input for Aggregate Function(s)\" ) if args [ \"time_col\" ] not in all_columns : raise TypeError ( \"Invalid input for time_col\" ) if func_name in [ \"lagged_ts\" ]: if not str ( args [ \"lag\" ]) . isnumeric (): raise TypeError ( \"Invalid input for Lag\" ) if args [ \"output_type\" ] not in ( \"ts\" , \"ts_diff\" ): raise TypeError ( \"Invalid input for output_type\" ) return list_of_cols def timestamp_to_unix ( spark , idf , list_of_cols , precision = \"s\" , tz = \"local\" , output_mode = \"replace\" ): \"\"\" Convert timestamp columns in a specified time zone to Unix time stamp in seconds or milliseconds. Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". precision \"ms\", \"s\". \"ms\" option returns the number of milliseconds from the unix epoch (1970-01-01 00:00:00 UTC) . \"s\" option returns the number of seconds from the unix epoch. (Default value = \"s\") tz \"local\", \"gmt\", \"utc\". Timezone of the input column(s) (Default value = \"local\") output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column. \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_unix\" e.g. column X is appended as X_unix. (Default value = \"replace\") Returns ------- DataFrame \"\"\" tz = tz . lower () list_of_cols = argument_checker ( \"timestamp_to_unix\" , { \"list_of_cols\" : list_of_cols , \"all_columns\" : idf . columns , \"output_mode\" : output_mode , \"precision\" : precision , \"tz\" : tz , }, ) if not list_of_cols : return idf localtz = ( spark . sql ( \"SET spark.sql.session.timeZone\" ) . select ( \"value\" ) . rdd . flatMap ( lambda x : x ) . collect ()[ 0 ] ) factor = { \"ms\" : 1000 , \"s\" : 1 } odf = idf for i in list_of_cols : if ( tz in ( \"gmt\" , \"utc\" )) & ( localtz . lower () not in ( \"gmt\" , \"utc\" )): odf = odf . withColumn ( i + \"_local\" , F . from_utc_timestamp ( i , localtz )) else : odf = odf . withColumn ( i + \"_local\" , F . col ( i )) modify_col = { \"replace\" : i , \"append\" : i + \"_unix\" } odf = odf . withColumn ( modify_col [ output_mode ], ( F . col ( i + \"_local\" ) . cast ( \"double\" ) * factor [ precision ]) . cast ( \"long\" ), ) . drop ( i + \"_local\" ) return odf def unix_to_timestamp ( spark , idf , list_of_cols , precision = \"s\" , tz = \"local\" , output_mode = \"replace\" ): \"\"\" Convert the number of seconds or milliseconds from unix epoch (1970-01-01 00:00:00 UTC) to a timestamp column in the specified time zone. Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". precision \"ms\", \"s\". \"ms\" treats the input columns as the number of milliseconds from the unix epoch (1970-01-01 00:00:00 UTC) . \"s\" treats the input columns as the number of seconds from the unix epoch. (Default value = \"s\") tz \"local\", \"gmt\", \"utc\". timezone of the output column(s) (Default value = \"local\") output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column. \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_ts\" e.g. column X is appended as X_ts. (Default value = \"replace\") Returns ------- DataFrame \"\"\" tz = tz . lower () list_of_cols = argument_checker ( \"unix_to_timestamp\" , { \"list_of_cols\" : list_of_cols , \"all_columns\" : idf . columns , \"output_mode\" : output_mode , \"precision\" : precision , \"tz\" : tz , }, ) if not list_of_cols : return idf localtz = ( spark . sql ( \"SET spark.sql.session.timeZone\" ) . select ( \"value\" ) . rdd . flatMap ( lambda x : x ) . collect ()[ 0 ] ) factor = { \"ms\" : 1000 , \"s\" : 1 } odf = idf for i in list_of_cols : modify_col = { \"replace\" : i , \"append\" : i + \"_ts\" } odf = odf . withColumn ( modify_col [ output_mode ], F . to_timestamp ( F . col ( i ) / factor [ precision ]) ) if ( tz in ( \"gmt\" , \"utc\" )) & ( localtz . lower () not in ( \"gmt\" , \"utc\" )): odf = odf . withColumn ( modify_col [ output_mode ], F . to_utc_timestamp ( modify_col [ output_mode ], localtz ), ) return odf def timezone_conversion ( spark , idf , list_of_cols , given_tz , output_tz , output_mode = \"replace\" ): \"\"\" Convert timestamp columns from the given timezone (given_tz) to the output timezone (output_tz). Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". given_tz Timezone of the input column(s). If \"local\", the timezone of the spark session will be used. output_tz Timezone of the output column(s). If \"local\", the timezone of the spark session will be used. output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column. \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_tzconverted\" e.g. column X is appended as X_tzconverted. (Default value = \"replace\") Returns ------- DataFrame \"\"\" list_of_cols = argument_checker ( \"timezone_conversion\" , { \"list_of_cols\" : list_of_cols , \"all_columns\" : idf . columns , \"output_mode\" : output_mode , }, ) if not list_of_cols : return idf localtz = ( spark . sql ( \"SET spark.sql.session.timeZone\" ) . select ( \"value\" ) . rdd . flatMap ( lambda x : x ) . collect ()[ 0 ] ) if given_tz == \"local\" : given_tz = localtz if output_tz == \"local\" : output_tz = localtz odf = idf for i in list_of_cols : modify_col = { \"replace\" : i , \"append\" : i + \"_tzconverted\" } odf = odf . withColumn ( modify_col [ output_mode ], F . from_utc_timestamp ( F . to_utc_timestamp ( i , given_tz ), output_tz ), ) return odf def string_to_timestamp ( spark , idf , list_of_cols , input_format = \"%Y-%m- %d %H:%M:%S\" , output_type = \"ts\" , output_mode = \"replace\" , ): \"\"\" Convert time string columns with given input format (\"%Y-%m-%d %H:%M:%S\", by default) to TimestampType or DateType columns. Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". input_format Format of the input column(s) in string (Default value = \"%Y-%m-%d %H:%M:%S\") output_type \"ts\", \"dt\" \"ts\" option returns result in T.TimestampType() \"dt\" option returns result in T.DateType() (Default value = \"ts\") output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column. \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_ts\" e.g. column X is appended as X_ts. (Default value = \"replace\") Returns ------- DataFrame \"\"\" list_of_cols = argument_checker ( \"string_to_timestamp\" , { \"list_of_cols\" : list_of_cols , \"all_columns\" : idf . columns , \"output_mode\" : output_mode , \"output_type\" : output_type , }, ) if not list_of_cols : return idf localtz = ( spark . sql ( \"SET spark.sql.session.timeZone\" ) . select ( \"value\" ) . rdd . flatMap ( lambda x : x ) . collect ()[ 0 ] ) def conversion ( col , form ): output = pytz . timezone ( localtz ) . localize ( dt . strptime ( str ( col ), form )) return output data_type = { \"ts\" : T . TimestampType (), \"dt\" : T . DateType ()} f_conversion = F . udf ( conversion , data_type [ output_type ]) odf = idf for i in list_of_cols : modify_col = { \"replace\" : i , \"append\" : i + \"_ts\" } odf = odf . withColumn ( modify_col [ output_mode ], f_conversion ( F . col ( i ), F . lit ( input_format )) ) return odf def timestamp_to_string ( spark , idf , list_of_cols , output_format = \"%Y-%m- %d %H:%M:%S\" , output_mode = \"replace\" ): \"\"\" Convert timestamp/date columns to time string columns with given output format (\"%Y-%m-%d %H:%M:%S\", by default) Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". Columns must be of Datetime type or String type in \"%Y-%m-%d %H:%M:%S\" format. output_format Format of the output column(s) (Default value = \"%Y-%m-%d %H:%M:%S\") output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column. \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_str\" e.g. column X is appended as X_str. (Default value = \"replace\") Returns ------- DataFrame \"\"\" list_of_cols = argument_checker ( \"timestamp_to_string\" , { \"list_of_cols\" : list_of_cols , \"all_columns\" : idf . columns , \"output_mode\" : output_mode , }, ) if not list_of_cols : return idf localtz = ( spark . sql ( \"SET spark.sql.session.timeZone\" ) . select ( \"value\" ) . rdd . flatMap ( lambda x : x ) . collect ()[ 0 ] ) def conversion ( col , form ): output = col . astimezone ( pytz . timezone ( localtz )) . strftime ( form ) return output f_conversion = F . udf ( conversion , T . StringType ()) odf = idf for i in list_of_cols : modify_col = { \"replace\" : i , \"append\" : i + \"_str\" } odf = odf . withColumn ( modify_col [ output_mode ], f_conversion ( F . col ( i ), F . lit ( output_format )) ) return odf def dateformat_conversion ( spark , idf , list_of_cols , input_format = \"%Y-%m- %d %H:%M:%S\" , output_format = \"%Y-%m- %d %H:%M:%S\" , output_mode = \"replace\" , ): \"\"\" Convert time string columns with given input format (\"%Y-%m-%d %H:%M:%S\", by default) to time string columns with given output format (\"%Y-%m-%d %H:%M:%S\", by default). Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". input_format Format of the input column(s) in string (Default value = \"%Y-%m-%d %H:%M:%S\") output_format Format of the output column(s) in string (Default value = \"%Y-%m-%d %H:%M:%S\") output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column. \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_ts\" e.g. column X is appended as X_ts. (Default value = \"replace\") Returns ------- DataFrame \"\"\" list_of_cols = argument_checker ( \"dateformat_conversion\" , { \"list_of_cols\" : list_of_cols , \"all_columns\" : idf . columns , \"output_mode\" : output_mode , }, ) if not list_of_cols : return idf odf_tmp = string_to_timestamp ( spark , idf , list_of_cols , input_format = input_format , output_type = \"ts\" , output_mode = output_mode , ) appended_cols = { \"append\" : [ col + \"_ts\" for col in list_of_cols ], \"replace\" : list_of_cols , } odf = timestamp_to_string ( spark , odf_tmp , appended_cols [ output_mode ], output_format = output_format , output_mode = \"replace\" , ) return odf def timeUnits_extraction ( idf , list_of_cols , units , output_mode = \"append\" ): \"\"\" Extract the unit(s) of given timestamp columns as integer. Currently the following units are supported: hour, minute, second, dayofmonth, dayofweek, dayofyear, weekofyear, month, quarter, year. Multiple units can be calculated at the same time by inputting a list of units or a string of units separated by pipe delimiter \u201c|\u201d. Parameters ---------- idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". units List of unit(s) to extract. Alternatively, unit(s) can be specified in a string format, where different units are separated by pipe delimiter \u201c|\u201d e.g., \"hour|minute\". Supported units to extract: \"hour\", \"minute\", \"second\",\"dayofmonth\",\"dayofweek\", \"dayofyear\",\"weekofyear\",\"month\",\"quarter\",\"year\". \"all\" can be passed to compute all supported metrics. output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived columns with a postfix \"_<unit>\", e.g. column X is replaced with X_second for units=\"second\". \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_<unit>\", e.g. column X is appended as X_second for units=\"second\". (Default value = \"append\") Returns ------- DataFrame \"\"\" all_units = [ \"hour\" , \"minute\" , \"second\" , \"dayofmonth\" , \"dayofweek\" , \"dayofyear\" , \"weekofyear\" , \"month\" , \"quarter\" , \"year\" , ] if units == \"all\" : units = all_units if isinstance ( units , str ): units = [ x . strip () for x in units . split ( \"|\" )] list_of_cols = argument_checker ( \"timeUnits_extraction\" , { \"list_of_cols\" : list_of_cols , \"all_columns\" : idf . columns , \"output_mode\" : output_mode , \"units\" : units , \"all_units\" : all_units , }, ) if not list_of_cols : return idf odf = idf for i in list_of_cols : for e in units : func = getattr ( F , e ) odf = odf . withColumn ( i + \"_\" + e , func ( i )) if output_mode == \"replace\" : odf = odf . drop ( i ) return odf def time_diff ( idf , ts1 , ts2 , unit , output_mode = \"append\" ): \"\"\" Calculate the time difference between 2 timestamp columns (Timestamp 1 - Timestamp 2) in a given unit. Currently the following units are supported: second, minute, hour, day, week, month, year. Parameters ---------- idf Input Dataframe ts1 First column to calculate the difference ts2 Second column to calculate the difference. unit \"second\", \"minute\", \"hour\", \"day\", \"week\", \"month\", \"year\". Unit of the output values. output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column <ts1>_<ts2>_<unit>diff, e.g. Given ts1=X, ts2=Y , X and Y are replaced with X_Y_daydiff for unit=\"day\". \u201cappend\u201d option appends derived column to the input dataset with name = <ts1>_<ts2>_<unit>diff, e.g. Given ts1=X, ts2=Y, X_Y_daydiff is appended for unit=\"day\". (Default value = \"append\") Returns ------- DataFrame \"\"\" argument_checker ( \"time_diff\" , { \"list_of_cols\" : [ ts1 , ts2 ], \"all_columns\" : idf . columns , \"output_mode\" : output_mode , }, ) factor_mapping = { \"second\" : 1 , \"minute\" : 60 , \"hour\" : 3600 , \"day\" : 86400 , \"week\" : 604800 , \"month\" : 2628000 , \"year\" : 31536000 , } if unit in factor_mapping . keys (): factor = factor_mapping [ unit ] elif unit in [( e + \"s\" ) for e in factor_mapping . keys ()]: unit = unit [: - 1 ] factor = factor_mapping [ unit ] else : raise TypeError ( \"Invalid input of unit\" ) odf = idf . withColumn ( ts1 + \"_\" + ts2 + \"_\" + unit + \"diff\" , F . abs (( F . col ( ts1 ) . cast ( \"double\" ) - F . col ( ts2 ) . cast ( \"double\" ))) / factor , ) if output_mode == \"replace\" : odf = odf . drop ( ts1 , ts2 ) return odf def time_elapsed ( idf , list_of_cols , unit , output_mode = \"append\" ): \"\"\" Calculate time difference between the current and the given timestamp (Current - Given Timestamp) in a given unit. Currently the following units are supported: second, minute, hour, day, week, month, year. Parameters ---------- idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". unit \"second\", \"minute\", \"hour\", \"day\", \"week\", \"month\", \"year\". Unit of the output values. output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived columns with a postfix \"_<unit>diff\", e.g. column X is replaced with X_daydiff for unit=\"day\". \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_<unit>diff\", e.g. column X is appended as X_daydiff for unit=\"day\". (Default value = \"append\") Returns ------- DataFrame \"\"\" list_of_cols = argument_checker ( \"time_elapsed\" , { \"list_of_cols\" : list_of_cols , \"all_columns\" : idf . columns , \"output_mode\" : output_mode , }, ) if not list_of_cols : return idf factor_mapping = { \"second\" : 1 , \"minute\" : 60 , \"hour\" : 3600 , \"day\" : 86400 , \"week\" : 604800 , \"month\" : 2628000 , \"year\" : 31536000 , } if unit in factor_mapping . keys (): factor = factor_mapping [ unit ] elif unit in [( e + \"s\" ) for e in factor_mapping . keys ()]: unit = unit [: - 1 ] factor = factor_mapping [ unit ] else : raise TypeError ( \"Invalid input of unit\" ) odf = idf for i in list_of_cols : odf = odf . withColumn ( i + \"_\" + unit + \"diff\" , F . abs ( ( F . lit ( F . current_timestamp ()) . cast ( \"double\" ) - F . col ( i ) . cast ( \"double\" )) ) / factor , ) if output_mode == \"replace\" : odf = odf . drop ( i ) return odf def adding_timeUnits ( idf , list_of_cols , unit , unit_value , output_mode = \"append\" ): \"\"\" Add or subtract given time units to/from timestamp columns. Currently the following units are supported: second, minute, hour, day, week, month, year. Subtraction can be performed by setting a negative unit_value. Parameters ---------- idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". unit \"second\", \"minute\", \"hour\", \"day\", \"week\", \"month\", \"year\". Unit of the added value. unit_value The value to be added to input column(s). output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived columns with a postfix \"_adjusted\", e.g. column X is replaced with X_adjusted. \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_adjusted\", e.g. column X is appended as X_adjusted. (Default value = \"append\") Returns ------- DataFrame \"\"\" all_units = [ \"hour\" , \"minute\" , \"second\" , \"day\" , \"week\" , \"month\" , \"year\" ] list_of_cols = argument_checker ( \"adding_timeUnits\" , { \"list_of_cols\" : list_of_cols , \"all_columns\" : idf . columns , \"output_mode\" : output_mode , \"unit\" : unit , \"all_units\" : all_units , }, ) if not list_of_cols : return idf odf = idf for i in list_of_cols : odf = odf . withColumn ( i + \"_adjusted\" , F . col ( i ) + F . expr ( \"Interval \" + str ( unit_value ) + \" \" + unit ), ) if output_mode == \"replace\" : odf = odf . drop ( i ) return odf def timestamp_comparison ( spark , idf , list_of_cols , comparison_type , comparison_value , comparison_format = \"%Y-%m- %d %H:%M:%S\" , output_mode = \"append\" , ): \"\"\" Compare timestamp columns with a given timestamp/date value (comparison_value) of given format ( comparison_format). Supported comparison types include greater_than, less_than, greaterThan_equalTo and lessThan_equalTo. The derived values are 1 if True and 0 if False. Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". comparison_type greater_than\", \"less_than\", \"greaterThan_equalTo\", \"lessThan_equalTo\" The comparison type of the transformation. comparison_value The timestamp / date value to compare with in string. comparison_format The format of comparison_value in string. (Default value = \"%Y-%m-%d %H:%M:%S\") output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived columns with a postfix \"_compared\", e.g. column X is replaced with X_compared. \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_compared\", e.g. column X is appended as X_compared. (Default value = \"append\") Returns ------- DataFrame \"\"\" all_types = [ \"greater_than\" , \"less_than\" , \"greaterThan_equalTo\" , \"lessThan_equalTo\" ] list_of_cols = argument_checker ( \"timestamp_comparison\" , { \"list_of_cols\" : list_of_cols , \"all_columns\" : idf . columns , \"output_mode\" : output_mode , \"comparison_type\" : comparison_type , \"all_types\" : all_types , }, ) if not list_of_cols : return idf localtz = ( spark . sql ( \"SET spark.sql.session.timeZone\" ) . select ( \"value\" ) . rdd . flatMap ( lambda x : x ) . collect ()[ 0 ] ) base_ts = pytz . timezone ( localtz ) . localize ( dt . strptime ( comparison_value , comparison_format ) ) odf = idf for i in list_of_cols : if comparison_type == \"greater_than\" : odf = odf . withColumn ( i + \"_compared\" , F . when ( F . col ( i ) > F . lit ( base_ts ), 1 ) . otherwise ( 0 ) ) elif comparison_type == \"less_than\" : odf = odf . withColumn ( i + \"_compared\" , F . when ( F . col ( i ) < F . lit ( base_ts ), 1 ) . otherwise ( 0 ) ) elif comparison_type == \"greaterThan_equalTo\" : odf = odf . withColumn ( i + \"_compared\" , F . when ( F . col ( i ) >= F . lit ( base_ts ), 1 ) . otherwise ( 0 ) ) else : odf = odf . withColumn ( i + \"_compared\" , F . when ( F . col ( i ) <= F . lit ( base_ts ), 1 ) . otherwise ( 0 ) ) if output_mode == \"replace\" : odf = odf . drop ( i ) return odf def start_of_month ( idf , list_of_cols , output_mode = \"append\" ): \"\"\" Extract the first day of the month of given timestamp/date columns. Parameters ---------- idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column with a postfix \"_monthStart\". \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_monthStart\", e.g. column X is appended as X_monthStart. (Default value = \"append\") Returns ------- DataFrame \"\"\" list_of_cols = argument_checker ( \"start_of_month\" , { \"list_of_cols\" : list_of_cols , \"all_columns\" : idf . columns , \"output_mode\" : output_mode , }, ) if not list_of_cols : return idf odf = idf for i in list_of_cols : odf = odf . withColumn ( i + \"_monthStart\" , F . trunc ( i , \"month\" )) if output_mode == \"replace\" : odf = odf . drop ( i ) return odf def is_monthStart ( idf , list_of_cols , output_mode = \"append\" ): \"\"\" Check if values in given timestamp/date columns are the first day of a month. The derived values are 1 if True and 0 if False. Parameters ---------- idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column with a postfix \"_ismonthStart\". \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_ismonthStart\", e.g. column X is appended as X_ismonthStart. (Default value = \"append\") Returns ------- DataFrame \"\"\" list_of_cols = argument_checker ( \"is_monthStart\" , { \"list_of_cols\" : list_of_cols , \"all_columns\" : idf . columns , \"output_mode\" : output_mode , }, ) if not list_of_cols : return idf odf = start_of_month ( idf , list_of_cols , output_mode = \"append\" ) for i in list_of_cols : odf = odf . withColumn ( i + \"_ismonthStart\" , F . when ( F . to_date ( F . col ( i )) == F . col ( i + \"_monthStart\" ), 1 ) . otherwise ( 0 ), ) . drop ( i + \"_monthStart\" ) if output_mode == \"replace\" : odf = odf . drop ( i ) return odf def end_of_month ( idf , list_of_cols , output_mode = \"append\" ): \"\"\" Extract the last day of the month of given timestamp/date columns. Parameters ---------- idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column with a postfix \"_monthEnd\". \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_monthEnd\", e.g. column X is appended as X_monthEnd. (Default value = \"append\") Returns ------- DataFrame \"\"\" list_of_cols = argument_checker ( \"end_of_month\" , { \"list_of_cols\" : list_of_cols , \"all_columns\" : idf . columns , \"output_mode\" : output_mode , }, ) if not list_of_cols : return idf odf = idf for i in list_of_cols : odf = odf . withColumn ( i + \"_monthEnd\" , F . last_day ( i )) if output_mode == \"replace\" : odf = odf . drop ( i ) return odf def is_monthEnd ( idf , list_of_cols , output_mode = \"append\" ): \"\"\" Check if values in given timestamp/date columns are the last day of a month. The derived values are 1 if True and 0 if False. Parameters ---------- idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column with a postfix \"_ismonthEnd\". \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_ismonthEnd\", e.g. column X is appended as X_ismonthEnd. (Default value = \"append\") Returns ------- DataFrame \"\"\" list_of_cols = argument_checker ( \"is_monthEnd\" , { \"list_of_cols\" : list_of_cols , \"all_columns\" : idf . columns , \"output_mode\" : output_mode , }, ) if not list_of_cols : return idf odf = end_of_month ( idf , list_of_cols , output_mode = \"append\" ) for i in list_of_cols : odf = odf . withColumn ( i + \"_ismonthEnd\" , F . when ( F . to_date ( F . col ( i )) == F . col ( i + \"_monthEnd\" ), 1 ) . otherwise ( 0 ), ) . drop ( i + \"_monthEnd\" ) if output_mode == \"replace\" : odf = odf . drop ( i ) return odf def start_of_year ( idf , list_of_cols , output_mode = \"append\" ): \"\"\" Extract the first day of the year of given timestamp/date columns. Parameters ---------- idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column with a postfix \"_yearStart\". \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_yearStart\", e.g. column X is appended as X_yearStart. (Default value = \"append\") Returns ------- DataFrame \"\"\" list_of_cols = argument_checker ( \"start_of_year\" , { \"list_of_cols\" : list_of_cols , \"all_columns\" : idf . columns , \"output_mode\" : output_mode , }, ) if not list_of_cols : return idf odf = idf for i in list_of_cols : odf = odf . withColumn ( i + \"_yearStart\" , F . trunc ( i , \"year\" )) if output_mode == \"replace\" : odf = odf . drop ( i ) return odf def is_yearStart ( idf , list_of_cols , output_mode = \"append\" ): \"\"\" Check if values in given timestamp/date columns are the first day of a year. The derived values are 1 if True and 0 if False. Parameters ---------- idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column with a postfix \"_isyearStart\". \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_isyearStart\", e.g. column X is appended as X_isyearStart. (Default value = \"append\") Returns ------- DataFrame \"\"\" list_of_cols = argument_checker ( \"is_yearStart\" , { \"list_of_cols\" : list_of_cols , \"all_columns\" : idf . columns , \"output_mode\" : output_mode , }, ) if not list_of_cols : return idf odf = start_of_year ( idf , list_of_cols , output_mode = \"append\" ) for i in list_of_cols : odf = odf . withColumn ( i + \"_isyearStart\" , F . when ( F . to_date ( F . col ( i )) == F . col ( i + \"_yearStart\" ), 1 ) . otherwise ( 0 ), ) . drop ( i + \"_yearStart\" ) if output_mode == \"replace\" : odf = odf . drop ( i ) return odf def end_of_year ( idf , list_of_cols , output_mode = \"append\" ): \"\"\" Extract the last day of the year of given timestamp/date columns. Parameters ---------- idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column with a postfix \"_yearEnd\". \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_yearEnd\", e.g. column X is appended as X_yearEnd. (Default value = \"append\") Returns ------- DataFrame \"\"\" list_of_cols = argument_checker ( \"end_of_year\" , { \"list_of_cols\" : list_of_cols , \"all_columns\" : idf . columns , \"output_mode\" : output_mode , }, ) if not list_of_cols : return idf odf = idf for i in list_of_cols : odf = odf . withColumn ( i + \"_yearEnd\" , F . concat_ws ( \"-\" , F . year ( i ), F . lit ( 12 ), F . lit ( 31 )) . cast ( \"date\" ), ) if output_mode == \"replace\" : odf = odf . drop ( i ) return odf def is_yearEnd ( idf , list_of_cols , output_mode = \"append\" ): \"\"\" Check if values in given timestamp/date columns are the last day of a year. The derived values are 1 if True and 0 if False. Parameters ---------- idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column with a postfix \"_isyearEnd\". \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_isyearEnd\", e.g. column X is appended as X_isyearEnd. (Default value = \"append\") Returns ------- DataFrame \"\"\" list_of_cols = argument_checker ( \"is_yearEnd\" , { \"list_of_cols\" : list_of_cols , \"all_columns\" : idf . columns , \"output_mode\" : output_mode , }, ) if not list_of_cols : return idf odf = end_of_year ( idf , list_of_cols , output_mode = \"append\" ) for i in list_of_cols : odf = odf . withColumn ( i + \"_isyearEnd\" , F . when ( F . to_date ( F . col ( i )) == F . col ( i + \"_yearEnd\" ), 1 ) . otherwise ( 0 ), ) . drop ( i + \"_yearEnd\" ) if output_mode == \"replace\" : odf = odf . drop ( i ) return odf def start_of_quarter ( idf , list_of_cols , output_mode = \"append\" ): \"\"\" Extract the first day of the quarter of given timestamp/date columns. Parameters ---------- idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column with a postfix \"_quarterStart. \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_quarterStart\", e.g. column X is appended as X_quarterStart. (Default value = \"append\") Returns ------- DataFrame \"\"\" list_of_cols = argument_checker ( \"start_of_quarter\" , { \"list_of_cols\" : list_of_cols , \"all_columns\" : idf . columns , \"output_mode\" : output_mode , }, ) if not list_of_cols : return idf odf = idf for i in list_of_cols : odf = odf . withColumn ( i + \"_quarterStart\" , F . to_date ( F . date_trunc ( \"quarter\" , i ))) if output_mode == \"replace\" : odf = odf . drop ( i ) return odf def is_quarterStart ( idf , list_of_cols , output_mode = \"append\" ): \"\"\" Check if values in given timestamp/date columns are the first day of a quarter. The derived values are 1 if True and 0 if False. Parameters ---------- idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column with a postfix \"_isquarterStart\". \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_isquarterStart\", e.g. column X is appended as X_isquarterStart. (Default value = \"append\") Returns ------- DataFrame \"\"\" list_of_cols = argument_checker ( \"is_quarterStart\" , { \"list_of_cols\" : list_of_cols , \"all_columns\" : idf . columns , \"output_mode\" : output_mode , }, ) if not list_of_cols : return idf odf = start_of_quarter ( idf , list_of_cols , output_mode = \"append\" ) for i in list_of_cols : odf = odf . withColumn ( i + \"_isquarterStart\" , F . when ( F . to_date ( F . col ( i )) == F . col ( i + \"_quarterStart\" ), 1 ) . otherwise ( 0 ), ) . drop ( i + \"_quarterStart\" ) if output_mode == \"replace\" : odf = odf . drop ( i ) return odf def end_of_quarter ( idf , list_of_cols , output_mode = \"append\" ): \"\"\" Extract the last day of the quarter of given timestamp/date columns. Parameters ---------- idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column with a postfix \"_quarterEnd\". \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_quarterEnd\", e.g. column X is appended as X_quarterEnd. (Default value = \"append\") Returns ------- DataFrame \"\"\" list_of_cols = argument_checker ( \"end_of_quarter\" , { \"list_of_cols\" : list_of_cols , \"all_columns\" : idf . columns , \"output_mode\" : output_mode , }, ) if not list_of_cols : return idf odf = idf for i in list_of_cols : odf = odf . withColumn ( i + \"_quarterEnd\" , F . to_date ( F . date_trunc ( \"quarter\" , i )) + F . expr ( \"Interval 3 months\" ) + F . expr ( \"Interval -1 day\" ), ) if output_mode == \"replace\" : odf = odf . drop ( i ) return odf def is_quarterEnd ( idf , list_of_cols , output_mode = \"append\" ): \"\"\" Check if values in given timestamp/date columns are the last day of a quarter. The derived values are 1 if True and 0 if False. Parameters ---------- idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column with a postfix \"_isquarterEnd\". \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_isquarterEnd\", e.g. column X is appended as X_isquarterEnd. (Default value = \"append\") Returns ------- DataFrame \"\"\" list_of_cols = argument_checker ( \"is_quarterEnd\" , { \"list_of_cols\" : list_of_cols , \"all_columns\" : idf . columns , \"output_mode\" : output_mode , }, ) if not list_of_cols : return idf odf = end_of_quarter ( idf , list_of_cols , output_mode = \"append\" ) for i in list_of_cols : odf = odf . withColumn ( i + \"_isquarterEnd\" , F . when ( F . to_date ( F . col ( i )) == F . col ( i + \"_quarterEnd\" ), 1 ) . otherwise ( 0 ), ) . drop ( i + \"_quarterEnd\" ) if output_mode == \"replace\" : odf = odf . drop ( i ) return odf def is_yearFirstHalf ( idf , list_of_cols , output_mode = \"append\" ): \"\"\" Check if values in given timestamp/date columns are in the first half of a year. The derived values are 1 if True and 0 if False. Parameters ---------- idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column with a postfix \"_isFirstHalf\". \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_isFirstHalf\", e.g. column X is appended as X_isFirstHalf. (Default value = \"append\") Returns ------- DataFrame \"\"\" list_of_cols = argument_checker ( \"is_yearFirstHalf\" , { \"list_of_cols\" : list_of_cols , \"all_columns\" : idf . columns , \"output_mode\" : output_mode , }, ) if not list_of_cols : return idf odf = idf for i in list_of_cols : odf = odf . withColumn ( i + \"_isFirstHalf\" , F . when ( F . month ( F . col ( i )) . isin ( * range ( 1 , 7 )), 1 ) . otherwise ( 0 ), ) if output_mode == \"replace\" : odf = odf . drop ( i ) return odf def is_selectedHour ( idf , list_of_cols , start_hour , end_hour , output_mode = \"append\" ): \"\"\" Check if the hour component of given timestamp columns are between start hour (inclusive) and end hour ( inclusive). The derived values are 1 if True and 0 if False. Start hour can be larger than end hour, for example, start_hour=22 and end_hour=3 can be used to check whether the hour component is in [22, 23, 0, 1, 2, 3]. Parameters ---------- idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". start_hour The starting hour of the hour range (inclusive) end_hour The ending hour of the hour range (inclusive) output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column with a postfix \"_isselectedHour\". \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_isselectedHour\", e.g. column X is appended as X_isselectedHour. (Default value = \"append\") Returns ------- DataFrame \"\"\" list_of_cols = argument_checker ( \"is_selectedHour\" , { \"list_of_cols\" : list_of_cols , \"all_columns\" : idf . columns , \"start_hour\" : start_hour , \"end_hour\" : end_hour , \"output_mode\" : output_mode , }, ) if not list_of_cols : return idf odf = idf if start_hour < end_hour : list_of_hrs = range ( start_hour , end_hour + 1 ) elif start_hour > end_hour : list_of_hrs = list ( range ( start_hour , 24 )) + list ( range ( 0 , end_hour + 1 )) else : list_of_hrs = [ start_hour ] for i in list_of_cols : odf = odf . withColumn ( i + \"_isselectedHour\" , F . when ( F . hour ( F . col ( i )) . isin ( * list_of_hrs ), 1 ) . otherwise ( 0 ), ) if output_mode == \"replace\" : odf = odf . drop ( i ) return odf def is_leapYear ( idf , list_of_cols , output_mode = \"append\" ): \"\"\" Check if values in given timestamp/date columns are in a leap year. The derived values are 1 if True and 0 if False. Parameters ---------- idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column with a postfix \"_isleapYear\". \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_isleapYear\", e.g. column X is appended as X_isleapYear. (Default value = \"append\") Returns ------- DataFrame \"\"\" list_of_cols = argument_checker ( \"is_leapYear\" , { \"list_of_cols\" : list_of_cols , \"all_columns\" : idf . columns , \"output_mode\" : output_mode , }, ) if not list_of_cols : return idf def check ( year ): if calendar . isleap ( year ): return 1 else : return 0 f_check = F . udf ( check , T . IntegerType ()) odf = idf for i in list_of_cols : odf = odf . withColumn ( i + \"_isleapYear\" , f_check ( F . year ( i ))) if output_mode == \"replace\" : odf = odf . drop ( i ) return odf def is_weekend ( idf , list_of_cols , output_mode = \"append\" ): \"\"\" Check if values in given timestamp/date columns are on weekends. The derived values are 1 if True and 0 if False. Parameters ---------- idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column with a postfix \"_isweekend\". \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_isweekend\", e.g. column X is appended as X_isweekend. (Default value = \"append\") Returns ------- DataFrame \"\"\" list_of_cols = argument_checker ( \"is_weekend\" , { \"list_of_cols\" : list_of_cols , \"all_columns\" : idf . columns , \"output_mode\" : output_mode , }, ) if not list_of_cols : return idf odf = idf for i in list_of_cols : odf = odf . withColumn ( i + \"_isweekend\" , F . when ( F . dayofweek ( F . col ( i )) . isin ([ 1 , 7 ]), 1 ) . otherwise ( 0 ) ) if output_mode == \"replace\" : odf = odf . drop ( i ) return odf def aggregator ( spark , idf , list_of_cols , list_of_aggs , time_col , granularity_format = \"%Y-%m- %d \" ): \"\"\" aggregator performs groupBy over the timestamp/date column and calcuates a list of aggregate metrics over all input columns. The timestamp column is firstly converted to the given granularity format (\"%Y-%m-%d\", by default) before applying groupBy and the conversion step can be skipped by setting granularity format to be an empty string. The following aggregate metrics are supported: count, min, max, sum, mean, median, stddev, countDistinct, sumDistinct, collect_list, collect_set. Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of columns to aggregate e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". list_of_aggs List of aggregate metrics to compute e.g., [\"f1\",\"f2\"]. Alternatively, metrics can be specified in a string format, where different metrics are separated by pipe delimiter \u201c|\u201d e.g., \"f1|f2\". Supported metrics: \"count\", \"min\", \"max\", \"sum\",\"mean\",\"median\",\"stddev\", \"countDistinct\",\"sumDistinct\",\"collect_list\",\"collect_set\". time_col Timestamp) Column to group by. granularity_format Format to be applied to time_col before groupBy. The default value is '%Y-%m-%d', which means grouping by the date component of time_col. Alternatively, '' can be used if no formatting is necessary. Returns ------- DataFrame \"\"\" all_aggs = [ \"count\" , \"min\" , \"max\" , \"sum\" , \"mean\" , \"median\" , \"stddev\" , \"countDistinct\" , \"sumDistinct\" , \"collect_list\" , \"collect_set\" , ] if isinstance ( list_of_aggs , str ): list_of_aggs = [ x . strip () for x in list_of_aggs . split ( \"|\" )] list_of_cols = argument_checker ( \"aggregator\" , { \"list_of_cols\" : list_of_cols , \"all_columns\" : idf . columns , \"list_of_aggs\" : list_of_aggs , \"all_aggs\" : all_aggs , \"time_col\" : time_col , }, ) if not list_of_cols : return idf if granularity_format != \"\" : idf = timestamp_to_string ( spark , idf , time_col , output_format = granularity_format , output_mode = \"replace\" , ) def agg_funcs ( col , agg ): mapping = { \"count\" : F . count ( col ) . alias ( col + \"_count\" ), \"min\" : F . min ( col ) . alias ( col + \"_min\" ), \"max\" : F . max ( col ) . alias ( col + \"_max\" ), \"sum\" : F . sum ( col ) . alias ( col + \"_sum\" ), \"mean\" : F . mean ( col ) . alias ( col + \"_mean\" ), \"median\" : F . expr ( \"percentile_approx(\" + col + \", 0.5)\" ) . alias ( col + \"_median\" ), \"stddev\" : F . stddev ( col ) . alias ( col + \"_stddev\" ), \"countDistinct\" : F . countDistinct ( col ) . alias ( col + \"_countDistinct\" ), \"sumDistinct\" : F . sumDistinct ( col ) . alias ( col + \"_sumDistinct\" ), \"collect_list\" : F . collect_list ( col ) . alias ( col + \"_collect_list\" ), \"collect_set\" : F . collect_set ( col ) . alias ( col + \"_collect_set\" ), } return mapping [ agg ] derived_cols = [] for i in list_of_cols : for j in list_of_aggs : derived_cols . append ( agg_funcs ( i , j )) odf = idf . groupBy ( time_col ) . agg ( * derived_cols ) return odf def window_aggregator ( idf , list_of_cols , list_of_aggs , order_col , window_type = \"expanding\" , window_size = \"unbounded\" , partition_col = \"\" , output_mode = \"append\" , ): \"\"\" window_aggregator calcuates a list of aggregate metrics for all input columns over a window frame (expanding by default, or rolling type) ordered by the given timestamp column and partitioned by partition_col (\"\" by default, to indicate no partition). Window size needs to be provided as an integer for rolling window type. The following aggregate metrics are supported: count, min, max, sum, mean, median. Parameters ---------- idf Input Dataframe list_of_cols List of columns to aggregate e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". list_of_aggs List of aggregate metrics to compute e.g., [\"f1\",\"f2\"]. Alternatively, metrics can be specified in a string format, where different metrics are separated by pipe delimiter \u201c|\u201d e.g., \"f1|f2\". Supported metrics: \"count\", \"min\", \"max\", \"sum\", \"mean\", \"median\" order_col Timestamp Column to order window window_type \"expanding\", \"rolling\" \"expanding\" option has a fixed lower bound (first row in the partition) \"rolling\" option has a fixed window size defined by window_size param (Default value = \"expanding\") window_size window size for rolling window type. Integer value with value >= 1. (Default value = \"unbounded\") partition_col Rows partitioned by this column before creating window. (Default value = \"\") output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column(s) with metric name as postfix. \u201cappend\u201d option appends derived column(s) to the input dataset with metric name as postfix, e.g. \"_count\", \"_mean\". (Default value = \"append\") Returns ------- DataFrame \"\"\" if isinstance ( list_of_aggs , str ): list_of_aggs = [ x . strip () for x in list_of_aggs . split ( \"|\" )] all_aggs = [ \"count\" , \"min\" , \"max\" , \"sum\" , \"mean\" , \"median\" ] list_of_cols = argument_checker ( \"window_aggregator\" , { \"list_of_cols\" : list_of_cols , \"all_columns\" : idf . columns , \"list_of_aggs\" : list_of_aggs , \"all_aggs\" : all_aggs , \"output_mode\" : output_mode , \"window_type\" : window_type , \"window_size\" : window_size , }, ) if not list_of_cols : return idf odf = idf window_upper = ( Window . unboundedPreceding if window_type == \"expanding\" else - int ( window_size ) ) if partition_col : window = ( Window . partitionBy ( partition_col ) . orderBy ( order_col ) . rowsBetween ( window_upper , 0 ) ) else : window = Window . partitionBy () . orderBy ( order_col ) . rowsBetween ( window_upper , 0 ) def agg_funcs ( col ): mapping = { \"count\" : F . count ( col ) . over ( window ) . alias ( col + \"_count\" ), \"min\" : F . min ( col ) . over ( window ) . alias ( col + \"_min\" ), \"max\" : F . max ( col ) . over ( window ) . alias ( col + \"_max\" ), \"sum\" : F . sum ( col ) . over ( window ) . alias ( col + \"_sum\" ), \"mean\" : F . mean ( col ) . over ( window ) . alias ( col + \"_mean\" ), \"median\" : F . expr ( \"percentile_approx(\" + col + \", 0.5)\" ) . over ( window ) . alias ( col + \"_median\" ), } derived_cols = [] for agg in list_of_aggs : derived_cols . append ( mapping [ agg ]) return derived_cols for i in list_of_cols : derived_cols = agg_funcs ( i ) odf = odf . select ( odf . columns + derived_cols ) if output_mode == \"replace\" : odf = odf . drop ( i ) return odf def lagged_ts ( idf , list_of_cols , lag , output_type = \"ts\" , tsdiff_unit = \"days\" , partition_col = \"\" , output_mode = \"append\" , ): \"\"\" lagged_ts returns the values that are *lag* rows before the current rows, and None if there is less than *lag* rows before the current rows. If output_type is \"ts_diff\", an additional column is generated with values being the time difference between the original timestamp and the lagged timestamp in given unit *tsdiff_unit*. Currently the following units are supported: second, minute, hour, day, week, month, year. Parameters ---------- idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". lag Number of row(s) to extend. output_type \"ts\", \"ts_diff\". \"ts\" option generats a lag column for each input column having the value that is <lag> rows before the current row, and None if there is less than <lag> rows before the current row. \"ts_diff\" option generates the lag column in the same way as the \"ts\" option. On top of that, it appends a column which represents the time_diff between the original and the lag column. (Default value = \"ts\") tsdiff_unit \"second\", \"minute\", \"hour\", \"day\", \"week\", \"month\", \"year\". Unit of the time_diff if output_type=\"ts_diff\". (Default value = \"days\") partition_col Rows partitioned by this column before creating window. (Default value = \"\") output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column: <col>_lag<lag> for \"ts\" output_type, <col>_lag<lag> and <col>_<col>_lag<lag>_<tsdiff_unit>diff for \"ts_diff\" output_type. \u201cappend\u201d option appends derived column to the input dataset, e.g. given output_type=\"ts_diff\", lag=5, tsdiff_unit=\"days\", column X is appended as X_lag5 and X_X_lag5_daydiff. (Default value = \"append\") Returns ------- DataFrame \"\"\" list_of_cols = argument_checker ( \"lagged_ts\" , { \"list_of_cols\" : list_of_cols , \"all_columns\" : idf . columns , \"lag\" : lag , \"output_type\" : output_type , \"output_mode\" : output_mode , }, ) if not list_of_cols : return idf odf = idf for i in list_of_cols : if partition_col : window = Window . partitionBy ( partition_col ) . orderBy ( i ) else : window = Window . partitionBy () . orderBy ( i ) lag = int ( lag ) odf = odf . withColumn ( i + \"_lag\" + str ( lag ), F . lag ( F . col ( i ), lag ) . over ( window )) if output_type == \"ts_diff\" : odf = time_diff ( odf , i , i + \"_lag\" + str ( lag ), unit = tsdiff_unit , output_mode = \"append\" ) if output_mode == \"replace\" : odf = odf . drop ( i ) return odf Functions def adding_timeUnits ( idf, list_of_cols, unit, unit_value, output_mode='append') Add or subtract given time units to/from timestamp columns. Currently the following units are supported: second, minute, hour, day, week, month, year. Subtraction can be performed by setting a negative unit_value. Parameters idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". unit \"second\", \"minute\", \"hour\", \"day\", \"week\", \"month\", \"year\". Unit of the added value. unit_value The value to be added to input column(s). output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived columns with a postfix \"_adjusted\", e.g. column X is replaced with X_adjusted. \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_adjusted\", e.g. column X is appended as X_adjusted. (Default value = \"append\") Returns DataFrame Expand source code def adding_timeUnits ( idf , list_of_cols , unit , unit_value , output_mode = \"append\" ): \"\"\" Add or subtract given time units to/from timestamp columns. Currently the following units are supported: second, minute, hour, day, week, month, year. Subtraction can be performed by setting a negative unit_value. Parameters ---------- idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". unit \"second\", \"minute\", \"hour\", \"day\", \"week\", \"month\", \"year\". Unit of the added value. unit_value The value to be added to input column(s). output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived columns with a postfix \"_adjusted\", e.g. column X is replaced with X_adjusted. \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_adjusted\", e.g. column X is appended as X_adjusted. (Default value = \"append\") Returns ------- DataFrame \"\"\" all_units = [ \"hour\" , \"minute\" , \"second\" , \"day\" , \"week\" , \"month\" , \"year\" ] list_of_cols = argument_checker ( \"adding_timeUnits\" , { \"list_of_cols\" : list_of_cols , \"all_columns\" : idf . columns , \"output_mode\" : output_mode , \"unit\" : unit , \"all_units\" : all_units , }, ) if not list_of_cols : return idf odf = idf for i in list_of_cols : odf = odf . withColumn ( i + \"_adjusted\" , F . col ( i ) + F . expr ( \"Interval \" + str ( unit_value ) + \" \" + unit ), ) if output_mode == \"replace\" : odf = odf . drop ( i ) return odf def aggregator ( spark, idf, list_of_cols, list_of_aggs, time_col, granularity_format='%Y-%m-%d') aggregator performs groupBy over the timestamp/date column and calcuates a list of aggregate metrics over all input columns. The timestamp column is firstly converted to the given granularity format (\"%Y-%m-%d\", by default) before applying groupBy and the conversion step can be skipped by setting granularity format to be an empty string. The following aggregate metrics are supported: count, min, max, sum, mean, median, stddev, countDistinct, sumDistinct, collect_list, collect_set. Parameters spark Spark Session idf Input Dataframe list_of_cols List of columns to aggregate e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". list_of_aggs List of aggregate metrics to compute e.g., [\"f1\",\"f2\"]. Alternatively, metrics can be specified in a string format, where different metrics are separated by pipe delimiter \u201c|\u201d e.g., \"f1|f2\". Supported metrics: \"count\", \"min\", \"max\", \"sum\",\"mean\",\"median\",\"stddev\", \"countDistinct\",\"sumDistinct\",\"collect_list\",\"collect_set\". time_col Timestamp) Column to group by. granularity_format Format to be applied to time_col before groupBy. The default value is '%Y-%m-%d', which means grouping by the date component of time_col. Alternatively, '' can be used if no formatting is necessary. Returns DataFrame Expand source code def aggregator ( spark , idf , list_of_cols , list_of_aggs , time_col , granularity_format = \"%Y-%m- %d \" ): \"\"\" aggregator performs groupBy over the timestamp/date column and calcuates a list of aggregate metrics over all input columns. The timestamp column is firstly converted to the given granularity format (\"%Y-%m-%d\", by default) before applying groupBy and the conversion step can be skipped by setting granularity format to be an empty string. The following aggregate metrics are supported: count, min, max, sum, mean, median, stddev, countDistinct, sumDistinct, collect_list, collect_set. Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of columns to aggregate e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". list_of_aggs List of aggregate metrics to compute e.g., [\"f1\",\"f2\"]. Alternatively, metrics can be specified in a string format, where different metrics are separated by pipe delimiter \u201c|\u201d e.g., \"f1|f2\". Supported metrics: \"count\", \"min\", \"max\", \"sum\",\"mean\",\"median\",\"stddev\", \"countDistinct\",\"sumDistinct\",\"collect_list\",\"collect_set\". time_col Timestamp) Column to group by. granularity_format Format to be applied to time_col before groupBy. The default value is '%Y-%m-%d', which means grouping by the date component of time_col. Alternatively, '' can be used if no formatting is necessary. Returns ------- DataFrame \"\"\" all_aggs = [ \"count\" , \"min\" , \"max\" , \"sum\" , \"mean\" , \"median\" , \"stddev\" , \"countDistinct\" , \"sumDistinct\" , \"collect_list\" , \"collect_set\" , ] if isinstance ( list_of_aggs , str ): list_of_aggs = [ x . strip () for x in list_of_aggs . split ( \"|\" )] list_of_cols = argument_checker ( \"aggregator\" , { \"list_of_cols\" : list_of_cols , \"all_columns\" : idf . columns , \"list_of_aggs\" : list_of_aggs , \"all_aggs\" : all_aggs , \"time_col\" : time_col , }, ) if not list_of_cols : return idf if granularity_format != \"\" : idf = timestamp_to_string ( spark , idf , time_col , output_format = granularity_format , output_mode = \"replace\" , ) def agg_funcs ( col , agg ): mapping = { \"count\" : F . count ( col ) . alias ( col + \"_count\" ), \"min\" : F . min ( col ) . alias ( col + \"_min\" ), \"max\" : F . max ( col ) . alias ( col + \"_max\" ), \"sum\" : F . sum ( col ) . alias ( col + \"_sum\" ), \"mean\" : F . mean ( col ) . alias ( col + \"_mean\" ), \"median\" : F . expr ( \"percentile_approx(\" + col + \", 0.5)\" ) . alias ( col + \"_median\" ), \"stddev\" : F . stddev ( col ) . alias ( col + \"_stddev\" ), \"countDistinct\" : F . countDistinct ( col ) . alias ( col + \"_countDistinct\" ), \"sumDistinct\" : F . sumDistinct ( col ) . alias ( col + \"_sumDistinct\" ), \"collect_list\" : F . collect_list ( col ) . alias ( col + \"_collect_list\" ), \"collect_set\" : F . collect_set ( col ) . alias ( col + \"_collect_set\" ), } return mapping [ agg ] derived_cols = [] for i in list_of_cols : for j in list_of_aggs : derived_cols . append ( agg_funcs ( i , j )) odf = idf . groupBy ( time_col ) . agg ( * derived_cols ) return odf def argument_checker ( func_name, args) Parameters func_name function name for which argument needs to be check args arguments to check in dictionary format Returns List list of columns to analyze Expand source code def argument_checker ( func_name , args ): \"\"\" Parameters ---------- func_name function name for which argument needs to be check args arguments to check in dictionary format Returns ------- List list of columns to analyze \"\"\" list_of_cols = args [ \"list_of_cols\" ] all_columns = args [ \"all_columns\" ] if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if any ( x not in all_columns for x in list_of_cols ): raise TypeError ( \"Invalid input for Column(s)\" ) if len ( list_of_cols ) == 0 : warnings . warn ( \"No timestamp conversion - No column(s) to convert\" ) return [] if func_name not in [ \"aggregator\" ]: if args [ \"output_mode\" ] not in ( \"replace\" , \"append\" ): raise TypeError ( \"Invalid input for output_mode\" ) if func_name in [ \"timestamp_to_unix\" , \"unix_to_timestamp\" ]: if args [ \"precision\" ] not in ( \"ms\" , \"s\" ): raise TypeError ( \"Invalid input for precision\" ) if args [ \"tz\" ] not in ( \"local\" , \"gmt\" , \"utc\" ): raise TypeError ( \"Invalid input for timezone\" ) if func_name in [ \"string_to_timestamp\" ]: if args [ \"output_type\" ] not in ( \"ts\" , \"dt\" ): raise TypeError ( \"Invalid input for output_type\" ) if func_name in [ \"timeUnits_extraction\" ]: if any ( x not in args [ \"all_units\" ] for x in args [ \"units\" ]): raise TypeError ( \"Invalid input for Unit(s)\" ) if func_name in [ \"adding_timeUnits\" ]: if args [ \"unit\" ] not in ( args [ \"all_units\" ] + [( e + \"s\" ) for e in args [ \"all_units\" ]] ): raise TypeError ( \"Invalid input for Unit\" ) if func_name in [ \"timestamp_comparison\" ]: if args [ \"comparison_type\" ] not in args [ \"all_types\" ]: raise TypeError ( \"Invalid input for comparison_type\" ) if func_name in [ \"is_selectedHour\" ]: hours = list ( range ( 0 , 24 )) if args [ \"start_hour\" ] not in hours : raise TypeError ( \"Invalid input for start_hour\" ) if args [ \"end_hour\" ] not in hours : raise TypeError ( \"Invalid input for end_hour\" ) if func_name in [ \"window_aggregator\" ]: if any ( x not in args [ \"all_aggs\" ] for x in args [ \"list_of_aggs\" ]): raise TypeError ( \"Invalid input for Aggregate Function(s)\" ) if args [ \"window_type\" ] not in ( \"expanding\" , \"rolling\" ): raise TypeError ( \"Invalid input for Window Type\" ) if ( args [ \"window_type\" ] == \"rolling\" ) & ( not str ( args [ \"window_size\" ]) . isnumeric () ): raise TypeError ( \"Invalid input for Window Size\" ) if func_name in [ \"aggregator\" ]: if any ( x not in args [ \"all_aggs\" ] for x in args [ \"list_of_aggs\" ]): raise TypeError ( \"Invalid input for Aggregate Function(s)\" ) if args [ \"time_col\" ] not in all_columns : raise TypeError ( \"Invalid input for time_col\" ) if func_name in [ \"lagged_ts\" ]: if not str ( args [ \"lag\" ]) . isnumeric (): raise TypeError ( \"Invalid input for Lag\" ) if args [ \"output_type\" ] not in ( \"ts\" , \"ts_diff\" ): raise TypeError ( \"Invalid input for output_type\" ) return list_of_cols def dateformat_conversion ( spark, idf, list_of_cols, input_format='%Y-%m-%d %H:%M:%S', output_format='%Y-%m-%d %H:%M:%S', output_mode='replace') Convert time string columns with given input format (\"%Y-%m-%d %H:%M:%S\", by default) to time string columns with given output format (\"%Y-%m-%d %H:%M:%S\", by default). Parameters spark Spark Session idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". input_format Format of the input column(s) in string (Default value = \"%Y-%m-%d %H:%M:%S\") output_format Format of the output column(s) in string (Default value = \"%Y-%m-%d %H:%M:%S\") output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column. \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_ts\" e.g. column X is appended as X_ts. (Default value = \"replace\") Returns DataFrame Expand source code def dateformat_conversion ( spark , idf , list_of_cols , input_format = \"%Y-%m- %d %H:%M:%S\" , output_format = \"%Y-%m- %d %H:%M:%S\" , output_mode = \"replace\" , ): \"\"\" Convert time string columns with given input format (\"%Y-%m-%d %H:%M:%S\", by default) to time string columns with given output format (\"%Y-%m-%d %H:%M:%S\", by default). Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". input_format Format of the input column(s) in string (Default value = \"%Y-%m-%d %H:%M:%S\") output_format Format of the output column(s) in string (Default value = \"%Y-%m-%d %H:%M:%S\") output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column. \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_ts\" e.g. column X is appended as X_ts. (Default value = \"replace\") Returns ------- DataFrame \"\"\" list_of_cols = argument_checker ( \"dateformat_conversion\" , { \"list_of_cols\" : list_of_cols , \"all_columns\" : idf . columns , \"output_mode\" : output_mode , }, ) if not list_of_cols : return idf odf_tmp = string_to_timestamp ( spark , idf , list_of_cols , input_format = input_format , output_type = \"ts\" , output_mode = output_mode , ) appended_cols = { \"append\" : [ col + \"_ts\" for col in list_of_cols ], \"replace\" : list_of_cols , } odf = timestamp_to_string ( spark , odf_tmp , appended_cols [ output_mode ], output_format = output_format , output_mode = \"replace\" , ) return odf def end_of_month ( idf, list_of_cols, output_mode='append') Extract the last day of the month of given timestamp/date columns. Parameters idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column with a postfix \"_monthEnd\". \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_monthEnd\", e.g. column X is appended as X_monthEnd. (Default value = \"append\") Returns DataFrame Expand source code def end_of_month ( idf , list_of_cols , output_mode = \"append\" ): \"\"\" Extract the last day of the month of given timestamp/date columns. Parameters ---------- idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column with a postfix \"_monthEnd\". \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_monthEnd\", e.g. column X is appended as X_monthEnd. (Default value = \"append\") Returns ------- DataFrame \"\"\" list_of_cols = argument_checker ( \"end_of_month\" , { \"list_of_cols\" : list_of_cols , \"all_columns\" : idf . columns , \"output_mode\" : output_mode , }, ) if not list_of_cols : return idf odf = idf for i in list_of_cols : odf = odf . withColumn ( i + \"_monthEnd\" , F . last_day ( i )) if output_mode == \"replace\" : odf = odf . drop ( i ) return odf def end_of_quarter ( idf, list_of_cols, output_mode='append') Extract the last day of the quarter of given timestamp/date columns. Parameters idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column with a postfix \"_quarterEnd\". \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_quarterEnd\", e.g. column X is appended as X_quarterEnd. (Default value = \"append\") Returns DataFrame Expand source code def end_of_quarter ( idf , list_of_cols , output_mode = \"append\" ): \"\"\" Extract the last day of the quarter of given timestamp/date columns. Parameters ---------- idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column with a postfix \"_quarterEnd\". \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_quarterEnd\", e.g. column X is appended as X_quarterEnd. (Default value = \"append\") Returns ------- DataFrame \"\"\" list_of_cols = argument_checker ( \"end_of_quarter\" , { \"list_of_cols\" : list_of_cols , \"all_columns\" : idf . columns , \"output_mode\" : output_mode , }, ) if not list_of_cols : return idf odf = idf for i in list_of_cols : odf = odf . withColumn ( i + \"_quarterEnd\" , F . to_date ( F . date_trunc ( \"quarter\" , i )) + F . expr ( \"Interval 3 months\" ) + F . expr ( \"Interval -1 day\" ), ) if output_mode == \"replace\" : odf = odf . drop ( i ) return odf def end_of_year ( idf, list_of_cols, output_mode='append') Extract the last day of the year of given timestamp/date columns. Parameters idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column with a postfix \"_yearEnd\". \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_yearEnd\", e.g. column X is appended as X_yearEnd. (Default value = \"append\") Returns DataFrame Expand source code def end_of_year ( idf , list_of_cols , output_mode = \"append\" ): \"\"\" Extract the last day of the year of given timestamp/date columns. Parameters ---------- idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column with a postfix \"_yearEnd\". \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_yearEnd\", e.g. column X is appended as X_yearEnd. (Default value = \"append\") Returns ------- DataFrame \"\"\" list_of_cols = argument_checker ( \"end_of_year\" , { \"list_of_cols\" : list_of_cols , \"all_columns\" : idf . columns , \"output_mode\" : output_mode , }, ) if not list_of_cols : return idf odf = idf for i in list_of_cols : odf = odf . withColumn ( i + \"_yearEnd\" , F . concat_ws ( \"-\" , F . year ( i ), F . lit ( 12 ), F . lit ( 31 )) . cast ( \"date\" ), ) if output_mode == \"replace\" : odf = odf . drop ( i ) return odf def is_leapYear ( idf, list_of_cols, output_mode='append') Check if values in given timestamp/date columns are in a leap year. The derived values are 1 if True and 0 if False. Parameters idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column with a postfix \"_isleapYear\". \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_isleapYear\", e.g. column X is appended as X_isleapYear. (Default value = \"append\") Returns DataFrame Expand source code def is_leapYear ( idf , list_of_cols , output_mode = \"append\" ): \"\"\" Check if values in given timestamp/date columns are in a leap year. The derived values are 1 if True and 0 if False. Parameters ---------- idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column with a postfix \"_isleapYear\". \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_isleapYear\", e.g. column X is appended as X_isleapYear. (Default value = \"append\") Returns ------- DataFrame \"\"\" list_of_cols = argument_checker ( \"is_leapYear\" , { \"list_of_cols\" : list_of_cols , \"all_columns\" : idf . columns , \"output_mode\" : output_mode , }, ) if not list_of_cols : return idf def check ( year ): if calendar . isleap ( year ): return 1 else : return 0 f_check = F . udf ( check , T . IntegerType ()) odf = idf for i in list_of_cols : odf = odf . withColumn ( i + \"_isleapYear\" , f_check ( F . year ( i ))) if output_mode == \"replace\" : odf = odf . drop ( i ) return odf def is_monthEnd ( idf, list_of_cols, output_mode='append') Check if values in given timestamp/date columns are the last day of a month. The derived values are 1 if True and 0 if False. Parameters idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column with a postfix \"_ismonthEnd\". \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_ismonthEnd\", e.g. column X is appended as X_ismonthEnd. (Default value = \"append\") Returns DataFrame Expand source code def is_monthEnd ( idf , list_of_cols , output_mode = \"append\" ): \"\"\" Check if values in given timestamp/date columns are the last day of a month. The derived values are 1 if True and 0 if False. Parameters ---------- idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column with a postfix \"_ismonthEnd\". \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_ismonthEnd\", e.g. column X is appended as X_ismonthEnd. (Default value = \"append\") Returns ------- DataFrame \"\"\" list_of_cols = argument_checker ( \"is_monthEnd\" , { \"list_of_cols\" : list_of_cols , \"all_columns\" : idf . columns , \"output_mode\" : output_mode , }, ) if not list_of_cols : return idf odf = end_of_month ( idf , list_of_cols , output_mode = \"append\" ) for i in list_of_cols : odf = odf . withColumn ( i + \"_ismonthEnd\" , F . when ( F . to_date ( F . col ( i )) == F . col ( i + \"_monthEnd\" ), 1 ) . otherwise ( 0 ), ) . drop ( i + \"_monthEnd\" ) if output_mode == \"replace\" : odf = odf . drop ( i ) return odf def is_monthStart ( idf, list_of_cols, output_mode='append') Check if values in given timestamp/date columns are the first day of a month. The derived values are 1 if True and 0 if False. Parameters idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column with a postfix \"_ismonthStart\". \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_ismonthStart\", e.g. column X is appended as X_ismonthStart. (Default value = \"append\") Returns DataFrame Expand source code def is_monthStart ( idf , list_of_cols , output_mode = \"append\" ): \"\"\" Check if values in given timestamp/date columns are the first day of a month. The derived values are 1 if True and 0 if False. Parameters ---------- idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column with a postfix \"_ismonthStart\". \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_ismonthStart\", e.g. column X is appended as X_ismonthStart. (Default value = \"append\") Returns ------- DataFrame \"\"\" list_of_cols = argument_checker ( \"is_monthStart\" , { \"list_of_cols\" : list_of_cols , \"all_columns\" : idf . columns , \"output_mode\" : output_mode , }, ) if not list_of_cols : return idf odf = start_of_month ( idf , list_of_cols , output_mode = \"append\" ) for i in list_of_cols : odf = odf . withColumn ( i + \"_ismonthStart\" , F . when ( F . to_date ( F . col ( i )) == F . col ( i + \"_monthStart\" ), 1 ) . otherwise ( 0 ), ) . drop ( i + \"_monthStart\" ) if output_mode == \"replace\" : odf = odf . drop ( i ) return odf def is_quarterEnd ( idf, list_of_cols, output_mode='append') Check if values in given timestamp/date columns are the last day of a quarter. The derived values are 1 if True and 0 if False. Parameters idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column with a postfix \"_isquarterEnd\". \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_isquarterEnd\", e.g. column X is appended as X_isquarterEnd. (Default value = \"append\") Returns DataFrame Expand source code def is_quarterEnd ( idf , list_of_cols , output_mode = \"append\" ): \"\"\" Check if values in given timestamp/date columns are the last day of a quarter. The derived values are 1 if True and 0 if False. Parameters ---------- idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column with a postfix \"_isquarterEnd\". \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_isquarterEnd\", e.g. column X is appended as X_isquarterEnd. (Default value = \"append\") Returns ------- DataFrame \"\"\" list_of_cols = argument_checker ( \"is_quarterEnd\" , { \"list_of_cols\" : list_of_cols , \"all_columns\" : idf . columns , \"output_mode\" : output_mode , }, ) if not list_of_cols : return idf odf = end_of_quarter ( idf , list_of_cols , output_mode = \"append\" ) for i in list_of_cols : odf = odf . withColumn ( i + \"_isquarterEnd\" , F . when ( F . to_date ( F . col ( i )) == F . col ( i + \"_quarterEnd\" ), 1 ) . otherwise ( 0 ), ) . drop ( i + \"_quarterEnd\" ) if output_mode == \"replace\" : odf = odf . drop ( i ) return odf def is_quarterStart ( idf, list_of_cols, output_mode='append') Check if values in given timestamp/date columns are the first day of a quarter. The derived values are 1 if True and 0 if False. Parameters idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column with a postfix \"_isquarterStart\". \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_isquarterStart\", e.g. column X is appended as X_isquarterStart. (Default value = \"append\") Returns DataFrame Expand source code def is_quarterStart ( idf , list_of_cols , output_mode = \"append\" ): \"\"\" Check if values in given timestamp/date columns are the first day of a quarter. The derived values are 1 if True and 0 if False. Parameters ---------- idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column with a postfix \"_isquarterStart\". \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_isquarterStart\", e.g. column X is appended as X_isquarterStart. (Default value = \"append\") Returns ------- DataFrame \"\"\" list_of_cols = argument_checker ( \"is_quarterStart\" , { \"list_of_cols\" : list_of_cols , \"all_columns\" : idf . columns , \"output_mode\" : output_mode , }, ) if not list_of_cols : return idf odf = start_of_quarter ( idf , list_of_cols , output_mode = \"append\" ) for i in list_of_cols : odf = odf . withColumn ( i + \"_isquarterStart\" , F . when ( F . to_date ( F . col ( i )) == F . col ( i + \"_quarterStart\" ), 1 ) . otherwise ( 0 ), ) . drop ( i + \"_quarterStart\" ) if output_mode == \"replace\" : odf = odf . drop ( i ) return odf def is_selectedHour ( idf, list_of_cols, start_hour, end_hour, output_mode='append') Check if the hour component of given timestamp columns are between start hour (inclusive) and end hour ( inclusive). The derived values are 1 if True and 0 if False. Start hour can be larger than end hour, for example, start_hour=22 and end_hour=3 can be used to check whether the hour component is in [22, 23, 0, 1, 2, 3]. Parameters idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". start_hour The starting hour of the hour range (inclusive) end_hour The ending hour of the hour range (inclusive) output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column with a postfix \"_isselectedHour\". \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_isselectedHour\", e.g. column X is appended as X_isselectedHour. (Default value = \"append\") Returns DataFrame Expand source code def is_selectedHour ( idf , list_of_cols , start_hour , end_hour , output_mode = \"append\" ): \"\"\" Check if the hour component of given timestamp columns are between start hour (inclusive) and end hour ( inclusive). The derived values are 1 if True and 0 if False. Start hour can be larger than end hour, for example, start_hour=22 and end_hour=3 can be used to check whether the hour component is in [22, 23, 0, 1, 2, 3]. Parameters ---------- idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". start_hour The starting hour of the hour range (inclusive) end_hour The ending hour of the hour range (inclusive) output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column with a postfix \"_isselectedHour\". \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_isselectedHour\", e.g. column X is appended as X_isselectedHour. (Default value = \"append\") Returns ------- DataFrame \"\"\" list_of_cols = argument_checker ( \"is_selectedHour\" , { \"list_of_cols\" : list_of_cols , \"all_columns\" : idf . columns , \"start_hour\" : start_hour , \"end_hour\" : end_hour , \"output_mode\" : output_mode , }, ) if not list_of_cols : return idf odf = idf if start_hour < end_hour : list_of_hrs = range ( start_hour , end_hour + 1 ) elif start_hour > end_hour : list_of_hrs = list ( range ( start_hour , 24 )) + list ( range ( 0 , end_hour + 1 )) else : list_of_hrs = [ start_hour ] for i in list_of_cols : odf = odf . withColumn ( i + \"_isselectedHour\" , F . when ( F . hour ( F . col ( i )) . isin ( * list_of_hrs ), 1 ) . otherwise ( 0 ), ) if output_mode == \"replace\" : odf = odf . drop ( i ) return odf def is_weekend ( idf, list_of_cols, output_mode='append') Check if values in given timestamp/date columns are on weekends. The derived values are 1 if True and 0 if False. Parameters idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column with a postfix \"_isweekend\". \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_isweekend\", e.g. column X is appended as X_isweekend. (Default value = \"append\") Returns DataFrame Expand source code def is_weekend ( idf , list_of_cols , output_mode = \"append\" ): \"\"\" Check if values in given timestamp/date columns are on weekends. The derived values are 1 if True and 0 if False. Parameters ---------- idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column with a postfix \"_isweekend\". \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_isweekend\", e.g. column X is appended as X_isweekend. (Default value = \"append\") Returns ------- DataFrame \"\"\" list_of_cols = argument_checker ( \"is_weekend\" , { \"list_of_cols\" : list_of_cols , \"all_columns\" : idf . columns , \"output_mode\" : output_mode , }, ) if not list_of_cols : return idf odf = idf for i in list_of_cols : odf = odf . withColumn ( i + \"_isweekend\" , F . when ( F . dayofweek ( F . col ( i )) . isin ([ 1 , 7 ]), 1 ) . otherwise ( 0 ) ) if output_mode == \"replace\" : odf = odf . drop ( i ) return odf def is_yearEnd ( idf, list_of_cols, output_mode='append') Check if values in given timestamp/date columns are the last day of a year. The derived values are 1 if True and 0 if False. Parameters idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column with a postfix \"_isyearEnd\". \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_isyearEnd\", e.g. column X is appended as X_isyearEnd. (Default value = \"append\") Returns DataFrame Expand source code def is_yearEnd ( idf , list_of_cols , output_mode = \"append\" ): \"\"\" Check if values in given timestamp/date columns are the last day of a year. The derived values are 1 if True and 0 if False. Parameters ---------- idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column with a postfix \"_isyearEnd\". \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_isyearEnd\", e.g. column X is appended as X_isyearEnd. (Default value = \"append\") Returns ------- DataFrame \"\"\" list_of_cols = argument_checker ( \"is_yearEnd\" , { \"list_of_cols\" : list_of_cols , \"all_columns\" : idf . columns , \"output_mode\" : output_mode , }, ) if not list_of_cols : return idf odf = end_of_year ( idf , list_of_cols , output_mode = \"append\" ) for i in list_of_cols : odf = odf . withColumn ( i + \"_isyearEnd\" , F . when ( F . to_date ( F . col ( i )) == F . col ( i + \"_yearEnd\" ), 1 ) . otherwise ( 0 ), ) . drop ( i + \"_yearEnd\" ) if output_mode == \"replace\" : odf = odf . drop ( i ) return odf def is_yearFirstHalf ( idf, list_of_cols, output_mode='append') Check if values in given timestamp/date columns are in the first half of a year. The derived values are 1 if True and 0 if False. Parameters idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column with a postfix \"_isFirstHalf\". \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_isFirstHalf\", e.g. column X is appended as X_isFirstHalf. (Default value = \"append\") Returns DataFrame Expand source code def is_yearFirstHalf ( idf , list_of_cols , output_mode = \"append\" ): \"\"\" Check if values in given timestamp/date columns are in the first half of a year. The derived values are 1 if True and 0 if False. Parameters ---------- idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column with a postfix \"_isFirstHalf\". \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_isFirstHalf\", e.g. column X is appended as X_isFirstHalf. (Default value = \"append\") Returns ------- DataFrame \"\"\" list_of_cols = argument_checker ( \"is_yearFirstHalf\" , { \"list_of_cols\" : list_of_cols , \"all_columns\" : idf . columns , \"output_mode\" : output_mode , }, ) if not list_of_cols : return idf odf = idf for i in list_of_cols : odf = odf . withColumn ( i + \"_isFirstHalf\" , F . when ( F . month ( F . col ( i )) . isin ( * range ( 1 , 7 )), 1 ) . otherwise ( 0 ), ) if output_mode == \"replace\" : odf = odf . drop ( i ) return odf def is_yearStart ( idf, list_of_cols, output_mode='append') Check if values in given timestamp/date columns are the first day of a year. The derived values are 1 if True and 0 if False. Parameters idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column with a postfix \"_isyearStart\". \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_isyearStart\", e.g. column X is appended as X_isyearStart. (Default value = \"append\") Returns DataFrame Expand source code def is_yearStart ( idf , list_of_cols , output_mode = \"append\" ): \"\"\" Check if values in given timestamp/date columns are the first day of a year. The derived values are 1 if True and 0 if False. Parameters ---------- idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column with a postfix \"_isyearStart\". \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_isyearStart\", e.g. column X is appended as X_isyearStart. (Default value = \"append\") Returns ------- DataFrame \"\"\" list_of_cols = argument_checker ( \"is_yearStart\" , { \"list_of_cols\" : list_of_cols , \"all_columns\" : idf . columns , \"output_mode\" : output_mode , }, ) if not list_of_cols : return idf odf = start_of_year ( idf , list_of_cols , output_mode = \"append\" ) for i in list_of_cols : odf = odf . withColumn ( i + \"_isyearStart\" , F . when ( F . to_date ( F . col ( i )) == F . col ( i + \"_yearStart\" ), 1 ) . otherwise ( 0 ), ) . drop ( i + \"_yearStart\" ) if output_mode == \"replace\" : odf = odf . drop ( i ) return odf def lagged_ts ( idf, list_of_cols, lag, output_type='ts', tsdiff_unit='days', partition_col='', output_mode='append') lagged_ts returns the values that are lag rows before the current rows, and None if there is less than lag rows before the current rows. If output_type is \"ts_diff\", an additional column is generated with values being the time difference between the original timestamp and the lagged timestamp in given unit tsdiff_unit . Currently the following units are supported: second, minute, hour, day, week, month, year. Parameters idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". lag Number of row(s) to extend. output_type \"ts\", \"ts_diff\". \"ts\" option generats a lag column for each input column having the value that is rows before the current row, and None if there is less than rows before the current row. \"ts_diff\" option generates the lag column in the same way as the \"ts\" option. On top of that, it appends a column which represents the time_diff between the original and the lag column. (Default value = \"ts\") tsdiff_unit \"second\", \"minute\", \"hour\", \"day\", \"week\", \"month\", \"year\". Unit of the time_diff if output_type=\"ts_diff\". (Default value = \"days\") partition_col Rows partitioned by this column before creating window. (Default value = \"\") output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column: lag for \"ts\" output_type, _lag and lag diff for \"ts_diff\" output_type. \u201cappend\u201d option appends derived column to the input dataset, e.g. given output_type=\"ts_diff\", lag=5, tsdiff_unit=\"days\", column X is appended as X_lag5 and X_X_lag5_daydiff. (Default value = \"append\") Returns DataFrame Expand source code def lagged_ts ( idf , list_of_cols , lag , output_type = \"ts\" , tsdiff_unit = \"days\" , partition_col = \"\" , output_mode = \"append\" , ): \"\"\" lagged_ts returns the values that are *lag* rows before the current rows, and None if there is less than *lag* rows before the current rows. If output_type is \"ts_diff\", an additional column is generated with values being the time difference between the original timestamp and the lagged timestamp in given unit *tsdiff_unit*. Currently the following units are supported: second, minute, hour, day, week, month, year. Parameters ---------- idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". lag Number of row(s) to extend. output_type \"ts\", \"ts_diff\". \"ts\" option generats a lag column for each input column having the value that is <lag> rows before the current row, and None if there is less than <lag> rows before the current row. \"ts_diff\" option generates the lag column in the same way as the \"ts\" option. On top of that, it appends a column which represents the time_diff between the original and the lag column. (Default value = \"ts\") tsdiff_unit \"second\", \"minute\", \"hour\", \"day\", \"week\", \"month\", \"year\". Unit of the time_diff if output_type=\"ts_diff\". (Default value = \"days\") partition_col Rows partitioned by this column before creating window. (Default value = \"\") output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column: <col>_lag<lag> for \"ts\" output_type, <col>_lag<lag> and <col>_<col>_lag<lag>_<tsdiff_unit>diff for \"ts_diff\" output_type. \u201cappend\u201d option appends derived column to the input dataset, e.g. given output_type=\"ts_diff\", lag=5, tsdiff_unit=\"days\", column X is appended as X_lag5 and X_X_lag5_daydiff. (Default value = \"append\") Returns ------- DataFrame \"\"\" list_of_cols = argument_checker ( \"lagged_ts\" , { \"list_of_cols\" : list_of_cols , \"all_columns\" : idf . columns , \"lag\" : lag , \"output_type\" : output_type , \"output_mode\" : output_mode , }, ) if not list_of_cols : return idf odf = idf for i in list_of_cols : if partition_col : window = Window . partitionBy ( partition_col ) . orderBy ( i ) else : window = Window . partitionBy () . orderBy ( i ) lag = int ( lag ) odf = odf . withColumn ( i + \"_lag\" + str ( lag ), F . lag ( F . col ( i ), lag ) . over ( window )) if output_type == \"ts_diff\" : odf = time_diff ( odf , i , i + \"_lag\" + str ( lag ), unit = tsdiff_unit , output_mode = \"append\" ) if output_mode == \"replace\" : odf = odf . drop ( i ) return odf def start_of_month ( idf, list_of_cols, output_mode='append') Extract the first day of the month of given timestamp/date columns. Parameters idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column with a postfix \"_monthStart\". \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_monthStart\", e.g. column X is appended as X_monthStart. (Default value = \"append\") Returns DataFrame Expand source code def start_of_month ( idf , list_of_cols , output_mode = \"append\" ): \"\"\" Extract the first day of the month of given timestamp/date columns. Parameters ---------- idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column with a postfix \"_monthStart\". \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_monthStart\", e.g. column X is appended as X_monthStart. (Default value = \"append\") Returns ------- DataFrame \"\"\" list_of_cols = argument_checker ( \"start_of_month\" , { \"list_of_cols\" : list_of_cols , \"all_columns\" : idf . columns , \"output_mode\" : output_mode , }, ) if not list_of_cols : return idf odf = idf for i in list_of_cols : odf = odf . withColumn ( i + \"_monthStart\" , F . trunc ( i , \"month\" )) if output_mode == \"replace\" : odf = odf . drop ( i ) return odf def start_of_quarter ( idf, list_of_cols, output_mode='append') Extract the first day of the quarter of given timestamp/date columns. Parameters idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column with a postfix \"_quarterStart. \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_quarterStart\", e.g. column X is appended as X_quarterStart. (Default value = \"append\") Returns DataFrame Expand source code def start_of_quarter ( idf , list_of_cols , output_mode = \"append\" ): \"\"\" Extract the first day of the quarter of given timestamp/date columns. Parameters ---------- idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column with a postfix \"_quarterStart. \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_quarterStart\", e.g. column X is appended as X_quarterStart. (Default value = \"append\") Returns ------- DataFrame \"\"\" list_of_cols = argument_checker ( \"start_of_quarter\" , { \"list_of_cols\" : list_of_cols , \"all_columns\" : idf . columns , \"output_mode\" : output_mode , }, ) if not list_of_cols : return idf odf = idf for i in list_of_cols : odf = odf . withColumn ( i + \"_quarterStart\" , F . to_date ( F . date_trunc ( \"quarter\" , i ))) if output_mode == \"replace\" : odf = odf . drop ( i ) return odf def start_of_year ( idf, list_of_cols, output_mode='append') Extract the first day of the year of given timestamp/date columns. Parameters idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column with a postfix \"_yearStart\". \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_yearStart\", e.g. column X is appended as X_yearStart. (Default value = \"append\") Returns DataFrame Expand source code def start_of_year ( idf , list_of_cols , output_mode = \"append\" ): \"\"\" Extract the first day of the year of given timestamp/date columns. Parameters ---------- idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column with a postfix \"_yearStart\". \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_yearStart\", e.g. column X is appended as X_yearStart. (Default value = \"append\") Returns ------- DataFrame \"\"\" list_of_cols = argument_checker ( \"start_of_year\" , { \"list_of_cols\" : list_of_cols , \"all_columns\" : idf . columns , \"output_mode\" : output_mode , }, ) if not list_of_cols : return idf odf = idf for i in list_of_cols : odf = odf . withColumn ( i + \"_yearStart\" , F . trunc ( i , \"year\" )) if output_mode == \"replace\" : odf = odf . drop ( i ) return odf def string_to_timestamp ( spark, idf, list_of_cols, input_format='%Y-%m-%d %H:%M:%S', output_type='ts', output_mode='replace') Convert time string columns with given input format (\"%Y-%m-%d %H:%M:%S\", by default) to TimestampType or DateType columns. Parameters spark Spark Session idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". input_format Format of the input column(s) in string (Default value = \"%Y-%m-%d %H:%M:%S\") output_type \"ts\", \"dt\" \"ts\" option returns result in T.TimestampType() \"dt\" option returns result in T.DateType() (Default value = \"ts\") output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column. \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_ts\" e.g. column X is appended as X_ts. (Default value = \"replace\") Returns DataFrame Expand source code def string_to_timestamp ( spark , idf , list_of_cols , input_format = \"%Y-%m- %d %H:%M:%S\" , output_type = \"ts\" , output_mode = \"replace\" , ): \"\"\" Convert time string columns with given input format (\"%Y-%m-%d %H:%M:%S\", by default) to TimestampType or DateType columns. Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". input_format Format of the input column(s) in string (Default value = \"%Y-%m-%d %H:%M:%S\") output_type \"ts\", \"dt\" \"ts\" option returns result in T.TimestampType() \"dt\" option returns result in T.DateType() (Default value = \"ts\") output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column. \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_ts\" e.g. column X is appended as X_ts. (Default value = \"replace\") Returns ------- DataFrame \"\"\" list_of_cols = argument_checker ( \"string_to_timestamp\" , { \"list_of_cols\" : list_of_cols , \"all_columns\" : idf . columns , \"output_mode\" : output_mode , \"output_type\" : output_type , }, ) if not list_of_cols : return idf localtz = ( spark . sql ( \"SET spark.sql.session.timeZone\" ) . select ( \"value\" ) . rdd . flatMap ( lambda x : x ) . collect ()[ 0 ] ) def conversion ( col , form ): output = pytz . timezone ( localtz ) . localize ( dt . strptime ( str ( col ), form )) return output data_type = { \"ts\" : T . TimestampType (), \"dt\" : T . DateType ()} f_conversion = F . udf ( conversion , data_type [ output_type ]) odf = idf for i in list_of_cols : modify_col = { \"replace\" : i , \"append\" : i + \"_ts\" } odf = odf . withColumn ( modify_col [ output_mode ], f_conversion ( F . col ( i ), F . lit ( input_format )) ) return odf def timeUnits_extraction ( idf, list_of_cols, units, output_mode='append') Extract the unit(s) of given timestamp columns as integer. Currently the following units are supported: hour, minute, second, dayofmonth, dayofweek, dayofyear, weekofyear, month, quarter, year. Multiple units can be calculated at the same time by inputting a list of units or a string of units separated by pipe delimiter \u201c|\u201d. Parameters idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". units List of unit(s) to extract. Alternatively, unit(s) can be specified in a string format, where different units are separated by pipe delimiter \u201c|\u201d e.g., \"hour|minute\". Supported units to extract: \"hour\", \"minute\", \"second\",\"dayofmonth\",\"dayofweek\", \"dayofyear\",\"weekofyear\",\"month\",\"quarter\",\"year\". \"all\" can be passed to compute all supported metrics. output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived columns with a postfix \" \", e.g. column X is replaced with X_second for units=\"second\". \u201cappend\u201d option appends derived column to the input dataset with a postfix \" \", e.g. column X is appended as X_second for units=\"second\". (Default value = \"append\") Returns DataFrame Expand source code def timeUnits_extraction ( idf , list_of_cols , units , output_mode = \"append\" ): \"\"\" Extract the unit(s) of given timestamp columns as integer. Currently the following units are supported: hour, minute, second, dayofmonth, dayofweek, dayofyear, weekofyear, month, quarter, year. Multiple units can be calculated at the same time by inputting a list of units or a string of units separated by pipe delimiter \u201c|\u201d. Parameters ---------- idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". units List of unit(s) to extract. Alternatively, unit(s) can be specified in a string format, where different units are separated by pipe delimiter \u201c|\u201d e.g., \"hour|minute\". Supported units to extract: \"hour\", \"minute\", \"second\",\"dayofmonth\",\"dayofweek\", \"dayofyear\",\"weekofyear\",\"month\",\"quarter\",\"year\". \"all\" can be passed to compute all supported metrics. output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived columns with a postfix \"_<unit>\", e.g. column X is replaced with X_second for units=\"second\". \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_<unit>\", e.g. column X is appended as X_second for units=\"second\". (Default value = \"append\") Returns ------- DataFrame \"\"\" all_units = [ \"hour\" , \"minute\" , \"second\" , \"dayofmonth\" , \"dayofweek\" , \"dayofyear\" , \"weekofyear\" , \"month\" , \"quarter\" , \"year\" , ] if units == \"all\" : units = all_units if isinstance ( units , str ): units = [ x . strip () for x in units . split ( \"|\" )] list_of_cols = argument_checker ( \"timeUnits_extraction\" , { \"list_of_cols\" : list_of_cols , \"all_columns\" : idf . columns , \"output_mode\" : output_mode , \"units\" : units , \"all_units\" : all_units , }, ) if not list_of_cols : return idf odf = idf for i in list_of_cols : for e in units : func = getattr ( F , e ) odf = odf . withColumn ( i + \"_\" + e , func ( i )) if output_mode == \"replace\" : odf = odf . drop ( i ) return odf def time_diff ( idf, ts1, ts2, unit, output_mode='append') Calculate the time difference between 2 timestamp columns (Timestamp 1 - Timestamp 2) in a given unit. Currently the following units are supported: second, minute, hour, day, week, month, year. Parameters idf Input Dataframe ts1 First column to calculate the difference ts2 Second column to calculate the difference. unit \"second\", \"minute\", \"hour\", \"day\", \"week\", \"month\", \"year\". Unit of the output values. output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column diff, e.g. Given ts1=X, ts2=Y , X and Y are replaced with X_Y_daydiff for unit=\"day\". \u201cappend\u201d option appends derived column to the input dataset with name = diff, e.g. Given ts1=X, ts2=Y, X_Y_daydiff is appended for unit=\"day\". (Default value = \"append\") Returns DataFrame Expand source code def time_diff ( idf , ts1 , ts2 , unit , output_mode = \"append\" ): \"\"\" Calculate the time difference between 2 timestamp columns (Timestamp 1 - Timestamp 2) in a given unit. Currently the following units are supported: second, minute, hour, day, week, month, year. Parameters ---------- idf Input Dataframe ts1 First column to calculate the difference ts2 Second column to calculate the difference. unit \"second\", \"minute\", \"hour\", \"day\", \"week\", \"month\", \"year\". Unit of the output values. output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column <ts1>_<ts2>_<unit>diff, e.g. Given ts1=X, ts2=Y , X and Y are replaced with X_Y_daydiff for unit=\"day\". \u201cappend\u201d option appends derived column to the input dataset with name = <ts1>_<ts2>_<unit>diff, e.g. Given ts1=X, ts2=Y, X_Y_daydiff is appended for unit=\"day\". (Default value = \"append\") Returns ------- DataFrame \"\"\" argument_checker ( \"time_diff\" , { \"list_of_cols\" : [ ts1 , ts2 ], \"all_columns\" : idf . columns , \"output_mode\" : output_mode , }, ) factor_mapping = { \"second\" : 1 , \"minute\" : 60 , \"hour\" : 3600 , \"day\" : 86400 , \"week\" : 604800 , \"month\" : 2628000 , \"year\" : 31536000 , } if unit in factor_mapping . keys (): factor = factor_mapping [ unit ] elif unit in [( e + \"s\" ) for e in factor_mapping . keys ()]: unit = unit [: - 1 ] factor = factor_mapping [ unit ] else : raise TypeError ( \"Invalid input of unit\" ) odf = idf . withColumn ( ts1 + \"_\" + ts2 + \"_\" + unit + \"diff\" , F . abs (( F . col ( ts1 ) . cast ( \"double\" ) - F . col ( ts2 ) . cast ( \"double\" ))) / factor , ) if output_mode == \"replace\" : odf = odf . drop ( ts1 , ts2 ) return odf def time_elapsed ( idf, list_of_cols, unit, output_mode='append') Calculate time difference between the current and the given timestamp (Current - Given Timestamp) in a given unit. Currently the following units are supported: second, minute, hour, day, week, month, year. Parameters idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". unit \"second\", \"minute\", \"hour\", \"day\", \"week\", \"month\", \"year\". Unit of the output values. output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived columns with a postfix \" diff\", e.g. column X is replaced with X_daydiff for unit=\"day\". \u201cappend\u201d option appends derived column to the input dataset with a postfix \" diff\", e.g. column X is appended as X_daydiff for unit=\"day\". (Default value = \"append\") Returns DataFrame Expand source code def time_elapsed ( idf , list_of_cols , unit , output_mode = \"append\" ): \"\"\" Calculate time difference between the current and the given timestamp (Current - Given Timestamp) in a given unit. Currently the following units are supported: second, minute, hour, day, week, month, year. Parameters ---------- idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". unit \"second\", \"minute\", \"hour\", \"day\", \"week\", \"month\", \"year\". Unit of the output values. output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived columns with a postfix \"_<unit>diff\", e.g. column X is replaced with X_daydiff for unit=\"day\". \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_<unit>diff\", e.g. column X is appended as X_daydiff for unit=\"day\". (Default value = \"append\") Returns ------- DataFrame \"\"\" list_of_cols = argument_checker ( \"time_elapsed\" , { \"list_of_cols\" : list_of_cols , \"all_columns\" : idf . columns , \"output_mode\" : output_mode , }, ) if not list_of_cols : return idf factor_mapping = { \"second\" : 1 , \"minute\" : 60 , \"hour\" : 3600 , \"day\" : 86400 , \"week\" : 604800 , \"month\" : 2628000 , \"year\" : 31536000 , } if unit in factor_mapping . keys (): factor = factor_mapping [ unit ] elif unit in [( e + \"s\" ) for e in factor_mapping . keys ()]: unit = unit [: - 1 ] factor = factor_mapping [ unit ] else : raise TypeError ( \"Invalid input of unit\" ) odf = idf for i in list_of_cols : odf = odf . withColumn ( i + \"_\" + unit + \"diff\" , F . abs ( ( F . lit ( F . current_timestamp ()) . cast ( \"double\" ) - F . col ( i ) . cast ( \"double\" )) ) / factor , ) if output_mode == \"replace\" : odf = odf . drop ( i ) return odf def timestamp_comparison ( spark, idf, list_of_cols, comparison_type, comparison_value, comparison_format='%Y-%m-%d %H:%M:%S', output_mode='append') Compare timestamp columns with a given timestamp/date value (comparison_value) of given format ( comparison_format). Supported comparison types include greater_than, less_than, greaterThan_equalTo and lessThan_equalTo. The derived values are 1 if True and 0 if False. Parameters spark Spark Session idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". comparison_type greater_than\", \"less_than\", \"greaterThan_equalTo\", \"lessThan_equalTo\" The comparison type of the transformation. comparison_value The timestamp / date value to compare with in string. comparison_format The format of comparison_value in string. (Default value = \"%Y-%m-%d %H:%M:%S\") output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived columns with a postfix \"_compared\", e.g. column X is replaced with X_compared. \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_compared\", e.g. column X is appended as X_compared. (Default value = \"append\") Returns DataFrame Expand source code def timestamp_comparison ( spark , idf , list_of_cols , comparison_type , comparison_value , comparison_format = \"%Y-%m- %d %H:%M:%S\" , output_mode = \"append\" , ): \"\"\" Compare timestamp columns with a given timestamp/date value (comparison_value) of given format ( comparison_format). Supported comparison types include greater_than, less_than, greaterThan_equalTo and lessThan_equalTo. The derived values are 1 if True and 0 if False. Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". comparison_type greater_than\", \"less_than\", \"greaterThan_equalTo\", \"lessThan_equalTo\" The comparison type of the transformation. comparison_value The timestamp / date value to compare with in string. comparison_format The format of comparison_value in string. (Default value = \"%Y-%m-%d %H:%M:%S\") output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived columns with a postfix \"_compared\", e.g. column X is replaced with X_compared. \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_compared\", e.g. column X is appended as X_compared. (Default value = \"append\") Returns ------- DataFrame \"\"\" all_types = [ \"greater_than\" , \"less_than\" , \"greaterThan_equalTo\" , \"lessThan_equalTo\" ] list_of_cols = argument_checker ( \"timestamp_comparison\" , { \"list_of_cols\" : list_of_cols , \"all_columns\" : idf . columns , \"output_mode\" : output_mode , \"comparison_type\" : comparison_type , \"all_types\" : all_types , }, ) if not list_of_cols : return idf localtz = ( spark . sql ( \"SET spark.sql.session.timeZone\" ) . select ( \"value\" ) . rdd . flatMap ( lambda x : x ) . collect ()[ 0 ] ) base_ts = pytz . timezone ( localtz ) . localize ( dt . strptime ( comparison_value , comparison_format ) ) odf = idf for i in list_of_cols : if comparison_type == \"greater_than\" : odf = odf . withColumn ( i + \"_compared\" , F . when ( F . col ( i ) > F . lit ( base_ts ), 1 ) . otherwise ( 0 ) ) elif comparison_type == \"less_than\" : odf = odf . withColumn ( i + \"_compared\" , F . when ( F . col ( i ) < F . lit ( base_ts ), 1 ) . otherwise ( 0 ) ) elif comparison_type == \"greaterThan_equalTo\" : odf = odf . withColumn ( i + \"_compared\" , F . when ( F . col ( i ) >= F . lit ( base_ts ), 1 ) . otherwise ( 0 ) ) else : odf = odf . withColumn ( i + \"_compared\" , F . when ( F . col ( i ) <= F . lit ( base_ts ), 1 ) . otherwise ( 0 ) ) if output_mode == \"replace\" : odf = odf . drop ( i ) return odf def timestamp_to_string ( spark, idf, list_of_cols, output_format='%Y-%m-%d %H:%M:%S', output_mode='replace') Convert timestamp/date columns to time string columns with given output format (\"%Y-%m-%d %H:%M:%S\", by default) Parameters spark Spark Session idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". Columns must be of Datetime type or String type in \"%Y-%m-%d %H:%M:%S\" format. output_format Format of the output column(s) (Default value = \"%Y-%m-%d %H:%M:%S\") output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column. \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_str\" e.g. column X is appended as X_str. (Default value = \"replace\") Returns DataFrame Expand source code def timestamp_to_string ( spark , idf , list_of_cols , output_format = \"%Y-%m- %d %H:%M:%S\" , output_mode = \"replace\" ): \"\"\" Convert timestamp/date columns to time string columns with given output format (\"%Y-%m-%d %H:%M:%S\", by default) Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". Columns must be of Datetime type or String type in \"%Y-%m-%d %H:%M:%S\" format. output_format Format of the output column(s) (Default value = \"%Y-%m-%d %H:%M:%S\") output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column. \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_str\" e.g. column X is appended as X_str. (Default value = \"replace\") Returns ------- DataFrame \"\"\" list_of_cols = argument_checker ( \"timestamp_to_string\" , { \"list_of_cols\" : list_of_cols , \"all_columns\" : idf . columns , \"output_mode\" : output_mode , }, ) if not list_of_cols : return idf localtz = ( spark . sql ( \"SET spark.sql.session.timeZone\" ) . select ( \"value\" ) . rdd . flatMap ( lambda x : x ) . collect ()[ 0 ] ) def conversion ( col , form ): output = col . astimezone ( pytz . timezone ( localtz )) . strftime ( form ) return output f_conversion = F . udf ( conversion , T . StringType ()) odf = idf for i in list_of_cols : modify_col = { \"replace\" : i , \"append\" : i + \"_str\" } odf = odf . withColumn ( modify_col [ output_mode ], f_conversion ( F . col ( i ), F . lit ( output_format )) ) return odf def timestamp_to_unix ( spark, idf, list_of_cols, precision='s', tz='local', output_mode='replace') Convert timestamp columns in a specified time zone to Unix time stamp in seconds or milliseconds. Parameters spark Spark Session idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". precision \"ms\", \"s\". \"ms\" option returns the number of milliseconds from the unix epoch (1970-01-01 00:00:00 UTC) . \"s\" option returns the number of seconds from the unix epoch. (Default value = \"s\") tz \"local\", \"gmt\", \"utc\". Timezone of the input column(s) (Default value = \"local\") output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column. \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_unix\" e.g. column X is appended as X_unix. (Default value = \"replace\") Returns DataFrame Expand source code def timestamp_to_unix ( spark , idf , list_of_cols , precision = \"s\" , tz = \"local\" , output_mode = \"replace\" ): \"\"\" Convert timestamp columns in a specified time zone to Unix time stamp in seconds or milliseconds. Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". precision \"ms\", \"s\". \"ms\" option returns the number of milliseconds from the unix epoch (1970-01-01 00:00:00 UTC) . \"s\" option returns the number of seconds from the unix epoch. (Default value = \"s\") tz \"local\", \"gmt\", \"utc\". Timezone of the input column(s) (Default value = \"local\") output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column. \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_unix\" e.g. column X is appended as X_unix. (Default value = \"replace\") Returns ------- DataFrame \"\"\" tz = tz . lower () list_of_cols = argument_checker ( \"timestamp_to_unix\" , { \"list_of_cols\" : list_of_cols , \"all_columns\" : idf . columns , \"output_mode\" : output_mode , \"precision\" : precision , \"tz\" : tz , }, ) if not list_of_cols : return idf localtz = ( spark . sql ( \"SET spark.sql.session.timeZone\" ) . select ( \"value\" ) . rdd . flatMap ( lambda x : x ) . collect ()[ 0 ] ) factor = { \"ms\" : 1000 , \"s\" : 1 } odf = idf for i in list_of_cols : if ( tz in ( \"gmt\" , \"utc\" )) & ( localtz . lower () not in ( \"gmt\" , \"utc\" )): odf = odf . withColumn ( i + \"_local\" , F . from_utc_timestamp ( i , localtz )) else : odf = odf . withColumn ( i + \"_local\" , F . col ( i )) modify_col = { \"replace\" : i , \"append\" : i + \"_unix\" } odf = odf . withColumn ( modify_col [ output_mode ], ( F . col ( i + \"_local\" ) . cast ( \"double\" ) * factor [ precision ]) . cast ( \"long\" ), ) . drop ( i + \"_local\" ) return odf def timezone_conversion ( spark, idf, list_of_cols, given_tz, output_tz, output_mode='replace') Convert timestamp columns from the given timezone (given_tz) to the output timezone (output_tz). Parameters spark Spark Session idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". given_tz Timezone of the input column(s). If \"local\", the timezone of the spark session will be used. output_tz Timezone of the output column(s). If \"local\", the timezone of the spark session will be used. output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column. \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_tzconverted\" e.g. column X is appended as X_tzconverted. (Default value = \"replace\") Returns DataFrame Expand source code def timezone_conversion ( spark , idf , list_of_cols , given_tz , output_tz , output_mode = \"replace\" ): \"\"\" Convert timestamp columns from the given timezone (given_tz) to the output timezone (output_tz). Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". given_tz Timezone of the input column(s). If \"local\", the timezone of the spark session will be used. output_tz Timezone of the output column(s). If \"local\", the timezone of the spark session will be used. output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column. \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_tzconverted\" e.g. column X is appended as X_tzconverted. (Default value = \"replace\") Returns ------- DataFrame \"\"\" list_of_cols = argument_checker ( \"timezone_conversion\" , { \"list_of_cols\" : list_of_cols , \"all_columns\" : idf . columns , \"output_mode\" : output_mode , }, ) if not list_of_cols : return idf localtz = ( spark . sql ( \"SET spark.sql.session.timeZone\" ) . select ( \"value\" ) . rdd . flatMap ( lambda x : x ) . collect ()[ 0 ] ) if given_tz == \"local\" : given_tz = localtz if output_tz == \"local\" : output_tz = localtz odf = idf for i in list_of_cols : modify_col = { \"replace\" : i , \"append\" : i + \"_tzconverted\" } odf = odf . withColumn ( modify_col [ output_mode ], F . from_utc_timestamp ( F . to_utc_timestamp ( i , given_tz ), output_tz ), ) return odf def unix_to_timestamp ( spark, idf, list_of_cols, precision='s', tz='local', output_mode='replace') Convert the number of seconds or milliseconds from unix epoch (1970-01-01 00:00:00 UTC) to a timestamp column in the specified time zone. Parameters spark Spark Session idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". precision \"ms\", \"s\". \"ms\" treats the input columns as the number of milliseconds from the unix epoch (1970-01-01 00:00:00 UTC) . \"s\" treats the input columns as the number of seconds from the unix epoch. (Default value = \"s\") tz \"local\", \"gmt\", \"utc\". timezone of the output column(s) (Default value = \"local\") output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column. \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_ts\" e.g. column X is appended as X_ts. (Default value = \"replace\") Returns DataFrame Expand source code def unix_to_timestamp ( spark , idf , list_of_cols , precision = \"s\" , tz = \"local\" , output_mode = \"replace\" ): \"\"\" Convert the number of seconds or milliseconds from unix epoch (1970-01-01 00:00:00 UTC) to a timestamp column in the specified time zone. Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". precision \"ms\", \"s\". \"ms\" treats the input columns as the number of milliseconds from the unix epoch (1970-01-01 00:00:00 UTC) . \"s\" treats the input columns as the number of seconds from the unix epoch. (Default value = \"s\") tz \"local\", \"gmt\", \"utc\". timezone of the output column(s) (Default value = \"local\") output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column. \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_ts\" e.g. column X is appended as X_ts. (Default value = \"replace\") Returns ------- DataFrame \"\"\" tz = tz . lower () list_of_cols = argument_checker ( \"unix_to_timestamp\" , { \"list_of_cols\" : list_of_cols , \"all_columns\" : idf . columns , \"output_mode\" : output_mode , \"precision\" : precision , \"tz\" : tz , }, ) if not list_of_cols : return idf localtz = ( spark . sql ( \"SET spark.sql.session.timeZone\" ) . select ( \"value\" ) . rdd . flatMap ( lambda x : x ) . collect ()[ 0 ] ) factor = { \"ms\" : 1000 , \"s\" : 1 } odf = idf for i in list_of_cols : modify_col = { \"replace\" : i , \"append\" : i + \"_ts\" } odf = odf . withColumn ( modify_col [ output_mode ], F . to_timestamp ( F . col ( i ) / factor [ precision ]) ) if ( tz in ( \"gmt\" , \"utc\" )) & ( localtz . lower () not in ( \"gmt\" , \"utc\" )): odf = odf . withColumn ( modify_col [ output_mode ], F . to_utc_timestamp ( modify_col [ output_mode ], localtz ), ) return odf def window_aggregator ( idf, list_of_cols, list_of_aggs, order_col, window_type='expanding', window_size='unbounded', partition_col='', output_mode='append') window_aggregator calcuates a list of aggregate metrics for all input columns over a window frame (expanding by default, or rolling type) ordered by the given timestamp column and partitioned by partition_col (\"\" by default, to indicate no partition). Window size needs to be provided as an integer for rolling window type. The following aggregate metrics are supported: count, min, max, sum, mean, median. Parameters idf Input Dataframe list_of_cols List of columns to aggregate e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". list_of_aggs List of aggregate metrics to compute e.g., [\"f1\",\"f2\"]. Alternatively, metrics can be specified in a string format, where different metrics are separated by pipe delimiter \u201c|\u201d e.g., \"f1|f2\". Supported metrics: \"count\", \"min\", \"max\", \"sum\", \"mean\", \"median\" order_col Timestamp Column to order window window_type \"expanding\", \"rolling\" \"expanding\" option has a fixed lower bound (first row in the partition) \"rolling\" option has a fixed window size defined by window_size param (Default value = \"expanding\") window_size window size for rolling window type. Integer value with value >= 1. (Default value = \"unbounded\") partition_col Rows partitioned by this column before creating window. (Default value = \"\") output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column(s) with metric name as postfix. \u201cappend\u201d option appends derived column(s) to the input dataset with metric name as postfix, e.g. \"_count\", \"_mean\". (Default value = \"append\") Returns DataFrame Expand source code def window_aggregator ( idf , list_of_cols , list_of_aggs , order_col , window_type = \"expanding\" , window_size = \"unbounded\" , partition_col = \"\" , output_mode = \"append\" , ): \"\"\" window_aggregator calcuates a list of aggregate metrics for all input columns over a window frame (expanding by default, or rolling type) ordered by the given timestamp column and partitioned by partition_col (\"\" by default, to indicate no partition). Window size needs to be provided as an integer for rolling window type. The following aggregate metrics are supported: count, min, max, sum, mean, median. Parameters ---------- idf Input Dataframe list_of_cols List of columns to aggregate e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". list_of_aggs List of aggregate metrics to compute e.g., [\"f1\",\"f2\"]. Alternatively, metrics can be specified in a string format, where different metrics are separated by pipe delimiter \u201c|\u201d e.g., \"f1|f2\". Supported metrics: \"count\", \"min\", \"max\", \"sum\", \"mean\", \"median\" order_col Timestamp Column to order window window_type \"expanding\", \"rolling\" \"expanding\" option has a fixed lower bound (first row in the partition) \"rolling\" option has a fixed window size defined by window_size param (Default value = \"expanding\") window_size window size for rolling window type. Integer value with value >= 1. (Default value = \"unbounded\") partition_col Rows partitioned by this column before creating window. (Default value = \"\") output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column(s) with metric name as postfix. \u201cappend\u201d option appends derived column(s) to the input dataset with metric name as postfix, e.g. \"_count\", \"_mean\". (Default value = \"append\") Returns ------- DataFrame \"\"\" if isinstance ( list_of_aggs , str ): list_of_aggs = [ x . strip () for x in list_of_aggs . split ( \"|\" )] all_aggs = [ \"count\" , \"min\" , \"max\" , \"sum\" , \"mean\" , \"median\" ] list_of_cols = argument_checker ( \"window_aggregator\" , { \"list_of_cols\" : list_of_cols , \"all_columns\" : idf . columns , \"list_of_aggs\" : list_of_aggs , \"all_aggs\" : all_aggs , \"output_mode\" : output_mode , \"window_type\" : window_type , \"window_size\" : window_size , }, ) if not list_of_cols : return idf odf = idf window_upper = ( Window . unboundedPreceding if window_type == \"expanding\" else - int ( window_size ) ) if partition_col : window = ( Window . partitionBy ( partition_col ) . orderBy ( order_col ) . rowsBetween ( window_upper , 0 ) ) else : window = Window . partitionBy () . orderBy ( order_col ) . rowsBetween ( window_upper , 0 ) def agg_funcs ( col ): mapping = { \"count\" : F . count ( col ) . over ( window ) . alias ( col + \"_count\" ), \"min\" : F . min ( col ) . over ( window ) . alias ( col + \"_min\" ), \"max\" : F . max ( col ) . over ( window ) . alias ( col + \"_max\" ), \"sum\" : F . sum ( col ) . over ( window ) . alias ( col + \"_sum\" ), \"mean\" : F . mean ( col ) . over ( window ) . alias ( col + \"_mean\" ), \"median\" : F . expr ( \"percentile_approx(\" + col + \", 0.5)\" ) . over ( window ) . alias ( col + \"_median\" ), } derived_cols = [] for agg in list_of_aggs : derived_cols . append ( mapping [ agg ]) return derived_cols for i in list_of_cols : derived_cols = agg_funcs ( i ) odf = odf . select ( odf . columns + derived_cols ) if output_mode == \"replace\" : odf = odf . drop ( i ) return odf","title":"<code>datetime</code>"},{"location":"api/data_transformer/datetime.html#datetime","text":"Datetime module supports various transformations related to date and timestamp datatype columns. All available functions in this release can be classified into the following 4 categories: Conversion: - Between Timestamp and Epoch (timestamp_to_unix and unix_to_timestamp) - Between Timestamp and String (timestamp_to_string and string_to_timestamp) - Between Date Formats (dateformat_conversion) - Between Time Zones (timezone_conversion) Calculation: - Time difference - [Timestamp 1 - Timestamp 2] (time_diff) - Time elapsed - [Current - Given Timestamp] (time_elapsed) - Adding/subtracting time units (adding_timeUnits) - Aggregate features at X granularity level (aggregator) - Aggregate features with window frame (window_aggregator) - Lagged features - lagged date and time diff from the lagged date (lagged_ts) Extraction: - Time component extraction (timeUnits_extraction) - Start/end of month/year/quarter (start_of_month, end_of_month, start_of_year, end_of_year, start_of_quarter and end_of_quarter) Binary features: - Timestamp comparison (timestamp_comparison) - Is start/end of month/year/quarter nor not (is_monthStart, is_monthEnd, is_yearStart, is_yearEnd, is_quarterStart, is_quarterEnd) - Is first half of the year/selected hours/leap year/weekend or not (is_yearFirstHalf, is_selectedHour, is_leapYear and is_weekend) Expand source code \"\"\" Datetime module supports various transformations related to date and timestamp datatype columns. All available functions in this release can be classified into the following 4 categories: Conversion: - Between Timestamp and Epoch (timestamp_to_unix and unix_to_timestamp) - Between Timestamp and String (timestamp_to_string and string_to_timestamp) - Between Date Formats (dateformat_conversion) - Between Time Zones (timezone_conversion) Calculation: - Time difference - [Timestamp 1 - Timestamp 2] (time_diff) - Time elapsed - [Current - Given Timestamp] (time_elapsed) - Adding/subtracting time units (adding_timeUnits) - Aggregate features at X granularity level (aggregator) - Aggregate features with window frame (window_aggregator) - Lagged features - lagged date and time diff from the lagged date (lagged_ts) Extraction: - Time component extraction (timeUnits_extraction) - Start/end of month/year/quarter (start_of_month, end_of_month, start_of_year, end_of_year, start_of_quarter and end_of_quarter) Binary features: - Timestamp comparison (timestamp_comparison) - Is start/end of month/year/quarter nor not (is_monthStart, is_monthEnd, is_yearStart, is_yearEnd, is_quarterStart, is_quarterEnd) - Is first half of the year/selected hours/leap year/weekend or not (is_yearFirstHalf, is_selectedHour, is_leapYear and is_weekend) \"\"\" import calendar import warnings import pytz from pyspark.sql import Window from pyspark.sql import functions as F from pyspark.sql import types as T from datetime import datetime as dt def argument_checker ( func_name , args ): \"\"\" Parameters ---------- func_name function name for which argument needs to be check args arguments to check in dictionary format Returns ------- List list of columns to analyze \"\"\" list_of_cols = args [ \"list_of_cols\" ] all_columns = args [ \"all_columns\" ] if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if any ( x not in all_columns for x in list_of_cols ): raise TypeError ( \"Invalid input for Column(s)\" ) if len ( list_of_cols ) == 0 : warnings . warn ( \"No timestamp conversion - No column(s) to convert\" ) return [] if func_name not in [ \"aggregator\" ]: if args [ \"output_mode\" ] not in ( \"replace\" , \"append\" ): raise TypeError ( \"Invalid input for output_mode\" ) if func_name in [ \"timestamp_to_unix\" , \"unix_to_timestamp\" ]: if args [ \"precision\" ] not in ( \"ms\" , \"s\" ): raise TypeError ( \"Invalid input for precision\" ) if args [ \"tz\" ] not in ( \"local\" , \"gmt\" , \"utc\" ): raise TypeError ( \"Invalid input for timezone\" ) if func_name in [ \"string_to_timestamp\" ]: if args [ \"output_type\" ] not in ( \"ts\" , \"dt\" ): raise TypeError ( \"Invalid input for output_type\" ) if func_name in [ \"timeUnits_extraction\" ]: if any ( x not in args [ \"all_units\" ] for x in args [ \"units\" ]): raise TypeError ( \"Invalid input for Unit(s)\" ) if func_name in [ \"adding_timeUnits\" ]: if args [ \"unit\" ] not in ( args [ \"all_units\" ] + [( e + \"s\" ) for e in args [ \"all_units\" ]] ): raise TypeError ( \"Invalid input for Unit\" ) if func_name in [ \"timestamp_comparison\" ]: if args [ \"comparison_type\" ] not in args [ \"all_types\" ]: raise TypeError ( \"Invalid input for comparison_type\" ) if func_name in [ \"is_selectedHour\" ]: hours = list ( range ( 0 , 24 )) if args [ \"start_hour\" ] not in hours : raise TypeError ( \"Invalid input for start_hour\" ) if args [ \"end_hour\" ] not in hours : raise TypeError ( \"Invalid input for end_hour\" ) if func_name in [ \"window_aggregator\" ]: if any ( x not in args [ \"all_aggs\" ] for x in args [ \"list_of_aggs\" ]): raise TypeError ( \"Invalid input for Aggregate Function(s)\" ) if args [ \"window_type\" ] not in ( \"expanding\" , \"rolling\" ): raise TypeError ( \"Invalid input for Window Type\" ) if ( args [ \"window_type\" ] == \"rolling\" ) & ( not str ( args [ \"window_size\" ]) . isnumeric () ): raise TypeError ( \"Invalid input for Window Size\" ) if func_name in [ \"aggregator\" ]: if any ( x not in args [ \"all_aggs\" ] for x in args [ \"list_of_aggs\" ]): raise TypeError ( \"Invalid input for Aggregate Function(s)\" ) if args [ \"time_col\" ] not in all_columns : raise TypeError ( \"Invalid input for time_col\" ) if func_name in [ \"lagged_ts\" ]: if not str ( args [ \"lag\" ]) . isnumeric (): raise TypeError ( \"Invalid input for Lag\" ) if args [ \"output_type\" ] not in ( \"ts\" , \"ts_diff\" ): raise TypeError ( \"Invalid input for output_type\" ) return list_of_cols def timestamp_to_unix ( spark , idf , list_of_cols , precision = \"s\" , tz = \"local\" , output_mode = \"replace\" ): \"\"\" Convert timestamp columns in a specified time zone to Unix time stamp in seconds or milliseconds. Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". precision \"ms\", \"s\". \"ms\" option returns the number of milliseconds from the unix epoch (1970-01-01 00:00:00 UTC) . \"s\" option returns the number of seconds from the unix epoch. (Default value = \"s\") tz \"local\", \"gmt\", \"utc\". Timezone of the input column(s) (Default value = \"local\") output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column. \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_unix\" e.g. column X is appended as X_unix. (Default value = \"replace\") Returns ------- DataFrame \"\"\" tz = tz . lower () list_of_cols = argument_checker ( \"timestamp_to_unix\" , { \"list_of_cols\" : list_of_cols , \"all_columns\" : idf . columns , \"output_mode\" : output_mode , \"precision\" : precision , \"tz\" : tz , }, ) if not list_of_cols : return idf localtz = ( spark . sql ( \"SET spark.sql.session.timeZone\" ) . select ( \"value\" ) . rdd . flatMap ( lambda x : x ) . collect ()[ 0 ] ) factor = { \"ms\" : 1000 , \"s\" : 1 } odf = idf for i in list_of_cols : if ( tz in ( \"gmt\" , \"utc\" )) & ( localtz . lower () not in ( \"gmt\" , \"utc\" )): odf = odf . withColumn ( i + \"_local\" , F . from_utc_timestamp ( i , localtz )) else : odf = odf . withColumn ( i + \"_local\" , F . col ( i )) modify_col = { \"replace\" : i , \"append\" : i + \"_unix\" } odf = odf . withColumn ( modify_col [ output_mode ], ( F . col ( i + \"_local\" ) . cast ( \"double\" ) * factor [ precision ]) . cast ( \"long\" ), ) . drop ( i + \"_local\" ) return odf def unix_to_timestamp ( spark , idf , list_of_cols , precision = \"s\" , tz = \"local\" , output_mode = \"replace\" ): \"\"\" Convert the number of seconds or milliseconds from unix epoch (1970-01-01 00:00:00 UTC) to a timestamp column in the specified time zone. Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". precision \"ms\", \"s\". \"ms\" treats the input columns as the number of milliseconds from the unix epoch (1970-01-01 00:00:00 UTC) . \"s\" treats the input columns as the number of seconds from the unix epoch. (Default value = \"s\") tz \"local\", \"gmt\", \"utc\". timezone of the output column(s) (Default value = \"local\") output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column. \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_ts\" e.g. column X is appended as X_ts. (Default value = \"replace\") Returns ------- DataFrame \"\"\" tz = tz . lower () list_of_cols = argument_checker ( \"unix_to_timestamp\" , { \"list_of_cols\" : list_of_cols , \"all_columns\" : idf . columns , \"output_mode\" : output_mode , \"precision\" : precision , \"tz\" : tz , }, ) if not list_of_cols : return idf localtz = ( spark . sql ( \"SET spark.sql.session.timeZone\" ) . select ( \"value\" ) . rdd . flatMap ( lambda x : x ) . collect ()[ 0 ] ) factor = { \"ms\" : 1000 , \"s\" : 1 } odf = idf for i in list_of_cols : modify_col = { \"replace\" : i , \"append\" : i + \"_ts\" } odf = odf . withColumn ( modify_col [ output_mode ], F . to_timestamp ( F . col ( i ) / factor [ precision ]) ) if ( tz in ( \"gmt\" , \"utc\" )) & ( localtz . lower () not in ( \"gmt\" , \"utc\" )): odf = odf . withColumn ( modify_col [ output_mode ], F . to_utc_timestamp ( modify_col [ output_mode ], localtz ), ) return odf def timezone_conversion ( spark , idf , list_of_cols , given_tz , output_tz , output_mode = \"replace\" ): \"\"\" Convert timestamp columns from the given timezone (given_tz) to the output timezone (output_tz). Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". given_tz Timezone of the input column(s). If \"local\", the timezone of the spark session will be used. output_tz Timezone of the output column(s). If \"local\", the timezone of the spark session will be used. output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column. \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_tzconverted\" e.g. column X is appended as X_tzconverted. (Default value = \"replace\") Returns ------- DataFrame \"\"\" list_of_cols = argument_checker ( \"timezone_conversion\" , { \"list_of_cols\" : list_of_cols , \"all_columns\" : idf . columns , \"output_mode\" : output_mode , }, ) if not list_of_cols : return idf localtz = ( spark . sql ( \"SET spark.sql.session.timeZone\" ) . select ( \"value\" ) . rdd . flatMap ( lambda x : x ) . collect ()[ 0 ] ) if given_tz == \"local\" : given_tz = localtz if output_tz == \"local\" : output_tz = localtz odf = idf for i in list_of_cols : modify_col = { \"replace\" : i , \"append\" : i + \"_tzconverted\" } odf = odf . withColumn ( modify_col [ output_mode ], F . from_utc_timestamp ( F . to_utc_timestamp ( i , given_tz ), output_tz ), ) return odf def string_to_timestamp ( spark , idf , list_of_cols , input_format = \"%Y-%m- %d %H:%M:%S\" , output_type = \"ts\" , output_mode = \"replace\" , ): \"\"\" Convert time string columns with given input format (\"%Y-%m-%d %H:%M:%S\", by default) to TimestampType or DateType columns. Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". input_format Format of the input column(s) in string (Default value = \"%Y-%m-%d %H:%M:%S\") output_type \"ts\", \"dt\" \"ts\" option returns result in T.TimestampType() \"dt\" option returns result in T.DateType() (Default value = \"ts\") output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column. \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_ts\" e.g. column X is appended as X_ts. (Default value = \"replace\") Returns ------- DataFrame \"\"\" list_of_cols = argument_checker ( \"string_to_timestamp\" , { \"list_of_cols\" : list_of_cols , \"all_columns\" : idf . columns , \"output_mode\" : output_mode , \"output_type\" : output_type , }, ) if not list_of_cols : return idf localtz = ( spark . sql ( \"SET spark.sql.session.timeZone\" ) . select ( \"value\" ) . rdd . flatMap ( lambda x : x ) . collect ()[ 0 ] ) def conversion ( col , form ): output = pytz . timezone ( localtz ) . localize ( dt . strptime ( str ( col ), form )) return output data_type = { \"ts\" : T . TimestampType (), \"dt\" : T . DateType ()} f_conversion = F . udf ( conversion , data_type [ output_type ]) odf = idf for i in list_of_cols : modify_col = { \"replace\" : i , \"append\" : i + \"_ts\" } odf = odf . withColumn ( modify_col [ output_mode ], f_conversion ( F . col ( i ), F . lit ( input_format )) ) return odf def timestamp_to_string ( spark , idf , list_of_cols , output_format = \"%Y-%m- %d %H:%M:%S\" , output_mode = \"replace\" ): \"\"\" Convert timestamp/date columns to time string columns with given output format (\"%Y-%m-%d %H:%M:%S\", by default) Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". Columns must be of Datetime type or String type in \"%Y-%m-%d %H:%M:%S\" format. output_format Format of the output column(s) (Default value = \"%Y-%m-%d %H:%M:%S\") output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column. \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_str\" e.g. column X is appended as X_str. (Default value = \"replace\") Returns ------- DataFrame \"\"\" list_of_cols = argument_checker ( \"timestamp_to_string\" , { \"list_of_cols\" : list_of_cols , \"all_columns\" : idf . columns , \"output_mode\" : output_mode , }, ) if not list_of_cols : return idf localtz = ( spark . sql ( \"SET spark.sql.session.timeZone\" ) . select ( \"value\" ) . rdd . flatMap ( lambda x : x ) . collect ()[ 0 ] ) def conversion ( col , form ): output = col . astimezone ( pytz . timezone ( localtz )) . strftime ( form ) return output f_conversion = F . udf ( conversion , T . StringType ()) odf = idf for i in list_of_cols : modify_col = { \"replace\" : i , \"append\" : i + \"_str\" } odf = odf . withColumn ( modify_col [ output_mode ], f_conversion ( F . col ( i ), F . lit ( output_format )) ) return odf def dateformat_conversion ( spark , idf , list_of_cols , input_format = \"%Y-%m- %d %H:%M:%S\" , output_format = \"%Y-%m- %d %H:%M:%S\" , output_mode = \"replace\" , ): \"\"\" Convert time string columns with given input format (\"%Y-%m-%d %H:%M:%S\", by default) to time string columns with given output format (\"%Y-%m-%d %H:%M:%S\", by default). Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". input_format Format of the input column(s) in string (Default value = \"%Y-%m-%d %H:%M:%S\") output_format Format of the output column(s) in string (Default value = \"%Y-%m-%d %H:%M:%S\") output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column. \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_ts\" e.g. column X is appended as X_ts. (Default value = \"replace\") Returns ------- DataFrame \"\"\" list_of_cols = argument_checker ( \"dateformat_conversion\" , { \"list_of_cols\" : list_of_cols , \"all_columns\" : idf . columns , \"output_mode\" : output_mode , }, ) if not list_of_cols : return idf odf_tmp = string_to_timestamp ( spark , idf , list_of_cols , input_format = input_format , output_type = \"ts\" , output_mode = output_mode , ) appended_cols = { \"append\" : [ col + \"_ts\" for col in list_of_cols ], \"replace\" : list_of_cols , } odf = timestamp_to_string ( spark , odf_tmp , appended_cols [ output_mode ], output_format = output_format , output_mode = \"replace\" , ) return odf def timeUnits_extraction ( idf , list_of_cols , units , output_mode = \"append\" ): \"\"\" Extract the unit(s) of given timestamp columns as integer. Currently the following units are supported: hour, minute, second, dayofmonth, dayofweek, dayofyear, weekofyear, month, quarter, year. Multiple units can be calculated at the same time by inputting a list of units or a string of units separated by pipe delimiter \u201c|\u201d. Parameters ---------- idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". units List of unit(s) to extract. Alternatively, unit(s) can be specified in a string format, where different units are separated by pipe delimiter \u201c|\u201d e.g., \"hour|minute\". Supported units to extract: \"hour\", \"minute\", \"second\",\"dayofmonth\",\"dayofweek\", \"dayofyear\",\"weekofyear\",\"month\",\"quarter\",\"year\". \"all\" can be passed to compute all supported metrics. output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived columns with a postfix \"_<unit>\", e.g. column X is replaced with X_second for units=\"second\". \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_<unit>\", e.g. column X is appended as X_second for units=\"second\". (Default value = \"append\") Returns ------- DataFrame \"\"\" all_units = [ \"hour\" , \"minute\" , \"second\" , \"dayofmonth\" , \"dayofweek\" , \"dayofyear\" , \"weekofyear\" , \"month\" , \"quarter\" , \"year\" , ] if units == \"all\" : units = all_units if isinstance ( units , str ): units = [ x . strip () for x in units . split ( \"|\" )] list_of_cols = argument_checker ( \"timeUnits_extraction\" , { \"list_of_cols\" : list_of_cols , \"all_columns\" : idf . columns , \"output_mode\" : output_mode , \"units\" : units , \"all_units\" : all_units , }, ) if not list_of_cols : return idf odf = idf for i in list_of_cols : for e in units : func = getattr ( F , e ) odf = odf . withColumn ( i + \"_\" + e , func ( i )) if output_mode == \"replace\" : odf = odf . drop ( i ) return odf def time_diff ( idf , ts1 , ts2 , unit , output_mode = \"append\" ): \"\"\" Calculate the time difference between 2 timestamp columns (Timestamp 1 - Timestamp 2) in a given unit. Currently the following units are supported: second, minute, hour, day, week, month, year. Parameters ---------- idf Input Dataframe ts1 First column to calculate the difference ts2 Second column to calculate the difference. unit \"second\", \"minute\", \"hour\", \"day\", \"week\", \"month\", \"year\". Unit of the output values. output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column <ts1>_<ts2>_<unit>diff, e.g. Given ts1=X, ts2=Y , X and Y are replaced with X_Y_daydiff for unit=\"day\". \u201cappend\u201d option appends derived column to the input dataset with name = <ts1>_<ts2>_<unit>diff, e.g. Given ts1=X, ts2=Y, X_Y_daydiff is appended for unit=\"day\". (Default value = \"append\") Returns ------- DataFrame \"\"\" argument_checker ( \"time_diff\" , { \"list_of_cols\" : [ ts1 , ts2 ], \"all_columns\" : idf . columns , \"output_mode\" : output_mode , }, ) factor_mapping = { \"second\" : 1 , \"minute\" : 60 , \"hour\" : 3600 , \"day\" : 86400 , \"week\" : 604800 , \"month\" : 2628000 , \"year\" : 31536000 , } if unit in factor_mapping . keys (): factor = factor_mapping [ unit ] elif unit in [( e + \"s\" ) for e in factor_mapping . keys ()]: unit = unit [: - 1 ] factor = factor_mapping [ unit ] else : raise TypeError ( \"Invalid input of unit\" ) odf = idf . withColumn ( ts1 + \"_\" + ts2 + \"_\" + unit + \"diff\" , F . abs (( F . col ( ts1 ) . cast ( \"double\" ) - F . col ( ts2 ) . cast ( \"double\" ))) / factor , ) if output_mode == \"replace\" : odf = odf . drop ( ts1 , ts2 ) return odf def time_elapsed ( idf , list_of_cols , unit , output_mode = \"append\" ): \"\"\" Calculate time difference between the current and the given timestamp (Current - Given Timestamp) in a given unit. Currently the following units are supported: second, minute, hour, day, week, month, year. Parameters ---------- idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". unit \"second\", \"minute\", \"hour\", \"day\", \"week\", \"month\", \"year\". Unit of the output values. output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived columns with a postfix \"_<unit>diff\", e.g. column X is replaced with X_daydiff for unit=\"day\". \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_<unit>diff\", e.g. column X is appended as X_daydiff for unit=\"day\". (Default value = \"append\") Returns ------- DataFrame \"\"\" list_of_cols = argument_checker ( \"time_elapsed\" , { \"list_of_cols\" : list_of_cols , \"all_columns\" : idf . columns , \"output_mode\" : output_mode , }, ) if not list_of_cols : return idf factor_mapping = { \"second\" : 1 , \"minute\" : 60 , \"hour\" : 3600 , \"day\" : 86400 , \"week\" : 604800 , \"month\" : 2628000 , \"year\" : 31536000 , } if unit in factor_mapping . keys (): factor = factor_mapping [ unit ] elif unit in [( e + \"s\" ) for e in factor_mapping . keys ()]: unit = unit [: - 1 ] factor = factor_mapping [ unit ] else : raise TypeError ( \"Invalid input of unit\" ) odf = idf for i in list_of_cols : odf = odf . withColumn ( i + \"_\" + unit + \"diff\" , F . abs ( ( F . lit ( F . current_timestamp ()) . cast ( \"double\" ) - F . col ( i ) . cast ( \"double\" )) ) / factor , ) if output_mode == \"replace\" : odf = odf . drop ( i ) return odf def adding_timeUnits ( idf , list_of_cols , unit , unit_value , output_mode = \"append\" ): \"\"\" Add or subtract given time units to/from timestamp columns. Currently the following units are supported: second, minute, hour, day, week, month, year. Subtraction can be performed by setting a negative unit_value. Parameters ---------- idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". unit \"second\", \"minute\", \"hour\", \"day\", \"week\", \"month\", \"year\". Unit of the added value. unit_value The value to be added to input column(s). output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived columns with a postfix \"_adjusted\", e.g. column X is replaced with X_adjusted. \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_adjusted\", e.g. column X is appended as X_adjusted. (Default value = \"append\") Returns ------- DataFrame \"\"\" all_units = [ \"hour\" , \"minute\" , \"second\" , \"day\" , \"week\" , \"month\" , \"year\" ] list_of_cols = argument_checker ( \"adding_timeUnits\" , { \"list_of_cols\" : list_of_cols , \"all_columns\" : idf . columns , \"output_mode\" : output_mode , \"unit\" : unit , \"all_units\" : all_units , }, ) if not list_of_cols : return idf odf = idf for i in list_of_cols : odf = odf . withColumn ( i + \"_adjusted\" , F . col ( i ) + F . expr ( \"Interval \" + str ( unit_value ) + \" \" + unit ), ) if output_mode == \"replace\" : odf = odf . drop ( i ) return odf def timestamp_comparison ( spark , idf , list_of_cols , comparison_type , comparison_value , comparison_format = \"%Y-%m- %d %H:%M:%S\" , output_mode = \"append\" , ): \"\"\" Compare timestamp columns with a given timestamp/date value (comparison_value) of given format ( comparison_format). Supported comparison types include greater_than, less_than, greaterThan_equalTo and lessThan_equalTo. The derived values are 1 if True and 0 if False. Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". comparison_type greater_than\", \"less_than\", \"greaterThan_equalTo\", \"lessThan_equalTo\" The comparison type of the transformation. comparison_value The timestamp / date value to compare with in string. comparison_format The format of comparison_value in string. (Default value = \"%Y-%m-%d %H:%M:%S\") output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived columns with a postfix \"_compared\", e.g. column X is replaced with X_compared. \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_compared\", e.g. column X is appended as X_compared. (Default value = \"append\") Returns ------- DataFrame \"\"\" all_types = [ \"greater_than\" , \"less_than\" , \"greaterThan_equalTo\" , \"lessThan_equalTo\" ] list_of_cols = argument_checker ( \"timestamp_comparison\" , { \"list_of_cols\" : list_of_cols , \"all_columns\" : idf . columns , \"output_mode\" : output_mode , \"comparison_type\" : comparison_type , \"all_types\" : all_types , }, ) if not list_of_cols : return idf localtz = ( spark . sql ( \"SET spark.sql.session.timeZone\" ) . select ( \"value\" ) . rdd . flatMap ( lambda x : x ) . collect ()[ 0 ] ) base_ts = pytz . timezone ( localtz ) . localize ( dt . strptime ( comparison_value , comparison_format ) ) odf = idf for i in list_of_cols : if comparison_type == \"greater_than\" : odf = odf . withColumn ( i + \"_compared\" , F . when ( F . col ( i ) > F . lit ( base_ts ), 1 ) . otherwise ( 0 ) ) elif comparison_type == \"less_than\" : odf = odf . withColumn ( i + \"_compared\" , F . when ( F . col ( i ) < F . lit ( base_ts ), 1 ) . otherwise ( 0 ) ) elif comparison_type == \"greaterThan_equalTo\" : odf = odf . withColumn ( i + \"_compared\" , F . when ( F . col ( i ) >= F . lit ( base_ts ), 1 ) . otherwise ( 0 ) ) else : odf = odf . withColumn ( i + \"_compared\" , F . when ( F . col ( i ) <= F . lit ( base_ts ), 1 ) . otherwise ( 0 ) ) if output_mode == \"replace\" : odf = odf . drop ( i ) return odf def start_of_month ( idf , list_of_cols , output_mode = \"append\" ): \"\"\" Extract the first day of the month of given timestamp/date columns. Parameters ---------- idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column with a postfix \"_monthStart\". \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_monthStart\", e.g. column X is appended as X_monthStart. (Default value = \"append\") Returns ------- DataFrame \"\"\" list_of_cols = argument_checker ( \"start_of_month\" , { \"list_of_cols\" : list_of_cols , \"all_columns\" : idf . columns , \"output_mode\" : output_mode , }, ) if not list_of_cols : return idf odf = idf for i in list_of_cols : odf = odf . withColumn ( i + \"_monthStart\" , F . trunc ( i , \"month\" )) if output_mode == \"replace\" : odf = odf . drop ( i ) return odf def is_monthStart ( idf , list_of_cols , output_mode = \"append\" ): \"\"\" Check if values in given timestamp/date columns are the first day of a month. The derived values are 1 if True and 0 if False. Parameters ---------- idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column with a postfix \"_ismonthStart\". \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_ismonthStart\", e.g. column X is appended as X_ismonthStart. (Default value = \"append\") Returns ------- DataFrame \"\"\" list_of_cols = argument_checker ( \"is_monthStart\" , { \"list_of_cols\" : list_of_cols , \"all_columns\" : idf . columns , \"output_mode\" : output_mode , }, ) if not list_of_cols : return idf odf = start_of_month ( idf , list_of_cols , output_mode = \"append\" ) for i in list_of_cols : odf = odf . withColumn ( i + \"_ismonthStart\" , F . when ( F . to_date ( F . col ( i )) == F . col ( i + \"_monthStart\" ), 1 ) . otherwise ( 0 ), ) . drop ( i + \"_monthStart\" ) if output_mode == \"replace\" : odf = odf . drop ( i ) return odf def end_of_month ( idf , list_of_cols , output_mode = \"append\" ): \"\"\" Extract the last day of the month of given timestamp/date columns. Parameters ---------- idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column with a postfix \"_monthEnd\". \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_monthEnd\", e.g. column X is appended as X_monthEnd. (Default value = \"append\") Returns ------- DataFrame \"\"\" list_of_cols = argument_checker ( \"end_of_month\" , { \"list_of_cols\" : list_of_cols , \"all_columns\" : idf . columns , \"output_mode\" : output_mode , }, ) if not list_of_cols : return idf odf = idf for i in list_of_cols : odf = odf . withColumn ( i + \"_monthEnd\" , F . last_day ( i )) if output_mode == \"replace\" : odf = odf . drop ( i ) return odf def is_monthEnd ( idf , list_of_cols , output_mode = \"append\" ): \"\"\" Check if values in given timestamp/date columns are the last day of a month. The derived values are 1 if True and 0 if False. Parameters ---------- idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column with a postfix \"_ismonthEnd\". \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_ismonthEnd\", e.g. column X is appended as X_ismonthEnd. (Default value = \"append\") Returns ------- DataFrame \"\"\" list_of_cols = argument_checker ( \"is_monthEnd\" , { \"list_of_cols\" : list_of_cols , \"all_columns\" : idf . columns , \"output_mode\" : output_mode , }, ) if not list_of_cols : return idf odf = end_of_month ( idf , list_of_cols , output_mode = \"append\" ) for i in list_of_cols : odf = odf . withColumn ( i + \"_ismonthEnd\" , F . when ( F . to_date ( F . col ( i )) == F . col ( i + \"_monthEnd\" ), 1 ) . otherwise ( 0 ), ) . drop ( i + \"_monthEnd\" ) if output_mode == \"replace\" : odf = odf . drop ( i ) return odf def start_of_year ( idf , list_of_cols , output_mode = \"append\" ): \"\"\" Extract the first day of the year of given timestamp/date columns. Parameters ---------- idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column with a postfix \"_yearStart\". \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_yearStart\", e.g. column X is appended as X_yearStart. (Default value = \"append\") Returns ------- DataFrame \"\"\" list_of_cols = argument_checker ( \"start_of_year\" , { \"list_of_cols\" : list_of_cols , \"all_columns\" : idf . columns , \"output_mode\" : output_mode , }, ) if not list_of_cols : return idf odf = idf for i in list_of_cols : odf = odf . withColumn ( i + \"_yearStart\" , F . trunc ( i , \"year\" )) if output_mode == \"replace\" : odf = odf . drop ( i ) return odf def is_yearStart ( idf , list_of_cols , output_mode = \"append\" ): \"\"\" Check if values in given timestamp/date columns are the first day of a year. The derived values are 1 if True and 0 if False. Parameters ---------- idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column with a postfix \"_isyearStart\". \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_isyearStart\", e.g. column X is appended as X_isyearStart. (Default value = \"append\") Returns ------- DataFrame \"\"\" list_of_cols = argument_checker ( \"is_yearStart\" , { \"list_of_cols\" : list_of_cols , \"all_columns\" : idf . columns , \"output_mode\" : output_mode , }, ) if not list_of_cols : return idf odf = start_of_year ( idf , list_of_cols , output_mode = \"append\" ) for i in list_of_cols : odf = odf . withColumn ( i + \"_isyearStart\" , F . when ( F . to_date ( F . col ( i )) == F . col ( i + \"_yearStart\" ), 1 ) . otherwise ( 0 ), ) . drop ( i + \"_yearStart\" ) if output_mode == \"replace\" : odf = odf . drop ( i ) return odf def end_of_year ( idf , list_of_cols , output_mode = \"append\" ): \"\"\" Extract the last day of the year of given timestamp/date columns. Parameters ---------- idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column with a postfix \"_yearEnd\". \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_yearEnd\", e.g. column X is appended as X_yearEnd. (Default value = \"append\") Returns ------- DataFrame \"\"\" list_of_cols = argument_checker ( \"end_of_year\" , { \"list_of_cols\" : list_of_cols , \"all_columns\" : idf . columns , \"output_mode\" : output_mode , }, ) if not list_of_cols : return idf odf = idf for i in list_of_cols : odf = odf . withColumn ( i + \"_yearEnd\" , F . concat_ws ( \"-\" , F . year ( i ), F . lit ( 12 ), F . lit ( 31 )) . cast ( \"date\" ), ) if output_mode == \"replace\" : odf = odf . drop ( i ) return odf def is_yearEnd ( idf , list_of_cols , output_mode = \"append\" ): \"\"\" Check if values in given timestamp/date columns are the last day of a year. The derived values are 1 if True and 0 if False. Parameters ---------- idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column with a postfix \"_isyearEnd\". \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_isyearEnd\", e.g. column X is appended as X_isyearEnd. (Default value = \"append\") Returns ------- DataFrame \"\"\" list_of_cols = argument_checker ( \"is_yearEnd\" , { \"list_of_cols\" : list_of_cols , \"all_columns\" : idf . columns , \"output_mode\" : output_mode , }, ) if not list_of_cols : return idf odf = end_of_year ( idf , list_of_cols , output_mode = \"append\" ) for i in list_of_cols : odf = odf . withColumn ( i + \"_isyearEnd\" , F . when ( F . to_date ( F . col ( i )) == F . col ( i + \"_yearEnd\" ), 1 ) . otherwise ( 0 ), ) . drop ( i + \"_yearEnd\" ) if output_mode == \"replace\" : odf = odf . drop ( i ) return odf def start_of_quarter ( idf , list_of_cols , output_mode = \"append\" ): \"\"\" Extract the first day of the quarter of given timestamp/date columns. Parameters ---------- idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column with a postfix \"_quarterStart. \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_quarterStart\", e.g. column X is appended as X_quarterStart. (Default value = \"append\") Returns ------- DataFrame \"\"\" list_of_cols = argument_checker ( \"start_of_quarter\" , { \"list_of_cols\" : list_of_cols , \"all_columns\" : idf . columns , \"output_mode\" : output_mode , }, ) if not list_of_cols : return idf odf = idf for i in list_of_cols : odf = odf . withColumn ( i + \"_quarterStart\" , F . to_date ( F . date_trunc ( \"quarter\" , i ))) if output_mode == \"replace\" : odf = odf . drop ( i ) return odf def is_quarterStart ( idf , list_of_cols , output_mode = \"append\" ): \"\"\" Check if values in given timestamp/date columns are the first day of a quarter. The derived values are 1 if True and 0 if False. Parameters ---------- idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column with a postfix \"_isquarterStart\". \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_isquarterStart\", e.g. column X is appended as X_isquarterStart. (Default value = \"append\") Returns ------- DataFrame \"\"\" list_of_cols = argument_checker ( \"is_quarterStart\" , { \"list_of_cols\" : list_of_cols , \"all_columns\" : idf . columns , \"output_mode\" : output_mode , }, ) if not list_of_cols : return idf odf = start_of_quarter ( idf , list_of_cols , output_mode = \"append\" ) for i in list_of_cols : odf = odf . withColumn ( i + \"_isquarterStart\" , F . when ( F . to_date ( F . col ( i )) == F . col ( i + \"_quarterStart\" ), 1 ) . otherwise ( 0 ), ) . drop ( i + \"_quarterStart\" ) if output_mode == \"replace\" : odf = odf . drop ( i ) return odf def end_of_quarter ( idf , list_of_cols , output_mode = \"append\" ): \"\"\" Extract the last day of the quarter of given timestamp/date columns. Parameters ---------- idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column with a postfix \"_quarterEnd\". \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_quarterEnd\", e.g. column X is appended as X_quarterEnd. (Default value = \"append\") Returns ------- DataFrame \"\"\" list_of_cols = argument_checker ( \"end_of_quarter\" , { \"list_of_cols\" : list_of_cols , \"all_columns\" : idf . columns , \"output_mode\" : output_mode , }, ) if not list_of_cols : return idf odf = idf for i in list_of_cols : odf = odf . withColumn ( i + \"_quarterEnd\" , F . to_date ( F . date_trunc ( \"quarter\" , i )) + F . expr ( \"Interval 3 months\" ) + F . expr ( \"Interval -1 day\" ), ) if output_mode == \"replace\" : odf = odf . drop ( i ) return odf def is_quarterEnd ( idf , list_of_cols , output_mode = \"append\" ): \"\"\" Check if values in given timestamp/date columns are the last day of a quarter. The derived values are 1 if True and 0 if False. Parameters ---------- idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column with a postfix \"_isquarterEnd\". \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_isquarterEnd\", e.g. column X is appended as X_isquarterEnd. (Default value = \"append\") Returns ------- DataFrame \"\"\" list_of_cols = argument_checker ( \"is_quarterEnd\" , { \"list_of_cols\" : list_of_cols , \"all_columns\" : idf . columns , \"output_mode\" : output_mode , }, ) if not list_of_cols : return idf odf = end_of_quarter ( idf , list_of_cols , output_mode = \"append\" ) for i in list_of_cols : odf = odf . withColumn ( i + \"_isquarterEnd\" , F . when ( F . to_date ( F . col ( i )) == F . col ( i + \"_quarterEnd\" ), 1 ) . otherwise ( 0 ), ) . drop ( i + \"_quarterEnd\" ) if output_mode == \"replace\" : odf = odf . drop ( i ) return odf def is_yearFirstHalf ( idf , list_of_cols , output_mode = \"append\" ): \"\"\" Check if values in given timestamp/date columns are in the first half of a year. The derived values are 1 if True and 0 if False. Parameters ---------- idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column with a postfix \"_isFirstHalf\". \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_isFirstHalf\", e.g. column X is appended as X_isFirstHalf. (Default value = \"append\") Returns ------- DataFrame \"\"\" list_of_cols = argument_checker ( \"is_yearFirstHalf\" , { \"list_of_cols\" : list_of_cols , \"all_columns\" : idf . columns , \"output_mode\" : output_mode , }, ) if not list_of_cols : return idf odf = idf for i in list_of_cols : odf = odf . withColumn ( i + \"_isFirstHalf\" , F . when ( F . month ( F . col ( i )) . isin ( * range ( 1 , 7 )), 1 ) . otherwise ( 0 ), ) if output_mode == \"replace\" : odf = odf . drop ( i ) return odf def is_selectedHour ( idf , list_of_cols , start_hour , end_hour , output_mode = \"append\" ): \"\"\" Check if the hour component of given timestamp columns are between start hour (inclusive) and end hour ( inclusive). The derived values are 1 if True and 0 if False. Start hour can be larger than end hour, for example, start_hour=22 and end_hour=3 can be used to check whether the hour component is in [22, 23, 0, 1, 2, 3]. Parameters ---------- idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". start_hour The starting hour of the hour range (inclusive) end_hour The ending hour of the hour range (inclusive) output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column with a postfix \"_isselectedHour\". \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_isselectedHour\", e.g. column X is appended as X_isselectedHour. (Default value = \"append\") Returns ------- DataFrame \"\"\" list_of_cols = argument_checker ( \"is_selectedHour\" , { \"list_of_cols\" : list_of_cols , \"all_columns\" : idf . columns , \"start_hour\" : start_hour , \"end_hour\" : end_hour , \"output_mode\" : output_mode , }, ) if not list_of_cols : return idf odf = idf if start_hour < end_hour : list_of_hrs = range ( start_hour , end_hour + 1 ) elif start_hour > end_hour : list_of_hrs = list ( range ( start_hour , 24 )) + list ( range ( 0 , end_hour + 1 )) else : list_of_hrs = [ start_hour ] for i in list_of_cols : odf = odf . withColumn ( i + \"_isselectedHour\" , F . when ( F . hour ( F . col ( i )) . isin ( * list_of_hrs ), 1 ) . otherwise ( 0 ), ) if output_mode == \"replace\" : odf = odf . drop ( i ) return odf def is_leapYear ( idf , list_of_cols , output_mode = \"append\" ): \"\"\" Check if values in given timestamp/date columns are in a leap year. The derived values are 1 if True and 0 if False. Parameters ---------- idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column with a postfix \"_isleapYear\". \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_isleapYear\", e.g. column X is appended as X_isleapYear. (Default value = \"append\") Returns ------- DataFrame \"\"\" list_of_cols = argument_checker ( \"is_leapYear\" , { \"list_of_cols\" : list_of_cols , \"all_columns\" : idf . columns , \"output_mode\" : output_mode , }, ) if not list_of_cols : return idf def check ( year ): if calendar . isleap ( year ): return 1 else : return 0 f_check = F . udf ( check , T . IntegerType ()) odf = idf for i in list_of_cols : odf = odf . withColumn ( i + \"_isleapYear\" , f_check ( F . year ( i ))) if output_mode == \"replace\" : odf = odf . drop ( i ) return odf def is_weekend ( idf , list_of_cols , output_mode = \"append\" ): \"\"\" Check if values in given timestamp/date columns are on weekends. The derived values are 1 if True and 0 if False. Parameters ---------- idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column with a postfix \"_isweekend\". \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_isweekend\", e.g. column X is appended as X_isweekend. (Default value = \"append\") Returns ------- DataFrame \"\"\" list_of_cols = argument_checker ( \"is_weekend\" , { \"list_of_cols\" : list_of_cols , \"all_columns\" : idf . columns , \"output_mode\" : output_mode , }, ) if not list_of_cols : return idf odf = idf for i in list_of_cols : odf = odf . withColumn ( i + \"_isweekend\" , F . when ( F . dayofweek ( F . col ( i )) . isin ([ 1 , 7 ]), 1 ) . otherwise ( 0 ) ) if output_mode == \"replace\" : odf = odf . drop ( i ) return odf def aggregator ( spark , idf , list_of_cols , list_of_aggs , time_col , granularity_format = \"%Y-%m- %d \" ): \"\"\" aggregator performs groupBy over the timestamp/date column and calcuates a list of aggregate metrics over all input columns. The timestamp column is firstly converted to the given granularity format (\"%Y-%m-%d\", by default) before applying groupBy and the conversion step can be skipped by setting granularity format to be an empty string. The following aggregate metrics are supported: count, min, max, sum, mean, median, stddev, countDistinct, sumDistinct, collect_list, collect_set. Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of columns to aggregate e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". list_of_aggs List of aggregate metrics to compute e.g., [\"f1\",\"f2\"]. Alternatively, metrics can be specified in a string format, where different metrics are separated by pipe delimiter \u201c|\u201d e.g., \"f1|f2\". Supported metrics: \"count\", \"min\", \"max\", \"sum\",\"mean\",\"median\",\"stddev\", \"countDistinct\",\"sumDistinct\",\"collect_list\",\"collect_set\". time_col Timestamp) Column to group by. granularity_format Format to be applied to time_col before groupBy. The default value is '%Y-%m-%d', which means grouping by the date component of time_col. Alternatively, '' can be used if no formatting is necessary. Returns ------- DataFrame \"\"\" all_aggs = [ \"count\" , \"min\" , \"max\" , \"sum\" , \"mean\" , \"median\" , \"stddev\" , \"countDistinct\" , \"sumDistinct\" , \"collect_list\" , \"collect_set\" , ] if isinstance ( list_of_aggs , str ): list_of_aggs = [ x . strip () for x in list_of_aggs . split ( \"|\" )] list_of_cols = argument_checker ( \"aggregator\" , { \"list_of_cols\" : list_of_cols , \"all_columns\" : idf . columns , \"list_of_aggs\" : list_of_aggs , \"all_aggs\" : all_aggs , \"time_col\" : time_col , }, ) if not list_of_cols : return idf if granularity_format != \"\" : idf = timestamp_to_string ( spark , idf , time_col , output_format = granularity_format , output_mode = \"replace\" , ) def agg_funcs ( col , agg ): mapping = { \"count\" : F . count ( col ) . alias ( col + \"_count\" ), \"min\" : F . min ( col ) . alias ( col + \"_min\" ), \"max\" : F . max ( col ) . alias ( col + \"_max\" ), \"sum\" : F . sum ( col ) . alias ( col + \"_sum\" ), \"mean\" : F . mean ( col ) . alias ( col + \"_mean\" ), \"median\" : F . expr ( \"percentile_approx(\" + col + \", 0.5)\" ) . alias ( col + \"_median\" ), \"stddev\" : F . stddev ( col ) . alias ( col + \"_stddev\" ), \"countDistinct\" : F . countDistinct ( col ) . alias ( col + \"_countDistinct\" ), \"sumDistinct\" : F . sumDistinct ( col ) . alias ( col + \"_sumDistinct\" ), \"collect_list\" : F . collect_list ( col ) . alias ( col + \"_collect_list\" ), \"collect_set\" : F . collect_set ( col ) . alias ( col + \"_collect_set\" ), } return mapping [ agg ] derived_cols = [] for i in list_of_cols : for j in list_of_aggs : derived_cols . append ( agg_funcs ( i , j )) odf = idf . groupBy ( time_col ) . agg ( * derived_cols ) return odf def window_aggregator ( idf , list_of_cols , list_of_aggs , order_col , window_type = \"expanding\" , window_size = \"unbounded\" , partition_col = \"\" , output_mode = \"append\" , ): \"\"\" window_aggregator calcuates a list of aggregate metrics for all input columns over a window frame (expanding by default, or rolling type) ordered by the given timestamp column and partitioned by partition_col (\"\" by default, to indicate no partition). Window size needs to be provided as an integer for rolling window type. The following aggregate metrics are supported: count, min, max, sum, mean, median. Parameters ---------- idf Input Dataframe list_of_cols List of columns to aggregate e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". list_of_aggs List of aggregate metrics to compute e.g., [\"f1\",\"f2\"]. Alternatively, metrics can be specified in a string format, where different metrics are separated by pipe delimiter \u201c|\u201d e.g., \"f1|f2\". Supported metrics: \"count\", \"min\", \"max\", \"sum\", \"mean\", \"median\" order_col Timestamp Column to order window window_type \"expanding\", \"rolling\" \"expanding\" option has a fixed lower bound (first row in the partition) \"rolling\" option has a fixed window size defined by window_size param (Default value = \"expanding\") window_size window size for rolling window type. Integer value with value >= 1. (Default value = \"unbounded\") partition_col Rows partitioned by this column before creating window. (Default value = \"\") output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column(s) with metric name as postfix. \u201cappend\u201d option appends derived column(s) to the input dataset with metric name as postfix, e.g. \"_count\", \"_mean\". (Default value = \"append\") Returns ------- DataFrame \"\"\" if isinstance ( list_of_aggs , str ): list_of_aggs = [ x . strip () for x in list_of_aggs . split ( \"|\" )] all_aggs = [ \"count\" , \"min\" , \"max\" , \"sum\" , \"mean\" , \"median\" ] list_of_cols = argument_checker ( \"window_aggregator\" , { \"list_of_cols\" : list_of_cols , \"all_columns\" : idf . columns , \"list_of_aggs\" : list_of_aggs , \"all_aggs\" : all_aggs , \"output_mode\" : output_mode , \"window_type\" : window_type , \"window_size\" : window_size , }, ) if not list_of_cols : return idf odf = idf window_upper = ( Window . unboundedPreceding if window_type == \"expanding\" else - int ( window_size ) ) if partition_col : window = ( Window . partitionBy ( partition_col ) . orderBy ( order_col ) . rowsBetween ( window_upper , 0 ) ) else : window = Window . partitionBy () . orderBy ( order_col ) . rowsBetween ( window_upper , 0 ) def agg_funcs ( col ): mapping = { \"count\" : F . count ( col ) . over ( window ) . alias ( col + \"_count\" ), \"min\" : F . min ( col ) . over ( window ) . alias ( col + \"_min\" ), \"max\" : F . max ( col ) . over ( window ) . alias ( col + \"_max\" ), \"sum\" : F . sum ( col ) . over ( window ) . alias ( col + \"_sum\" ), \"mean\" : F . mean ( col ) . over ( window ) . alias ( col + \"_mean\" ), \"median\" : F . expr ( \"percentile_approx(\" + col + \", 0.5)\" ) . over ( window ) . alias ( col + \"_median\" ), } derived_cols = [] for agg in list_of_aggs : derived_cols . append ( mapping [ agg ]) return derived_cols for i in list_of_cols : derived_cols = agg_funcs ( i ) odf = odf . select ( odf . columns + derived_cols ) if output_mode == \"replace\" : odf = odf . drop ( i ) return odf def lagged_ts ( idf , list_of_cols , lag , output_type = \"ts\" , tsdiff_unit = \"days\" , partition_col = \"\" , output_mode = \"append\" , ): \"\"\" lagged_ts returns the values that are *lag* rows before the current rows, and None if there is less than *lag* rows before the current rows. If output_type is \"ts_diff\", an additional column is generated with values being the time difference between the original timestamp and the lagged timestamp in given unit *tsdiff_unit*. Currently the following units are supported: second, minute, hour, day, week, month, year. Parameters ---------- idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". lag Number of row(s) to extend. output_type \"ts\", \"ts_diff\". \"ts\" option generats a lag column for each input column having the value that is <lag> rows before the current row, and None if there is less than <lag> rows before the current row. \"ts_diff\" option generates the lag column in the same way as the \"ts\" option. On top of that, it appends a column which represents the time_diff between the original and the lag column. (Default value = \"ts\") tsdiff_unit \"second\", \"minute\", \"hour\", \"day\", \"week\", \"month\", \"year\". Unit of the time_diff if output_type=\"ts_diff\". (Default value = \"days\") partition_col Rows partitioned by this column before creating window. (Default value = \"\") output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column: <col>_lag<lag> for \"ts\" output_type, <col>_lag<lag> and <col>_<col>_lag<lag>_<tsdiff_unit>diff for \"ts_diff\" output_type. \u201cappend\u201d option appends derived column to the input dataset, e.g. given output_type=\"ts_diff\", lag=5, tsdiff_unit=\"days\", column X is appended as X_lag5 and X_X_lag5_daydiff. (Default value = \"append\") Returns ------- DataFrame \"\"\" list_of_cols = argument_checker ( \"lagged_ts\" , { \"list_of_cols\" : list_of_cols , \"all_columns\" : idf . columns , \"lag\" : lag , \"output_type\" : output_type , \"output_mode\" : output_mode , }, ) if not list_of_cols : return idf odf = idf for i in list_of_cols : if partition_col : window = Window . partitionBy ( partition_col ) . orderBy ( i ) else : window = Window . partitionBy () . orderBy ( i ) lag = int ( lag ) odf = odf . withColumn ( i + \"_lag\" + str ( lag ), F . lag ( F . col ( i ), lag ) . over ( window )) if output_type == \"ts_diff\" : odf = time_diff ( odf , i , i + \"_lag\" + str ( lag ), unit = tsdiff_unit , output_mode = \"append\" ) if output_mode == \"replace\" : odf = odf . drop ( i ) return odf","title":"datetime"},{"location":"api/data_transformer/datetime.html#functions","text":"def adding_timeUnits ( idf, list_of_cols, unit, unit_value, output_mode='append') Add or subtract given time units to/from timestamp columns. Currently the following units are supported: second, minute, hour, day, week, month, year. Subtraction can be performed by setting a negative unit_value.","title":"Functions"},{"location":"api/data_transformer/transformers.html","text":"transformers The data transformer module supports selected pre-processing & transformation functions, such as binning, encoding, scaling, imputation, to name a few, which are required for statistics generation and quality checks. Functions supported through this modules are listed below: attribute_binning monotonic_binning cat_to_num_unsupervised cat_to_num_supervised z_standardization IQR_standardization normalization imputation_MMM imputation_sklearn imputation_matrixFactorization auto_imputation autoencoder_latentFeatures PCA_latentFeatures feature_transformation boxcox_transformation outlier_categories expression_parser Expand source code # coding=utf-8 \"\"\" The data transformer module supports selected pre-processing & transformation functions, such as binning, encoding, scaling, imputation, to name a few, which are required for statistics generation and quality checks. Functions supported through this modules are listed below: - attribute_binning - monotonic_binning - cat_to_num_unsupervised - cat_to_num_supervised - z_standardization - IQR_standardization - normalization - imputation_MMM - imputation_sklearn - imputation_matrixFactorization - auto_imputation - autoencoder_latentFeatures - PCA_latentFeatures - feature_transformation - boxcox_transformation - outlier_categories - expression_parser \"\"\" import copy import os import pickle import random import subprocess import tempfile import warnings from itertools import chain import numpy as np import pandas as pd import pyspark from packaging import version from scipy import stats if version . parse ( pyspark . __version__ ) < version . parse ( \"3.0.0\" ): from pyspark.ml.feature import OneHotEncoderEstimator as OneHotEncoder else : from pyspark.ml.feature import OneHotEncoder from pyspark.sql import functions as F from pyspark.sql import types as T from pyspark.sql.window import Window from pyspark.mllib.stat import Statistics from pyspark.ml.feature import StringIndexerModel from pyspark.ml.recommendation import ALS from pyspark.ml.evaluation import RegressionEvaluator from pyspark.ml.feature import Imputer , ImputerModel , StringIndexer , IndexToString from pyspark.ml.feature import ( VectorAssembler , MinMaxScaler , MinMaxScalerModel , PCA , PCAModel , ) from pyspark.ml.linalg import DenseVector from anovos.data_analyzer.stats_generator import ( missingCount_computation , uniqueCount_computation , ) from anovos.data_ingest.data_ingest import read_dataset , recast_column from anovos.shared.utils import attributeType_segregation , get_dtype from ..shared.utils import platform_root_path # enable_iterative_imputer is prequisite for importing IterativeImputer # check the following issue for more details https://github.com/scikit-learn/scikit-learn/issues/16833 from sklearn.experimental import enable_iterative_imputer # noqa from sklearn.impute import KNNImputer , IterativeImputer import tensorflow from tensorflow.keras.models import load_model , Model from tensorflow.keras.layers import Dense , Input , BatchNormalization , LeakyReLU def attribute_binning ( spark , idf , list_of_cols = \"all\" , drop_cols = [], method_type = \"equal_range\" , bin_size = 10 , bin_dtype = \"numerical\" , pre_existing_model = False , model_path = \"NA\" , output_mode = \"replace\" , print_impact = False , ): \"\"\" Attribute binning (or discretization) is a method of numerical attribute into discrete (integer or categorical values) using pre-defined number of bins. This data pre-processing technique is used to reduce the effects of minor observation errors. Also, Binning introduces non-linearity and tends to improve the performance of the model. In this function, we are focussing on unsupervised way of binning i.e. without taking the target variable into account - Equal Range Binning, Equal Frequency Binning. In Equal Range method, each bin is of equal size/width and computed as: w = max- min / no. of bins *bins cutoff=[min, min+w,min+2w\u2026..,max-w,max]* whereas in Equal Frequency binning method, bins are created in such a way that each bin has equal no. of rows, though the width of bins may vary from each other. w = 1 / no. of bins *bins cutoff=[min, wthpctile, 2wthpctile\u2026.,max ]* Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of numerical columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all numerical columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) method_type \"equal_frequency\", \"equal_range\". In \"equal_range\" method, each bin is of equal size/width and in \"equal_frequency\", each bin has equal no. of rows, though the width of bins may vary. (Default value = \"equal_range\") bin_size Number of bins. (Default value = 10) bin_dtype \"numerical\", \"categorical\". With \"numerical\" option, original value is replaced with an Integer (1,2,\u2026) and with \"categorical\" option, original replaced with a string describing min and max value allowed in the bin (\"minval-maxval\"). (Default value = \"numerical\") pre_existing_model Boolean argument \u2013 True or False. True if binning model exists already, False Otherwise. (Default value = False) model_path If pre_existing_model is True, this argument is path for referring the pre-saved model. If pre_existing_model is False, this argument can be used for saving the model. Default \"NA\" means there is neither pre-existing model nor there is a need to save one. output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with transformed column. \u201cappend\u201d option append transformed column to the input dataset with a postfix \"_binned\" e.g. column X is appended as X_binned. (Default value = \"replace\") print_impact True, False (Default value = False) This argument is to print the number of categories generated for each attribute (may or may be not same as bin_size) Returns ------- DataFrame Binned Dataframe \"\"\" num_cols = attributeType_segregation ( idf )[ 0 ] if list_of_cols == \"all\" : list_of_cols = num_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in drop_cols ])) if any ( x not in num_cols for x in list_of_cols ): raise TypeError ( \"Invalid input for Column(s)\" ) if len ( list_of_cols ) == 0 : warnings . warn ( \"No Binning Performed - No numerical column(s) to transform\" ) return idf if method_type not in ( \"equal_frequency\" , \"equal_range\" ): raise TypeError ( \"Invalid input for method_type\" ) if bin_size < 2 : raise TypeError ( \"Invalid input for bin_size\" ) if output_mode not in ( \"replace\" , \"append\" ): raise TypeError ( \"Invalid input for output_mode\" ) if pre_existing_model : df_model = spark . read . parquet ( model_path + \"/attribute_binning\" ) bin_cutoffs = [] for i in list_of_cols : mapped_value = ( df_model . where ( F . col ( \"attribute\" ) == i ) . select ( \"parameters\" ) . rdd . flatMap ( lambda x : x ) . collect ()[ 0 ] ) bin_cutoffs . append ( mapped_value ) else : if method_type == \"equal_frequency\" : pctile_width = 1 / bin_size pctile_cutoff = [] for j in range ( 1 , bin_size ): pctile_cutoff . append ( j * pctile_width ) bin_cutoffs = idf . approxQuantile ( list_of_cols , pctile_cutoff , 0.01 ) else : bin_cutoffs = [] for i in list_of_cols : max_val = ( idf . select ( F . col ( i )) . groupBy () . max () . rdd . flatMap ( lambda x : x ) . collect () + [ None ] )[ 0 ] min_val = ( idf . select ( F . col ( i )) . groupBy () . min () . rdd . flatMap ( lambda x : x ) . collect () + [ None ] )[ 0 ] bin_cutoff = [] if max_val is not None : bin_width = ( max_val - min_val ) / bin_size for j in range ( 1 , bin_size ): bin_cutoff . append ( min_val + j * bin_width ) bin_cutoffs . append ( bin_cutoff ) if model_path != \"NA\" : df_model = spark . createDataFrame ( zip ( list_of_cols , bin_cutoffs ), schema = [ \"attribute\" , \"parameters\" ] ) df_model . write . parquet ( model_path + \"/attribute_binning\" , mode = \"overwrite\" ) def bucket_label ( value , index ): if value is None : return None for i in range ( 0 , len ( bin_cutoffs [ index ])): if value <= bin_cutoffs [ index ][ i ]: if bin_dtype == \"numerical\" : return i + 1 else : if i == 0 : return \"<= \" + str ( round ( bin_cutoffs [ index ][ i ], 4 )) else : return ( str ( round ( bin_cutoffs [ index ][ i - 1 ], 4 )) + \"-\" + str ( round ( bin_cutoffs [ index ][ i ], 4 )) ) else : next if bin_dtype == \"numerical\" : return len ( bin_cutoffs [ 0 ]) + 1 else : return \"> \" + str ( round ( bin_cutoffs [ index ][ len ( bin_cutoffs [ 0 ]) - 1 ], 4 )) if bin_dtype == \"numerical\" : f_bucket_label = F . udf ( bucket_label , T . IntegerType ()) else : f_bucket_label = F . udf ( bucket_label , T . StringType ()) odf = idf for idx , i in enumerate ( list_of_cols ): odf = odf . withColumn ( i + \"_binned\" , f_bucket_label ( F . col ( i ), F . lit ( idx ))) if idx % 5 == 0 : odf . persist ( pyspark . StorageLevel . MEMORY_AND_DISK ) . count () if output_mode == \"replace\" : for col in list_of_cols : odf = odf . drop ( col ) . withColumnRenamed ( col + \"_binned\" , col ) if print_impact : if output_mode == \"replace\" : output_cols = list_of_cols else : output_cols = [( i + \"_binned\" ) for i in list_of_cols ] uniqueCount_computation ( spark , odf , output_cols ) . show ( len ( output_cols ), False ) return odf def monotonic_binning ( spark , idf , list_of_cols = \"all\" , drop_cols = [], label_col = \"label\" , event_label = 1 , bin_method = \"equal_range\" , bin_size = 10 , bin_dtype = \"numerical\" , output_mode = \"replace\" , ): \"\"\" This function constitutes supervised way of binning the numerical attribute into discrete (integer or categorical values) attribute. Instead of pre-defined fixed number of bins, number of bins are dynamically computed to ensure the monotonic nature of bins i.e. % event should increase or decrease with the bin. Monotonic nature of bins is evaluated by looking at spearman rank correlation, which should be either +1 or -1, between the bin index and % event. In case, the monotonic nature is not attained, user defined fixed number of bins are used for the binning. Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of numerical columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all numerical columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) label_col Label/Target column (Default value = \"label\") event_label Value of (positive) event (i.e label 1) (Default value = 1) bin_method \"equal_frequency\", \"equal_range\". In \"equal_range\" method, each bin is of equal size/width and in \"equal_frequency\", each bin has equal no. of rows, though the width of bins may vary. (Default value = \"equal_range\") bin_size Default number of bins in case monotonicity is not achieved. bin_dtype \"numerical\", \"categorical\". With \"numerical\" option, original value is replaced with an Integer (1,2,\u2026) and with \"categorical\" option, original replaced with a string describing min and max value allowed in the bin (\"minval-maxval\"). (Default value = \"numerical\") output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with transformed column. \u201cappend\u201d option append transformed column to the input dataset with a postfix \"_binned\" e.g. column X is appended as X_binned. (Default value = \"replace\") Returns ------- DataFrame Binned Dataframe \"\"\" num_cols = attributeType_segregation ( idf )[ 0 ] if list_of_cols == \"all\" : list_of_cols = num_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in ( drop_cols + [ label_col ])]) ) if any ( x not in num_cols for x in list_of_cols ): raise TypeError ( \"Invalid input for Column(s)\" ) attribute_binning ( spark , idf , list_of_cols = \"all\" , drop_cols = [], method_type = \"equal_range\" , bin_size = 10 , pre_existing_model = False , model_path = \"NA\" , output_mode = \"replace\" , print_impact = False , ) odf = idf for col in list_of_cols : n = 20 r = 0 while n > 2 : tmp = ( attribute_binning ( spark , idf , [ col ], drop_cols = [], method_type = bin_method , bin_size = n , output_mode = \"append\" , ) . select ( label_col , col , col + \"_binned\" ) . withColumn ( label_col , F . when ( F . col ( label_col ) == event_label , 1 ) . otherwise ( 0 ) ) . groupBy ( col + \"_binned\" ) . agg ( F . avg ( col ) . alias ( \"mean_val\" ), F . avg ( label_col ) . alias ( \"mean_label\" )) . dropna () ) r , p = stats . spearmanr ( tmp . toPandas ()[[ \"mean_val\" ]], tmp . toPandas ()[[ \"mean_label\" ]] ) if r == 1.0 : odf = attribute_binning ( spark , odf , [ col ], drop_cols = [], method_type = bin_method , bin_size = n , bin_dtype = bin_dtype , output_mode = output_mode , ) break n = n - 1 r = 0 if r < 1.0 : odf = attribute_binning ( spark , odf , [ col ], drop_cols = [], method_type = bin_method , bin_size = bin_size , bin_dtype = bin_dtype , output_mode = output_mode , ) return odf def cat_to_num_unsupervised ( spark , idf , list_of_cols = \"all\" , drop_cols = [], method_type = 1 , index_order = \"frequencyDesc\" , cardinality_threshold = 100 , pre_existing_model = False , model_path = \"NA\" , stats_unique = {}, output_mode = \"replace\" , print_impact = False , ): \"\"\" This is unsupervised method of converting a categorical attribute into numerical attribute(s). This is among the most important transformations required for any modelling exercise, as most of the machine learning algorithms cannot process categorical values. It covers two popular encoding techniques \u2013 label encoding & one-hot encoding. In label encoding, each categorical value is assigned a unique integer based on alphabetical or frequency ordering (both ascending & descending options are available \u2013 can be selected by index_order argument). One of the pitfalls of using this technique is that the model may learn some spurious relationship, which doesn't exist or might not make any logical sense in the real world settings. In one-hot encoding, every unique value in the attribute will be added as a feature in a form of dummy/binary attribute. However, using this method on high cardinality attributes can further aggravate the dimensionality issue. Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of categorical columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all categorical columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) method_type 1 for Label Encoding or 0 for One hot encoding. In label encoding, each categorical value is assigned a unique integer based on alphabetical or frequency ordering (both ascending & descending options are available that can be selected by index_order argument). In one-hot encoding, every unique value in the column will be added in a form of dummy/binary column. (Default value = 1) index_order \"frequencyDesc\", \"frequencyAsc\", \"alphabetDesc\", \"alphabetAsc\". Valid only for Label Encoding method_type. (Default value = \"frequencyDesc\") cardinality_threshold Defines threshold to skip columns with higher cardinality values from encoding (Warning is issued). (Default value = 100) pre_existing_model Boolean argument \u2013 True or False. True if encoding model exists already, False Otherwise. (Default value = False) model_path If pre_existing_model is True, this argument is path for referring the pre-saved model. If pre_existing_model is False, this argument can be used for saving the model. Default \"NA\" means there is neither pre existing model nor there is a need to save one. stats_unique Takes arguments for read_dataset (data_ingest module) function in a dictionary format to read pre-saved statistics on unique value count i.e. if measures_of_cardinality or uniqueCount_computation (data_analyzer.stats_generator module) has been computed & saved before. (Default value = {}) output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with transformed column. \u201cappend\u201d option append transformed column to the input dataset with a postfix \"_index\" for label encoding e.g. column X is appended as X_index, or a postfix \"_{n}\" for one hot encoding, n varies from 0 to unique value count e.g. column X is appended as X_0, X_1, X_2 (n = 3 i.e. no. of unique values for X). (Default value = \"replace\") print_impact True, False (Default value = False) This argument is to print out the change in schema (one hot encoding) or descriptive statistics (label encoding) Returns ------- DataFrame Encoded Dataframe \"\"\" cat_cols = attributeType_segregation ( idf )[ 1 ] if list_of_cols == \"all\" : list_of_cols = cat_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] if any ( x not in cat_cols for x in list_of_cols ): raise TypeError ( \"Invalid input for Column(s)\" ) if method_type not in ( 0 , 1 ): raise TypeError ( \"Invalid input for method_type\" ) if index_order not in ( \"frequencyDesc\" , \"frequencyAsc\" , \"alphabetDesc\" , \"alphabetAsc\" , ): raise TypeError ( \"Invalid input for Encoding Index Order\" ) if output_mode not in ( \"replace\" , \"append\" ): raise TypeError ( \"Invalid input for output_mode\" ) skip_cols = [] if method_type == 0 : if stats_unique == {}: skip_cols = ( uniqueCount_computation ( spark , idf , list_of_cols ) . where ( F . col ( \"unique_values\" ) > cardinality_threshold ) . select ( \"attribute\" ) . rdd . flatMap ( lambda x : x ) . collect () ) else : skip_cols = ( read_dataset ( spark , ** stats_unique ) . where ( F . col ( \"unique_values\" ) > cardinality_threshold ) . select ( \"attribute\" ) . rdd . flatMap ( lambda x : x ) . collect () ) skip_cols = list ( set ([ e for e in skip_cols if e in list_of_cols and e not in drop_cols ]) ) if skip_cols : warnings . warn ( \"Columns dropped from one-hot encoding due to high cardinality: \" + \",\" . join ( skip_cols ) ) list_of_cols = list ( set ([ e for e in list_of_cols if e not in drop_cols + skip_cols ]) ) if len ( list_of_cols ) == 0 : warnings . warn ( \"No Encoding Computation - No categorical column(s) to transform\" ) return idf list_of_cols_vec = [] list_of_cols_idx = [] for i in list_of_cols : list_of_cols_vec . append ( i + \"_vec\" ) list_of_cols_idx . append ( i + \"_index\" ) idf_id = idf . withColumn ( \"tempID\" , F . monotonically_increasing_id ()) idf_indexed = idf_id . select ([ \"tempID\" ] + list_of_cols ) if version . parse ( pyspark . __version__ ) < version . parse ( \"3.0.0\" ): for idx , i in enumerate ( list_of_cols ): if pre_existing_model : indexerModel = StringIndexerModel . load ( model_path + \"/cat_to_num_unsupervised/indexer-model/\" + i ) else : stringIndexer = StringIndexer ( inputCol = i , outputCol = i + \"_index\" , stringOrderType = index_order , handleInvalid = \"keep\" , ) indexerModel = stringIndexer . fit ( idf . select ( i )) if model_path != \"NA\" : indexerModel . write () . overwrite () . save ( model_path + \"/cat_to_num_unsupervised/indexer-model/\" + i ) idf_indexed = indexerModel . transform ( idf_indexed ) if idx % 5 == 0 : idf_indexed . persist ( pyspark . StorageLevel . MEMORY_AND_DISK ) . count () else : if pre_existing_model : indexerModel = StringIndexerModel . load ( model_path + \"/cat_to_num_unsupervised/indexer\" ) else : stringIndexer = StringIndexer ( inputCols = list_of_cols , outputCols = list_of_cols_idx , stringOrderType = index_order , handleInvalid = \"keep\" , ) indexerModel = stringIndexer . fit ( idf_indexed ) if model_path != \"NA\" : indexerModel . write () . overwrite () . save ( model_path + \"/cat_to_num_unsupervised/indexer\" ) idf_indexed = indexerModel . transform ( idf_indexed ) odf_indexed = idf_id . join ( idf_indexed . drop ( * list_of_cols ), \"tempID\" , \"left_outer\" ) . drop ( \"tempID\" ) odf_indexed . persist ( pyspark . StorageLevel . MEMORY_AND_DISK ) . count () if method_type == 0 : if pre_existing_model : encoder = OneHotEncoder . load ( model_path + \"/cat_to_num_unsupervised/encoder\" ) else : encoder = OneHotEncoder ( inputCols = list_of_cols_idx , outputCols = list_of_cols_vec , handleInvalid = \"keep\" , ) if model_path != \"NA\" : encoder . write () . overwrite () . save ( model_path + \"/cat_to_num_unsupervised/encoder\" ) odf_encoded = encoder . fit ( odf_indexed ) . transform ( odf_indexed ) odf = odf_encoded def vector_to_array ( v ): v = DenseVector ( v ) new_array = list ([ int ( x ) for x in v ]) return new_array f_vector_to_array = F . udf ( vector_to_array , T . ArrayType ( T . IntegerType ())) odf_sample = odf . take ( 1 ) for i in list_of_cols : uniq_cats = odf_sample [ 0 ] . asDict ()[ i + \"_vec\" ] . size odf_schema = odf . schema . add ( T . StructField ( \"tmp\" , T . ArrayType ( T . IntegerType ())) ) for j in range ( 0 , uniq_cats ): odf_schema = odf_schema . add ( T . StructField ( i + \"_\" + str ( j ), T . IntegerType ()) ) odf = ( odf . withColumn ( \"tmp\" , f_vector_to_array ( i + \"_vec\" )) . rdd . map ( lambda x : ( * x , * x [ \"tmp\" ])) . toDF ( schema = odf_schema ) ) if output_mode == \"replace\" : odf = odf . drop ( i , i + \"_vec\" , i + \"_index\" , \"tmp\" ) else : odf = odf . drop ( i + \"_vec\" , i + \"_index\" , \"tmp\" ) else : odf = odf_indexed for i in list_of_cols : odf = odf . withColumn ( i + \"_index\" , F . when ( F . col ( i ) . isNull (), None ) . otherwise ( F . col ( i + \"_index\" ) . cast ( T . IntegerType ()) ), ) if output_mode == \"replace\" : for i in list_of_cols : odf = odf . drop ( i ) . withColumnRenamed ( i + \"_index\" , i ) odf = odf . select ( idf . columns ) if print_impact and method_type == 1 : print ( \"Before\" ) idf . describe () . where ( F . col ( \"summary\" ) . isin ( \"count\" , \"min\" , \"max\" )) . show ( 3 , False ) print ( \"After\" ) odf . describe () . where ( F . col ( \"summary\" ) . isin ( \"count\" , \"min\" , \"max\" )) . show ( 3 , False ) if print_impact and method_type == 0 : print ( \"Before\" ) idf . printSchema () print ( \"After\" ) odf . printSchema () return odf def cat_to_num_supervised ( spark , idf , list_of_cols = \"all\" , drop_cols = [], label_col = \"label\" , event_label = 1 , pre_existing_model = False , model_path = \"NA\" , output_mode = \"replace\" , run_type = \"local\" , print_impact = False , ): \"\"\" This is a supervised method to convert a categorical attribute into a numerical attribute. It takes a label/target column to indicate whether the event is positive or negative. For each column, the positive event rate for each categorical value is used as the encoded numerical value. For example, there are 3 distinct values in a categorical attribute X: X1, X2 and X3. Within the input dataframe, there are - 15 positive events and 5 negative events with X==X1; - 10 positive events and 40 negative events with X==X2; - 20 positive events and 20 negative events with X==X3. Thus, value X1 is mapped to 15/(15+5) = 0.75, value X2 is mapped to 10/(10+40) = 0.2 and value X3 is mapped to 20/(20+20) = 0.5. This mapping will be applied to all values in attribute X. This encoding method can be helpful in avoiding creating too many dummy variables which may cause dimensionality issue and it also works with categorical attributes without an order or rank. Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of categorical columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all categorical columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) label_col Label/Target column (Default value = \"label\") event_label Value of (positive) event (i.e label 1) (Default value = 1) pre_existing_model Boolean argument \u2013 True or False. True if model (original and mapped numerical value for each column) exists already, False Otherwise. (Default value = False) model_path If pre_existing_model is True, this argument is path for referring the pre-saved model. If pre_existing_model is False, this argument can be used for saving the model. Default \"NA\" is used to save the model in \"intermediate_data/\" folder for optimization purpose. output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with transformed column. \u201cappend\u201d option append transformed column to the input dataset with a postfix \"_encoded\" e.g. column X is appended as X_encoded. (Default value = \"replace\") run_type \"local\", \"emr\", \"databricks\" (Default value = \"local\") print_impact True, False (Default value = False) This argument is to print out the descriptive statistics of encoded columns. Returns ------- DataFrame Encoded Dataframe \"\"\" cat_cols = attributeType_segregation ( idf )[ 1 ] if list_of_cols == \"all\" : list_of_cols = cat_cols elif isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if ( e not in drop_cols ) & ( e != label_col )]) ) if any ( x not in cat_cols for x in list_of_cols ): raise TypeError ( \"Invalid input for Column(s)\" ) if len ( list_of_cols ) == 0 : warnings . warn ( \"No Categorical Encoding - No categorical column(s) to transform\" ) return idf if label_col not in idf . columns : raise TypeError ( \"Invalid input for Label Column\" ) idf_id = idf . withColumn ( \"tempID\" , F . monotonically_increasing_id ()) odf_partial = idf_id . select ([ \"tempID\" ] + list_of_cols ) for index , i in enumerate ( list_of_cols ): if pre_existing_model : df_tmp = spark . read . csv ( model_path + \"/cat_to_num_supervised/\" + i , header = True , inferSchema = True , ) else : df_tmp = ( idf . withColumn ( label_col , F . when ( F . col ( label_col ) == event_label , \"1\" ) . otherwise ( \"0\" ), ) . groupBy ( i ) . pivot ( label_col ) . count () . fillna ( 0 ) . withColumn ( i + \"_encoded\" , F . round ( F . col ( \"1\" ) / ( F . col ( \"1\" ) + F . col ( \"0\" )), 4 ) ) . drop ( * [ \"1\" , \"0\" ]) ) if run_type in list ( platform_root_path . keys ()): root_path = platform_root_path [ run_type ] else : root_path = \"\" if model_path == \"NA\" : model_path = root_path + \"intermediate_data\" df_tmp . coalesce ( 1 ) . write . csv ( model_path + \"/cat_to_num_supervised/\" + i , header = True , mode = \"overwrite\" , ) df_tmp = spark . read . csv ( model_path + \"/cat_to_num_supervised/\" + i , header = True , inferSchema = True , ) if df_tmp . count () > 1 : odf_partial = odf_partial . join ( df_tmp , i , \"left_outer\" ) else : odf_partial = odf_partial . crossJoin ( df_tmp ) odf = idf_id . join ( odf_partial . drop ( * list_of_cols ), \"tempID\" , \"left_outer\" ) . drop ( \"tempID\" ) if output_mode == \"replace\" : for i in list_of_cols : odf = odf . drop ( i ) . withColumnRenamed ( i + \"_encoded\" , i ) odf = odf . select ( idf . columns ) if print_impact : if output_mode == \"replace\" : output_cols = list_of_cols else : output_cols = [( i + \"_encoded\" ) for i in list_of_cols ] print ( \"Before: \" ) idf . select ( list_of_cols ) . describe () . where ( F . col ( \"summary\" ) . isin ( \"count\" , \"min\" , \"max\" ) ) . show ( 3 , False ) print ( \"After: \" ) odf . select ( output_cols ) . describe () . where ( F . col ( \"summary\" ) . isin ( \"count\" , \"min\" , \"max\" ) ) . show ( 3 , False ) return odf def z_standardization ( spark , idf , list_of_cols = \"all\" , drop_cols = [], pre_existing_model = False , model_path = \"NA\" , output_mode = \"replace\" , print_impact = False , ): \"\"\" Standardization is commonly used in data pre-processing process. z_standardization standardizes the selected attributes of an input dataframe by normalizing each attribute to have standard deviation of 1 and mean of 0. For each attribute, the standard deviation (s) and mean (u) are calculated and a sample x will be standardized into ( x-u)/s. If the standard deviation of an attribute is 0, it will be excluded in standardization and a warning will be shown. None values will be kept as None in the output dataframe. Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of numerical columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all numerical columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) pre_existing_model Boolean argument \u2013 True or False. True if model files (Mean/stddev for each feature) exists already, False Otherwise (Default value = False) model_path If pre_existing_model is True, this argument is path for referring the pre-saved model. If pre_existing_model is False, this argument can be used for saving the model. Default \"NA\" means there is neither pre-existing model nor there is a need to save one. output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with transformed column. \u201cappend\u201d option append transformed column to the input dataset with a postfix \"_scaled\" e.g. column X is appended as X_scaled. (Default value = \"replace\") print_impact True, False (Default value = False) This argument is to print out the before and after descriptive statistics of rescaled columns. Returns ------- DataFrame Rescaled Dataframe \"\"\" num_cols = attributeType_segregation ( idf )[ 0 ] if list_of_cols == \"all\" : list_of_cols = num_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in drop_cols ])) if any ( x not in num_cols for x in list_of_cols ): raise TypeError ( \"Invalid input for Column(s)\" ) if len ( list_of_cols ) == 0 : warnings . warn ( \"No Standardization Performed - No numerical column(s) to transform\" ) return idf if output_mode not in ( \"replace\" , \"append\" ): raise TypeError ( \"Invalid input for output_mode\" ) parameters = [] excluded_cols = [] if pre_existing_model : df_model = spark . read . parquet ( model_path + \"/z_standardization\" ) for i in list_of_cols : mapped_value = ( df_model . where ( F . col ( \"feature\" ) == i ) . select ( \"parameters\" ) . rdd . flatMap ( lambda x : x ) . collect ()[ 0 ] ) parameters . append ( mapped_value ) else : for i in list_of_cols : mean , stddev = idf . select ( F . mean ( i ), F . stddev ( i )) . first () parameters . append ( [ float ( mean ) if mean else None , float ( stddev ) if stddev else None ] ) if stddev : if round ( stddev , 5 ) == 0.0 : excluded_cols . append ( i ) else : excluded_cols . append ( i ) if len ( excluded_cols ) > 0 : warnings . warn ( \"The following column(s) are excluded from standardization because the standard deviation is zero:\" + str ( excluded_cols ) ) odf = idf for index , i in enumerate ( list_of_cols ): if i not in excluded_cols : modify_col = ( i + \"_scaled\" ) if ( output_mode == \"append\" ) else i odf = odf . withColumn ( modify_col , ( F . col ( i ) - parameters [ index ][ 0 ]) / parameters [ index ][ 1 ] ) if ( not pre_existing_model ) & ( model_path != \"NA\" ): df_model = spark . createDataFrame ( zip ( list_of_cols , parameters ), schema = [ \"feature\" , \"parameters\" ] ) df_model . coalesce ( 1 ) . write . parquet ( model_path + \"/z_standardization\" , mode = \"overwrite\" ) if print_impact : if output_mode == \"replace\" : output_cols = list_of_cols else : output_cols = [ ( i + \"_scaled\" ) for i in list_of_cols if i not in excluded_cols ] print ( \"Before: \" ) idf . select ( list_of_cols ) . describe () . show ( 5 , False ) print ( \"After: \" ) odf . select ( output_cols ) . describe () . show ( 5 , False ) return odf def IQR_standardization ( spark , idf , list_of_cols = \"all\" , drop_cols = [], pre_existing_model = False , model_path = \"NA\" , output_mode = \"replace\" , print_impact = False , ): \"\"\" Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of numerical columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all numerical columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) pre_existing_model Boolean argument \u2013 True or False. True if model files (25/50/75 percentile for each feature) exists already, False Otherwise (Default value = False) model_path If pre_existing_model is True, this argument is path for referring the pre-saved model. If pre_existing_model is False, this argument can be used for saving the model. Default \"NA\" means there is neither pre-existing model nor there is a need to save one. output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with transformed column. \u201cappend\u201d option append transformed column to the input dataset with a postfix \"_scaled\" e.g. column X is appended as X_scaled. (Default value = \"replace\") print_impact True, False (Default value = False) This argument is to print out the before and after descriptive statistics of rescaled columns. Returns ------- DataFrame Rescaled Dataframe \"\"\" num_cols = attributeType_segregation ( idf )[ 0 ] if list_of_cols == \"all\" : list_of_cols = num_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in drop_cols ])) if any ( x not in num_cols for x in list_of_cols ): raise TypeError ( \"Invalid input for Column(s)\" ) if len ( list_of_cols ) == 0 : warnings . warn ( \"No Standardization Performed - No numerical column(s) to transform\" ) return idf if output_mode not in ( \"replace\" , \"append\" ): raise TypeError ( \"Invalid input for output_mode\" ) if pre_existing_model : df_model = spark . read . parquet ( model_path + \"/IQR_standardization\" ) parameters = [] for i in list_of_cols : mapped_value = ( df_model . where ( F . col ( \"feature\" ) == i ) . select ( \"parameters\" ) . rdd . flatMap ( lambda x : x ) . collect ()[ 0 ] ) parameters . append ( mapped_value ) else : parameters = idf . approxQuantile ( list_of_cols , [ 0.25 , 0.5 , 0.75 ], 0.01 ) excluded_cols = [] for i , param in zip ( list_of_cols , parameters ): if len ( param ) > 0 : if round ( param [ 0 ], 5 ) == round ( param [ 2 ], 5 ): excluded_cols . append ( i ) else : excluded_cols . append ( i ) if len ( excluded_cols ) > 0 : warnings . warn ( \"The following column(s) are excluded from standardization because the 75th and 25th percentiles are the same:\" + str ( excluded_cols ) ) odf = idf for index , i in enumerate ( list_of_cols ): if i not in excluded_cols : modify_col = ( i + \"_scaled\" ) if ( output_mode == \"append\" ) else i odf = odf . withColumn ( modify_col , ( F . col ( i ) - parameters [ index ][ 1 ]) / ( parameters [ index ][ 2 ] - parameters [ index ][ 0 ]), ) if ( not pre_existing_model ) & ( model_path != \"NA\" ): df_model = spark . createDataFrame ( zip ( list_of_cols , parameters ), schema = [ \"feature\" , \"parameters\" ] ) df_model . coalesce ( 1 ) . write . parquet ( model_path + \"/IQR_standardization\" , mode = \"overwrite\" ) if print_impact : if output_mode == \"replace\" : output_cols = list_of_cols else : output_cols = [ ( i + \"_scaled\" ) for i in list_of_cols if i not in excluded_cols ] print ( \"Before: \" ) idf . select ( list_of_cols ) . describe () . show ( 5 , False ) print ( \"After: \" ) odf . select ( output_cols ) . describe () . show ( 5 , False ) return odf def normalization ( idf , list_of_cols = \"all\" , drop_cols = [], pre_existing_model = False , model_path = \"NA\" , output_mode = \"replace\" , print_impact = False , ): \"\"\" Parameters ---------- idf Input Dataframe list_of_cols List of numerical columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all numerical columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) pre_existing_model Boolean argument \u2013 True or False. True if normalization/scalar model exists already, False Otherwise (Default value = False) model_path If pre_existing_model is True, this argument is path for referring the pre-saved model. If pre_existing_model is False, this argument can be used for saving the model. Default \"NA\" means there is neither pre-existing model nor there is a need to save one. output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with transformed column. \u201cappend\u201d option append transformed column to the input dataset with a postfix \"_scaled\" e.g. column X is appended as X_scaled. (Default value = \"replace\") print_impact True, False (Default value = False) This argument is to print out before and after descriptive statistics of rescaled columns. Returns ------- DataFrame Rescaled Dataframe \"\"\" num_cols = attributeType_segregation ( idf )[ 0 ] if list_of_cols == \"all\" : list_of_cols = num_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in drop_cols ])) if any ( x not in num_cols for x in list_of_cols ): raise TypeError ( \"Invalid input for Column(s)\" ) if len ( list_of_cols ) == 0 : warnings . warn ( \"No Normalization Performed - No numerical column(s) to transform\" ) return idf if output_mode not in ( \"replace\" , \"append\" ): raise TypeError ( \"Invalid input for output_mode\" ) idf_id = idf . withColumn ( \"tempID\" , F . monotonically_increasing_id ()) idf_partial = idf_id . select ([ \"tempID\" ] + list_of_cols ) assembler = VectorAssembler ( inputCols = list_of_cols , outputCol = \"list_of_cols_vector\" , handleInvalid = \"keep\" ) assembled_data = assembler . transform ( idf_partial ) if pre_existing_model : scalerModel = MinMaxScalerModel . load ( model_path + \"/normalization\" ) else : scaler = MinMaxScaler ( inputCol = \"list_of_cols_vector\" , outputCol = \"list_of_cols_scaled\" ) scalerModel = scaler . fit ( assembled_data ) if model_path != \"NA\" : scalerModel . write () . overwrite () . save ( model_path + \"/normalization\" ) scaledData = scalerModel . transform ( assembled_data ) def vector_to_array ( v ): return v . toArray () . tolist () f_vector_to_array = F . udf ( vector_to_array , T . ArrayType ( T . FloatType ())) odf_partial = scaledData . withColumn ( \"list_of_cols_array\" , f_vector_to_array ( \"list_of_cols_scaled\" ) ) . drop ( * [ \"list_of_cols_scaled\" , \"list_of_cols_vector\" ]) odf_schema = odf_partial . schema for i in list_of_cols : odf_schema = odf_schema . add ( T . StructField ( i + \"_scaled\" , T . FloatType ())) odf_partial = ( odf_partial . rdd . map ( lambda x : ( * x , * x [ \"list_of_cols_array\" ])) . toDF ( schema = odf_schema ) . drop ( \"list_of_cols_array\" ) ) odf = idf_id . join ( odf_partial . drop ( * list_of_cols ), \"tempID\" , \"left_outer\" ) . select ( idf . columns + [ ( F . when ( F . isnan ( F . col ( i + \"_scaled\" )), None ) . otherwise ( F . col ( i + \"_scaled\" ) ) ) . alias ( i + \"_scaled\" ) for i in list_of_cols ] ) if output_mode == \"replace\" : for i in list_of_cols : odf = odf . drop ( i ) . withColumnRenamed ( i + \"_scaled\" , i ) odf = odf . select ( idf . columns ) if print_impact : if output_mode == \"replace\" : output_cols = list_of_cols else : output_cols = [( i + \"_scaled\" ) for i in list_of_cols ] print ( \"Before: \" ) idf . select ( list_of_cols ) . describe () . show ( 5 , False ) print ( \"After: \" ) odf . select ( output_cols ) . describe () . show ( 5 , False ) return odf def imputation_MMM ( spark , idf , list_of_cols = \"missing\" , drop_cols = [], method_type = \"median\" , pre_existing_model = False , model_path = \"NA\" , output_mode = \"replace\" , stats_missing = {}, stats_mode = {}, print_impact = False , ): \"\"\" This function handles missing value related issues by substituting null values by the measure of central tendency (mode for categorical features and mean/median for numerical features). For numerical attributes, it leverages [Imputer](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.feature.Imputer .html) functionality of Spark MLlib. Though, Imputer can be used for categorical attributes but this feature is available only in Spark3.x, therefore for categorical features, we compute mode or leverage mode computation from Measures of Central Tendency. Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of columns to impute e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all (non-array) columns for analysis. This is super useful instead of specifying all column names manually. \"missing\" (default) can be passed to include only those columns with missing values. One of the usecases where \"all\" may be preferable over \"missing\" is when the user wants to save the imputation model for the future use e.g. a column may not have missing value in the training dataset but missing values may possibly appear in the prediction dataset. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) method_type \"median\", \"mean\" (valid only for for numerical columns attributes). Mode is only option for categorical columns. (Default value = \"median\") pre_existing_model Boolean argument \u2013 True or False. True if imputation model exists already, False otherwise. (Default value = False) model_path If pre_existing_model is True, this argument is path for referring the pre-saved model. If pre_existing_model is False, this argument can be used for saving the model. Default \"NA\" means there is neither pre-existing model nor there is a need to save one. output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with transformed column. \u201cappend\u201d option append transformed column to the input dataset with a postfix \"_imputed\" e.g. column X is appended as X_imputed. (Default value = \"replace\") stats_missing Takes arguments for read_dataset (data_ingest module) function in a dictionary format to read pre-saved statistics on missing count/pct i.e. if measures_of_counts or missingCount_computation (data_analyzer.stats_generator module) has been computed & saved before. (Default value = {}) stats_mode Takes arguments for read_dataset (data_ingest module) function in a dictionary format to read pre-saved statistics on most frequently seen values i.e. if measures_of_centralTendency or mode_computation (data_analyzer.stats_generator module) has been computed & saved before. (Default value = {}) print_impact True, False (Default value = False) This argument is to print out before and after missing counts of imputed columns. Returns ------- DataFrame Imputed Dataframe \"\"\" if stats_missing == {}: missing_df = missingCount_computation ( spark , idf ) else : missing_df = read_dataset ( spark , ** stats_missing ) . select ( \"attribute\" , \"missing_count\" , \"missing_pct\" ) missing_cols = ( missing_df . where ( F . col ( \"missing_count\" ) > 0 ) . select ( \"attribute\" ) . rdd . flatMap ( lambda x : x ) . collect () ) if str ( pre_existing_model ) . lower () == \"true\" : pre_existing_model = True elif str ( pre_existing_model ) . lower () == \"false\" : pre_existing_model = False else : raise TypeError ( \"Non-Boolean input for pre_existing_model\" ) if ( len ( missing_cols ) == 0 ) & ( not pre_existing_model ) & ( model_path == \"NA\" ): return idf num_cols , cat_cols , other_cols = attributeType_segregation ( idf ) if list_of_cols == \"all\" : list_of_cols = num_cols + cat_cols if list_of_cols == \"missing\" : list_of_cols = [ x for x in missing_cols if x in num_cols + cat_cols ] if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in drop_cols ])) if len ( list_of_cols ) == 0 : warnings . warn ( \"No Imputation performed- No column(s) to impute\" ) return idf if any ( x not in num_cols + cat_cols for x in list_of_cols ): raise TypeError ( \"Invalid input for Column(s)\" ) if method_type not in ( \"mode\" , \"mean\" , \"median\" ): raise TypeError ( \"Invalid input for method_type\" ) if output_mode not in ( \"replace\" , \"append\" ): raise TypeError ( \"Invalid input for output_mode\" ) num_cols , cat_cols , other_cols = attributeType_segregation ( idf . select ( list_of_cols )) odf = idf if len ( num_cols ) > 0 : recast_cols = [] recast_type = [] for i in num_cols : if get_dtype ( idf , i ) not in ( \"float\" , \"double\" ): odf = odf . withColumn ( i , F . col ( i ) . cast ( T . DoubleType ())) recast_cols . append ( i + \"_imputed\" ) recast_type . append ( get_dtype ( idf , i )) # For mode imputation if method_type == \"mode\" : if stats_mode == {}: parameters = [ str ( ( idf . select ( i ) . dropna () . groupby ( i ) . count () . orderBy ( \"count\" , ascending = False ) . first () or [ None ] )[ 0 ] ) for i in num_cols ] else : mode_df = read_dataset ( spark , ** stats_mode ) . replace ( \"None\" , None ) mode_df_cols = list ( mode_df . select ( \"attribute\" ) . toPandas ()[ \"attribute\" ]) parameters = [] for i in num_cols : if i not in mode_df_cols : parameters . append ( str ( ( idf . select ( i ) . dropna () . groupby ( i ) . count () . orderBy ( \"count\" , ascending = False ) . first () or [ None ] )[ 0 ] ) ) else : parameters . append ( mode_df . where ( F . col ( \"attribute\" ) == i ) . select ( \"mode\" ) . rdd . flatMap ( list ) . collect ()[ 0 ] ) for index , i in enumerate ( num_cols ): odf = odf . withColumn ( i + \"_imputed\" , F . when ( F . col ( i ) . isNull (), parameters [ index ]) . otherwise ( F . col ( i )), ) else : # For mean, median imputation # Building new imputer model or uploading the existing model if pre_existing_model : imputerModel = ImputerModel . load ( model_path + \"/imputation_MMM/num_imputer-model\" ) else : imputer = Imputer ( strategy = method_type , inputCols = num_cols , outputCols = [( e + \"_imputed\" ) for e in num_cols ], ) imputerModel = imputer . fit ( odf ) # Applying model # odf = recast_column(imputerModel.transform(odf), recast_cols, recast_type) odf = imputerModel . transform ( odf ) for i , j in zip ( recast_cols , recast_type ): odf = odf . withColumn ( i , F . col ( i ) . cast ( j )) # Saving model if required if ( not pre_existing_model ) & ( model_path != \"NA\" ): imputerModel . write () . overwrite () . save ( model_path + \"/imputation_MMM/num_imputer-model\" ) if len ( cat_cols ) > 0 : if pre_existing_model : df_model = spark . read . csv ( model_path + \"/imputation_MMM/cat_imputer\" , header = True , inferSchema = True , ) parameters = [] for i in cat_cols : mapped_value = ( df_model . where ( F . col ( \"attribute\" ) == i ) . select ( \"parameters\" ) . rdd . flatMap ( lambda x : x ) . collect ()[ 0 ] ) parameters . append ( mapped_value ) else : if stats_mode == {}: parameters = [ str ( ( idf . select ( i ) . dropna () . groupby ( i ) . count () . orderBy ( \"count\" , ascending = False ) . first () or [ None ] )[ 0 ] ) for i in cat_cols ] else : mode_df = read_dataset ( spark , ** stats_mode ) . replace ( \"None\" , None ) parameters = [ mode_df . where ( F . col ( \"attribute\" ) == i ) . select ( \"mode\" ) . rdd . flatMap ( list ) . collect ()[ 0 ] for i in cat_cols ] for index , i in enumerate ( cat_cols ): odf = odf . withColumn ( i + \"_imputed\" , F . when ( F . col ( i ) . isNull (), parameters [ index ]) . otherwise ( F . col ( i )), ) if ( not pre_existing_model ) & ( model_path != \"NA\" ): df_model = spark . createDataFrame ( zip ( cat_cols , parameters ), schema = [ \"attribute\" , \"parameters\" ] ) df_model . repartition ( 1 ) . write . csv ( model_path + \"/imputation_MMM/cat_imputer\" , header = True , mode = \"overwrite\" , ) for i in num_cols + cat_cols : if i not in missing_cols : odf = odf . drop ( i + \"_imputed\" ) elif output_mode == \"replace\" : odf = odf . drop ( i ) . withColumnRenamed ( i + \"_imputed\" , i ) if print_impact : if output_mode == \"replace\" : odf_print = missing_df . select ( \"attribute\" , F . col ( \"missing_count\" ) . alias ( \"missingCount_before\" ) ) . join ( missingCount_computation ( spark , odf , list_of_cols ) . select ( \"attribute\" , F . col ( \"missing_count\" ) . alias ( \"missingCount_after\" ) ), \"attribute\" , \"inner\" , ) else : output_cols = [ ( i + \"_imputed\" ) for i in [ e for e in ( num_cols + cat_cols ) if e in missing_cols ] ] odf_print = missing_df . select ( \"attribute\" , F . col ( \"missing_count\" ) . alias ( \"missingCount_before\" ) ) . join ( missingCount_computation ( spark , odf , output_cols ) . withColumnRenamed ( \"attribute\" , \"attribute_after\" ) . withColumn ( \"attribute\" , F . expr ( \"substring(attribute_after, 1, length(attribute_after)-8)\" ), ) . drop ( \"missing_pct\" ), \"attribute\" , \"inner\" , ) odf_print . show ( len ( list_of_cols ), False ) return odf def imputation_sklearn ( spark , idf , list_of_cols = \"missing\" , drop_cols = [], method_type = \"KNN\" , sample_size = 500000 , pre_existing_model = False , model_path = \"NA\" , output_mode = \"replace\" , stats_missing = {}, run_type = \"local\" , print_impact = False , ): \"\"\" The function \"imputation_sklearn\" leverages sklearn imputer algorithms. Two methods are supported via this function: \u201cKNN\u201d and \u201cregression\u201d. \u201cKNN\u201d option trains a sklearn.impute.KNNImputer which is based on k-Nearest Neighbors algorithm. The missing values of a sample are imputed using the mean of its 5 nearest neighbors in the training set. \u201cregression\u201d option trains a sklearn.impute.IterativeImputer which models attribute to impute as a function of rest of the attributes and imputes using the estimation. Imputation is performed in an iterative way from attributes with fewest number of missing values to most. All the hyperparameters used in the above mentioned imputers are their default values. However, sklearn imputers are not scalable, which might be slow if the size of the input dataframe is large. Thus, an input sample_size (the default value is 500,000) can be set to control the number of samples to be used to train the imputer. If the total number of input dataset exceeds sample_size, the rest of the samples will be imputed using the trained imputer in a scalable manner. This is one of the way to demonstrate how Anovos has been designed as a scalable feature engineering library. Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of columns to impute e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all (non-array) columns for analysis. This is super useful instead of specifying all column names manually. \"missing\" (default) can be passed to include only those columns with missing values. One of the usecases where \"all\" may be preferable over \"missing\" is when the user wants to save the imputation model for the future use e.g. a column may not have missing value in the training dataset but missing values may possibly appear in the prediction dataset. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) method_type \"KNN\", \"regression\". \"KNN\" option trains a sklearn.impute.KNNImputer. \"regression\" option trains a sklearn.impute.IterativeImputer (Default value = \"KNN\") sample_size Maximum rows for training the sklearn imputer (Default value = 500000) pre_existing_model Boolean argument \u2013 True or False. True if imputation model exists already, False otherwise. (Default value = False) model_path If pre_existing_model is True, this argument is path for referring the pre-saved model. If pre_existing_model is False, this argument can be used for saving the model. Default \"NA\" means there is neither pre-existing model nor there is a need to save one. output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with transformed column. \u201cappend\u201d option append transformed column to the input dataset with a postfix \"_imputed\" e.g. column X is appended as X_imputed. (Default value = \"replace\") stats_missing Takes arguments for read_dataset (data_ingest module) function in a dictionary format to read pre-saved statistics on missing count/pct i.e. if measures_of_counts or missingCount_computation (data_analyzer.stats_generator module) has been computed & saved before. (Default value = {}) run_type \"local\", \"emr\", \"databricks\" (Default value = \"local\") print_impact True, False (Default value = False) This argument is to print out before and after missing counts of imputed columns. Returns ------- DataFrame Imputed Dataframe \"\"\" num_cols = attributeType_segregation ( idf )[ 0 ] if stats_missing == {}: missing_df = missingCount_computation ( spark , idf , num_cols ) else : missing_df = ( read_dataset ( spark , ** stats_missing ) . select ( \"attribute\" , \"missing_count\" , \"missing_pct\" ) . where ( F . col ( \"attribute\" ) . isin ( num_cols )) ) empty_cols = ( missing_df . where ( F . col ( \"missing_pct\" ) == 1.0 ) . select ( \"attribute\" ) . rdd . flatMap ( lambda x : x ) . collect () ) if len ( empty_cols ) > 0 : warnings . warn ( \"Following columns dropped from the imputation as all values are null: \" + \",\" . join ( empty_cols ) ) missing_cols = ( missing_df . where ( F . col ( \"missing_count\" ) > 0 ) . where ( F . col ( \"missing_pct\" ) < 1.0 ) . select ( \"attribute\" ) . rdd . flatMap ( lambda x : x ) . collect () ) if list_of_cols == \"all\" : list_of_cols = num_cols if list_of_cols == \"missing\" : list_of_cols = missing_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if ( e not in drop_cols ) & ( e not in empty_cols )]) ) if len ( list_of_cols ) <= 1 : warnings . warn ( \"No Imputation Performed - No Column(s) or Insufficient Column(s) to Impute\" ) return idf if str ( pre_existing_model ) . lower () == \"true\" : pre_existing_model = True elif str ( pre_existing_model ) . lower () == \"false\" : pre_existing_model = False else : raise TypeError ( \"Non-Boolean input for pre_existing_model\" ) if ( ( len ([ e for e in list_of_cols if e in missing_cols ]) == 0 ) & ( not pre_existing_model ) & ( model_path == \"NA\" ) ): warnings . warn ( \"No Imputation Performed - No Column(s) to Impute and No Imputation Model to be saved\" ) return idf if any ( x not in num_cols for x in list_of_cols ): raise TypeError ( \"Invalid input for Column(s)\" ) if method_type not in ( \"KNN\" , \"regression\" ): raise TypeError ( \"Invalid input for method_type\" ) if output_mode not in ( \"replace\" , \"append\" ): raise TypeError ( \"Invalid input for output_mode\" ) if pre_existing_model : if run_type == \"emr\" : bash_cmd = \"aws s3 cp \" + model_path + \"/imputation_sklearn.sav .\" output = subprocess . check_output ([ \"bash\" , \"-c\" , bash_cmd ]) imputer = pickle . load ( open ( \"imputation_sklearn.sav\" , \"rb\" )) else : imputer = pickle . load ( open ( model_path + \"/imputation_sklearn.sav\" , \"rb\" )) idf_rest = idf else : sample_ratio = min ( 1.0 , float ( sample_size ) / idf . count ()) idf_model = idf . sample ( False , sample_ratio , 0 ) idf_pd = idf_model . select ( list_of_cols ) . toPandas () if method_type == \"KNN\" : imputer = KNNImputer ( n_neighbors = 5 , weights = \"uniform\" , metric = \"nan_euclidean\" ) if method_type == \"regression\" : imputer = IterativeImputer () imputer . fit ( idf_pd ) if ( not pre_existing_model ) & ( model_path != \"NA\" ): if run_type == \"emr\" : pickle . dump ( imputer , open ( \"imputation_sklearn.sav\" , \"wb\" )) bash_cmd = ( \"aws s3 cp imputation_sklearn.sav \" + model_path + \"/imputation_sklearn.sav\" ) output = subprocess . check_output ([ \"bash\" , \"-c\" , bash_cmd ]) imputer = pickle . load ( open ( \"imputation_sklearn.sav\" , \"rb\" )) else : local_path = model_path + \"/imputation_sklearn.sav\" os . makedirs ( os . path . dirname ( local_path ), exist_ok = True ) pickle . dump ( imputer , open ( local_path , \"wb\" )) imputer = pickle . load ( open ( model_path + \"/imputation_sklearn.sav\" , \"rb\" ) ) @F . pandas_udf ( returnType = T . ArrayType ( T . DoubleType ())) def prediction ( * cols ): X = pd . concat ( cols , axis = 1 ) return pd . Series ( row . tolist () for row in imputer . transform ( X )) odf = idf . withColumn ( \"features\" , prediction ( * list_of_cols )) odf . persist ( pyspark . StorageLevel . MEMORY_AND_DISK ) . count () odf_schema = odf . schema for i in list_of_cols : odf_schema = odf_schema . add ( T . StructField ( i + \"_imputed\" , T . FloatType ())) odf = ( odf . rdd . map ( lambda x : ( * x , * x [ \"features\" ])) . toDF ( schema = odf_schema ) . drop ( \"features\" ) ) output_cols = [] for i in list_of_cols : if output_mode == \"append\" : if i not in missing_cols : odf = odf . drop ( i + \"_imputed\" ) else : output_cols . append ( i + \"_imputed\" ) else : odf = odf . drop ( i ) . withColumnRenamed ( i + \"_imputed\" , i ) odf = odf . select ( idf . columns + output_cols ) if print_impact : if output_mode == \"replace\" : odf_print = missing_df . select ( \"attribute\" , F . col ( \"missing_count\" ) . alias ( \"missingCount_before\" ) ) . join ( missingCount_computation ( spark , odf , list_of_cols ) . select ( \"attribute\" , F . col ( \"missing_count\" ) . alias ( \"missingCount_after\" ) ), \"attribute\" , \"inner\" , ) else : odf_print = missing_df . select ( \"attribute\" , F . col ( \"missing_count\" ) . alias ( \"missingCount_before\" ) ) . join ( missingCount_computation ( spark , odf , output_cols ) . withColumnRenamed ( \"attribute\" , \"attribute_after\" ) . withColumn ( \"attribute\" , F . expr ( \"substring(attribute_after, 1, length(attribute_after)-8)\" ), ) . drop ( \"missing_pct\" ), \"attribute\" , \"inner\" , ) odf_print . show ( len ( list_of_cols ), False ) return odf def imputation_matrixFactorization ( spark , idf , list_of_cols = \"missing\" , drop_cols = [], id_col = \"\" , output_mode = \"replace\" , stats_missing = {}, print_impact = False , ): \"\"\" imputation_matrixFactorization uses collaborative filtering technique to impute missing values. Collaborative filtering is commonly used in recommender systems to fill the missing user-item entries and PySpark provides an implementation using alternating least squares (ALS) algorithm, which is used in this function. To fit our problem into the ALS model, each attribute is treated as an item and an id column needs to be specified by the user to generate the user-item pairs. In the case, ID column doesn't exist in the dataset and proxu ID column is implicitly generated by the function. Subsequently, all user-item pairs with known values will be used to train the ALS model and the trained model can be used to predict the user-item pairs with missing values. Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of columns to impute e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all (non-array) columns for analysis. This is super useful instead of specifying all column names manually. \"missing\" (default) can be passed to include only those columns with missing values. One of the usecases where \"all\" may be preferable over \"missing\" is when the user wants to save the imputation model for the future use e.g. a column may not have missing value in the training dataset but missing values may possibly appear in the prediction dataset. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) id_col ID column (Default value = \"\") output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with transformed column. \u201cappend\u201d option append transformed column to the input dataset with a postfix \"_imputed\" e.g. column X is appended as X_imputed. (Default value = \"replace\") stats_missing Takes arguments for read_dataset (data_ingest module) function in a dictionary format to read pre-saved statistics on missing count/pct i.e. if measures_of_counts or missingCount_computation (data_analyzer.stats_generator module) has been computed & saved before. (Default value = {}) print_impact True, False (Default value = False) This argument is to print out before and after missing counts of imputed columns. Returns ------- DataFrame Imputed Dataframe \"\"\" num_cols = attributeType_segregation ( idf )[ 0 ] if stats_missing == {}: missing_df = missingCount_computation ( spark , idf , num_cols ) else : missing_df = ( read_dataset ( spark , ** stats_missing ) . select ( \"attribute\" , \"missing_count\" , \"missing_pct\" ) . where ( F . col ( \"attribute\" ) . isin ( num_cols )) ) empty_cols = ( missing_df . where ( F . col ( \"missing_pct\" ) == 1.0 ) . select ( \"attribute\" ) . rdd . flatMap ( lambda x : x ) . collect () ) if len ( empty_cols ) > 0 : warnings . warn ( \"Following columns dropped from the imputation as all values are null: \" + \",\" . join ( empty_cols ) ) missing_cols = ( missing_df . where ( F . col ( \"missing_count\" ) > 0 ) . where ( F . col ( \"missing_pct\" ) < 1.0 ) . select ( \"attribute\" ) . rdd . flatMap ( lambda x : x ) . collect () ) if list_of_cols == \"all\" : list_of_cols = num_cols if list_of_cols == \"missing\" : list_of_cols = missing_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ( [ e for e in list_of_cols if ( e not in drop_cols ) & ( e != id_col ) & ( e not in empty_cols ) ] ) ) if ( len ( list_of_cols ) == 0 ) | ( len ([ e for e in list_of_cols if e in missing_cols ]) == 0 ): warnings . warn ( \"No Imputation Performed - No Column(s) to Impute\" ) return idf if len ( list_of_cols ) == 1 : warnings . warn ( \"No Imputation Performed - Needs more than 1 column for matrix factorization\" ) return idf if any ( x not in num_cols for x in list_of_cols ): raise TypeError ( \"Invalid input for Column(s)\" ) if output_mode not in ( \"replace\" , \"append\" ): raise TypeError ( \"Invalid input for output_mode\" ) remove_id = False if id_col == \"\" : idf = idf . withColumn ( \"id\" , F . monotonically_increasing_id ()) . withColumn ( \"id\" , F . row_number () . over ( Window . orderBy ( \"id\" )) ) id_col = \"id\" remove_id = True key_and_val = F . create_map ( list ( chain . from_iterable ([[ F . lit ( c ), F . col ( c )] for c in list_of_cols ])) ) df_flatten = idf . select ( id_col , F . explode ( key_and_val )) . withColumn ( \"key\" , F . concat ( F . col ( \"key\" ), F . lit ( \"_imputed\" )) ) id_type = get_dtype ( idf , id_col ) if id_type == \"string\" : id_indexer = StringIndexer () . setInputCol ( id_col ) . setOutputCol ( \"IDLabel\" ) id_indexer_model = id_indexer . fit ( df_flatten ) df_flatten = id_indexer_model . transform ( df_flatten ) . drop ( id_col ) else : df_flatten = df_flatten . withColumnRenamed ( id_col , \"IDLabel\" ) indexer = StringIndexer () . setInputCol ( \"key\" ) . setOutputCol ( \"keyLabel\" ) indexer_model = indexer . fit ( df_flatten ) df_encoded = indexer_model . transform ( df_flatten ) . drop ( \"key\" ) df_model = df_encoded . where ( F . col ( \"value\" ) . isNotNull ()) df_test = df_encoded . where ( F . col ( \"value\" ) . isNull ()) if ( df_model . select ( \"IDLabel\" ) . distinct () . count () < df_encoded . select ( \"IDLabel\" ) . distinct () . count () ): warnings . warn ( \"The returned odf may not be fully imputed because values for all list_of_cols are null for some IDs\" ) als = ALS ( maxIter = 20 , regParam = 0.01 , userCol = \"IDLabel\" , itemCol = \"keyLabel\" , ratingCol = \"value\" , coldStartStrategy = \"drop\" , ) model = als . fit ( df_model ) df_pred = ( model . transform ( df_test ) . drop ( \"value\" ) . withColumnRenamed ( \"prediction\" , \"value\" ) ) df_encoded_pred = df_model . union ( df_pred . select ( df_model . columns )) if id_type == \"string\" : IDlabelReverse = IndexToString () . setInputCol ( \"IDLabel\" ) . setOutputCol ( id_col ) df_encoded_pred = IDlabelReverse . transform ( df_encoded_pred ) else : df_encoded_pred = df_encoded_pred . withColumnRenamed ( \"IDLabel\" , id_col ) keylabelReverse = IndexToString () . setInputCol ( \"keyLabel\" ) . setOutputCol ( \"key\" ) odf_imputed = ( keylabelReverse . transform ( df_encoded_pred ) . groupBy ( id_col ) . pivot ( \"key\" ) . agg ( F . first ( \"value\" )) . select ( [ id_col ] + [( i + \"_imputed\" ) for i in list_of_cols if i in missing_cols ] ) ) odf = idf . join ( odf_imputed , id_col , \"left_outer\" ) for i in list_of_cols : if i not in missing_cols : odf = odf . drop ( i + \"_imputed\" ) elif output_mode == \"replace\" : odf = odf . drop ( i ) . withColumnRenamed ( i + \"_imputed\" , i ) if remove_id : odf = odf . drop ( \"id\" ) if print_impact : if output_mode == \"replace\" : odf_print = missing_df . select ( \"attribute\" , F . col ( \"missing_count\" ) . alias ( \"missingCount_before\" ) ) . join ( missingCount_computation ( spark , odf , list_of_cols ) . select ( \"attribute\" , F . col ( \"missing_count\" ) . alias ( \"missingCount_after\" ) ), \"attribute\" , \"inner\" , ) else : output_cols = [ ( i + \"_imputed\" ) for i in [ e for e in list_of_cols if e in missing_cols ] ] odf_print = missing_df . select ( \"attribute\" , F . col ( \"missing_count\" ) . alias ( \"missingCount_before\" ) ) . join ( missingCount_computation ( spark , odf , output_cols ) . withColumnRenamed ( \"attribute\" , \"attribute_after\" ) . withColumn ( \"attribute\" , F . expr ( \"substring(attribute_after, 1, length(attribute_after)-8)\" ), ) . drop ( \"missing_pct\" ), \"attribute\" , \"inner\" , ) odf_print . show ( len ( list_of_cols ), False ) return odf def auto_imputation ( spark , idf , list_of_cols = \"missing\" , drop_cols = [], id_col = \"\" , null_pct = 0.1 , stats_missing = {}, output_mode = \"replace\" , run_type = \"local\" , print_impact = True , ): \"\"\" auto_imputation tests for 5 imputation methods using the other imputation functions provided in this module and returns the one with the best performance. The 5 methods are: (1) imputation_MMM with method_type=\"mean\" (2) imputation_MMM with method_type=\"median\" (3) imputation_sklearn with method_type=\"KNN\" (4) imputation_sklearn with method_type=\"regression\" (5) imputation_matrixFactorization Samples without missing values in attributes to impute are used for testing by removing some % of values and impute them again using the above 5 methods. RMSE/attribute_mean is used as the evaluation metric for each attribute to reduce the effect of unit difference among attributes. The final error of a method is calculated by the sum of ( RMSE/attribute_mean) for all numerical attributes to impute and the method with the least error will be selected. The above testing is only applicable for numerical attributes. If categorical attributes are included, they will be automatically imputed using imputation_MMM. In addition, if there is only one numerical attribute to impute, only method (1) and (2) will be tested because the rest of the methods require more than one column. Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of columns to impute e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all (non-array) columns for analysis. This is super useful instead of specifying all column names manually. \"missing\" (default) can be passed to include only those columns with missing values. One of the usecases where \"all\" may be preferable over \"missing\" is when the user wants to save the imputation model for the future use e.g. a column may not have missing value in the training dataset but missing values may possibly appear in the prediction dataset. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) id_col ID column (Default value = \"\") null_pct proportion of the valid input data to be replaced by None to form the test data (Default value = 0.1) stats_missing Takes arguments for read_dataset (data_ingest module) function in a dictionary format to read pre-saved statistics on missing count/pct i.e. if measures_of_counts or missingCount_computation (data_analyzer.stats_generator module) has been computed & saved before. (Default value = {}) output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with transformed column. \u201cappend\u201d option append transformed column to the input dataset with a postfix \"_imputed\" e.g. column X is appended as X_imputed. (Default value = \"replace\") run_type \"local\", \"emr\", \"databricks\" (Default value = \"local\") print_impact True, False (Default value = False) This argument is to print out before and after missing counts of imputed columns. It also print the name of best performing imputation method along with RMSE details. Returns ------- DataFrame Imputed Dataframe \"\"\" if run_type in list ( platform_root_path . keys ()): root_path = platform_root_path [ run_type ] else : root_path = \"\" if stats_missing == {}: missing_df = missingCount_computation ( spark , idf ) missing_df . write . parquet ( root_path + \"intermediate_data/imputation_comparison/missingCount_computation\" , mode = \"overwrite\" , ) stats_missing = { \"file_path\" : root_path + \"intermediate_data/imputation_comparison/missingCount_computation\" , \"file_type\" : \"parquet\" , } else : missing_df = read_dataset ( spark , ** stats_missing ) . select ( \"attribute\" , \"missing_count\" , \"missing_pct\" ) empty_cols = ( missing_df . where ( F . col ( \"missing_pct\" ) == 1.0 ) . select ( \"attribute\" ) . rdd . flatMap ( lambda x : x ) . collect () ) if len ( empty_cols ) > 0 : warnings . warn ( \"Following columns have all null values: \" + \",\" . join ( empty_cols )) missing_cols = ( missing_df . where ( F . col ( \"missing_count\" ) > 0 ) . select ( \"attribute\" ) . rdd . flatMap ( lambda x : x ) . collect () ) if list_of_cols == \"all\" : list_of_cols = idf . columns if list_of_cols == \"missing\" : list_of_cols = missing_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if ( e not in drop_cols ) & ( e != id_col )]) ) if any ( x not in idf . columns for x in list_of_cols ): raise TypeError ( \"Invalid input for Column(s)\" ) del_cols = [ e for e in list_of_cols if e in empty_cols ] odf_del = idf . drop ( * del_cols ) list_of_cols = [ e for e in list_of_cols if e not in empty_cols ] num_cols , cat_cols , other_cols = attributeType_segregation ( odf_del . select ( list_of_cols ) ) missing_catcols = [ e for e in cat_cols if e in missing_cols ] missing_numcols = [ e for e in num_cols if e in missing_cols ] if missing_catcols : odf_imputed_cat = imputation_MMM ( spark , odf_del , list_of_cols = missing_catcols , stats_missing = stats_missing ) else : odf_imputed_cat = odf_del if len ( missing_numcols ) == 0 : warnings . warn ( \"No Imputation Performed for numerical columns - No Column(s) to Impute\" ) return odf_imputed_cat idf_test = ( odf_imputed_cat . dropna ( subset = missing_numcols ) . withColumn ( \"index\" , F . monotonically_increasing_id ()) . withColumn ( \"index\" , F . row_number () . over ( Window . orderBy ( \"index\" ))) ) null_count = int ( null_pct * idf_test . count ()) idf_null = idf_test for i in missing_numcols : null_index = random . sample ( range ( idf_test . count ()), null_count ) idf_null = idf_null . withColumn ( i , F . when ( F . col ( \"index\" ) . isin ( null_index ), None ) . otherwise ( F . col ( i )) ) idf_null . write . parquet ( root_path + \"intermediate_data/imputation_comparison/test_dataset\" , mode = \"overwrite\" , ) idf_null = spark . read . parquet ( root_path + \"intermediate_data/imputation_comparison/test_dataset\" ) method1 = imputation_MMM ( spark , idf_null , list_of_cols = missing_numcols , method_type = \"mean\" , stats_missing = stats_missing , output_mode = output_mode , ) method2 = imputation_MMM ( spark , idf_null , list_of_cols = missing_numcols , method_type = \"median\" , stats_missing = stats_missing , output_mode = output_mode , ) valid_methods = [ method1 , method2 ] if len ( num_cols ) > 1 : method3 = imputation_sklearn ( spark , idf_null , list_of_cols = num_cols , method_type = \"KNN\" , stats_missing = stats_missing , output_mode = output_mode , ) method4 = imputation_sklearn ( spark , idf_null , list_of_cols = num_cols , method_type = \"regression\" , stats_missing = stats_missing , output_mode = output_mode , ) method5 = imputation_matrixFactorization ( spark , idf_null , list_of_cols = num_cols , id_col = id_col , stats_missing = stats_missing , output_mode = output_mode , ) valid_methods = [ method1 , method2 , method3 , method4 , method5 ] nrmse_all = [] method_all = [ \"MMM-mean\" , \"MMM-median\" , \"KNN\" , \"regression\" , \"matrix_factorization\" ] for index , method in enumerate ( valid_methods ): nrmse = 0 for i in missing_numcols : pred_col = ( i + \"_imputed\" ) if output_mode == \"append\" else i idf_joined = ( idf_test . select ( \"index\" , F . col ( i ) . alias ( \"val\" )) . join ( method . select ( \"index\" , F . col ( pred_col ) . alias ( \"pred\" )), \"index\" , \"left_outer\" , ) . dropna () ) idf_joined = recast_column ( idf = idf_joined , list_of_cols = [ \"val\" , \"pred\" ], list_of_dtypes = [ \"double\" , \"double\" ], ) pred_mean = float ( method . select ( F . mean ( pred_col )) . rdd . flatMap ( lambda x : x ) . collect ()[ 0 ] ) i_nrmse = ( RegressionEvaluator ( metricName = \"rmse\" , labelCol = \"val\" , predictionCol = \"pred\" ) . evaluate ( idf_joined ) ) / abs ( pred_mean ) nrmse += i_nrmse nrmse_all . append ( nrmse ) min_index = nrmse_all . index ( np . min ( nrmse_all )) best_method = method_all [ min_index ] odf = valid_methods [ min_index ] if print_impact : print ( list ( zip ( method_all , nrmse_all ))) print ( \"Best Imputation Method: \" , best_method ) return odf def autoencoder_latentFeatures ( spark , idf , list_of_cols = \"all\" , drop_cols = [], reduction_params = 0.5 , sample_size = 500000 , epochs = 100 , batch_size = 256 , pre_existing_model = False , model_path = \"NA\" , standardization = True , standardization_configs = { \"pre_existing_model\" : False , \"model_path\" : \"NA\" }, imputation = False , imputation_configs = { \"imputation_function\" : \"imputation_MMM\" }, stats_missing = {}, output_mode = \"replace\" , run_type = \"local\" , print_impact = False , ): \"\"\" Many machine learning models suffer from \"the curse of dimensionality\" when the number of features is too large. autoencoder_latentFeatures is able to reduce the dimensionality by compressing input attributes to a smaller number of latent features. To be more specific, it trains a neural network model using TensorFlow library. The neural network contains an encoder and a decoder, where the encoder learns to represent the input using smaller number of latent features controlled by the input *reduction_params* and the decoder learns to reproduce the input using the latent features. In the end, only the encoder will be kept and the latent features generated by the encoder will be added to the output dataframe. However, the neural network model is not trained in a scalable manner, which might not be able to handle large input dataframe. Thus, an input *sample_size* (the default value is 500,000) can be set to control the number of samples to be used to train the model. If the total number of samples exceeds *sample_size*, the rest of the samples will be predicted using the fitted encoder. Standardization is highly recommended if the input attributes are not of the same scale. Otherwise, the model might not converge smoothly. Inputs *standardization* and *standardization_configs* can be set accordingly to perform standardization within the function. In addition, if a sample contains missing values in the model input, the output values for its latent features will all be None. Thus data imputation is also recommended if missing values exist, which can be done within the function by setting inputs *imputation* and *imputation_configs*. Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of numerical columns to encode e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all numerical columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) reduction_params Determines the number of encoded features in the result. If reduction_params < 1, int(reduction_params * <number of columns>) columns will be generated. Else, reduction_params columns will be generated. (Default value = 0.5) sample_size Maximum rows for training the autoencoder model using tensorflow. (Default value = 500000) epochs Number of epochs to train the tensorflow model. (Default value = 100) batch_size Number of samples per gradient update when fitting the tensorflow model. (Default value = 256) pre_existing_model Boolean argument \u2013 True or False. True if model exists already, False Otherwise (Default value = False) model_path If pre_existing_model is True, this argument is path for referring the pre-saved model. If pre_existing_model is False, this argument can be used for saving the model. Default \"NA\" means there is neither pre-existing model nor there is a need to save one. standardization Boolean argument \u2013 True or False. True, if the standardization required. (Default value = True) standardization_configs z_standardization function arguments in dictionary format. (Default value = {\"pre_existing_model\": False) imputation Boolean argument \u2013 True or False. True, if the imputation required. (Default value = False) imputation_configs Takes input in dictionary format. Imputation function name is provided with key \"imputation_name\". optional arguments pertaining to that imputation function can be provided with argument name as key. (Default value = {\"imputation_function\": \"imputation_MMM\"}) stats_missing Takes arguments for read_dataset (data_ingest module) function in a dictionary format to read pre-saved statistics on missing count/pct i.e. if measures_of_counts or missingCount_computation (data_analyzer.stats_generator module) has been computed & saved before. (Default value = {}) output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with transformed columns: latent_<col_index>. \u201cappend\u201d option append transformed columns with format latent_<col_index> to the input dataset, e.g. latent_0, latent_1 will be appended if reduction_params=2. (Default value = \"replace\") run_type \"local\", \"emr\", \"databricks\" (Default value = \"local\") print_impact True, False This argument is to print descriptive statistics of the latest features (Default value = False) Returns ------- DataFrame Dataframe with Latent Features \"\"\" num_cols = attributeType_segregation ( idf )[ 0 ] if list_of_cols == \"all\" : list_of_cols = num_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in drop_cols ])) if any ( x not in num_cols for x in list_of_cols ): raise TypeError ( \"Invalid input for Column(s)\" ) if len ( list_of_cols ) == 0 : warnings . warn ( \"No Latent Features Generated - No Column(s) to Transform\" ) return idf if run_type in list ( platform_root_path . keys ()): root_path = platform_root_path [ run_type ] else : root_path = \"\" if stats_missing == {}: missing_df = missingCount_computation ( spark , idf , list_of_cols ) missing_df . write . parquet ( root_path + \"intermediate_data/autoencoder_latentFeatures/missingCount_computation\" , mode = \"overwrite\" , ) stats_missing = { \"file_path\" : root_path + \"intermediate_data/autoencoder_latentFeatures/missingCount_computation\" , \"file_type\" : \"parquet\" , } else : missing_df = ( read_dataset ( spark , ** stats_missing ) . select ( \"attribute\" , \"missing_count\" , \"missing_pct\" ) . where ( F . col ( \"attribute\" ) . isin ( list_of_cols )) ) empty_cols = ( missing_df . where ( F . col ( \"missing_pct\" ) == 1.0 ) . select ( \"attribute\" ) . rdd . flatMap ( lambda x : x ) . collect () ) if len ( empty_cols ) > 0 : warnings . warn ( \"The following column(s) are excluded from dimensionality reduction as all values are null: \" + \",\" . join ( empty_cols ) ) list_of_cols = [ e for e in list_of_cols if e not in empty_cols ] if standardization : idf_standardized = z_standardization ( spark , idf , list_of_cols = list_of_cols , output_mode = \"append\" , ** standardization_configs , ) list_of_cols_scaled = [ i + \"_scaled\" for i in list_of_cols if ( i + \"_scaled\" ) in idf_standardized . columns ] else : idf_standardized = idf for i in list_of_cols : idf_standardized = idf_standardized . withColumn ( i + \"_scaled\" , F . col ( i )) list_of_cols_scaled = [ i + \"_scaled\" for i in list_of_cols ] if imputation : all_functions = globals () . copy () all_functions . update ( locals ()) f = all_functions . get ( imputation_configs [ \"imputation_function\" ]) args = copy . deepcopy ( imputation_configs ) args . pop ( \"imputation_function\" , None ) missing_df_scaled = ( read_dataset ( spark , ** stats_missing ) . select ( \"attribute\" , \"missing_count\" , \"missing_pct\" ) . withColumn ( \"attribute\" , F . concat ( F . col ( \"attribute\" ), F . lit ( \"_scaled\" ))) ) missing_df_scaled . write . parquet ( root_path + \"intermediate_data/autoencoder_latentFeatures/missingCount_computation_scaled\" , mode = \"overwrite\" , ) stats_missing_scaled = { \"file_path\" : root_path + \"intermediate_data/autoencoder_latentFeatures/missingCount_computation_scaled\" , \"file_type\" : \"parquet\" , } idf_imputed = f ( spark , idf_standardized , list_of_cols_scaled , stats_missing = stats_missing_scaled , ** args , ) else : idf_imputed = idf_standardized . dropna ( subset = list_of_cols_scaled ) n_inputs = len ( list_of_cols_scaled ) if reduction_params < 1 : n_bottleneck = int ( reduction_params * n_inputs ) else : n_bottleneck = int ( reduction_params ) if pre_existing_model : if run_type == \"emr\" : bash_cmd = ( \"aws s3 cp \" + model_path + \"/autoencoders_latentFeatures/encoder.h5 .\" ) output = subprocess . check_output ([ \"bash\" , \"-c\" , bash_cmd ]) bash_cmd = ( \"aws s3 cp \" + model_path + \"/autoencoders_latentFeatures/model.h5 .\" ) output = subprocess . check_output ([ \"bash\" , \"-c\" , bash_cmd ]) encoder = load_model ( \"encoder.h5\" ) model = load_model ( \"model.h5\" ) else : encoder = load_model ( model_path + \"/autoencoders_latentFeatures/encoder.h5\" ) model = load_model ( model_path + \"/autoencoders_latentFeatures/model.h5\" ) else : idf_valid = idf_imputed . select ( list_of_cols_scaled ) idf_model = idf_valid . sample ( False , min ( 1.0 , float ( sample_size ) / idf_valid . count ()), 0 ) idf_train = idf_model . sample ( False , 0.8 , 0 ) idf_test = idf_model . subtract ( idf_train ) X_train = idf_train . toPandas () X_test = idf_test . toPandas () visible = Input ( shape = ( n_inputs ,)) e = Dense ( n_inputs * 2 )( visible ) e = BatchNormalization ()( e ) e = LeakyReLU ()( e ) e = Dense ( n_inputs )( e ) e = BatchNormalization ()( e ) e = LeakyReLU ()( e ) bottleneck = Dense ( n_bottleneck )( e ) d = Dense ( n_inputs )( bottleneck ) d = BatchNormalization ()( d ) d = LeakyReLU ()( d ) d = Dense ( n_inputs * 2 )( d ) d = BatchNormalization ()( d ) d = LeakyReLU ()( d ) output = Dense ( n_inputs , activation = \"linear\" )( d ) model = Model ( inputs = visible , outputs = output ) encoder = Model ( inputs = visible , outputs = bottleneck ) model . compile ( optimizer = \"adam\" , loss = \"mse\" ) history = model . fit ( X_train , X_train , epochs = int ( epochs ), batch_size = int ( batch_size ), verbose = 2 , validation_data = ( X_test , X_test ), ) if ( not pre_existing_model ) & ( model_path != \"NA\" ): if run_type == \"emr\" : encoder . save ( \"encoder.h5\" ) model . save ( \"model.h5\" ) bash_cmd = ( \"aws s3 cp encoder.h5 \" + model_path + \"/autoencoders_latentFeatures/encoder.h5\" ) output = subprocess . check_output ([ \"bash\" , \"-c\" , bash_cmd ]) bash_cmd = ( \"aws s3 cp model.h5 \" + model_path + \"/autoencoders_latentFeatures/model.h5\" ) output = subprocess . check_output ([ \"bash\" , \"-c\" , bash_cmd ]) else : if not os . path . exists ( model_path + \"/autoencoders_latentFeatures/\" ): os . makedirs ( model_path + \"/autoencoders_latentFeatures/\" ) encoder . save ( model_path + \"/autoencoders_latentFeatures/encoder.h5\" ) model . save ( model_path + \"/autoencoders_latentFeatures/model.h5\" ) class ModelWrapperPickable : def __init__ ( self , model ): self . model = model def __getstate__ ( self ): model_str = \"\" with tempfile . NamedTemporaryFile ( suffix = \".hdf5\" , delete = True ) as fd : tensorflow . keras . models . save_model ( self . model , fd . name , overwrite = True ) model_str = fd . read () d = { \"model_str\" : model_str } return d def __setstate__ ( self , state ): with tempfile . NamedTemporaryFile ( suffix = \".hdf5\" , delete = True ) as fd : fd . write ( state [ \"model_str\" ]) fd . flush () self . model = tensorflow . keras . models . load_model ( fd . name ) model_wrapper = ModelWrapperPickable ( encoder ) def compute_output_pandas_udf ( model_wrapper ): @F . pandas_udf ( returnType = T . ArrayType ( T . DoubleType ())) def predict_pandas_udf ( * cols ): X = pd . concat ( cols , axis = 1 ) return pd . Series ( row . tolist () for row in model_wrapper . model . predict ( X )) return predict_pandas_udf odf = idf_imputed . withColumn ( \"predicted_output\" , compute_output_pandas_udf ( model_wrapper )( * list_of_cols_scaled ), ) odf_schema = odf . schema for i in range ( 0 , n_bottleneck ): odf_schema = odf_schema . add ( T . StructField ( \"latent_\" + str ( i ), T . FloatType ())) odf = ( odf . rdd . map ( lambda x : ( * x , * x [ \"predicted_output\" ])) . toDF ( schema = odf_schema ) . drop ( \"predicted_output\" ) ) if output_mode == \"replace\" : odf = odf . drop ( * list_of_cols ) if print_impact : output_cols = [ \"latent_\" + str ( j ) for j in range ( 0 , n_bottleneck )] odf . select ( output_cols ) . describe () . show ( 5 , False ) return odf def PCA_latentFeatures ( spark , idf , list_of_cols = \"all\" , drop_cols = [], explained_variance_cutoff = 0.95 , pre_existing_model = False , model_path = \"NA\" , standardization = True , standardization_configs = { \"pre_existing_model\" : False , \"model_path\" : \"NA\" }, imputation = False , imputation_configs = { \"imputation_function\" : \"imputation_MMM\" }, stats_missing = {}, output_mode = \"replace\" , run_type = \"local\" , print_impact = False , ): \"\"\" Similar to autoencoder_latentFeatures, PCA_latentFeatures also generates latent features which reduces the dimensionality of the input dataframe but through a different technique: Principal Component Analysis (PCA). PCA algorithm produces principal components such that it can describe most of the remaining variance and all the principal components are orthogonal to each other. The final number of generated principal components is controlled by the input *explained_variance_cutoff*. In other words, the number of selected principal components, k, is the minimum value such that top k components (latent features) can explain at least *explained_variance_cutoff* of the total variance. Standardization is highly recommended if the input attributes are not of the same scale. Otherwise the generated latent features might be dominated by the attributes with larger variance. Inputs *standardization* and *standardization_configs* can be set accordingly to perform standardization within the function. In addition, data imputation is also recommended if missing values exist, which can be done within the function by setting inputs *imputation* and *imputation_configs*. Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of numerical columns to encode e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all numerical columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) explained_variance_cutoff Determines the number of latent columns in the output. If N is the smallest integer such that top N latent columns explain more than explained_variance_cutoff variance, these N columns will be selected. (Default value = 0.95) pre_existing_model Boolean argument \u2013 True or False. True if model exists already, False Otherwise (Default value = False) model_path If pre_existing_model is True, this argument is path for referring the pre-saved model. If pre_existing_model is False, this argument can be used for saving the model. Default \"NA\" means there is neither pre-existing model nor there is a need to save one. standardization Boolean argument \u2013 True or False. True, if the standardization required. (Default value = True) standardization_configs z_standardization function arguments in dictionary format. (Default value = {\"pre_existing_model\": False) imputation Boolean argument \u2013 True or False. True, if the imputation required. (Default value = False) imputation_configs Takes input in dictionary format. Imputation function name is provided with key \"imputation_name\". optional arguments pertaining to that imputation function can be provided with argument name as key. (Default value = {\"imputation_function\": \"imputation_MMM\"}) stats_missing Takes arguments for read_dataset (data_ingest module) function in a dictionary format to read pre-saved statistics on missing count/pct i.e. if measures_of_counts or missingCount_computation (data_analyzer.stats_generator module) has been computed & saved before. (Default value = {}) output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with transformed columns: latent_<col_index>. \u201cappend\u201d option append transformed columns with format latent_<col_index> to the input dataset, e.g. latent_0, latent_1. (Default value = \"replace\") run_type \"local\", \"emr\", \"databricks\" (Default value = \"local\") print_impact True, False This argument is to print descriptive statistics of the latest features (Default value = False) Returns ------- DataFrame Dataframe with Latent Features \"\"\" num_cols = attributeType_segregation ( idf )[ 0 ] if list_of_cols == \"all\" : list_of_cols = num_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in drop_cols ])) if any ( x not in num_cols for x in list_of_cols ): raise TypeError ( \"Invalid input for Column(s)\" ) if len ( list_of_cols ) == 0 : warnings . warn ( \"No Latent Features Generated - No Column(s) to Transform\" ) return idf if run_type in list ( platform_root_path . keys ()): root_path = platform_root_path [ run_type ] else : root_path = \"\" if stats_missing == {}: missing_df = missingCount_computation ( spark , idf , list_of_cols ) missing_df . write . parquet ( root_path + \"intermediate_data/PCA_latentFeatures/missingCount_computation\" , mode = \"overwrite\" , ) stats_missing = { \"file_path\" : root_path + \"intermediate_data/PCA_latentFeatures/missingCount_computation\" , \"file_type\" : \"parquet\" , } else : missing_df = ( read_dataset ( spark , ** stats_missing ) . select ( \"attribute\" , \"missing_count\" , \"missing_pct\" ) . where ( F . col ( \"attribute\" ) . isin ( list_of_cols )) ) empty_cols = ( missing_df . where ( F . col ( \"missing_pct\" ) == 1.0 ) . select ( \"attribute\" ) . rdd . flatMap ( lambda x : x ) . collect () ) if len ( empty_cols ) > 0 : warnings . warn ( \"The following column(s) are excluded from dimensionality reduction as all values are null: \" + \",\" . join ( empty_cols ) ) list_of_cols = [ e for e in list_of_cols if e not in empty_cols ] if standardization : idf_standardized = z_standardization ( spark , idf , list_of_cols = list_of_cols , output_mode = \"append\" , ** standardization_configs , ) list_of_cols_scaled = [ i + \"_scaled\" for i in list_of_cols if ( i + \"_scaled\" ) in idf_standardized . columns ] else : idf_standardized = idf for i in list_of_cols : idf_standardized = idf_standardized . withColumn ( i + \"_scaled\" , F . col ( i )) list_of_cols_scaled = [ i + \"_scaled\" for i in list_of_cols ] if imputation : all_functions = globals () . copy () all_functions . update ( locals ()) f = all_functions . get ( imputation_configs [ \"imputation_function\" ]) args = copy . deepcopy ( imputation_configs ) args . pop ( \"imputation_function\" , None ) missing_df_scaled = ( read_dataset ( spark , ** stats_missing ) . select ( \"attribute\" , \"missing_count\" , \"missing_pct\" ) . withColumn ( \"attribute\" , F . concat ( F . col ( \"attribute\" ), F . lit ( \"_scaled\" ))) ) missing_df_scaled . write . parquet ( root_path + \"intermediate_data/PCA_latentFeatures/missingCount_computation_scaled\" , mode = \"overwrite\" , ) stats_missing_scaled = { \"file_path\" : root_path + \"intermediate_data/PCA_latentFeatures/missingCount_computation_scaled\" , \"file_type\" : \"parquet\" , } idf_imputed = f ( spark , idf_standardized , list_of_cols_scaled , stats_missing = stats_missing_scaled , ** args , ) else : idf_imputed = idf_standardized . dropna ( subset = list_of_cols_scaled ) idf_imputed . persist ( pyspark . StorageLevel . MEMORY_AND_DISK ) . count () assembler = VectorAssembler ( inputCols = list_of_cols_scaled , outputCol = \"features\" ) assembled_data = assembler . transform ( idf_imputed ) . drop ( * list_of_cols_scaled ) if pre_existing_model : pca = PCA . load ( model_path + \"/PCA_latentFeatures/pca_path\" ) pcaModel = PCAModel . load ( model_path + \"/PCA_latentFeatures/pcaModel_path\" ) n = pca . getK () else : pca = PCA ( k = len ( list_of_cols_scaled ), inputCol = \"features\" , outputCol = \"features_pca\" ) pcaModel = pca . fit ( assembled_data ) explained_variance = 0 for n in range ( 1 , len ( list_of_cols ) + 1 ): explained_variance += pcaModel . explainedVariance [ n - 1 ] if explained_variance > explained_variance_cutoff : break pca = PCA ( k = n , inputCol = \"features\" , outputCol = \"features_pca\" ) pcaModel = pca . fit ( assembled_data ) if ( not pre_existing_model ) & ( model_path != \"NA\" ): pcaModel . write () . overwrite () . save ( model_path + \"/PCA_latentFeatures/pcaModel_path\" ) pca . write () . overwrite () . save ( model_path + \"/PCA_latentFeatures/pca_path\" ) def vector_to_array ( v ): return v . toArray () . tolist () f_vector_to_array = F . udf ( vector_to_array , T . ArrayType ( T . FloatType ())) odf = ( pcaModel . transform ( assembled_data ) . withColumn ( \"features_pca_array\" , f_vector_to_array ( \"features_pca\" )) . drop ( * [ \"features\" , \"features_pca\" ]) ) odf_schema = odf . schema for i in range ( 0 , n ): odf_schema = odf_schema . add ( T . StructField ( \"latent_\" + str ( i ), T . FloatType ())) odf = ( odf . rdd . map ( lambda x : ( * x , * x [ \"features_pca_array\" ])) . toDF ( schema = odf_schema ) . drop ( \"features_pca_array\" ) . replace ( float ( \"nan\" ), None , subset = [ \"latent_\" + str ( j ) for j in range ( 0 , n )]) ) if output_mode == \"replace\" : odf = odf . drop ( * list_of_cols ) if print_impact : print ( \"Explained Variance: \" , round ( np . sum ( pcaModel . explainedVariance [ 0 : n ]), 4 )) output_cols = [ \"latent_\" + str ( j ) for j in range ( 0 , n )] odf . select ( output_cols ) . describe () . show ( 5 , False ) return odf def feature_transformation ( idf , list_of_cols = \"all\" , drop_cols = [], method_type = \"sqrt\" , N = None , output_mode = \"replace\" , print_impact = False , ): \"\"\" As the name indicates, feature_transformation performs mathematical transformation over selected attributes. The following methods are supported for an input attribute x: ln(x), log10(x), log2(x), e^x, 2^x, 10^x, N^x, square and cube root of x, x^2, x^3, x^N, trigonometric transformations of x (sin, cos, tan, asin, acos, atan), radians, x%N, x!, 1/x, floor and ceiling of x and x rounded to N decimal places. Some transformations only work with positive or non-negative input values such as log and square root and an error will be returned if violated. Parameters ---------- idf Input Dataframe list_of_cols List of numerical columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all numerical columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) method_type \"ln\", \"log10\", \"log2\", \"exp\", \"powOf2\" (2^x), \"powOf10\" (10^x), \"powOfN\" (N^x), \"sqrt\" (square root), \"cbrt\" (cube root), \"sq\" (square), \"cb\" (cube), \"toPowerN\" (x^N), \"sin\", \"cos\", \"tan\", \"asin\", \"acos\", \"atan\", \"radians\", \"remainderDivByN\" (x%N), \"factorial\" (x!), \"mul_inv\" (1/x), \"floor\", \"ceil\", \"roundN\" (round to N decimal places) (Default value = \"sqrt\") N None by default. If method_type is \"powOfN\", \"toPowerN\", \"remainderDivByN\" or \"roundN\", N will be used as the required constant. output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with transformed columns. \u201cappend\u201d option append transformed columns with a postfix (E.g. \"_ln\", \"_powOf<N>\") to the input dataset. (Default value = \"replace\") print_impact True, False This argument is to print before and after descriptive statistics of the transformed features (Default value = False) Returns ------- DataFrame Transformed Dataframe \"\"\" num_cols = attributeType_segregation ( idf )[ 0 ] if list_of_cols == \"all\" : list_of_cols = num_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in drop_cols ])) if ( len ( list_of_cols ) == 0 ) | ( any ( x not in num_cols for x in list_of_cols )): raise TypeError ( \"Invalid input for Column(s)\" ) if method_type not in [ \"ln\" , \"log10\" , \"log2\" , \"exp\" , \"powOf2\" , \"powOf10\" , \"powOfN\" , \"sqrt\" , \"cbrt\" , \"sq\" , \"cb\" , \"toPowerN\" , \"sin\" , \"cos\" , \"tan\" , \"asin\" , \"acos\" , \"atan\" , \"radians\" , \"remainderDivByN\" , \"factorial\" , \"mul_inv\" , \"floor\" , \"ceil\" , \"roundN\" , ]: raise TypeError ( \"Invalid input method_type\" ) num_cols = attributeType_segregation ( idf . select ( list_of_cols ))[ 0 ] list_of_cols = num_cols odf = idf transformation_function = { \"ln\" : F . log , \"log10\" : F . log10 , \"log2\" : F . log2 , \"exp\" : F . exp , \"powOf2\" : ( lambda x : F . pow ( 2.0 , x )), \"powOf10\" : ( lambda x : F . pow ( 10.0 , x )), \"powOfN\" : ( lambda x : F . pow ( N , x )), \"sqrt\" : F . sqrt , \"cbrt\" : F . cbrt , \"sq\" : ( lambda x : x ** 2 ), \"cb\" : ( lambda x : x ** 3 ), \"toPowerN\" : ( lambda x : x ** N ), \"sin\" : F . sin , \"cos\" : F . cos , \"tan\" : F . tan , \"asin\" : F . asin , \"acos\" : F . acos , \"atan\" : F . atan , \"radians\" : F . radians , \"remainderDivByN\" : ( lambda x : x % F . lit ( N )), \"factorial\" : F . factorial , \"mul_inv\" : ( lambda x : F . lit ( 1 ) / x ), \"floor\" : F . floor , \"ceil\" : F . ceil , \"roundN\" : ( lambda x : F . round ( x , N )), } def get_col_name ( i ): if output_mode == \"replace\" : return i else : if method_type in [ \"powOfN\" , \"toPowerN\" , \"remainderDivByN\" , \"roundN\" ]: return i + \"_\" + method_type [: - 1 ] + str ( N ) else : return i + \"_\" + method_type output_cols = [] for i in list_of_cols : modify_col = get_col_name ( i ) odf = odf . withColumn ( modify_col , transformation_function [ method_type ]( F . col ( i ))) output_cols . append ( modify_col ) if print_impact : print ( \"Before:\" ) idf . select ( list_of_cols ) . describe () . show ( 5 , False ) print ( \"After:\" ) odf . select ( output_cols ) . describe () . show ( 5 , False ) return odf def boxcox_transformation ( idf , list_of_cols = \"all\" , drop_cols = [], boxcox_lambda = None , output_mode = \"replace\" , print_impact = False , ): \"\"\" Some machine learning algorithms require the input data to follow normal distributions. Thus, when the input data is too skewed, boxcox_transformation can be used to transform it into a more normal-like distribution. The transformed value of a sample x depends on a coefficient lambda: (1) if lambda = 0, x is transformed into log(x); (2) if lambda !=0, x is transformed into (x^lambda-1)/lambda. The value of lambda can be either specified by the user or automatically selected within the function. If lambda needs to be selected within the function, a range of values (0, 1, -1, 0.5, -0.5, 2, -2, 0.25, -0.25, 3, -3, 4, -4, 5, -5) will be tested and the lambda, which optimizes the KolmogorovSmirnovTest by PySpark with the theoretical distribution being normal, will be used to perform the transformation. Different lambda values can be assigned to different attributes but one attribute can only be assigned one lambda value. Parameters ---------- idf Input Dataframe list_of_cols List of numerical columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all numerical columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) boxcox_lambda Lambda value for box_cox transormation. If boxcox_lambda is not None, it will be directly used for the transformation. It can be a (1) list: each element represents a lambda value for an attribute and the length of the list must be the same as the number of columns to transform. (2) int/float: all attributes will be assigned the same lambda value. Else, search for the best lambda among [1,-1,0.5,-0.5,2,-2,0.25,-0.25,3,-3,4,-4,5,-5] for each column and apply the transformation (Default value = None) output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with transformed columns. \u201cappend\u201d option append transformed columns with a postfix \"_bxcx_<lambda>\" to the input dataset. (Default value = \"replace\") print_impact True, False This argument is to print before and after descriptive statistics of the transformed features (Default value = False) Returns ------- DataFrame Transformed Dataframe \"\"\" num_cols = attributeType_segregation ( idf )[ 0 ] if list_of_cols == \"all\" : list_of_cols = num_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in drop_cols ])) if ( len ( list_of_cols ) == 0 ) | ( any ( x not in num_cols for x in list_of_cols )): raise TypeError ( \"Invalid input for Column(s)\" ) num_cols = attributeType_segregation ( idf . select ( list_of_cols ))[ 0 ] list_of_cols = num_cols odf = idf col_mins = idf . select ([ F . min ( i ) for i in list_of_cols ]) if any ([ i <= 0 for i in col_mins . rdd . flatMap ( lambda x : x ) . collect ()]): col_mins . show ( 1 , False ) raise ValueError ( \"Data must be positive\" ) if boxcox_lambda is not None : if isinstance ( boxcox_lambda , ( list , tuple )): if len ( boxcox_lambda ) != len ( list_of_cols ): raise TypeError ( \"Invalid input for boxcox_lambda\" ) elif not all ([ isinstance ( l , ( float , int )) for l in boxcox_lambda ]): raise TypeError ( \"Invalid input for boxcox_lambda\" ) else : boxcox_lambda_list = list ( boxcox_lambda ) elif isinstance ( boxcox_lambda , ( float , int )): boxcox_lambda_list = [ boxcox_lambda ] * len ( list_of_cols ) else : raise TypeError ( \"Invalid input for boxcox_lambda\" ) else : boxcox_lambda_list = [] for i in list_of_cols : lambdaVal = [ 1 , - 1 , 0.5 , - 0.5 , 2 , - 2 , 0.25 , - 0.25 , 3 , - 3 , 4 , - 4 , 5 , - 5 ] best_pVal = 0 for j in lambdaVal : pVal = Statistics . kolmogorovSmirnovTest ( odf . select ( F . pow ( F . col ( i ), j )) . rdd . flatMap ( lambda x : x ), \"norm\" ) . pValue if pVal > best_pVal : best_pVal = pVal best_lambdaVal = j pVal = Statistics . kolmogorovSmirnovTest ( odf . select ( F . log ( F . col ( i ))) . rdd . flatMap ( lambda x : x ), \"norm\" ) . pValue if pVal > best_pVal : best_pVal = pVal best_lambdaVal = 0 boxcox_lambda_list . append ( best_lambdaVal ) output_cols = [] for i , curr_lambdaVal in zip ( list_of_cols , boxcox_lambda_list ): if curr_lambdaVal != 1 : modify_col = ( ( i + \"_bxcx_\" + str ( curr_lambdaVal )) if output_mode == \"append\" else i ) output_cols . append ( modify_col ) if curr_lambdaVal == 0 : odf = odf . withColumn ( modify_col , F . log ( F . col ( i ))) else : odf = odf . withColumn ( modify_col , F . pow ( F . col ( i ), curr_lambdaVal )) if len ( output_cols ) == 0 : warnings . warn ( \"lambdaVal for all columns are 1 so no transformation is performed and idf is returned\" ) return idf if print_impact : print ( \"Transformed Columns: \" , list_of_cols ) print ( \"Best BoxCox Parameter(s): \" , boxcox_lambda_list ) print ( \"Before:\" ) idf . select ( list_of_cols ) . describe () . unionByName ( idf . select ([ F . skewness ( i ) . alias ( i ) for i in list_of_cols ]) . withColumn ( \"summary\" , F . lit ( \"skewness\" ) ) ) . show ( 6 , False ) print ( \"After:\" ) if output_mode == \"replace\" : odf . select ( list_of_cols ) . describe () . unionByName ( odf . select ([ F . skewness ( i ) . alias ( i ) for i in list_of_cols ]) . withColumn ( \"summary\" , F . lit ( \"skewness\" ) ) ) . show ( 6 , False ) else : output_cols = [( \"`\" + i + \"`\" ) for i in output_cols ] odf . select ( output_cols ) . describe () . unionByName ( odf . select ( [ F . skewness ( i ) . alias ( i [ 1 : - 1 ]) for i in output_cols ] ) . withColumn ( \"summary\" , F . lit ( \"skewness\" )) ) . show ( 6 , False ) return odf def outlier_categories ( spark , idf , list_of_cols = \"all\" , drop_cols = [], coverage = 1.0 , max_category = 50 , pre_existing_model = False , model_path = \"NA\" , output_mode = \"replace\" , print_impact = False , ): \"\"\" This function replaces less frequently seen values (called as outlier values in the current context) in a categorical column by 'others'. Outlier values can be defined in two ways \u2013 a) Max N categories, where N is used defined value. In this method, top N-1 frequently seen categories are considered and rest are clubbed under single category 'others'. or Alternatively, b) Coverage \u2013 top frequently seen categories are considered till it covers minimum N% of rows and rest lesser seen values are mapped to mapped to others. Even if the Coverage is less, maximum category constraint is given priority. Further, there is a caveat that when multiple categories have same rank. Then, number of categorical values can be more than max_category defined by the user. Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of categorical columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all categorical columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) coverage Defines the minimum % of rows that will be mapped to actual category name and the rest to be mapped to others and takes value between 0 to 1. Coverage of 0.8 can be interpreted as top frequently seen categories are considered till it covers minimum 80% of rows and rest lesser seen values are mapped to others. (Default value = 1.0) max_category Even if coverage is less, only (max_category - 1) categories will be mapped to actual name and rest to others. Caveat is when multiple categories have same rank, then #categories can be more than max_category. (Default value = 50) pre_existing_model Boolean argument \u2013 True or False. True if the model with the outlier/other values for each attribute exists already to be used, False Otherwise. (Default value = False) model_path If pre_existing_model is True, this argument is path for the pre-saved model. If pre_existing_model is False, this field can be used for saving the model. Default \"NA\" means there is neither pre-existing model nor there is a need to save one. output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with transformed column. \u201cappend\u201d option append transformed column to the input dataset with a postfix \"_outliered\" e.g. column X is appended as X_outliered. (Default value = \"replace\") print_impact True, False This argument is to print before and after unique count of the transformed features (Default value = False) Returns ------- DataFrame Transformed Dataframe \"\"\" cat_cols = attributeType_segregation ( idf )[ 1 ] if list_of_cols == \"all\" : list_of_cols = cat_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in drop_cols ])) if any ( x not in cat_cols for x in list_of_cols ): raise TypeError ( \"Invalid input for Column(s)\" ) if len ( list_of_cols ) == 0 : warnings . warn ( \"No Outlier Categories Computation - No categorical column(s) to transform\" ) return idf if ( coverage <= 0 ) | ( coverage > 1 ): raise TypeError ( \"Invalid input for Coverage Value\" ) if max_category < 2 : raise TypeError ( \"Invalid input for Maximum No. of Categories Allowed\" ) if output_mode not in ( \"replace\" , \"append\" ): raise TypeError ( \"Invalid input for output_mode\" ) if pre_existing_model : df_model = spark . read . csv ( model_path + \"/outlier_categories\" , header = True , inferSchema = True ) else : for index , i in enumerate ( list_of_cols ): window = Window . partitionBy () . orderBy ( F . desc ( \"count_pct\" )) df_cats = ( idf . groupBy ( i ) . count () . dropna () . withColumn ( \"count_pct\" , F . col ( \"count\" ) / F . sum ( \"count\" ) . over ( Window . partitionBy ()), ) . withColumn ( \"rank\" , F . rank () . over ( window )) . withColumn ( \"cumu\" , F . sum ( \"count_pct\" ) . over ( window . rowsBetween ( Window . unboundedPreceding , 0 ) ), ) . withColumn ( \"lag_cumu\" , F . lag ( \"cumu\" ) . over ( window )) . fillna ( 0 ) . where ( ~ (( F . col ( \"cumu\" ) >= coverage ) & ( F . col ( \"lag_cumu\" ) >= coverage ))) . where ( F . col ( \"rank\" ) <= ( max_category - 1 )) . select ( F . lit ( i ) . alias ( \"attribute\" ), F . col ( i ) . alias ( \"parameters\" )) ) if index == 0 : df_model = df_cats else : df_model = df_model . union ( df_cats ) odf = idf for i in list_of_cols : parameters = ( df_model . where ( F . col ( \"attribute\" ) == i ) . select ( \"parameters\" ) . rdd . flatMap ( lambda x : x ) . collect () ) if output_mode == \"replace\" : odf = odf . withColumn ( i , F . when ( ( F . col ( i ) . isin ( parameters )) | ( F . col ( i ) . isNull ()), F . col ( i ) ) . otherwise ( \"others\" ), ) else : odf = odf . withColumn ( i + \"_outliered\" , F . when ( ( F . col ( i ) . isin ( parameters )) | ( F . col ( i ) . isNull ()), F . col ( i ) ) . otherwise ( \"others\" ), ) if ( not pre_existing_model ) & ( model_path != \"NA\" ): df_model . repartition ( 1 ) . write . csv ( model_path + \"/outlier_categories\" , header = True , mode = \"overwrite\" ) if print_impact : if output_mode == \"replace\" : output_cols = list_of_cols else : output_cols = [( i + \"_outliered\" ) for i in list_of_cols ] uniqueCount_computation ( spark , idf , list_of_cols ) . select ( \"attribute\" , F . col ( \"unique_values\" ) . alias ( \"uniqueValues_before\" ) ) . show ( len ( list_of_cols ), False ) uniqueCount_computation ( spark , odf , output_cols ) . select ( \"attribute\" , F . col ( \"unique_values\" ) . alias ( \"uniqueValues_after\" ) ) . show ( len ( list_of_cols ), False ) return odf def expression_parser ( idf , list_of_expr , postfix = \"\" , print_impact = False ): \"\"\" expression_parser can be used to evaluate a list of SQL expressions and output the result as new features. It is able to handle column names containing special characters such as \u201c.\u201d, \u201c-\u201d, \u201c@\u201d, \u201c^\u201d, etc, by converting them to \u201c_\u201d first before the evaluation and convert them back to the original names before returning the output dataframe. Parameters ---------- idf Input Dataframe list_of_expr List of expressions to evaluate as new features e.g., [\"expr1\",\"expr2\"]. Alternatively, expressions can be specified in a string format, where different expressions are separated by pipe delimiter \u201c|\u201d e.g., \"expr1|expr2\". postfix postfix for new feature name.Naming convention \"f\" + expression_index + postfix e.g. with postfix of \"new\", new added features are named as f0new, f1new etc. (Default value = \"\") print_impact True, False This argument is to print the descriptive statistics of the parsed features (Default value = False) Returns ------- DataFrame Parsed Dataframe \"\"\" if isinstance ( list_of_expr , str ): list_of_expr = [ x . strip () for x in list_of_expr . split ( \"|\" )] special_chars = [ \"&\" , \"$\" , \";\" , \":\" , \",\" , \"*\" , \"#\" , \"@\" , \"?\" , \"%\" , \"!\" , \"^\" , \"(\" , \")\" , \"-\" , \"/\" , \"'\" , \".\" , '\"' , ] rename_cols = [] replace_chars = {} for char in special_chars : for col in idf . columns : if char in col : rename_cols . append ( col ) if col in replace_chars . keys (): ( replace_chars [ col ]) . append ( char ) else : replace_chars [ col ] = [ char ] rename_mapping_to_new , rename_mapping_to_old = {}, {} idf_renamed = idf for col in rename_cols : new_col = col for char in replace_chars [ col ]: new_col = new_col . replace ( char , \"_\" ) rename_mapping_to_old [ new_col ] = col rename_mapping_to_new [ col ] = new_col idf_renamed = idf_renamed . withColumnRenamed ( col , new_col ) list_of_expr_ = [] for expr in list_of_expr : new_expr = expr for col in rename_cols : if col in expr : new_expr = new_expr . replace ( col , rename_mapping_to_new [ col ]) list_of_expr_ . append ( new_expr ) list_of_expr = list_of_expr_ odf = idf_renamed new_cols = [] for index , exp in enumerate ( list_of_expr ): odf = odf . withColumn ( \"f\" + str ( index ) + postfix , F . expr ( exp )) new_cols . append ( \"f\" + str ( index ) + postfix ) for new_col , col in rename_mapping_to_old . items (): odf = odf . withColumnRenamed ( new_col , col ) if print_impact : print ( \"Columns Added: \" , new_cols ) odf . select ( new_cols ) . describe () . show ( 5 , False ) return odf Functions def IQR_standardization ( spark, idf, list_of_cols='all', drop_cols=[], pre_existing_model=False, model_path='NA', output_mode='replace', print_impact=False) Parameters spark Spark Session idf Input Dataframe list_of_cols List of numerical columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all numerical columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) pre_existing_model Boolean argument \u2013 True or False. True if model files (25/50/75 percentile for each feature) exists already, False Otherwise (Default value = False) model_path If pre_existing_model is True, this argument is path for referring the pre-saved model. If pre_existing_model is False, this argument can be used for saving the model. Default \"NA\" means there is neither pre-existing model nor there is a need to save one. output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with transformed column. \u201cappend\u201d option append transformed column to the input dataset with a postfix \"_scaled\" e.g. column X is appended as X_scaled. (Default value = \"replace\") print_impact True, False (Default value = False) This argument is to print out the before and after descriptive statistics of rescaled columns. Returns DataFrame Rescaled Dataframe Expand source code def IQR_standardization ( spark , idf , list_of_cols = \"all\" , drop_cols = [], pre_existing_model = False , model_path = \"NA\" , output_mode = \"replace\" , print_impact = False , ): \"\"\" Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of numerical columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all numerical columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) pre_existing_model Boolean argument \u2013 True or False. True if model files (25/50/75 percentile for each feature) exists already, False Otherwise (Default value = False) model_path If pre_existing_model is True, this argument is path for referring the pre-saved model. If pre_existing_model is False, this argument can be used for saving the model. Default \"NA\" means there is neither pre-existing model nor there is a need to save one. output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with transformed column. \u201cappend\u201d option append transformed column to the input dataset with a postfix \"_scaled\" e.g. column X is appended as X_scaled. (Default value = \"replace\") print_impact True, False (Default value = False) This argument is to print out the before and after descriptive statistics of rescaled columns. Returns ------- DataFrame Rescaled Dataframe \"\"\" num_cols = attributeType_segregation ( idf )[ 0 ] if list_of_cols == \"all\" : list_of_cols = num_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in drop_cols ])) if any ( x not in num_cols for x in list_of_cols ): raise TypeError ( \"Invalid input for Column(s)\" ) if len ( list_of_cols ) == 0 : warnings . warn ( \"No Standardization Performed - No numerical column(s) to transform\" ) return idf if output_mode not in ( \"replace\" , \"append\" ): raise TypeError ( \"Invalid input for output_mode\" ) if pre_existing_model : df_model = spark . read . parquet ( model_path + \"/IQR_standardization\" ) parameters = [] for i in list_of_cols : mapped_value = ( df_model . where ( F . col ( \"feature\" ) == i ) . select ( \"parameters\" ) . rdd . flatMap ( lambda x : x ) . collect ()[ 0 ] ) parameters . append ( mapped_value ) else : parameters = idf . approxQuantile ( list_of_cols , [ 0.25 , 0.5 , 0.75 ], 0.01 ) excluded_cols = [] for i , param in zip ( list_of_cols , parameters ): if len ( param ) > 0 : if round ( param [ 0 ], 5 ) == round ( param [ 2 ], 5 ): excluded_cols . append ( i ) else : excluded_cols . append ( i ) if len ( excluded_cols ) > 0 : warnings . warn ( \"The following column(s) are excluded from standardization because the 75th and 25th percentiles are the same:\" + str ( excluded_cols ) ) odf = idf for index , i in enumerate ( list_of_cols ): if i not in excluded_cols : modify_col = ( i + \"_scaled\" ) if ( output_mode == \"append\" ) else i odf = odf . withColumn ( modify_col , ( F . col ( i ) - parameters [ index ][ 1 ]) / ( parameters [ index ][ 2 ] - parameters [ index ][ 0 ]), ) if ( not pre_existing_model ) & ( model_path != \"NA\" ): df_model = spark . createDataFrame ( zip ( list_of_cols , parameters ), schema = [ \"feature\" , \"parameters\" ] ) df_model . coalesce ( 1 ) . write . parquet ( model_path + \"/IQR_standardization\" , mode = \"overwrite\" ) if print_impact : if output_mode == \"replace\" : output_cols = list_of_cols else : output_cols = [ ( i + \"_scaled\" ) for i in list_of_cols if i not in excluded_cols ] print ( \"Before: \" ) idf . select ( list_of_cols ) . describe () . show ( 5 , False ) print ( \"After: \" ) odf . select ( output_cols ) . describe () . show ( 5 , False ) return odf def PCA_latentFeatures ( spark, idf, list_of_cols='all', drop_cols=[], explained_variance_cutoff=0.95, pre_existing_model=False, model_path='NA', standardization=True, standardization_configs={'pre_existing_model': False, 'model_path': 'NA'}, imputation=False, imputation_configs={'imputation_function': 'imputation_MMM'}, stats_missing={}, output_mode='replace', run_type='local', print_impact=False) Similar to autoencoder_latentFeatures, PCA_latentFeatures also generates latent features which reduces the dimensionality of the input dataframe but through a different technique: Principal Component Analysis (PCA). PCA algorithm produces principal components such that it can describe most of the remaining variance and all the principal components are orthogonal to each other. The final number of generated principal components is controlled by the input explained_variance_cutoff . In other words, the number of selected principal components, k, is the minimum value such that top k components (latent features) can explain at least explained_variance_cutoff of the total variance. Standardization is highly recommended if the input attributes are not of the same scale. Otherwise the generated latent features might be dominated by the attributes with larger variance. Inputs standardization and standardization_configs can be set accordingly to perform standardization within the function. In addition, data imputation is also recommended if missing values exist, which can be done within the function by setting inputs imputation and imputation_configs . Parameters spark Spark Session idf Input Dataframe list_of_cols List of numerical columns to encode e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all numerical columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) explained_variance_cutoff Determines the number of latent columns in the output. If N is the smallest integer such that top N latent columns explain more than explained_variance_cutoff variance, these N columns will be selected. (Default value = 0.95) pre_existing_model Boolean argument \u2013 True or False. True if model exists already, False Otherwise (Default value = False) model_path If pre_existing_model is True, this argument is path for referring the pre-saved model. If pre_existing_model is False, this argument can be used for saving the model. Default \"NA\" means there is neither pre-existing model nor there is a need to save one. standardization Boolean argument \u2013 True or False. True, if the standardization required. (Default value = True) standardization_configs z_standardization function arguments in dictionary format. (Default value = {\"pre_existing_model\": False) imputation Boolean argument \u2013 True or False. True, if the imputation required. (Default value = False) imputation_configs Takes input in dictionary format. Imputation function name is provided with key \"imputation_name\". optional arguments pertaining to that imputation function can be provided with argument name as key. (Default value = {\"imputation_function\": \"imputation_MMM\"}) stats_missing Takes arguments for read_dataset (data_ingest module) function in a dictionary format to read pre-saved statistics on missing count/pct i.e. if measures_of_counts or missingCount_computation (data_analyzer.stats_generator module) has been computed & saved before. (Default value = {}) output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with transformed columns: latent_ . \u201cappend\u201d option append transformed columns with format latent_ to the input dataset, e.g. latent_0, latent_1. (Default value = \"replace\") run_type \"local\", \"emr\", \"databricks\" (Default value = \"local\") print_impact True, False This argument is to print descriptive statistics of the latest features (Default value = False) Returns DataFrame Dataframe with Latent Features Expand source code def PCA_latentFeatures ( spark , idf , list_of_cols = \"all\" , drop_cols = [], explained_variance_cutoff = 0.95 , pre_existing_model = False , model_path = \"NA\" , standardization = True , standardization_configs = { \"pre_existing_model\" : False , \"model_path\" : \"NA\" }, imputation = False , imputation_configs = { \"imputation_function\" : \"imputation_MMM\" }, stats_missing = {}, output_mode = \"replace\" , run_type = \"local\" , print_impact = False , ): \"\"\" Similar to autoencoder_latentFeatures, PCA_latentFeatures also generates latent features which reduces the dimensionality of the input dataframe but through a different technique: Principal Component Analysis (PCA). PCA algorithm produces principal components such that it can describe most of the remaining variance and all the principal components are orthogonal to each other. The final number of generated principal components is controlled by the input *explained_variance_cutoff*. In other words, the number of selected principal components, k, is the minimum value such that top k components (latent features) can explain at least *explained_variance_cutoff* of the total variance. Standardization is highly recommended if the input attributes are not of the same scale. Otherwise the generated latent features might be dominated by the attributes with larger variance. Inputs *standardization* and *standardization_configs* can be set accordingly to perform standardization within the function. In addition, data imputation is also recommended if missing values exist, which can be done within the function by setting inputs *imputation* and *imputation_configs*. Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of numerical columns to encode e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all numerical columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) explained_variance_cutoff Determines the number of latent columns in the output. If N is the smallest integer such that top N latent columns explain more than explained_variance_cutoff variance, these N columns will be selected. (Default value = 0.95) pre_existing_model Boolean argument \u2013 True or False. True if model exists already, False Otherwise (Default value = False) model_path If pre_existing_model is True, this argument is path for referring the pre-saved model. If pre_existing_model is False, this argument can be used for saving the model. Default \"NA\" means there is neither pre-existing model nor there is a need to save one. standardization Boolean argument \u2013 True or False. True, if the standardization required. (Default value = True) standardization_configs z_standardization function arguments in dictionary format. (Default value = {\"pre_existing_model\": False) imputation Boolean argument \u2013 True or False. True, if the imputation required. (Default value = False) imputation_configs Takes input in dictionary format. Imputation function name is provided with key \"imputation_name\". optional arguments pertaining to that imputation function can be provided with argument name as key. (Default value = {\"imputation_function\": \"imputation_MMM\"}) stats_missing Takes arguments for read_dataset (data_ingest module) function in a dictionary format to read pre-saved statistics on missing count/pct i.e. if measures_of_counts or missingCount_computation (data_analyzer.stats_generator module) has been computed & saved before. (Default value = {}) output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with transformed columns: latent_<col_index>. \u201cappend\u201d option append transformed columns with format latent_<col_index> to the input dataset, e.g. latent_0, latent_1. (Default value = \"replace\") run_type \"local\", \"emr\", \"databricks\" (Default value = \"local\") print_impact True, False This argument is to print descriptive statistics of the latest features (Default value = False) Returns ------- DataFrame Dataframe with Latent Features \"\"\" num_cols = attributeType_segregation ( idf )[ 0 ] if list_of_cols == \"all\" : list_of_cols = num_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in drop_cols ])) if any ( x not in num_cols for x in list_of_cols ): raise TypeError ( \"Invalid input for Column(s)\" ) if len ( list_of_cols ) == 0 : warnings . warn ( \"No Latent Features Generated - No Column(s) to Transform\" ) return idf if run_type in list ( platform_root_path . keys ()): root_path = platform_root_path [ run_type ] else : root_path = \"\" if stats_missing == {}: missing_df = missingCount_computation ( spark , idf , list_of_cols ) missing_df . write . parquet ( root_path + \"intermediate_data/PCA_latentFeatures/missingCount_computation\" , mode = \"overwrite\" , ) stats_missing = { \"file_path\" : root_path + \"intermediate_data/PCA_latentFeatures/missingCount_computation\" , \"file_type\" : \"parquet\" , } else : missing_df = ( read_dataset ( spark , ** stats_missing ) . select ( \"attribute\" , \"missing_count\" , \"missing_pct\" ) . where ( F . col ( \"attribute\" ) . isin ( list_of_cols )) ) empty_cols = ( missing_df . where ( F . col ( \"missing_pct\" ) == 1.0 ) . select ( \"attribute\" ) . rdd . flatMap ( lambda x : x ) . collect () ) if len ( empty_cols ) > 0 : warnings . warn ( \"The following column(s) are excluded from dimensionality reduction as all values are null: \" + \",\" . join ( empty_cols ) ) list_of_cols = [ e for e in list_of_cols if e not in empty_cols ] if standardization : idf_standardized = z_standardization ( spark , idf , list_of_cols = list_of_cols , output_mode = \"append\" , ** standardization_configs , ) list_of_cols_scaled = [ i + \"_scaled\" for i in list_of_cols if ( i + \"_scaled\" ) in idf_standardized . columns ] else : idf_standardized = idf for i in list_of_cols : idf_standardized = idf_standardized . withColumn ( i + \"_scaled\" , F . col ( i )) list_of_cols_scaled = [ i + \"_scaled\" for i in list_of_cols ] if imputation : all_functions = globals () . copy () all_functions . update ( locals ()) f = all_functions . get ( imputation_configs [ \"imputation_function\" ]) args = copy . deepcopy ( imputation_configs ) args . pop ( \"imputation_function\" , None ) missing_df_scaled = ( read_dataset ( spark , ** stats_missing ) . select ( \"attribute\" , \"missing_count\" , \"missing_pct\" ) . withColumn ( \"attribute\" , F . concat ( F . col ( \"attribute\" ), F . lit ( \"_scaled\" ))) ) missing_df_scaled . write . parquet ( root_path + \"intermediate_data/PCA_latentFeatures/missingCount_computation_scaled\" , mode = \"overwrite\" , ) stats_missing_scaled = { \"file_path\" : root_path + \"intermediate_data/PCA_latentFeatures/missingCount_computation_scaled\" , \"file_type\" : \"parquet\" , } idf_imputed = f ( spark , idf_standardized , list_of_cols_scaled , stats_missing = stats_missing_scaled , ** args , ) else : idf_imputed = idf_standardized . dropna ( subset = list_of_cols_scaled ) idf_imputed . persist ( pyspark . StorageLevel . MEMORY_AND_DISK ) . count () assembler = VectorAssembler ( inputCols = list_of_cols_scaled , outputCol = \"features\" ) assembled_data = assembler . transform ( idf_imputed ) . drop ( * list_of_cols_scaled ) if pre_existing_model : pca = PCA . load ( model_path + \"/PCA_latentFeatures/pca_path\" ) pcaModel = PCAModel . load ( model_path + \"/PCA_latentFeatures/pcaModel_path\" ) n = pca . getK () else : pca = PCA ( k = len ( list_of_cols_scaled ), inputCol = \"features\" , outputCol = \"features_pca\" ) pcaModel = pca . fit ( assembled_data ) explained_variance = 0 for n in range ( 1 , len ( list_of_cols ) + 1 ): explained_variance += pcaModel . explainedVariance [ n - 1 ] if explained_variance > explained_variance_cutoff : break pca = PCA ( k = n , inputCol = \"features\" , outputCol = \"features_pca\" ) pcaModel = pca . fit ( assembled_data ) if ( not pre_existing_model ) & ( model_path != \"NA\" ): pcaModel . write () . overwrite () . save ( model_path + \"/PCA_latentFeatures/pcaModel_path\" ) pca . write () . overwrite () . save ( model_path + \"/PCA_latentFeatures/pca_path\" ) def vector_to_array ( v ): return v . toArray () . tolist () f_vector_to_array = F . udf ( vector_to_array , T . ArrayType ( T . FloatType ())) odf = ( pcaModel . transform ( assembled_data ) . withColumn ( \"features_pca_array\" , f_vector_to_array ( \"features_pca\" )) . drop ( * [ \"features\" , \"features_pca\" ]) ) odf_schema = odf . schema for i in range ( 0 , n ): odf_schema = odf_schema . add ( T . StructField ( \"latent_\" + str ( i ), T . FloatType ())) odf = ( odf . rdd . map ( lambda x : ( * x , * x [ \"features_pca_array\" ])) . toDF ( schema = odf_schema ) . drop ( \"features_pca_array\" ) . replace ( float ( \"nan\" ), None , subset = [ \"latent_\" + str ( j ) for j in range ( 0 , n )]) ) if output_mode == \"replace\" : odf = odf . drop ( * list_of_cols ) if print_impact : print ( \"Explained Variance: \" , round ( np . sum ( pcaModel . explainedVariance [ 0 : n ]), 4 )) output_cols = [ \"latent_\" + str ( j ) for j in range ( 0 , n )] odf . select ( output_cols ) . describe () . show ( 5 , False ) return odf def attribute_binning ( spark, idf, list_of_cols='all', drop_cols=[], method_type='equal_range', bin_size=10, bin_dtype='numerical', pre_existing_model=False, model_path='NA', output_mode='replace', print_impact=False) Attribute binning (or discretization) is a method of numerical attribute into discrete (integer or categorical values) using pre-defined number of bins. This data pre-processing technique is used to reduce the effects of minor observation errors. Also, Binning introduces non-linearity and tends to improve the performance of the model. In this function, we are focussing on unsupervised way of binning i.e. without taking the target variable into account - Equal Range Binning, Equal Frequency Binning. In Equal Range method, each bin is of equal size/width and computed as: w = max- min / no. of bins bins cutoff=[min, min+w,min+2w\u2026..,max-w,max] whereas in Equal Frequency binning method, bins are created in such a way that each bin has equal no. of rows, though the width of bins may vary from each other. w = 1 / no. of bins bins cutoff=[min, wthpctile, 2wthpctile\u2026.,max ] Parameters spark Spark Session idf Input Dataframe list_of_cols List of numerical columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all numerical columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) method_type \"equal_frequency\", \"equal_range\". In \"equal_range\" method, each bin is of equal size/width and in \"equal_frequency\", each bin has equal no. of rows, though the width of bins may vary. (Default value = \"equal_range\") bin_size Number of bins. (Default value = 10) bin_dtype \"numerical\", \"categorical\". With \"numerical\" option, original value is replaced with an Integer (1,2,\u2026) and with \"categorical\" option, original replaced with a string describing min and max value allowed in the bin (\"minval-maxval\"). (Default value = \"numerical\") pre_existing_model Boolean argument \u2013 True or False. True if binning model exists already, False Otherwise. (Default value = False) model_path If pre_existing_model is True, this argument is path for referring the pre-saved model. If pre_existing_model is False, this argument can be used for saving the model. Default \"NA\" means there is neither pre-existing model nor there is a need to save one. output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with transformed column. \u201cappend\u201d option append transformed column to the input dataset with a postfix \"_binned\" e.g. column X is appended as X_binned. (Default value = \"replace\") print_impact True, False (Default value = False) This argument is to print the number of categories generated for each attribute (may or may be not same as bin_size) Returns DataFrame Binned Dataframe Expand source code def attribute_binning ( spark , idf , list_of_cols = \"all\" , drop_cols = [], method_type = \"equal_range\" , bin_size = 10 , bin_dtype = \"numerical\" , pre_existing_model = False , model_path = \"NA\" , output_mode = \"replace\" , print_impact = False , ): \"\"\" Attribute binning (or discretization) is a method of numerical attribute into discrete (integer or categorical values) using pre-defined number of bins. This data pre-processing technique is used to reduce the effects of minor observation errors. Also, Binning introduces non-linearity and tends to improve the performance of the model. In this function, we are focussing on unsupervised way of binning i.e. without taking the target variable into account - Equal Range Binning, Equal Frequency Binning. In Equal Range method, each bin is of equal size/width and computed as: w = max- min / no. of bins *bins cutoff=[min, min+w,min+2w\u2026..,max-w,max]* whereas in Equal Frequency binning method, bins are created in such a way that each bin has equal no. of rows, though the width of bins may vary from each other. w = 1 / no. of bins *bins cutoff=[min, wthpctile, 2wthpctile\u2026.,max ]* Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of numerical columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all numerical columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) method_type \"equal_frequency\", \"equal_range\". In \"equal_range\" method, each bin is of equal size/width and in \"equal_frequency\", each bin has equal no. of rows, though the width of bins may vary. (Default value = \"equal_range\") bin_size Number of bins. (Default value = 10) bin_dtype \"numerical\", \"categorical\". With \"numerical\" option, original value is replaced with an Integer (1,2,\u2026) and with \"categorical\" option, original replaced with a string describing min and max value allowed in the bin (\"minval-maxval\"). (Default value = \"numerical\") pre_existing_model Boolean argument \u2013 True or False. True if binning model exists already, False Otherwise. (Default value = False) model_path If pre_existing_model is True, this argument is path for referring the pre-saved model. If pre_existing_model is False, this argument can be used for saving the model. Default \"NA\" means there is neither pre-existing model nor there is a need to save one. output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with transformed column. \u201cappend\u201d option append transformed column to the input dataset with a postfix \"_binned\" e.g. column X is appended as X_binned. (Default value = \"replace\") print_impact True, False (Default value = False) This argument is to print the number of categories generated for each attribute (may or may be not same as bin_size) Returns ------- DataFrame Binned Dataframe \"\"\" num_cols = attributeType_segregation ( idf )[ 0 ] if list_of_cols == \"all\" : list_of_cols = num_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in drop_cols ])) if any ( x not in num_cols for x in list_of_cols ): raise TypeError ( \"Invalid input for Column(s)\" ) if len ( list_of_cols ) == 0 : warnings . warn ( \"No Binning Performed - No numerical column(s) to transform\" ) return idf if method_type not in ( \"equal_frequency\" , \"equal_range\" ): raise TypeError ( \"Invalid input for method_type\" ) if bin_size < 2 : raise TypeError ( \"Invalid input for bin_size\" ) if output_mode not in ( \"replace\" , \"append\" ): raise TypeError ( \"Invalid input for output_mode\" ) if pre_existing_model : df_model = spark . read . parquet ( model_path + \"/attribute_binning\" ) bin_cutoffs = [] for i in list_of_cols : mapped_value = ( df_model . where ( F . col ( \"attribute\" ) == i ) . select ( \"parameters\" ) . rdd . flatMap ( lambda x : x ) . collect ()[ 0 ] ) bin_cutoffs . append ( mapped_value ) else : if method_type == \"equal_frequency\" : pctile_width = 1 / bin_size pctile_cutoff = [] for j in range ( 1 , bin_size ): pctile_cutoff . append ( j * pctile_width ) bin_cutoffs = idf . approxQuantile ( list_of_cols , pctile_cutoff , 0.01 ) else : bin_cutoffs = [] for i in list_of_cols : max_val = ( idf . select ( F . col ( i )) . groupBy () . max () . rdd . flatMap ( lambda x : x ) . collect () + [ None ] )[ 0 ] min_val = ( idf . select ( F . col ( i )) . groupBy () . min () . rdd . flatMap ( lambda x : x ) . collect () + [ None ] )[ 0 ] bin_cutoff = [] if max_val is not None : bin_width = ( max_val - min_val ) / bin_size for j in range ( 1 , bin_size ): bin_cutoff . append ( min_val + j * bin_width ) bin_cutoffs . append ( bin_cutoff ) if model_path != \"NA\" : df_model = spark . createDataFrame ( zip ( list_of_cols , bin_cutoffs ), schema = [ \"attribute\" , \"parameters\" ] ) df_model . write . parquet ( model_path + \"/attribute_binning\" , mode = \"overwrite\" ) def bucket_label ( value , index ): if value is None : return None for i in range ( 0 , len ( bin_cutoffs [ index ])): if value <= bin_cutoffs [ index ][ i ]: if bin_dtype == \"numerical\" : return i + 1 else : if i == 0 : return \"<= \" + str ( round ( bin_cutoffs [ index ][ i ], 4 )) else : return ( str ( round ( bin_cutoffs [ index ][ i - 1 ], 4 )) + \"-\" + str ( round ( bin_cutoffs [ index ][ i ], 4 )) ) else : next if bin_dtype == \"numerical\" : return len ( bin_cutoffs [ 0 ]) + 1 else : return \"> \" + str ( round ( bin_cutoffs [ index ][ len ( bin_cutoffs [ 0 ]) - 1 ], 4 )) if bin_dtype == \"numerical\" : f_bucket_label = F . udf ( bucket_label , T . IntegerType ()) else : f_bucket_label = F . udf ( bucket_label , T . StringType ()) odf = idf for idx , i in enumerate ( list_of_cols ): odf = odf . withColumn ( i + \"_binned\" , f_bucket_label ( F . col ( i ), F . lit ( idx ))) if idx % 5 == 0 : odf . persist ( pyspark . StorageLevel . MEMORY_AND_DISK ) . count () if output_mode == \"replace\" : for col in list_of_cols : odf = odf . drop ( col ) . withColumnRenamed ( col + \"_binned\" , col ) if print_impact : if output_mode == \"replace\" : output_cols = list_of_cols else : output_cols = [( i + \"_binned\" ) for i in list_of_cols ] uniqueCount_computation ( spark , odf , output_cols ) . show ( len ( output_cols ), False ) return odf def auto_imputation ( spark, idf, list_of_cols='missing', drop_cols=[], id_col='', null_pct=0.1, stats_missing={}, output_mode='replace', run_type='local', print_impact=True) auto_imputation tests for 5 imputation methods using the other imputation functions provided in this module and returns the one with the best performance. The 5 methods are: (1) imputation_MMM with method_type=\"mean\" (2) imputation_MMM with method_type=\"median\" (3) imputation_sklearn with method_type=\"KNN\" (4) imputation_sklearn with method_type=\"regression\" (5) imputation_matrixFactorization Samples without missing values in attributes to impute are used for testing by removing some % of values and impute them again using the above 5 methods. RMSE/attribute_mean is used as the evaluation metric for each attribute to reduce the effect of unit difference among attributes. The final error of a method is calculated by the sum of ( RMSE/attribute_mean) for all numerical attributes to impute and the method with the least error will be selected. The above testing is only applicable for numerical attributes. If categorical attributes are included, they will be automatically imputed using imputation_MMM. In addition, if there is only one numerical attribute to impute, only method (1) and (2) will be tested because the rest of the methods require more than one column. Parameters spark Spark Session idf Input Dataframe list_of_cols List of columns to impute e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all (non-array) columns for analysis. This is super useful instead of specifying all column names manually. \"missing\" (default) can be passed to include only those columns with missing values. One of the usecases where \"all\" may be preferable over \"missing\" is when the user wants to save the imputation model for the future use e.g. a column may not have missing value in the training dataset but missing values may possibly appear in the prediction dataset. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) id_col ID column (Default value = \"\") null_pct proportion of the valid input data to be replaced by None to form the test data (Default value = 0.1) stats_missing Takes arguments for read_dataset (data_ingest module) function in a dictionary format to read pre-saved statistics on missing count/pct i.e. if measures_of_counts or missingCount_computation (data_analyzer.stats_generator module) has been computed & saved before. (Default value = {}) output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with transformed column. \u201cappend\u201d option append transformed column to the input dataset with a postfix \"_imputed\" e.g. column X is appended as X_imputed. (Default value = \"replace\") run_type \"local\", \"emr\", \"databricks\" (Default value = \"local\") print_impact True, False (Default value = False) This argument is to print out before and after missing counts of imputed columns. It also print the name of best performing imputation method along with RMSE details. Returns DataFrame Imputed Dataframe Expand source code def auto_imputation ( spark , idf , list_of_cols = \"missing\" , drop_cols = [], id_col = \"\" , null_pct = 0.1 , stats_missing = {}, output_mode = \"replace\" , run_type = \"local\" , print_impact = True , ): \"\"\" auto_imputation tests for 5 imputation methods using the other imputation functions provided in this module and returns the one with the best performance. The 5 methods are: (1) imputation_MMM with method_type=\"mean\" (2) imputation_MMM with method_type=\"median\" (3) imputation_sklearn with method_type=\"KNN\" (4) imputation_sklearn with method_type=\"regression\" (5) imputation_matrixFactorization Samples without missing values in attributes to impute are used for testing by removing some % of values and impute them again using the above 5 methods. RMSE/attribute_mean is used as the evaluation metric for each attribute to reduce the effect of unit difference among attributes. The final error of a method is calculated by the sum of ( RMSE/attribute_mean) for all numerical attributes to impute and the method with the least error will be selected. The above testing is only applicable for numerical attributes. If categorical attributes are included, they will be automatically imputed using imputation_MMM. In addition, if there is only one numerical attribute to impute, only method (1) and (2) will be tested because the rest of the methods require more than one column. Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of columns to impute e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all (non-array) columns for analysis. This is super useful instead of specifying all column names manually. \"missing\" (default) can be passed to include only those columns with missing values. One of the usecases where \"all\" may be preferable over \"missing\" is when the user wants to save the imputation model for the future use e.g. a column may not have missing value in the training dataset but missing values may possibly appear in the prediction dataset. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) id_col ID column (Default value = \"\") null_pct proportion of the valid input data to be replaced by None to form the test data (Default value = 0.1) stats_missing Takes arguments for read_dataset (data_ingest module) function in a dictionary format to read pre-saved statistics on missing count/pct i.e. if measures_of_counts or missingCount_computation (data_analyzer.stats_generator module) has been computed & saved before. (Default value = {}) output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with transformed column. \u201cappend\u201d option append transformed column to the input dataset with a postfix \"_imputed\" e.g. column X is appended as X_imputed. (Default value = \"replace\") run_type \"local\", \"emr\", \"databricks\" (Default value = \"local\") print_impact True, False (Default value = False) This argument is to print out before and after missing counts of imputed columns. It also print the name of best performing imputation method along with RMSE details. Returns ------- DataFrame Imputed Dataframe \"\"\" if run_type in list ( platform_root_path . keys ()): root_path = platform_root_path [ run_type ] else : root_path = \"\" if stats_missing == {}: missing_df = missingCount_computation ( spark , idf ) missing_df . write . parquet ( root_path + \"intermediate_data/imputation_comparison/missingCount_computation\" , mode = \"overwrite\" , ) stats_missing = { \"file_path\" : root_path + \"intermediate_data/imputation_comparison/missingCount_computation\" , \"file_type\" : \"parquet\" , } else : missing_df = read_dataset ( spark , ** stats_missing ) . select ( \"attribute\" , \"missing_count\" , \"missing_pct\" ) empty_cols = ( missing_df . where ( F . col ( \"missing_pct\" ) == 1.0 ) . select ( \"attribute\" ) . rdd . flatMap ( lambda x : x ) . collect () ) if len ( empty_cols ) > 0 : warnings . warn ( \"Following columns have all null values: \" + \",\" . join ( empty_cols )) missing_cols = ( missing_df . where ( F . col ( \"missing_count\" ) > 0 ) . select ( \"attribute\" ) . rdd . flatMap ( lambda x : x ) . collect () ) if list_of_cols == \"all\" : list_of_cols = idf . columns if list_of_cols == \"missing\" : list_of_cols = missing_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if ( e not in drop_cols ) & ( e != id_col )]) ) if any ( x not in idf . columns for x in list_of_cols ): raise TypeError ( \"Invalid input for Column(s)\" ) del_cols = [ e for e in list_of_cols if e in empty_cols ] odf_del = idf . drop ( * del_cols ) list_of_cols = [ e for e in list_of_cols if e not in empty_cols ] num_cols , cat_cols , other_cols = attributeType_segregation ( odf_del . select ( list_of_cols ) ) missing_catcols = [ e for e in cat_cols if e in missing_cols ] missing_numcols = [ e for e in num_cols if e in missing_cols ] if missing_catcols : odf_imputed_cat = imputation_MMM ( spark , odf_del , list_of_cols = missing_catcols , stats_missing = stats_missing ) else : odf_imputed_cat = odf_del if len ( missing_numcols ) == 0 : warnings . warn ( \"No Imputation Performed for numerical columns - No Column(s) to Impute\" ) return odf_imputed_cat idf_test = ( odf_imputed_cat . dropna ( subset = missing_numcols ) . withColumn ( \"index\" , F . monotonically_increasing_id ()) . withColumn ( \"index\" , F . row_number () . over ( Window . orderBy ( \"index\" ))) ) null_count = int ( null_pct * idf_test . count ()) idf_null = idf_test for i in missing_numcols : null_index = random . sample ( range ( idf_test . count ()), null_count ) idf_null = idf_null . withColumn ( i , F . when ( F . col ( \"index\" ) . isin ( null_index ), None ) . otherwise ( F . col ( i )) ) idf_null . write . parquet ( root_path + \"intermediate_data/imputation_comparison/test_dataset\" , mode = \"overwrite\" , ) idf_null = spark . read . parquet ( root_path + \"intermediate_data/imputation_comparison/test_dataset\" ) method1 = imputation_MMM ( spark , idf_null , list_of_cols = missing_numcols , method_type = \"mean\" , stats_missing = stats_missing , output_mode = output_mode , ) method2 = imputation_MMM ( spark , idf_null , list_of_cols = missing_numcols , method_type = \"median\" , stats_missing = stats_missing , output_mode = output_mode , ) valid_methods = [ method1 , method2 ] if len ( num_cols ) > 1 : method3 = imputation_sklearn ( spark , idf_null , list_of_cols = num_cols , method_type = \"KNN\" , stats_missing = stats_missing , output_mode = output_mode , ) method4 = imputation_sklearn ( spark , idf_null , list_of_cols = num_cols , method_type = \"regression\" , stats_missing = stats_missing , output_mode = output_mode , ) method5 = imputation_matrixFactorization ( spark , idf_null , list_of_cols = num_cols , id_col = id_col , stats_missing = stats_missing , output_mode = output_mode , ) valid_methods = [ method1 , method2 , method3 , method4 , method5 ] nrmse_all = [] method_all = [ \"MMM-mean\" , \"MMM-median\" , \"KNN\" , \"regression\" , \"matrix_factorization\" ] for index , method in enumerate ( valid_methods ): nrmse = 0 for i in missing_numcols : pred_col = ( i + \"_imputed\" ) if output_mode == \"append\" else i idf_joined = ( idf_test . select ( \"index\" , F . col ( i ) . alias ( \"val\" )) . join ( method . select ( \"index\" , F . col ( pred_col ) . alias ( \"pred\" )), \"index\" , \"left_outer\" , ) . dropna () ) idf_joined = recast_column ( idf = idf_joined , list_of_cols = [ \"val\" , \"pred\" ], list_of_dtypes = [ \"double\" , \"double\" ], ) pred_mean = float ( method . select ( F . mean ( pred_col )) . rdd . flatMap ( lambda x : x ) . collect ()[ 0 ] ) i_nrmse = ( RegressionEvaluator ( metricName = \"rmse\" , labelCol = \"val\" , predictionCol = \"pred\" ) . evaluate ( idf_joined ) ) / abs ( pred_mean ) nrmse += i_nrmse nrmse_all . append ( nrmse ) min_index = nrmse_all . index ( np . min ( nrmse_all )) best_method = method_all [ min_index ] odf = valid_methods [ min_index ] if print_impact : print ( list ( zip ( method_all , nrmse_all ))) print ( \"Best Imputation Method: \" , best_method ) return odf def autoencoder_latentFeatures ( spark, idf, list_of_cols='all', drop_cols=[], reduction_params=0.5, sample_size=500000, epochs=100, batch_size=256, pre_existing_model=False, model_path='NA', standardization=True, standardization_configs={'pre_existing_model': False, 'model_path': 'NA'}, imputation=False, imputation_configs={'imputation_function': 'imputation_MMM'}, stats_missing={}, output_mode='replace', run_type='local', print_impact=False) Many machine learning models suffer from \"the curse of dimensionality\" when the number of features is too large. autoencoder_latentFeatures is able to reduce the dimensionality by compressing input attributes to a smaller number of latent features. To be more specific, it trains a neural network model using TensorFlow library. The neural network contains an encoder and a decoder, where the encoder learns to represent the input using smaller number of latent features controlled by the input reduction_params and the decoder learns to reproduce the input using the latent features. In the end, only the encoder will be kept and the latent features generated by the encoder will be added to the output dataframe. However, the neural network model is not trained in a scalable manner, which might not be able to handle large input dataframe. Thus, an input sample_size (the default value is 500,000) can be set to control the number of samples to be used to train the model. If the total number of samples exceeds sample_size , the rest of the samples will be predicted using the fitted encoder. Standardization is highly recommended if the input attributes are not of the same scale. Otherwise, the model might not converge smoothly. Inputs standardization and standardization_configs can be set accordingly to perform standardization within the function. In addition, if a sample contains missing values in the model input, the output values for its latent features will all be None. Thus data imputation is also recommended if missing values exist, which can be done within the function by setting inputs imputation and imputation_configs . Parameters spark Spark Session idf Input Dataframe list_of_cols List of numerical columns to encode e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all numerical columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) reduction_params Determines the number of encoded features in the result. If reduction_params < 1, int(reduction_params * ) columns will be generated. Else, reduction_params columns will be generated. (Default value = 0.5) sample_size Maximum rows for training the autoencoder model using tensorflow. (Default value = 500000) epochs Number of epochs to train the tensorflow model. (Default value = 100) batch_size Number of samples per gradient update when fitting the tensorflow model. (Default value = 256) pre_existing_model Boolean argument \u2013 True or False. True if model exists already, False Otherwise (Default value = False) model_path If pre_existing_model is True, this argument is path for referring the pre-saved model. If pre_existing_model is False, this argument can be used for saving the model. Default \"NA\" means there is neither pre-existing model nor there is a need to save one. standardization Boolean argument \u2013 True or False. True, if the standardization required. (Default value = True) standardization_configs z_standardization function arguments in dictionary format. (Default value = {\"pre_existing_model\": False) imputation Boolean argument \u2013 True or False. True, if the imputation required. (Default value = False) imputation_configs Takes input in dictionary format. Imputation function name is provided with key \"imputation_name\". optional arguments pertaining to that imputation function can be provided with argument name as key. (Default value = {\"imputation_function\": \"imputation_MMM\"}) stats_missing Takes arguments for read_dataset (data_ingest module) function in a dictionary format to read pre-saved statistics on missing count/pct i.e. if measures_of_counts or missingCount_computation (data_analyzer.stats_generator module) has been computed & saved before. (Default value = {}) output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with transformed columns: latent_ . \u201cappend\u201d option append transformed columns with format latent_ to the input dataset, e.g. latent_0, latent_1 will be appended if reduction_params=2. (Default value = \"replace\") run_type \"local\", \"emr\", \"databricks\" (Default value = \"local\") print_impact True, False This argument is to print descriptive statistics of the latest features (Default value = False) Returns DataFrame Dataframe with Latent Features Expand source code def autoencoder_latentFeatures ( spark , idf , list_of_cols = \"all\" , drop_cols = [], reduction_params = 0.5 , sample_size = 500000 , epochs = 100 , batch_size = 256 , pre_existing_model = False , model_path = \"NA\" , standardization = True , standardization_configs = { \"pre_existing_model\" : False , \"model_path\" : \"NA\" }, imputation = False , imputation_configs = { \"imputation_function\" : \"imputation_MMM\" }, stats_missing = {}, output_mode = \"replace\" , run_type = \"local\" , print_impact = False , ): \"\"\" Many machine learning models suffer from \"the curse of dimensionality\" when the number of features is too large. autoencoder_latentFeatures is able to reduce the dimensionality by compressing input attributes to a smaller number of latent features. To be more specific, it trains a neural network model using TensorFlow library. The neural network contains an encoder and a decoder, where the encoder learns to represent the input using smaller number of latent features controlled by the input *reduction_params* and the decoder learns to reproduce the input using the latent features. In the end, only the encoder will be kept and the latent features generated by the encoder will be added to the output dataframe. However, the neural network model is not trained in a scalable manner, which might not be able to handle large input dataframe. Thus, an input *sample_size* (the default value is 500,000) can be set to control the number of samples to be used to train the model. If the total number of samples exceeds *sample_size*, the rest of the samples will be predicted using the fitted encoder. Standardization is highly recommended if the input attributes are not of the same scale. Otherwise, the model might not converge smoothly. Inputs *standardization* and *standardization_configs* can be set accordingly to perform standardization within the function. In addition, if a sample contains missing values in the model input, the output values for its latent features will all be None. Thus data imputation is also recommended if missing values exist, which can be done within the function by setting inputs *imputation* and *imputation_configs*. Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of numerical columns to encode e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all numerical columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) reduction_params Determines the number of encoded features in the result. If reduction_params < 1, int(reduction_params * <number of columns>) columns will be generated. Else, reduction_params columns will be generated. (Default value = 0.5) sample_size Maximum rows for training the autoencoder model using tensorflow. (Default value = 500000) epochs Number of epochs to train the tensorflow model. (Default value = 100) batch_size Number of samples per gradient update when fitting the tensorflow model. (Default value = 256) pre_existing_model Boolean argument \u2013 True or False. True if model exists already, False Otherwise (Default value = False) model_path If pre_existing_model is True, this argument is path for referring the pre-saved model. If pre_existing_model is False, this argument can be used for saving the model. Default \"NA\" means there is neither pre-existing model nor there is a need to save one. standardization Boolean argument \u2013 True or False. True, if the standardization required. (Default value = True) standardization_configs z_standardization function arguments in dictionary format. (Default value = {\"pre_existing_model\": False) imputation Boolean argument \u2013 True or False. True, if the imputation required. (Default value = False) imputation_configs Takes input in dictionary format. Imputation function name is provided with key \"imputation_name\". optional arguments pertaining to that imputation function can be provided with argument name as key. (Default value = {\"imputation_function\": \"imputation_MMM\"}) stats_missing Takes arguments for read_dataset (data_ingest module) function in a dictionary format to read pre-saved statistics on missing count/pct i.e. if measures_of_counts or missingCount_computation (data_analyzer.stats_generator module) has been computed & saved before. (Default value = {}) output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with transformed columns: latent_<col_index>. \u201cappend\u201d option append transformed columns with format latent_<col_index> to the input dataset, e.g. latent_0, latent_1 will be appended if reduction_params=2. (Default value = \"replace\") run_type \"local\", \"emr\", \"databricks\" (Default value = \"local\") print_impact True, False This argument is to print descriptive statistics of the latest features (Default value = False) Returns ------- DataFrame Dataframe with Latent Features \"\"\" num_cols = attributeType_segregation ( idf )[ 0 ] if list_of_cols == \"all\" : list_of_cols = num_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in drop_cols ])) if any ( x not in num_cols for x in list_of_cols ): raise TypeError ( \"Invalid input for Column(s)\" ) if len ( list_of_cols ) == 0 : warnings . warn ( \"No Latent Features Generated - No Column(s) to Transform\" ) return idf if run_type in list ( platform_root_path . keys ()): root_path = platform_root_path [ run_type ] else : root_path = \"\" if stats_missing == {}: missing_df = missingCount_computation ( spark , idf , list_of_cols ) missing_df . write . parquet ( root_path + \"intermediate_data/autoencoder_latentFeatures/missingCount_computation\" , mode = \"overwrite\" , ) stats_missing = { \"file_path\" : root_path + \"intermediate_data/autoencoder_latentFeatures/missingCount_computation\" , \"file_type\" : \"parquet\" , } else : missing_df = ( read_dataset ( spark , ** stats_missing ) . select ( \"attribute\" , \"missing_count\" , \"missing_pct\" ) . where ( F . col ( \"attribute\" ) . isin ( list_of_cols )) ) empty_cols = ( missing_df . where ( F . col ( \"missing_pct\" ) == 1.0 ) . select ( \"attribute\" ) . rdd . flatMap ( lambda x : x ) . collect () ) if len ( empty_cols ) > 0 : warnings . warn ( \"The following column(s) are excluded from dimensionality reduction as all values are null: \" + \",\" . join ( empty_cols ) ) list_of_cols = [ e for e in list_of_cols if e not in empty_cols ] if standardization : idf_standardized = z_standardization ( spark , idf , list_of_cols = list_of_cols , output_mode = \"append\" , ** standardization_configs , ) list_of_cols_scaled = [ i + \"_scaled\" for i in list_of_cols if ( i + \"_scaled\" ) in idf_standardized . columns ] else : idf_standardized = idf for i in list_of_cols : idf_standardized = idf_standardized . withColumn ( i + \"_scaled\" , F . col ( i )) list_of_cols_scaled = [ i + \"_scaled\" for i in list_of_cols ] if imputation : all_functions = globals () . copy () all_functions . update ( locals ()) f = all_functions . get ( imputation_configs [ \"imputation_function\" ]) args = copy . deepcopy ( imputation_configs ) args . pop ( \"imputation_function\" , None ) missing_df_scaled = ( read_dataset ( spark , ** stats_missing ) . select ( \"attribute\" , \"missing_count\" , \"missing_pct\" ) . withColumn ( \"attribute\" , F . concat ( F . col ( \"attribute\" ), F . lit ( \"_scaled\" ))) ) missing_df_scaled . write . parquet ( root_path + \"intermediate_data/autoencoder_latentFeatures/missingCount_computation_scaled\" , mode = \"overwrite\" , ) stats_missing_scaled = { \"file_path\" : root_path + \"intermediate_data/autoencoder_latentFeatures/missingCount_computation_scaled\" , \"file_type\" : \"parquet\" , } idf_imputed = f ( spark , idf_standardized , list_of_cols_scaled , stats_missing = stats_missing_scaled , ** args , ) else : idf_imputed = idf_standardized . dropna ( subset = list_of_cols_scaled ) n_inputs = len ( list_of_cols_scaled ) if reduction_params < 1 : n_bottleneck = int ( reduction_params * n_inputs ) else : n_bottleneck = int ( reduction_params ) if pre_existing_model : if run_type == \"emr\" : bash_cmd = ( \"aws s3 cp \" + model_path + \"/autoencoders_latentFeatures/encoder.h5 .\" ) output = subprocess . check_output ([ \"bash\" , \"-c\" , bash_cmd ]) bash_cmd = ( \"aws s3 cp \" + model_path + \"/autoencoders_latentFeatures/model.h5 .\" ) output = subprocess . check_output ([ \"bash\" , \"-c\" , bash_cmd ]) encoder = load_model ( \"encoder.h5\" ) model = load_model ( \"model.h5\" ) else : encoder = load_model ( model_path + \"/autoencoders_latentFeatures/encoder.h5\" ) model = load_model ( model_path + \"/autoencoders_latentFeatures/model.h5\" ) else : idf_valid = idf_imputed . select ( list_of_cols_scaled ) idf_model = idf_valid . sample ( False , min ( 1.0 , float ( sample_size ) / idf_valid . count ()), 0 ) idf_train = idf_model . sample ( False , 0.8 , 0 ) idf_test = idf_model . subtract ( idf_train ) X_train = idf_train . toPandas () X_test = idf_test . toPandas () visible = Input ( shape = ( n_inputs ,)) e = Dense ( n_inputs * 2 )( visible ) e = BatchNormalization ()( e ) e = LeakyReLU ()( e ) e = Dense ( n_inputs )( e ) e = BatchNormalization ()( e ) e = LeakyReLU ()( e ) bottleneck = Dense ( n_bottleneck )( e ) d = Dense ( n_inputs )( bottleneck ) d = BatchNormalization ()( d ) d = LeakyReLU ()( d ) d = Dense ( n_inputs * 2 )( d ) d = BatchNormalization ()( d ) d = LeakyReLU ()( d ) output = Dense ( n_inputs , activation = \"linear\" )( d ) model = Model ( inputs = visible , outputs = output ) encoder = Model ( inputs = visible , outputs = bottleneck ) model . compile ( optimizer = \"adam\" , loss = \"mse\" ) history = model . fit ( X_train , X_train , epochs = int ( epochs ), batch_size = int ( batch_size ), verbose = 2 , validation_data = ( X_test , X_test ), ) if ( not pre_existing_model ) & ( model_path != \"NA\" ): if run_type == \"emr\" : encoder . save ( \"encoder.h5\" ) model . save ( \"model.h5\" ) bash_cmd = ( \"aws s3 cp encoder.h5 \" + model_path + \"/autoencoders_latentFeatures/encoder.h5\" ) output = subprocess . check_output ([ \"bash\" , \"-c\" , bash_cmd ]) bash_cmd = ( \"aws s3 cp model.h5 \" + model_path + \"/autoencoders_latentFeatures/model.h5\" ) output = subprocess . check_output ([ \"bash\" , \"-c\" , bash_cmd ]) else : if not os . path . exists ( model_path + \"/autoencoders_latentFeatures/\" ): os . makedirs ( model_path + \"/autoencoders_latentFeatures/\" ) encoder . save ( model_path + \"/autoencoders_latentFeatures/encoder.h5\" ) model . save ( model_path + \"/autoencoders_latentFeatures/model.h5\" ) class ModelWrapperPickable : def __init__ ( self , model ): self . model = model def __getstate__ ( self ): model_str = \"\" with tempfile . NamedTemporaryFile ( suffix = \".hdf5\" , delete = True ) as fd : tensorflow . keras . models . save_model ( self . model , fd . name , overwrite = True ) model_str = fd . read () d = { \"model_str\" : model_str } return d def __setstate__ ( self , state ): with tempfile . NamedTemporaryFile ( suffix = \".hdf5\" , delete = True ) as fd : fd . write ( state [ \"model_str\" ]) fd . flush () self . model = tensorflow . keras . models . load_model ( fd . name ) model_wrapper = ModelWrapperPickable ( encoder ) def compute_output_pandas_udf ( model_wrapper ): @F . pandas_udf ( returnType = T . ArrayType ( T . DoubleType ())) def predict_pandas_udf ( * cols ): X = pd . concat ( cols , axis = 1 ) return pd . Series ( row . tolist () for row in model_wrapper . model . predict ( X )) return predict_pandas_udf odf = idf_imputed . withColumn ( \"predicted_output\" , compute_output_pandas_udf ( model_wrapper )( * list_of_cols_scaled ), ) odf_schema = odf . schema for i in range ( 0 , n_bottleneck ): odf_schema = odf_schema . add ( T . StructField ( \"latent_\" + str ( i ), T . FloatType ())) odf = ( odf . rdd . map ( lambda x : ( * x , * x [ \"predicted_output\" ])) . toDF ( schema = odf_schema ) . drop ( \"predicted_output\" ) ) if output_mode == \"replace\" : odf = odf . drop ( * list_of_cols ) if print_impact : output_cols = [ \"latent_\" + str ( j ) for j in range ( 0 , n_bottleneck )] odf . select ( output_cols ) . describe () . show ( 5 , False ) return odf def boxcox_transformation ( idf, list_of_cols='all', drop_cols=[], boxcox_lambda=None, output_mode='replace', print_impact=False) Some machine learning algorithms require the input data to follow normal distributions. Thus, when the input data is too skewed, boxcox_transformation can be used to transform it into a more normal-like distribution. The transformed value of a sample x depends on a coefficient lambda: (1) if lambda = 0, x is transformed into log(x); (2) if lambda !=0, x is transformed into (x^lambda-1)/lambda. The value of lambda can be either specified by the user or automatically selected within the function. If lambda needs to be selected within the function, a range of values (0, 1, -1, 0.5, -0.5, 2, -2, 0.25, -0.25, 3, -3, 4, -4, 5, -5) will be tested and the lambda, which optimizes the KolmogorovSmirnovTest by PySpark with the theoretical distribution being normal, will be used to perform the transformation. Different lambda values can be assigned to different attributes but one attribute can only be assigned one lambda value. Parameters idf Input Dataframe list_of_cols List of numerical columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all numerical columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) boxcox_lambda Lambda value for box_cox transormation. If boxcox_lambda is not None, it will be directly used for the transformation. It can be a (1) list: each element represents a lambda value for an attribute and the length of the list must be the same as the number of columns to transform. (2) int/float: all attributes will be assigned the same lambda value. Else, search for the best lambda among [1,-1,0.5,-0.5,2,-2,0.25,-0.25,3,-3,4,-4,5,-5] for each column and apply the transformation (Default value = None) output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with transformed columns. \u201cappend\u201d option append transformed columns with a postfix \" bxcx \" to the input dataset. (Default value = \"replace\") print_impact True, False This argument is to print before and after descriptive statistics of the transformed features (Default value = False) Returns DataFrame Transformed Dataframe Expand source code def boxcox_transformation ( idf , list_of_cols = \"all\" , drop_cols = [], boxcox_lambda = None , output_mode = \"replace\" , print_impact = False , ): \"\"\" Some machine learning algorithms require the input data to follow normal distributions. Thus, when the input data is too skewed, boxcox_transformation can be used to transform it into a more normal-like distribution. The transformed value of a sample x depends on a coefficient lambda: (1) if lambda = 0, x is transformed into log(x); (2) if lambda !=0, x is transformed into (x^lambda-1)/lambda. The value of lambda can be either specified by the user or automatically selected within the function. If lambda needs to be selected within the function, a range of values (0, 1, -1, 0.5, -0.5, 2, -2, 0.25, -0.25, 3, -3, 4, -4, 5, -5) will be tested and the lambda, which optimizes the KolmogorovSmirnovTest by PySpark with the theoretical distribution being normal, will be used to perform the transformation. Different lambda values can be assigned to different attributes but one attribute can only be assigned one lambda value. Parameters ---------- idf Input Dataframe list_of_cols List of numerical columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all numerical columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) boxcox_lambda Lambda value for box_cox transormation. If boxcox_lambda is not None, it will be directly used for the transformation. It can be a (1) list: each element represents a lambda value for an attribute and the length of the list must be the same as the number of columns to transform. (2) int/float: all attributes will be assigned the same lambda value. Else, search for the best lambda among [1,-1,0.5,-0.5,2,-2,0.25,-0.25,3,-3,4,-4,5,-5] for each column and apply the transformation (Default value = None) output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with transformed columns. \u201cappend\u201d option append transformed columns with a postfix \"_bxcx_<lambda>\" to the input dataset. (Default value = \"replace\") print_impact True, False This argument is to print before and after descriptive statistics of the transformed features (Default value = False) Returns ------- DataFrame Transformed Dataframe \"\"\" num_cols = attributeType_segregation ( idf )[ 0 ] if list_of_cols == \"all\" : list_of_cols = num_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in drop_cols ])) if ( len ( list_of_cols ) == 0 ) | ( any ( x not in num_cols for x in list_of_cols )): raise TypeError ( \"Invalid input for Column(s)\" ) num_cols = attributeType_segregation ( idf . select ( list_of_cols ))[ 0 ] list_of_cols = num_cols odf = idf col_mins = idf . select ([ F . min ( i ) for i in list_of_cols ]) if any ([ i <= 0 for i in col_mins . rdd . flatMap ( lambda x : x ) . collect ()]): col_mins . show ( 1 , False ) raise ValueError ( \"Data must be positive\" ) if boxcox_lambda is not None : if isinstance ( boxcox_lambda , ( list , tuple )): if len ( boxcox_lambda ) != len ( list_of_cols ): raise TypeError ( \"Invalid input for boxcox_lambda\" ) elif not all ([ isinstance ( l , ( float , int )) for l in boxcox_lambda ]): raise TypeError ( \"Invalid input for boxcox_lambda\" ) else : boxcox_lambda_list = list ( boxcox_lambda ) elif isinstance ( boxcox_lambda , ( float , int )): boxcox_lambda_list = [ boxcox_lambda ] * len ( list_of_cols ) else : raise TypeError ( \"Invalid input for boxcox_lambda\" ) else : boxcox_lambda_list = [] for i in list_of_cols : lambdaVal = [ 1 , - 1 , 0.5 , - 0.5 , 2 , - 2 , 0.25 , - 0.25 , 3 , - 3 , 4 , - 4 , 5 , - 5 ] best_pVal = 0 for j in lambdaVal : pVal = Statistics . kolmogorovSmirnovTest ( odf . select ( F . pow ( F . col ( i ), j )) . rdd . flatMap ( lambda x : x ), \"norm\" ) . pValue if pVal > best_pVal : best_pVal = pVal best_lambdaVal = j pVal = Statistics . kolmogorovSmirnovTest ( odf . select ( F . log ( F . col ( i ))) . rdd . flatMap ( lambda x : x ), \"norm\" ) . pValue if pVal > best_pVal : best_pVal = pVal best_lambdaVal = 0 boxcox_lambda_list . append ( best_lambdaVal ) output_cols = [] for i , curr_lambdaVal in zip ( list_of_cols , boxcox_lambda_list ): if curr_lambdaVal != 1 : modify_col = ( ( i + \"_bxcx_\" + str ( curr_lambdaVal )) if output_mode == \"append\" else i ) output_cols . append ( modify_col ) if curr_lambdaVal == 0 : odf = odf . withColumn ( modify_col , F . log ( F . col ( i ))) else : odf = odf . withColumn ( modify_col , F . pow ( F . col ( i ), curr_lambdaVal )) if len ( output_cols ) == 0 : warnings . warn ( \"lambdaVal for all columns are 1 so no transformation is performed and idf is returned\" ) return idf if print_impact : print ( \"Transformed Columns: \" , list_of_cols ) print ( \"Best BoxCox Parameter(s): \" , boxcox_lambda_list ) print ( \"Before:\" ) idf . select ( list_of_cols ) . describe () . unionByName ( idf . select ([ F . skewness ( i ) . alias ( i ) for i in list_of_cols ]) . withColumn ( \"summary\" , F . lit ( \"skewness\" ) ) ) . show ( 6 , False ) print ( \"After:\" ) if output_mode == \"replace\" : odf . select ( list_of_cols ) . describe () . unionByName ( odf . select ([ F . skewness ( i ) . alias ( i ) for i in list_of_cols ]) . withColumn ( \"summary\" , F . lit ( \"skewness\" ) ) ) . show ( 6 , False ) else : output_cols = [( \"`\" + i + \"`\" ) for i in output_cols ] odf . select ( output_cols ) . describe () . unionByName ( odf . select ( [ F . skewness ( i ) . alias ( i [ 1 : - 1 ]) for i in output_cols ] ) . withColumn ( \"summary\" , F . lit ( \"skewness\" )) ) . show ( 6 , False ) return odf def cat_to_num_supervised ( spark, idf, list_of_cols='all', drop_cols=[], label_col='label', event_label=1, pre_existing_model=False, model_path='NA', output_mode='replace', run_type='local', print_impact=False) This is a supervised method to convert a categorical attribute into a numerical attribute. It takes a label/target column to indicate whether the event is positive or negative. For each column, the positive event rate for each categorical value is used as the encoded numerical value. For example, there are 3 distinct values in a categorical attribute X: X1, X2 and X3. Within the input dataframe, there are - 15 positive events and 5 negative events with X==X1; - 10 positive events and 40 negative events with X==X2; - 20 positive events and 20 negative events with X==X3. Thus, value X1 is mapped to 15/(15+5) = 0.75, value X2 is mapped to 10/(10+40) = 0.2 and value X3 is mapped to 20/(20+20) = 0.5. This mapping will be applied to all values in attribute X. This encoding method can be helpful in avoiding creating too many dummy variables which may cause dimensionality issue and it also works with categorical attributes without an order or rank. Parameters spark Spark Session idf Input Dataframe list_of_cols List of categorical columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all categorical columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) label_col Label/Target column (Default value = \"label\") event_label Value of (positive) event (i.e label 1) (Default value = 1) pre_existing_model Boolean argument \u2013 True or False. True if model (original and mapped numerical value for each column) exists already, False Otherwise. (Default value = False) model_path If pre_existing_model is True, this argument is path for referring the pre-saved model. If pre_existing_model is False, this argument can be used for saving the model. Default \"NA\" is used to save the model in \"intermediate_data/\" folder for optimization purpose. output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with transformed column. \u201cappend\u201d option append transformed column to the input dataset with a postfix \"_encoded\" e.g. column X is appended as X_encoded. (Default value = \"replace\") run_type \"local\", \"emr\", \"databricks\" (Default value = \"local\") print_impact True, False (Default value = False) This argument is to print out the descriptive statistics of encoded columns. Returns DataFrame Encoded Dataframe Expand source code def cat_to_num_supervised ( spark , idf , list_of_cols = \"all\" , drop_cols = [], label_col = \"label\" , event_label = 1 , pre_existing_model = False , model_path = \"NA\" , output_mode = \"replace\" , run_type = \"local\" , print_impact = False , ): \"\"\" This is a supervised method to convert a categorical attribute into a numerical attribute. It takes a label/target column to indicate whether the event is positive or negative. For each column, the positive event rate for each categorical value is used as the encoded numerical value. For example, there are 3 distinct values in a categorical attribute X: X1, X2 and X3. Within the input dataframe, there are - 15 positive events and 5 negative events with X==X1; - 10 positive events and 40 negative events with X==X2; - 20 positive events and 20 negative events with X==X3. Thus, value X1 is mapped to 15/(15+5) = 0.75, value X2 is mapped to 10/(10+40) = 0.2 and value X3 is mapped to 20/(20+20) = 0.5. This mapping will be applied to all values in attribute X. This encoding method can be helpful in avoiding creating too many dummy variables which may cause dimensionality issue and it also works with categorical attributes without an order or rank. Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of categorical columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all categorical columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) label_col Label/Target column (Default value = \"label\") event_label Value of (positive) event (i.e label 1) (Default value = 1) pre_existing_model Boolean argument \u2013 True or False. True if model (original and mapped numerical value for each column) exists already, False Otherwise. (Default value = False) model_path If pre_existing_model is True, this argument is path for referring the pre-saved model. If pre_existing_model is False, this argument can be used for saving the model. Default \"NA\" is used to save the model in \"intermediate_data/\" folder for optimization purpose. output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with transformed column. \u201cappend\u201d option append transformed column to the input dataset with a postfix \"_encoded\" e.g. column X is appended as X_encoded. (Default value = \"replace\") run_type \"local\", \"emr\", \"databricks\" (Default value = \"local\") print_impact True, False (Default value = False) This argument is to print out the descriptive statistics of encoded columns. Returns ------- DataFrame Encoded Dataframe \"\"\" cat_cols = attributeType_segregation ( idf )[ 1 ] if list_of_cols == \"all\" : list_of_cols = cat_cols elif isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if ( e not in drop_cols ) & ( e != label_col )]) ) if any ( x not in cat_cols for x in list_of_cols ): raise TypeError ( \"Invalid input for Column(s)\" ) if len ( list_of_cols ) == 0 : warnings . warn ( \"No Categorical Encoding - No categorical column(s) to transform\" ) return idf if label_col not in idf . columns : raise TypeError ( \"Invalid input for Label Column\" ) idf_id = idf . withColumn ( \"tempID\" , F . monotonically_increasing_id ()) odf_partial = idf_id . select ([ \"tempID\" ] + list_of_cols ) for index , i in enumerate ( list_of_cols ): if pre_existing_model : df_tmp = spark . read . csv ( model_path + \"/cat_to_num_supervised/\" + i , header = True , inferSchema = True , ) else : df_tmp = ( idf . withColumn ( label_col , F . when ( F . col ( label_col ) == event_label , \"1\" ) . otherwise ( \"0\" ), ) . groupBy ( i ) . pivot ( label_col ) . count () . fillna ( 0 ) . withColumn ( i + \"_encoded\" , F . round ( F . col ( \"1\" ) / ( F . col ( \"1\" ) + F . col ( \"0\" )), 4 ) ) . drop ( * [ \"1\" , \"0\" ]) ) if run_type in list ( platform_root_path . keys ()): root_path = platform_root_path [ run_type ] else : root_path = \"\" if model_path == \"NA\" : model_path = root_path + \"intermediate_data\" df_tmp . coalesce ( 1 ) . write . csv ( model_path + \"/cat_to_num_supervised/\" + i , header = True , mode = \"overwrite\" , ) df_tmp = spark . read . csv ( model_path + \"/cat_to_num_supervised/\" + i , header = True , inferSchema = True , ) if df_tmp . count () > 1 : odf_partial = odf_partial . join ( df_tmp , i , \"left_outer\" ) else : odf_partial = odf_partial . crossJoin ( df_tmp ) odf = idf_id . join ( odf_partial . drop ( * list_of_cols ), \"tempID\" , \"left_outer\" ) . drop ( \"tempID\" ) if output_mode == \"replace\" : for i in list_of_cols : odf = odf . drop ( i ) . withColumnRenamed ( i + \"_encoded\" , i ) odf = odf . select ( idf . columns ) if print_impact : if output_mode == \"replace\" : output_cols = list_of_cols else : output_cols = [( i + \"_encoded\" ) for i in list_of_cols ] print ( \"Before: \" ) idf . select ( list_of_cols ) . describe () . where ( F . col ( \"summary\" ) . isin ( \"count\" , \"min\" , \"max\" ) ) . show ( 3 , False ) print ( \"After: \" ) odf . select ( output_cols ) . describe () . where ( F . col ( \"summary\" ) . isin ( \"count\" , \"min\" , \"max\" ) ) . show ( 3 , False ) return odf def cat_to_num_unsupervised ( spark, idf, list_of_cols='all', drop_cols=[], method_type=1, index_order='frequencyDesc', cardinality_threshold=100, pre_existing_model=False, model_path='NA', stats_unique={}, output_mode='replace', print_impact=False) This is unsupervised method of converting a categorical attribute into numerical attribute(s). This is among the most important transformations required for any modelling exercise, as most of the machine learning algorithms cannot process categorical values. It covers two popular encoding techniques \u2013 label encoding & one-hot encoding. In label encoding, each categorical value is assigned a unique integer based on alphabetical or frequency ordering (both ascending & descending options are available \u2013 can be selected by index_order argument). One of the pitfalls of using this technique is that the model may learn some spurious relationship, which doesn't exist or might not make any logical sense in the real world settings. In one-hot encoding, every unique value in the attribute will be added as a feature in a form of dummy/binary attribute. However, using this method on high cardinality attributes can further aggravate the dimensionality issue. Parameters spark Spark Session idf Input Dataframe list_of_cols List of categorical columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all categorical columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) method_type 1 for Label Encoding or 0 for One hot encoding. In label encoding, each categorical value is assigned a unique integer based on alphabetical or frequency ordering (both ascending & descending options are available that can be selected by index_order argument). In one-hot encoding, every unique value in the column will be added in a form of dummy/binary column. (Default value = 1) index_order \"frequencyDesc\", \"frequencyAsc\", \"alphabetDesc\", \"alphabetAsc\". Valid only for Label Encoding method_type. (Default value = \"frequencyDesc\") cardinality_threshold Defines threshold to skip columns with higher cardinality values from encoding (Warning is issued). (Default value = 100) pre_existing_model Boolean argument \u2013 True or False. True if encoding model exists already, False Otherwise. (Default value = False) model_path If pre_existing_model is True, this argument is path for referring the pre-saved model. If pre_existing_model is False, this argument can be used for saving the model. Default \"NA\" means there is neither pre existing model nor there is a need to save one. stats_unique Takes arguments for read_dataset (data_ingest module) function in a dictionary format to read pre-saved statistics on unique value count i.e. if measures_of_cardinality or uniqueCount_computation (data_analyzer.stats_generator module) has been computed & saved before. (Default value = {}) output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with transformed column. \u201cappend\u201d option append transformed column to the input dataset with a postfix \" index\" for label encoding e.g. column X is appended as X_index, or a postfix \" \" for one hot encoding, n varies from 0 to unique value count e.g. column X is appended as X_0, X_1, X_2 (n = 3 i.e. no. of unique values for X). (Default value = \"replace\") print_impact True, False (Default value = False) This argument is to print out the change in schema (one hot encoding) or descriptive statistics (label encoding) Returns DataFrame Encoded Dataframe Expand source code def cat_to_num_unsupervised ( spark , idf , list_of_cols = \"all\" , drop_cols = [], method_type = 1 , index_order = \"frequencyDesc\" , cardinality_threshold = 100 , pre_existing_model = False , model_path = \"NA\" , stats_unique = {}, output_mode = \"replace\" , print_impact = False , ): \"\"\" This is unsupervised method of converting a categorical attribute into numerical attribute(s). This is among the most important transformations required for any modelling exercise, as most of the machine learning algorithms cannot process categorical values. It covers two popular encoding techniques \u2013 label encoding & one-hot encoding. In label encoding, each categorical value is assigned a unique integer based on alphabetical or frequency ordering (both ascending & descending options are available \u2013 can be selected by index_order argument). One of the pitfalls of using this technique is that the model may learn some spurious relationship, which doesn't exist or might not make any logical sense in the real world settings. In one-hot encoding, every unique value in the attribute will be added as a feature in a form of dummy/binary attribute. However, using this method on high cardinality attributes can further aggravate the dimensionality issue. Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of categorical columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all categorical columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) method_type 1 for Label Encoding or 0 for One hot encoding. In label encoding, each categorical value is assigned a unique integer based on alphabetical or frequency ordering (both ascending & descending options are available that can be selected by index_order argument). In one-hot encoding, every unique value in the column will be added in a form of dummy/binary column. (Default value = 1) index_order \"frequencyDesc\", \"frequencyAsc\", \"alphabetDesc\", \"alphabetAsc\". Valid only for Label Encoding method_type. (Default value = \"frequencyDesc\") cardinality_threshold Defines threshold to skip columns with higher cardinality values from encoding (Warning is issued). (Default value = 100) pre_existing_model Boolean argument \u2013 True or False. True if encoding model exists already, False Otherwise. (Default value = False) model_path If pre_existing_model is True, this argument is path for referring the pre-saved model. If pre_existing_model is False, this argument can be used for saving the model. Default \"NA\" means there is neither pre existing model nor there is a need to save one. stats_unique Takes arguments for read_dataset (data_ingest module) function in a dictionary format to read pre-saved statistics on unique value count i.e. if measures_of_cardinality or uniqueCount_computation (data_analyzer.stats_generator module) has been computed & saved before. (Default value = {}) output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with transformed column. \u201cappend\u201d option append transformed column to the input dataset with a postfix \"_index\" for label encoding e.g. column X is appended as X_index, or a postfix \"_{n}\" for one hot encoding, n varies from 0 to unique value count e.g. column X is appended as X_0, X_1, X_2 (n = 3 i.e. no. of unique values for X). (Default value = \"replace\") print_impact True, False (Default value = False) This argument is to print out the change in schema (one hot encoding) or descriptive statistics (label encoding) Returns ------- DataFrame Encoded Dataframe \"\"\" cat_cols = attributeType_segregation ( idf )[ 1 ] if list_of_cols == \"all\" : list_of_cols = cat_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] if any ( x not in cat_cols for x in list_of_cols ): raise TypeError ( \"Invalid input for Column(s)\" ) if method_type not in ( 0 , 1 ): raise TypeError ( \"Invalid input for method_type\" ) if index_order not in ( \"frequencyDesc\" , \"frequencyAsc\" , \"alphabetDesc\" , \"alphabetAsc\" , ): raise TypeError ( \"Invalid input for Encoding Index Order\" ) if output_mode not in ( \"replace\" , \"append\" ): raise TypeError ( \"Invalid input for output_mode\" ) skip_cols = [] if method_type == 0 : if stats_unique == {}: skip_cols = ( uniqueCount_computation ( spark , idf , list_of_cols ) . where ( F . col ( \"unique_values\" ) > cardinality_threshold ) . select ( \"attribute\" ) . rdd . flatMap ( lambda x : x ) . collect () ) else : skip_cols = ( read_dataset ( spark , ** stats_unique ) . where ( F . col ( \"unique_values\" ) > cardinality_threshold ) . select ( \"attribute\" ) . rdd . flatMap ( lambda x : x ) . collect () ) skip_cols = list ( set ([ e for e in skip_cols if e in list_of_cols and e not in drop_cols ]) ) if skip_cols : warnings . warn ( \"Columns dropped from one-hot encoding due to high cardinality: \" + \",\" . join ( skip_cols ) ) list_of_cols = list ( set ([ e for e in list_of_cols if e not in drop_cols + skip_cols ]) ) if len ( list_of_cols ) == 0 : warnings . warn ( \"No Encoding Computation - No categorical column(s) to transform\" ) return idf list_of_cols_vec = [] list_of_cols_idx = [] for i in list_of_cols : list_of_cols_vec . append ( i + \"_vec\" ) list_of_cols_idx . append ( i + \"_index\" ) idf_id = idf . withColumn ( \"tempID\" , F . monotonically_increasing_id ()) idf_indexed = idf_id . select ([ \"tempID\" ] + list_of_cols ) if version . parse ( pyspark . __version__ ) < version . parse ( \"3.0.0\" ): for idx , i in enumerate ( list_of_cols ): if pre_existing_model : indexerModel = StringIndexerModel . load ( model_path + \"/cat_to_num_unsupervised/indexer-model/\" + i ) else : stringIndexer = StringIndexer ( inputCol = i , outputCol = i + \"_index\" , stringOrderType = index_order , handleInvalid = \"keep\" , ) indexerModel = stringIndexer . fit ( idf . select ( i )) if model_path != \"NA\" : indexerModel . write () . overwrite () . save ( model_path + \"/cat_to_num_unsupervised/indexer-model/\" + i ) idf_indexed = indexerModel . transform ( idf_indexed ) if idx % 5 == 0 : idf_indexed . persist ( pyspark . StorageLevel . MEMORY_AND_DISK ) . count () else : if pre_existing_model : indexerModel = StringIndexerModel . load ( model_path + \"/cat_to_num_unsupervised/indexer\" ) else : stringIndexer = StringIndexer ( inputCols = list_of_cols , outputCols = list_of_cols_idx , stringOrderType = index_order , handleInvalid = \"keep\" , ) indexerModel = stringIndexer . fit ( idf_indexed ) if model_path != \"NA\" : indexerModel . write () . overwrite () . save ( model_path + \"/cat_to_num_unsupervised/indexer\" ) idf_indexed = indexerModel . transform ( idf_indexed ) odf_indexed = idf_id . join ( idf_indexed . drop ( * list_of_cols ), \"tempID\" , \"left_outer\" ) . drop ( \"tempID\" ) odf_indexed . persist ( pyspark . StorageLevel . MEMORY_AND_DISK ) . count () if method_type == 0 : if pre_existing_model : encoder = OneHotEncoder . load ( model_path + \"/cat_to_num_unsupervised/encoder\" ) else : encoder = OneHotEncoder ( inputCols = list_of_cols_idx , outputCols = list_of_cols_vec , handleInvalid = \"keep\" , ) if model_path != \"NA\" : encoder . write () . overwrite () . save ( model_path + \"/cat_to_num_unsupervised/encoder\" ) odf_encoded = encoder . fit ( odf_indexed ) . transform ( odf_indexed ) odf = odf_encoded def vector_to_array ( v ): v = DenseVector ( v ) new_array = list ([ int ( x ) for x in v ]) return new_array f_vector_to_array = F . udf ( vector_to_array , T . ArrayType ( T . IntegerType ())) odf_sample = odf . take ( 1 ) for i in list_of_cols : uniq_cats = odf_sample [ 0 ] . asDict ()[ i + \"_vec\" ] . size odf_schema = odf . schema . add ( T . StructField ( \"tmp\" , T . ArrayType ( T . IntegerType ())) ) for j in range ( 0 , uniq_cats ): odf_schema = odf_schema . add ( T . StructField ( i + \"_\" + str ( j ), T . IntegerType ()) ) odf = ( odf . withColumn ( \"tmp\" , f_vector_to_array ( i + \"_vec\" )) . rdd . map ( lambda x : ( * x , * x [ \"tmp\" ])) . toDF ( schema = odf_schema ) ) if output_mode == \"replace\" : odf = odf . drop ( i , i + \"_vec\" , i + \"_index\" , \"tmp\" ) else : odf = odf . drop ( i + \"_vec\" , i + \"_index\" , \"tmp\" ) else : odf = odf_indexed for i in list_of_cols : odf = odf . withColumn ( i + \"_index\" , F . when ( F . col ( i ) . isNull (), None ) . otherwise ( F . col ( i + \"_index\" ) . cast ( T . IntegerType ()) ), ) if output_mode == \"replace\" : for i in list_of_cols : odf = odf . drop ( i ) . withColumnRenamed ( i + \"_index\" , i ) odf = odf . select ( idf . columns ) if print_impact and method_type == 1 : print ( \"Before\" ) idf . describe () . where ( F . col ( \"summary\" ) . isin ( \"count\" , \"min\" , \"max\" )) . show ( 3 , False ) print ( \"After\" ) odf . describe () . where ( F . col ( \"summary\" ) . isin ( \"count\" , \"min\" , \"max\" )) . show ( 3 , False ) if print_impact and method_type == 0 : print ( \"Before\" ) idf . printSchema () print ( \"After\" ) odf . printSchema () return odf def expression_parser ( idf, list_of_expr, postfix='', print_impact=False) expression_parser can be used to evaluate a list of SQL expressions and output the result as new features. It is able to handle column names containing special characters such as \u201c.\u201d, \u201c-\u201d, \u201c@\u201d, \u201c^\u201d, etc, by converting them to \u201c_\u201d first before the evaluation and convert them back to the original names before returning the output dataframe. Parameters idf Input Dataframe list_of_expr List of expressions to evaluate as new features e.g., [\"expr1\",\"expr2\"]. Alternatively, expressions can be specified in a string format, where different expressions are separated by pipe delimiter \u201c|\u201d e.g., \"expr1|expr2\". postfix postfix for new feature name.Naming convention \"f\" + expression_index + postfix e.g. with postfix of \"new\", new added features are named as f0new, f1new etc. (Default value = \"\") print_impact True, False This argument is to print the descriptive statistics of the parsed features (Default value = False) Returns DataFrame Parsed Dataframe Expand source code def expression_parser ( idf , list_of_expr , postfix = \"\" , print_impact = False ): \"\"\" expression_parser can be used to evaluate a list of SQL expressions and output the result as new features. It is able to handle column names containing special characters such as \u201c.\u201d, \u201c-\u201d, \u201c@\u201d, \u201c^\u201d, etc, by converting them to \u201c_\u201d first before the evaluation and convert them back to the original names before returning the output dataframe. Parameters ---------- idf Input Dataframe list_of_expr List of expressions to evaluate as new features e.g., [\"expr1\",\"expr2\"]. Alternatively, expressions can be specified in a string format, where different expressions are separated by pipe delimiter \u201c|\u201d e.g., \"expr1|expr2\". postfix postfix for new feature name.Naming convention \"f\" + expression_index + postfix e.g. with postfix of \"new\", new added features are named as f0new, f1new etc. (Default value = \"\") print_impact True, False This argument is to print the descriptive statistics of the parsed features (Default value = False) Returns ------- DataFrame Parsed Dataframe \"\"\" if isinstance ( list_of_expr , str ): list_of_expr = [ x . strip () for x in list_of_expr . split ( \"|\" )] special_chars = [ \"&\" , \"$\" , \";\" , \":\" , \",\" , \"*\" , \"#\" , \"@\" , \"?\" , \"%\" , \"!\" , \"^\" , \"(\" , \")\" , \"-\" , \"/\" , \"'\" , \".\" , '\"' , ] rename_cols = [] replace_chars = {} for char in special_chars : for col in idf . columns : if char in col : rename_cols . append ( col ) if col in replace_chars . keys (): ( replace_chars [ col ]) . append ( char ) else : replace_chars [ col ] = [ char ] rename_mapping_to_new , rename_mapping_to_old = {}, {} idf_renamed = idf for col in rename_cols : new_col = col for char in replace_chars [ col ]: new_col = new_col . replace ( char , \"_\" ) rename_mapping_to_old [ new_col ] = col rename_mapping_to_new [ col ] = new_col idf_renamed = idf_renamed . withColumnRenamed ( col , new_col ) list_of_expr_ = [] for expr in list_of_expr : new_expr = expr for col in rename_cols : if col in expr : new_expr = new_expr . replace ( col , rename_mapping_to_new [ col ]) list_of_expr_ . append ( new_expr ) list_of_expr = list_of_expr_ odf = idf_renamed new_cols = [] for index , exp in enumerate ( list_of_expr ): odf = odf . withColumn ( \"f\" + str ( index ) + postfix , F . expr ( exp )) new_cols . append ( \"f\" + str ( index ) + postfix ) for new_col , col in rename_mapping_to_old . items (): odf = odf . withColumnRenamed ( new_col , col ) if print_impact : print ( \"Columns Added: \" , new_cols ) odf . select ( new_cols ) . describe () . show ( 5 , False ) return odf def feature_transformation ( idf, list_of_cols='all', drop_cols=[], method_type='sqrt', N=None, output_mode='replace', print_impact=False) As the name indicates, feature_transformation performs mathematical transformation over selected attributes. The following methods are supported for an input attribute x: ln(x), log10(x), log2(x), e^x, 2^x, 10^x, N^x, square and cube root of x, x^2, x^3, x^N, trigonometric transformations of x (sin, cos, tan, asin, acos, atan), radians, x%N, x!, 1/x, floor and ceiling of x and x rounded to N decimal places. Some transformations only work with positive or non-negative input values such as log and square root and an error will be returned if violated. Parameters idf Input Dataframe list_of_cols List of numerical columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all numerical columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) method_type \"ln\", \"log10\", \"log2\", \"exp\", \"powOf2\" (2^x), \"powOf10\" (10^x), \"powOfN\" (N^x), \"sqrt\" (square root), \"cbrt\" (cube root), \"sq\" (square), \"cb\" (cube), \"toPowerN\" (x^N), \"sin\", \"cos\", \"tan\", \"asin\", \"acos\", \"atan\", \"radians\", \"remainderDivByN\" (x%N), \"factorial\" (x!), \"mul_inv\" (1/x), \"floor\", \"ceil\", \"roundN\" (round to N decimal places) (Default value = \"sqrt\") N None by default. If method_type is \"powOfN\", \"toPowerN\", \"remainderDivByN\" or \"roundN\", N will be used as the required constant. output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with transformed columns. \u201cappend\u201d option append transformed columns with a postfix (E.g. \"_ln\", \"_powOf \") to the input dataset. (Default value = \"replace\") print_impact True, False This argument is to print before and after descriptive statistics of the transformed features (Default value = False) Returns DataFrame Transformed Dataframe Expand source code def feature_transformation ( idf , list_of_cols = \"all\" , drop_cols = [], method_type = \"sqrt\" , N = None , output_mode = \"replace\" , print_impact = False , ): \"\"\" As the name indicates, feature_transformation performs mathematical transformation over selected attributes. The following methods are supported for an input attribute x: ln(x), log10(x), log2(x), e^x, 2^x, 10^x, N^x, square and cube root of x, x^2, x^3, x^N, trigonometric transformations of x (sin, cos, tan, asin, acos, atan), radians, x%N, x!, 1/x, floor and ceiling of x and x rounded to N decimal places. Some transformations only work with positive or non-negative input values such as log and square root and an error will be returned if violated. Parameters ---------- idf Input Dataframe list_of_cols List of numerical columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all numerical columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) method_type \"ln\", \"log10\", \"log2\", \"exp\", \"powOf2\" (2^x), \"powOf10\" (10^x), \"powOfN\" (N^x), \"sqrt\" (square root), \"cbrt\" (cube root), \"sq\" (square), \"cb\" (cube), \"toPowerN\" (x^N), \"sin\", \"cos\", \"tan\", \"asin\", \"acos\", \"atan\", \"radians\", \"remainderDivByN\" (x%N), \"factorial\" (x!), \"mul_inv\" (1/x), \"floor\", \"ceil\", \"roundN\" (round to N decimal places) (Default value = \"sqrt\") N None by default. If method_type is \"powOfN\", \"toPowerN\", \"remainderDivByN\" or \"roundN\", N will be used as the required constant. output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with transformed columns. \u201cappend\u201d option append transformed columns with a postfix (E.g. \"_ln\", \"_powOf<N>\") to the input dataset. (Default value = \"replace\") print_impact True, False This argument is to print before and after descriptive statistics of the transformed features (Default value = False) Returns ------- DataFrame Transformed Dataframe \"\"\" num_cols = attributeType_segregation ( idf )[ 0 ] if list_of_cols == \"all\" : list_of_cols = num_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in drop_cols ])) if ( len ( list_of_cols ) == 0 ) | ( any ( x not in num_cols for x in list_of_cols )): raise TypeError ( \"Invalid input for Column(s)\" ) if method_type not in [ \"ln\" , \"log10\" , \"log2\" , \"exp\" , \"powOf2\" , \"powOf10\" , \"powOfN\" , \"sqrt\" , \"cbrt\" , \"sq\" , \"cb\" , \"toPowerN\" , \"sin\" , \"cos\" , \"tan\" , \"asin\" , \"acos\" , \"atan\" , \"radians\" , \"remainderDivByN\" , \"factorial\" , \"mul_inv\" , \"floor\" , \"ceil\" , \"roundN\" , ]: raise TypeError ( \"Invalid input method_type\" ) num_cols = attributeType_segregation ( idf . select ( list_of_cols ))[ 0 ] list_of_cols = num_cols odf = idf transformation_function = { \"ln\" : F . log , \"log10\" : F . log10 , \"log2\" : F . log2 , \"exp\" : F . exp , \"powOf2\" : ( lambda x : F . pow ( 2.0 , x )), \"powOf10\" : ( lambda x : F . pow ( 10.0 , x )), \"powOfN\" : ( lambda x : F . pow ( N , x )), \"sqrt\" : F . sqrt , \"cbrt\" : F . cbrt , \"sq\" : ( lambda x : x ** 2 ), \"cb\" : ( lambda x : x ** 3 ), \"toPowerN\" : ( lambda x : x ** N ), \"sin\" : F . sin , \"cos\" : F . cos , \"tan\" : F . tan , \"asin\" : F . asin , \"acos\" : F . acos , \"atan\" : F . atan , \"radians\" : F . radians , \"remainderDivByN\" : ( lambda x : x % F . lit ( N )), \"factorial\" : F . factorial , \"mul_inv\" : ( lambda x : F . lit ( 1 ) / x ), \"floor\" : F . floor , \"ceil\" : F . ceil , \"roundN\" : ( lambda x : F . round ( x , N )), } def get_col_name ( i ): if output_mode == \"replace\" : return i else : if method_type in [ \"powOfN\" , \"toPowerN\" , \"remainderDivByN\" , \"roundN\" ]: return i + \"_\" + method_type [: - 1 ] + str ( N ) else : return i + \"_\" + method_type output_cols = [] for i in list_of_cols : modify_col = get_col_name ( i ) odf = odf . withColumn ( modify_col , transformation_function [ method_type ]( F . col ( i ))) output_cols . append ( modify_col ) if print_impact : print ( \"Before:\" ) idf . select ( list_of_cols ) . describe () . show ( 5 , False ) print ( \"After:\" ) odf . select ( output_cols ) . describe () . show ( 5 , False ) return odf def imputation_MMM ( spark, idf, list_of_cols='missing', drop_cols=[], method_type='median', pre_existing_model=False, model_path='NA', output_mode='replace', stats_missing={}, stats_mode={}, print_impact=False) This function handles missing value related issues by substituting null values by the measure of central tendency (mode for categorical features and mean/median for numerical features). For numerical attributes, it leverages Imputer functionality of Spark MLlib. Though, Imputer can be used for categorical attributes but this feature is available only in Spark3.x, therefore for categorical features, we compute mode or leverage mode computation from Measures of Central Tendency. Parameters spark Spark Session idf Input Dataframe list_of_cols List of columns to impute e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all (non-array) columns for analysis. This is super useful instead of specifying all column names manually. \"missing\" (default) can be passed to include only those columns with missing values. One of the usecases where \"all\" may be preferable over \"missing\" is when the user wants to save the imputation model for the future use e.g. a column may not have missing value in the training dataset but missing values may possibly appear in the prediction dataset. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) method_type \"median\", \"mean\" (valid only for for numerical columns attributes). Mode is only option for categorical columns. (Default value = \"median\") pre_existing_model Boolean argument \u2013 True or False. True if imputation model exists already, False otherwise. (Default value = False) model_path If pre_existing_model is True, this argument is path for referring the pre-saved model. If pre_existing_model is False, this argument can be used for saving the model. Default \"NA\" means there is neither pre-existing model nor there is a need to save one. output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with transformed column. \u201cappend\u201d option append transformed column to the input dataset with a postfix \"_imputed\" e.g. column X is appended as X_imputed. (Default value = \"replace\") stats_missing Takes arguments for read_dataset (data_ingest module) function in a dictionary format to read pre-saved statistics on missing count/pct i.e. if measures_of_counts or missingCount_computation (data_analyzer.stats_generator module) has been computed & saved before. (Default value = {}) stats_mode Takes arguments for read_dataset (data_ingest module) function in a dictionary format to read pre-saved statistics on most frequently seen values i.e. if measures_of_centralTendency or mode_computation (data_analyzer.stats_generator module) has been computed & saved before. (Default value = {}) print_impact True, False (Default value = False) This argument is to print out before and after missing counts of imputed columns. Returns DataFrame Imputed Dataframe Expand source code def imputation_MMM ( spark , idf , list_of_cols = \"missing\" , drop_cols = [], method_type = \"median\" , pre_existing_model = False , model_path = \"NA\" , output_mode = \"replace\" , stats_missing = {}, stats_mode = {}, print_impact = False , ): \"\"\" This function handles missing value related issues by substituting null values by the measure of central tendency (mode for categorical features and mean/median for numerical features). For numerical attributes, it leverages [Imputer](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.feature.Imputer .html) functionality of Spark MLlib. Though, Imputer can be used for categorical attributes but this feature is available only in Spark3.x, therefore for categorical features, we compute mode or leverage mode computation from Measures of Central Tendency. Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of columns to impute e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all (non-array) columns for analysis. This is super useful instead of specifying all column names manually. \"missing\" (default) can be passed to include only those columns with missing values. One of the usecases where \"all\" may be preferable over \"missing\" is when the user wants to save the imputation model for the future use e.g. a column may not have missing value in the training dataset but missing values may possibly appear in the prediction dataset. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) method_type \"median\", \"mean\" (valid only for for numerical columns attributes). Mode is only option for categorical columns. (Default value = \"median\") pre_existing_model Boolean argument \u2013 True or False. True if imputation model exists already, False otherwise. (Default value = False) model_path If pre_existing_model is True, this argument is path for referring the pre-saved model. If pre_existing_model is False, this argument can be used for saving the model. Default \"NA\" means there is neither pre-existing model nor there is a need to save one. output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with transformed column. \u201cappend\u201d option append transformed column to the input dataset with a postfix \"_imputed\" e.g. column X is appended as X_imputed. (Default value = \"replace\") stats_missing Takes arguments for read_dataset (data_ingest module) function in a dictionary format to read pre-saved statistics on missing count/pct i.e. if measures_of_counts or missingCount_computation (data_analyzer.stats_generator module) has been computed & saved before. (Default value = {}) stats_mode Takes arguments for read_dataset (data_ingest module) function in a dictionary format to read pre-saved statistics on most frequently seen values i.e. if measures_of_centralTendency or mode_computation (data_analyzer.stats_generator module) has been computed & saved before. (Default value = {}) print_impact True, False (Default value = False) This argument is to print out before and after missing counts of imputed columns. Returns ------- DataFrame Imputed Dataframe \"\"\" if stats_missing == {}: missing_df = missingCount_computation ( spark , idf ) else : missing_df = read_dataset ( spark , ** stats_missing ) . select ( \"attribute\" , \"missing_count\" , \"missing_pct\" ) missing_cols = ( missing_df . where ( F . col ( \"missing_count\" ) > 0 ) . select ( \"attribute\" ) . rdd . flatMap ( lambda x : x ) . collect () ) if str ( pre_existing_model ) . lower () == \"true\" : pre_existing_model = True elif str ( pre_existing_model ) . lower () == \"false\" : pre_existing_model = False else : raise TypeError ( \"Non-Boolean input for pre_existing_model\" ) if ( len ( missing_cols ) == 0 ) & ( not pre_existing_model ) & ( model_path == \"NA\" ): return idf num_cols , cat_cols , other_cols = attributeType_segregation ( idf ) if list_of_cols == \"all\" : list_of_cols = num_cols + cat_cols if list_of_cols == \"missing\" : list_of_cols = [ x for x in missing_cols if x in num_cols + cat_cols ] if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in drop_cols ])) if len ( list_of_cols ) == 0 : warnings . warn ( \"No Imputation performed- No column(s) to impute\" ) return idf if any ( x not in num_cols + cat_cols for x in list_of_cols ): raise TypeError ( \"Invalid input for Column(s)\" ) if method_type not in ( \"mode\" , \"mean\" , \"median\" ): raise TypeError ( \"Invalid input for method_type\" ) if output_mode not in ( \"replace\" , \"append\" ): raise TypeError ( \"Invalid input for output_mode\" ) num_cols , cat_cols , other_cols = attributeType_segregation ( idf . select ( list_of_cols )) odf = idf if len ( num_cols ) > 0 : recast_cols = [] recast_type = [] for i in num_cols : if get_dtype ( idf , i ) not in ( \"float\" , \"double\" ): odf = odf . withColumn ( i , F . col ( i ) . cast ( T . DoubleType ())) recast_cols . append ( i + \"_imputed\" ) recast_type . append ( get_dtype ( idf , i )) # For mode imputation if method_type == \"mode\" : if stats_mode == {}: parameters = [ str ( ( idf . select ( i ) . dropna () . groupby ( i ) . count () . orderBy ( \"count\" , ascending = False ) . first () or [ None ] )[ 0 ] ) for i in num_cols ] else : mode_df = read_dataset ( spark , ** stats_mode ) . replace ( \"None\" , None ) mode_df_cols = list ( mode_df . select ( \"attribute\" ) . toPandas ()[ \"attribute\" ]) parameters = [] for i in num_cols : if i not in mode_df_cols : parameters . append ( str ( ( idf . select ( i ) . dropna () . groupby ( i ) . count () . orderBy ( \"count\" , ascending = False ) . first () or [ None ] )[ 0 ] ) ) else : parameters . append ( mode_df . where ( F . col ( \"attribute\" ) == i ) . select ( \"mode\" ) . rdd . flatMap ( list ) . collect ()[ 0 ] ) for index , i in enumerate ( num_cols ): odf = odf . withColumn ( i + \"_imputed\" , F . when ( F . col ( i ) . isNull (), parameters [ index ]) . otherwise ( F . col ( i )), ) else : # For mean, median imputation # Building new imputer model or uploading the existing model if pre_existing_model : imputerModel = ImputerModel . load ( model_path + \"/imputation_MMM/num_imputer-model\" ) else : imputer = Imputer ( strategy = method_type , inputCols = num_cols , outputCols = [( e + \"_imputed\" ) for e in num_cols ], ) imputerModel = imputer . fit ( odf ) # Applying model # odf = recast_column(imputerModel.transform(odf), recast_cols, recast_type) odf = imputerModel . transform ( odf ) for i , j in zip ( recast_cols , recast_type ): odf = odf . withColumn ( i , F . col ( i ) . cast ( j )) # Saving model if required if ( not pre_existing_model ) & ( model_path != \"NA\" ): imputerModel . write () . overwrite () . save ( model_path + \"/imputation_MMM/num_imputer-model\" ) if len ( cat_cols ) > 0 : if pre_existing_model : df_model = spark . read . csv ( model_path + \"/imputation_MMM/cat_imputer\" , header = True , inferSchema = True , ) parameters = [] for i in cat_cols : mapped_value = ( df_model . where ( F . col ( \"attribute\" ) == i ) . select ( \"parameters\" ) . rdd . flatMap ( lambda x : x ) . collect ()[ 0 ] ) parameters . append ( mapped_value ) else : if stats_mode == {}: parameters = [ str ( ( idf . select ( i ) . dropna () . groupby ( i ) . count () . orderBy ( \"count\" , ascending = False ) . first () or [ None ] )[ 0 ] ) for i in cat_cols ] else : mode_df = read_dataset ( spark , ** stats_mode ) . replace ( \"None\" , None ) parameters = [ mode_df . where ( F . col ( \"attribute\" ) == i ) . select ( \"mode\" ) . rdd . flatMap ( list ) . collect ()[ 0 ] for i in cat_cols ] for index , i in enumerate ( cat_cols ): odf = odf . withColumn ( i + \"_imputed\" , F . when ( F . col ( i ) . isNull (), parameters [ index ]) . otherwise ( F . col ( i )), ) if ( not pre_existing_model ) & ( model_path != \"NA\" ): df_model = spark . createDataFrame ( zip ( cat_cols , parameters ), schema = [ \"attribute\" , \"parameters\" ] ) df_model . repartition ( 1 ) . write . csv ( model_path + \"/imputation_MMM/cat_imputer\" , header = True , mode = \"overwrite\" , ) for i in num_cols + cat_cols : if i not in missing_cols : odf = odf . drop ( i + \"_imputed\" ) elif output_mode == \"replace\" : odf = odf . drop ( i ) . withColumnRenamed ( i + \"_imputed\" , i ) if print_impact : if output_mode == \"replace\" : odf_print = missing_df . select ( \"attribute\" , F . col ( \"missing_count\" ) . alias ( \"missingCount_before\" ) ) . join ( missingCount_computation ( spark , odf , list_of_cols ) . select ( \"attribute\" , F . col ( \"missing_count\" ) . alias ( \"missingCount_after\" ) ), \"attribute\" , \"inner\" , ) else : output_cols = [ ( i + \"_imputed\" ) for i in [ e for e in ( num_cols + cat_cols ) if e in missing_cols ] ] odf_print = missing_df . select ( \"attribute\" , F . col ( \"missing_count\" ) . alias ( \"missingCount_before\" ) ) . join ( missingCount_computation ( spark , odf , output_cols ) . withColumnRenamed ( \"attribute\" , \"attribute_after\" ) . withColumn ( \"attribute\" , F . expr ( \"substring(attribute_after, 1, length(attribute_after)-8)\" ), ) . drop ( \"missing_pct\" ), \"attribute\" , \"inner\" , ) odf_print . show ( len ( list_of_cols ), False ) return odf def imputation_matrixFactorization ( spark, idf, list_of_cols='missing', drop_cols=[], id_col='', output_mode='replace', stats_missing={}, print_impact=False) imputation_matrixFactorization uses collaborative filtering technique to impute missing values. Collaborative filtering is commonly used in recommender systems to fill the missing user-item entries and PySpark provides an implementation using alternating least squares (ALS) algorithm, which is used in this function. To fit our problem into the ALS model, each attribute is treated as an item and an id column needs to be specified by the user to generate the user-item pairs. In the case, ID column doesn't exist in the dataset and proxu ID column is implicitly generated by the function. Subsequently, all user-item pairs with known values will be used to train the ALS model and the trained model can be used to predict the user-item pairs with missing values. Parameters spark Spark Session idf Input Dataframe list_of_cols List of columns to impute e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all (non-array) columns for analysis. This is super useful instead of specifying all column names manually. \"missing\" (default) can be passed to include only those columns with missing values. One of the usecases where \"all\" may be preferable over \"missing\" is when the user wants to save the imputation model for the future use e.g. a column may not have missing value in the training dataset but missing values may possibly appear in the prediction dataset. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) id_col ID column (Default value = \"\") output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with transformed column. \u201cappend\u201d option append transformed column to the input dataset with a postfix \"_imputed\" e.g. column X is appended as X_imputed. (Default value = \"replace\") stats_missing Takes arguments for read_dataset (data_ingest module) function in a dictionary format to read pre-saved statistics on missing count/pct i.e. if measures_of_counts or missingCount_computation (data_analyzer.stats_generator module) has been computed & saved before. (Default value = {}) print_impact True, False (Default value = False) This argument is to print out before and after missing counts of imputed columns. Returns DataFrame Imputed Dataframe Expand source code def imputation_matrixFactorization ( spark , idf , list_of_cols = \"missing\" , drop_cols = [], id_col = \"\" , output_mode = \"replace\" , stats_missing = {}, print_impact = False , ): \"\"\" imputation_matrixFactorization uses collaborative filtering technique to impute missing values. Collaborative filtering is commonly used in recommender systems to fill the missing user-item entries and PySpark provides an implementation using alternating least squares (ALS) algorithm, which is used in this function. To fit our problem into the ALS model, each attribute is treated as an item and an id column needs to be specified by the user to generate the user-item pairs. In the case, ID column doesn't exist in the dataset and proxu ID column is implicitly generated by the function. Subsequently, all user-item pairs with known values will be used to train the ALS model and the trained model can be used to predict the user-item pairs with missing values. Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of columns to impute e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all (non-array) columns for analysis. This is super useful instead of specifying all column names manually. \"missing\" (default) can be passed to include only those columns with missing values. One of the usecases where \"all\" may be preferable over \"missing\" is when the user wants to save the imputation model for the future use e.g. a column may not have missing value in the training dataset but missing values may possibly appear in the prediction dataset. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) id_col ID column (Default value = \"\") output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with transformed column. \u201cappend\u201d option append transformed column to the input dataset with a postfix \"_imputed\" e.g. column X is appended as X_imputed. (Default value = \"replace\") stats_missing Takes arguments for read_dataset (data_ingest module) function in a dictionary format to read pre-saved statistics on missing count/pct i.e. if measures_of_counts or missingCount_computation (data_analyzer.stats_generator module) has been computed & saved before. (Default value = {}) print_impact True, False (Default value = False) This argument is to print out before and after missing counts of imputed columns. Returns ------- DataFrame Imputed Dataframe \"\"\" num_cols = attributeType_segregation ( idf )[ 0 ] if stats_missing == {}: missing_df = missingCount_computation ( spark , idf , num_cols ) else : missing_df = ( read_dataset ( spark , ** stats_missing ) . select ( \"attribute\" , \"missing_count\" , \"missing_pct\" ) . where ( F . col ( \"attribute\" ) . isin ( num_cols )) ) empty_cols = ( missing_df . where ( F . col ( \"missing_pct\" ) == 1.0 ) . select ( \"attribute\" ) . rdd . flatMap ( lambda x : x ) . collect () ) if len ( empty_cols ) > 0 : warnings . warn ( \"Following columns dropped from the imputation as all values are null: \" + \",\" . join ( empty_cols ) ) missing_cols = ( missing_df . where ( F . col ( \"missing_count\" ) > 0 ) . where ( F . col ( \"missing_pct\" ) < 1.0 ) . select ( \"attribute\" ) . rdd . flatMap ( lambda x : x ) . collect () ) if list_of_cols == \"all\" : list_of_cols = num_cols if list_of_cols == \"missing\" : list_of_cols = missing_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ( [ e for e in list_of_cols if ( e not in drop_cols ) & ( e != id_col ) & ( e not in empty_cols ) ] ) ) if ( len ( list_of_cols ) == 0 ) | ( len ([ e for e in list_of_cols if e in missing_cols ]) == 0 ): warnings . warn ( \"No Imputation Performed - No Column(s) to Impute\" ) return idf if len ( list_of_cols ) == 1 : warnings . warn ( \"No Imputation Performed - Needs more than 1 column for matrix factorization\" ) return idf if any ( x not in num_cols for x in list_of_cols ): raise TypeError ( \"Invalid input for Column(s)\" ) if output_mode not in ( \"replace\" , \"append\" ): raise TypeError ( \"Invalid input for output_mode\" ) remove_id = False if id_col == \"\" : idf = idf . withColumn ( \"id\" , F . monotonically_increasing_id ()) . withColumn ( \"id\" , F . row_number () . over ( Window . orderBy ( \"id\" )) ) id_col = \"id\" remove_id = True key_and_val = F . create_map ( list ( chain . from_iterable ([[ F . lit ( c ), F . col ( c )] for c in list_of_cols ])) ) df_flatten = idf . select ( id_col , F . explode ( key_and_val )) . withColumn ( \"key\" , F . concat ( F . col ( \"key\" ), F . lit ( \"_imputed\" )) ) id_type = get_dtype ( idf , id_col ) if id_type == \"string\" : id_indexer = StringIndexer () . setInputCol ( id_col ) . setOutputCol ( \"IDLabel\" ) id_indexer_model = id_indexer . fit ( df_flatten ) df_flatten = id_indexer_model . transform ( df_flatten ) . drop ( id_col ) else : df_flatten = df_flatten . withColumnRenamed ( id_col , \"IDLabel\" ) indexer = StringIndexer () . setInputCol ( \"key\" ) . setOutputCol ( \"keyLabel\" ) indexer_model = indexer . fit ( df_flatten ) df_encoded = indexer_model . transform ( df_flatten ) . drop ( \"key\" ) df_model = df_encoded . where ( F . col ( \"value\" ) . isNotNull ()) df_test = df_encoded . where ( F . col ( \"value\" ) . isNull ()) if ( df_model . select ( \"IDLabel\" ) . distinct () . count () < df_encoded . select ( \"IDLabel\" ) . distinct () . count () ): warnings . warn ( \"The returned odf may not be fully imputed because values for all list_of_cols are null for some IDs\" ) als = ALS ( maxIter = 20 , regParam = 0.01 , userCol = \"IDLabel\" , itemCol = \"keyLabel\" , ratingCol = \"value\" , coldStartStrategy = \"drop\" , ) model = als . fit ( df_model ) df_pred = ( model . transform ( df_test ) . drop ( \"value\" ) . withColumnRenamed ( \"prediction\" , \"value\" ) ) df_encoded_pred = df_model . union ( df_pred . select ( df_model . columns )) if id_type == \"string\" : IDlabelReverse = IndexToString () . setInputCol ( \"IDLabel\" ) . setOutputCol ( id_col ) df_encoded_pred = IDlabelReverse . transform ( df_encoded_pred ) else : df_encoded_pred = df_encoded_pred . withColumnRenamed ( \"IDLabel\" , id_col ) keylabelReverse = IndexToString () . setInputCol ( \"keyLabel\" ) . setOutputCol ( \"key\" ) odf_imputed = ( keylabelReverse . transform ( df_encoded_pred ) . groupBy ( id_col ) . pivot ( \"key\" ) . agg ( F . first ( \"value\" )) . select ( [ id_col ] + [( i + \"_imputed\" ) for i in list_of_cols if i in missing_cols ] ) ) odf = idf . join ( odf_imputed , id_col , \"left_outer\" ) for i in list_of_cols : if i not in missing_cols : odf = odf . drop ( i + \"_imputed\" ) elif output_mode == \"replace\" : odf = odf . drop ( i ) . withColumnRenamed ( i + \"_imputed\" , i ) if remove_id : odf = odf . drop ( \"id\" ) if print_impact : if output_mode == \"replace\" : odf_print = missing_df . select ( \"attribute\" , F . col ( \"missing_count\" ) . alias ( \"missingCount_before\" ) ) . join ( missingCount_computation ( spark , odf , list_of_cols ) . select ( \"attribute\" , F . col ( \"missing_count\" ) . alias ( \"missingCount_after\" ) ), \"attribute\" , \"inner\" , ) else : output_cols = [ ( i + \"_imputed\" ) for i in [ e for e in list_of_cols if e in missing_cols ] ] odf_print = missing_df . select ( \"attribute\" , F . col ( \"missing_count\" ) . alias ( \"missingCount_before\" ) ) . join ( missingCount_computation ( spark , odf , output_cols ) . withColumnRenamed ( \"attribute\" , \"attribute_after\" ) . withColumn ( \"attribute\" , F . expr ( \"substring(attribute_after, 1, length(attribute_after)-8)\" ), ) . drop ( \"missing_pct\" ), \"attribute\" , \"inner\" , ) odf_print . show ( len ( list_of_cols ), False ) return odf def imputation_sklearn ( spark, idf, list_of_cols='missing', drop_cols=[], method_type='KNN', sample_size=500000, pre_existing_model=False, model_path='NA', output_mode='replace', stats_missing={}, run_type='local', print_impact=False) The function \"imputation_sklearn\" leverages sklearn imputer algorithms. Two methods are supported via this function: \u201cKNN\u201d and \u201cregression\u201d. \u201cKNN\u201d option trains a sklearn.impute.KNNImputer which is based on k-Nearest Neighbors algorithm. The missing values of a sample are imputed using the mean of its 5 nearest neighbors in the training set. \u201cregression\u201d option trains a sklearn.impute.IterativeImputer which models attribute to impute as a function of rest of the attributes and imputes using the estimation. Imputation is performed in an iterative way from attributes with fewest number of missing values to most. All the hyperparameters used in the above mentioned imputers are their default values. However, sklearn imputers are not scalable, which might be slow if the size of the input dataframe is large. Thus, an input sample_size (the default value is 500,000) can be set to control the number of samples to be used to train the imputer. If the total number of input dataset exceeds sample_size, the rest of the samples will be imputed using the trained imputer in a scalable manner. This is one of the way to demonstrate how Anovos has been designed as a scalable feature engineering library. Parameters spark Spark Session idf Input Dataframe list_of_cols List of columns to impute e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all (non-array) columns for analysis. This is super useful instead of specifying all column names manually. \"missing\" (default) can be passed to include only those columns with missing values. One of the usecases where \"all\" may be preferable over \"missing\" is when the user wants to save the imputation model for the future use e.g. a column may not have missing value in the training dataset but missing values may possibly appear in the prediction dataset. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) method_type \"KNN\", \"regression\". \"KNN\" option trains a sklearn.impute.KNNImputer. \"regression\" option trains a sklearn.impute.IterativeImputer (Default value = \"KNN\") sample_size Maximum rows for training the sklearn imputer (Default value = 500000) pre_existing_model Boolean argument \u2013 True or False. True if imputation model exists already, False otherwise. (Default value = False) model_path If pre_existing_model is True, this argument is path for referring the pre-saved model. If pre_existing_model is False, this argument can be used for saving the model. Default \"NA\" means there is neither pre-existing model nor there is a need to save one. output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with transformed column. \u201cappend\u201d option append transformed column to the input dataset with a postfix \"_imputed\" e.g. column X is appended as X_imputed. (Default value = \"replace\") stats_missing Takes arguments for read_dataset (data_ingest module) function in a dictionary format to read pre-saved statistics on missing count/pct i.e. if measures_of_counts or missingCount_computation (data_analyzer.stats_generator module) has been computed & saved before. (Default value = {}) run_type \"local\", \"emr\", \"databricks\" (Default value = \"local\") print_impact True, False (Default value = False) This argument is to print out before and after missing counts of imputed columns. Returns DataFrame Imputed Dataframe Expand source code def imputation_sklearn ( spark , idf , list_of_cols = \"missing\" , drop_cols = [], method_type = \"KNN\" , sample_size = 500000 , pre_existing_model = False , model_path = \"NA\" , output_mode = \"replace\" , stats_missing = {}, run_type = \"local\" , print_impact = False , ): \"\"\" The function \"imputation_sklearn\" leverages sklearn imputer algorithms. Two methods are supported via this function: \u201cKNN\u201d and \u201cregression\u201d. \u201cKNN\u201d option trains a sklearn.impute.KNNImputer which is based on k-Nearest Neighbors algorithm. The missing values of a sample are imputed using the mean of its 5 nearest neighbors in the training set. \u201cregression\u201d option trains a sklearn.impute.IterativeImputer which models attribute to impute as a function of rest of the attributes and imputes using the estimation. Imputation is performed in an iterative way from attributes with fewest number of missing values to most. All the hyperparameters used in the above mentioned imputers are their default values. However, sklearn imputers are not scalable, which might be slow if the size of the input dataframe is large. Thus, an input sample_size (the default value is 500,000) can be set to control the number of samples to be used to train the imputer. If the total number of input dataset exceeds sample_size, the rest of the samples will be imputed using the trained imputer in a scalable manner. This is one of the way to demonstrate how Anovos has been designed as a scalable feature engineering library. Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of columns to impute e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all (non-array) columns for analysis. This is super useful instead of specifying all column names manually. \"missing\" (default) can be passed to include only those columns with missing values. One of the usecases where \"all\" may be preferable over \"missing\" is when the user wants to save the imputation model for the future use e.g. a column may not have missing value in the training dataset but missing values may possibly appear in the prediction dataset. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) method_type \"KNN\", \"regression\". \"KNN\" option trains a sklearn.impute.KNNImputer. \"regression\" option trains a sklearn.impute.IterativeImputer (Default value = \"KNN\") sample_size Maximum rows for training the sklearn imputer (Default value = 500000) pre_existing_model Boolean argument \u2013 True or False. True if imputation model exists already, False otherwise. (Default value = False) model_path If pre_existing_model is True, this argument is path for referring the pre-saved model. If pre_existing_model is False, this argument can be used for saving the model. Default \"NA\" means there is neither pre-existing model nor there is a need to save one. output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with transformed column. \u201cappend\u201d option append transformed column to the input dataset with a postfix \"_imputed\" e.g. column X is appended as X_imputed. (Default value = \"replace\") stats_missing Takes arguments for read_dataset (data_ingest module) function in a dictionary format to read pre-saved statistics on missing count/pct i.e. if measures_of_counts or missingCount_computation (data_analyzer.stats_generator module) has been computed & saved before. (Default value = {}) run_type \"local\", \"emr\", \"databricks\" (Default value = \"local\") print_impact True, False (Default value = False) This argument is to print out before and after missing counts of imputed columns. Returns ------- DataFrame Imputed Dataframe \"\"\" num_cols = attributeType_segregation ( idf )[ 0 ] if stats_missing == {}: missing_df = missingCount_computation ( spark , idf , num_cols ) else : missing_df = ( read_dataset ( spark , ** stats_missing ) . select ( \"attribute\" , \"missing_count\" , \"missing_pct\" ) . where ( F . col ( \"attribute\" ) . isin ( num_cols )) ) empty_cols = ( missing_df . where ( F . col ( \"missing_pct\" ) == 1.0 ) . select ( \"attribute\" ) . rdd . flatMap ( lambda x : x ) . collect () ) if len ( empty_cols ) > 0 : warnings . warn ( \"Following columns dropped from the imputation as all values are null: \" + \",\" . join ( empty_cols ) ) missing_cols = ( missing_df . where ( F . col ( \"missing_count\" ) > 0 ) . where ( F . col ( \"missing_pct\" ) < 1.0 ) . select ( \"attribute\" ) . rdd . flatMap ( lambda x : x ) . collect () ) if list_of_cols == \"all\" : list_of_cols = num_cols if list_of_cols == \"missing\" : list_of_cols = missing_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if ( e not in drop_cols ) & ( e not in empty_cols )]) ) if len ( list_of_cols ) <= 1 : warnings . warn ( \"No Imputation Performed - No Column(s) or Insufficient Column(s) to Impute\" ) return idf if str ( pre_existing_model ) . lower () == \"true\" : pre_existing_model = True elif str ( pre_existing_model ) . lower () == \"false\" : pre_existing_model = False else : raise TypeError ( \"Non-Boolean input for pre_existing_model\" ) if ( ( len ([ e for e in list_of_cols if e in missing_cols ]) == 0 ) & ( not pre_existing_model ) & ( model_path == \"NA\" ) ): warnings . warn ( \"No Imputation Performed - No Column(s) to Impute and No Imputation Model to be saved\" ) return idf if any ( x not in num_cols for x in list_of_cols ): raise TypeError ( \"Invalid input for Column(s)\" ) if method_type not in ( \"KNN\" , \"regression\" ): raise TypeError ( \"Invalid input for method_type\" ) if output_mode not in ( \"replace\" , \"append\" ): raise TypeError ( \"Invalid input for output_mode\" ) if pre_existing_model : if run_type == \"emr\" : bash_cmd = \"aws s3 cp \" + model_path + \"/imputation_sklearn.sav .\" output = subprocess . check_output ([ \"bash\" , \"-c\" , bash_cmd ]) imputer = pickle . load ( open ( \"imputation_sklearn.sav\" , \"rb\" )) else : imputer = pickle . load ( open ( model_path + \"/imputation_sklearn.sav\" , \"rb\" )) idf_rest = idf else : sample_ratio = min ( 1.0 , float ( sample_size ) / idf . count ()) idf_model = idf . sample ( False , sample_ratio , 0 ) idf_pd = idf_model . select ( list_of_cols ) . toPandas () if method_type == \"KNN\" : imputer = KNNImputer ( n_neighbors = 5 , weights = \"uniform\" , metric = \"nan_euclidean\" ) if method_type == \"regression\" : imputer = IterativeImputer () imputer . fit ( idf_pd ) if ( not pre_existing_model ) & ( model_path != \"NA\" ): if run_type == \"emr\" : pickle . dump ( imputer , open ( \"imputation_sklearn.sav\" , \"wb\" )) bash_cmd = ( \"aws s3 cp imputation_sklearn.sav \" + model_path + \"/imputation_sklearn.sav\" ) output = subprocess . check_output ([ \"bash\" , \"-c\" , bash_cmd ]) imputer = pickle . load ( open ( \"imputation_sklearn.sav\" , \"rb\" )) else : local_path = model_path + \"/imputation_sklearn.sav\" os . makedirs ( os . path . dirname ( local_path ), exist_ok = True ) pickle . dump ( imputer , open ( local_path , \"wb\" )) imputer = pickle . load ( open ( model_path + \"/imputation_sklearn.sav\" , \"rb\" ) ) @F . pandas_udf ( returnType = T . ArrayType ( T . DoubleType ())) def prediction ( * cols ): X = pd . concat ( cols , axis = 1 ) return pd . Series ( row . tolist () for row in imputer . transform ( X )) odf = idf . withColumn ( \"features\" , prediction ( * list_of_cols )) odf . persist ( pyspark . StorageLevel . MEMORY_AND_DISK ) . count () odf_schema = odf . schema for i in list_of_cols : odf_schema = odf_schema . add ( T . StructField ( i + \"_imputed\" , T . FloatType ())) odf = ( odf . rdd . map ( lambda x : ( * x , * x [ \"features\" ])) . toDF ( schema = odf_schema ) . drop ( \"features\" ) ) output_cols = [] for i in list_of_cols : if output_mode == \"append\" : if i not in missing_cols : odf = odf . drop ( i + \"_imputed\" ) else : output_cols . append ( i + \"_imputed\" ) else : odf = odf . drop ( i ) . withColumnRenamed ( i + \"_imputed\" , i ) odf = odf . select ( idf . columns + output_cols ) if print_impact : if output_mode == \"replace\" : odf_print = missing_df . select ( \"attribute\" , F . col ( \"missing_count\" ) . alias ( \"missingCount_before\" ) ) . join ( missingCount_computation ( spark , odf , list_of_cols ) . select ( \"attribute\" , F . col ( \"missing_count\" ) . alias ( \"missingCount_after\" ) ), \"attribute\" , \"inner\" , ) else : odf_print = missing_df . select ( \"attribute\" , F . col ( \"missing_count\" ) . alias ( \"missingCount_before\" ) ) . join ( missingCount_computation ( spark , odf , output_cols ) . withColumnRenamed ( \"attribute\" , \"attribute_after\" ) . withColumn ( \"attribute\" , F . expr ( \"substring(attribute_after, 1, length(attribute_after)-8)\" ), ) . drop ( \"missing_pct\" ), \"attribute\" , \"inner\" , ) odf_print . show ( len ( list_of_cols ), False ) return odf def monotonic_binning ( spark, idf, list_of_cols='all', drop_cols=[], label_col='label', event_label=1, bin_method='equal_range', bin_size=10, bin_dtype='numerical', output_mode='replace') This function constitutes supervised way of binning the numerical attribute into discrete (integer or categorical values) attribute. Instead of pre-defined fixed number of bins, number of bins are dynamically computed to ensure the monotonic nature of bins i.e. % event should increase or decrease with the bin. Monotonic nature of bins is evaluated by looking at spearman rank correlation, which should be either +1 or -1, between the bin index and % event. In case, the monotonic nature is not attained, user defined fixed number of bins are used for the binning. Parameters spark Spark Session idf Input Dataframe list_of_cols List of numerical columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all numerical columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) label_col Label/Target column (Default value = \"label\") event_label Value of (positive) event (i.e label 1) (Default value = 1) bin_method \"equal_frequency\", \"equal_range\". In \"equal_range\" method, each bin is of equal size/width and in \"equal_frequency\", each bin has equal no. of rows, though the width of bins may vary. (Default value = \"equal_range\") bin_size Default number of bins in case monotonicity is not achieved. bin_dtype \"numerical\", \"categorical\". With \"numerical\" option, original value is replaced with an Integer (1,2,\u2026) and with \"categorical\" option, original replaced with a string describing min and max value allowed in the bin (\"minval-maxval\"). (Default value = \"numerical\") output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with transformed column. \u201cappend\u201d option append transformed column to the input dataset with a postfix \"_binned\" e.g. column X is appended as X_binned. (Default value = \"replace\") Returns DataFrame Binned Dataframe Expand source code def monotonic_binning ( spark , idf , list_of_cols = \"all\" , drop_cols = [], label_col = \"label\" , event_label = 1 , bin_method = \"equal_range\" , bin_size = 10 , bin_dtype = \"numerical\" , output_mode = \"replace\" , ): \"\"\" This function constitutes supervised way of binning the numerical attribute into discrete (integer or categorical values) attribute. Instead of pre-defined fixed number of bins, number of bins are dynamically computed to ensure the monotonic nature of bins i.e. % event should increase or decrease with the bin. Monotonic nature of bins is evaluated by looking at spearman rank correlation, which should be either +1 or -1, between the bin index and % event. In case, the monotonic nature is not attained, user defined fixed number of bins are used for the binning. Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of numerical columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all numerical columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) label_col Label/Target column (Default value = \"label\") event_label Value of (positive) event (i.e label 1) (Default value = 1) bin_method \"equal_frequency\", \"equal_range\". In \"equal_range\" method, each bin is of equal size/width and in \"equal_frequency\", each bin has equal no. of rows, though the width of bins may vary. (Default value = \"equal_range\") bin_size Default number of bins in case monotonicity is not achieved. bin_dtype \"numerical\", \"categorical\". With \"numerical\" option, original value is replaced with an Integer (1,2,\u2026) and with \"categorical\" option, original replaced with a string describing min and max value allowed in the bin (\"minval-maxval\"). (Default value = \"numerical\") output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with transformed column. \u201cappend\u201d option append transformed column to the input dataset with a postfix \"_binned\" e.g. column X is appended as X_binned. (Default value = \"replace\") Returns ------- DataFrame Binned Dataframe \"\"\" num_cols = attributeType_segregation ( idf )[ 0 ] if list_of_cols == \"all\" : list_of_cols = num_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in ( drop_cols + [ label_col ])]) ) if any ( x not in num_cols for x in list_of_cols ): raise TypeError ( \"Invalid input for Column(s)\" ) attribute_binning ( spark , idf , list_of_cols = \"all\" , drop_cols = [], method_type = \"equal_range\" , bin_size = 10 , pre_existing_model = False , model_path = \"NA\" , output_mode = \"replace\" , print_impact = False , ) odf = idf for col in list_of_cols : n = 20 r = 0 while n > 2 : tmp = ( attribute_binning ( spark , idf , [ col ], drop_cols = [], method_type = bin_method , bin_size = n , output_mode = \"append\" , ) . select ( label_col , col , col + \"_binned\" ) . withColumn ( label_col , F . when ( F . col ( label_col ) == event_label , 1 ) . otherwise ( 0 ) ) . groupBy ( col + \"_binned\" ) . agg ( F . avg ( col ) . alias ( \"mean_val\" ), F . avg ( label_col ) . alias ( \"mean_label\" )) . dropna () ) r , p = stats . spearmanr ( tmp . toPandas ()[[ \"mean_val\" ]], tmp . toPandas ()[[ \"mean_label\" ]] ) if r == 1.0 : odf = attribute_binning ( spark , odf , [ col ], drop_cols = [], method_type = bin_method , bin_size = n , bin_dtype = bin_dtype , output_mode = output_mode , ) break n = n - 1 r = 0 if r < 1.0 : odf = attribute_binning ( spark , odf , [ col ], drop_cols = [], method_type = bin_method , bin_size = bin_size , bin_dtype = bin_dtype , output_mode = output_mode , ) return odf def normalization ( idf, list_of_cols='all', drop_cols=[], pre_existing_model=False, model_path='NA', output_mode='replace', print_impact=False) Parameters idf Input Dataframe list_of_cols List of numerical columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all numerical columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) pre_existing_model Boolean argument \u2013 True or False. True if normalization/scalar model exists already, False Otherwise (Default value = False) model_path If pre_existing_model is True, this argument is path for referring the pre-saved model. If pre_existing_model is False, this argument can be used for saving the model. Default \"NA\" means there is neither pre-existing model nor there is a need to save one. output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with transformed column. \u201cappend\u201d option append transformed column to the input dataset with a postfix \"_scaled\" e.g. column X is appended as X_scaled. (Default value = \"replace\") print_impact True, False (Default value = False) This argument is to print out before and after descriptive statistics of rescaled columns. Returns DataFrame Rescaled Dataframe Expand source code def normalization ( idf , list_of_cols = \"all\" , drop_cols = [], pre_existing_model = False , model_path = \"NA\" , output_mode = \"replace\" , print_impact = False , ): \"\"\" Parameters ---------- idf Input Dataframe list_of_cols List of numerical columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all numerical columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) pre_existing_model Boolean argument \u2013 True or False. True if normalization/scalar model exists already, False Otherwise (Default value = False) model_path If pre_existing_model is True, this argument is path for referring the pre-saved model. If pre_existing_model is False, this argument can be used for saving the model. Default \"NA\" means there is neither pre-existing model nor there is a need to save one. output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with transformed column. \u201cappend\u201d option append transformed column to the input dataset with a postfix \"_scaled\" e.g. column X is appended as X_scaled. (Default value = \"replace\") print_impact True, False (Default value = False) This argument is to print out before and after descriptive statistics of rescaled columns. Returns ------- DataFrame Rescaled Dataframe \"\"\" num_cols = attributeType_segregation ( idf )[ 0 ] if list_of_cols == \"all\" : list_of_cols = num_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in drop_cols ])) if any ( x not in num_cols for x in list_of_cols ): raise TypeError ( \"Invalid input for Column(s)\" ) if len ( list_of_cols ) == 0 : warnings . warn ( \"No Normalization Performed - No numerical column(s) to transform\" ) return idf if output_mode not in ( \"replace\" , \"append\" ): raise TypeError ( \"Invalid input for output_mode\" ) idf_id = idf . withColumn ( \"tempID\" , F . monotonically_increasing_id ()) idf_partial = idf_id . select ([ \"tempID\" ] + list_of_cols ) assembler = VectorAssembler ( inputCols = list_of_cols , outputCol = \"list_of_cols_vector\" , handleInvalid = \"keep\" ) assembled_data = assembler . transform ( idf_partial ) if pre_existing_model : scalerModel = MinMaxScalerModel . load ( model_path + \"/normalization\" ) else : scaler = MinMaxScaler ( inputCol = \"list_of_cols_vector\" , outputCol = \"list_of_cols_scaled\" ) scalerModel = scaler . fit ( assembled_data ) if model_path != \"NA\" : scalerModel . write () . overwrite () . save ( model_path + \"/normalization\" ) scaledData = scalerModel . transform ( assembled_data ) def vector_to_array ( v ): return v . toArray () . tolist () f_vector_to_array = F . udf ( vector_to_array , T . ArrayType ( T . FloatType ())) odf_partial = scaledData . withColumn ( \"list_of_cols_array\" , f_vector_to_array ( \"list_of_cols_scaled\" ) ) . drop ( * [ \"list_of_cols_scaled\" , \"list_of_cols_vector\" ]) odf_schema = odf_partial . schema for i in list_of_cols : odf_schema = odf_schema . add ( T . StructField ( i + \"_scaled\" , T . FloatType ())) odf_partial = ( odf_partial . rdd . map ( lambda x : ( * x , * x [ \"list_of_cols_array\" ])) . toDF ( schema = odf_schema ) . drop ( \"list_of_cols_array\" ) ) odf = idf_id . join ( odf_partial . drop ( * list_of_cols ), \"tempID\" , \"left_outer\" ) . select ( idf . columns + [ ( F . when ( F . isnan ( F . col ( i + \"_scaled\" )), None ) . otherwise ( F . col ( i + \"_scaled\" ) ) ) . alias ( i + \"_scaled\" ) for i in list_of_cols ] ) if output_mode == \"replace\" : for i in list_of_cols : odf = odf . drop ( i ) . withColumnRenamed ( i + \"_scaled\" , i ) odf = odf . select ( idf . columns ) if print_impact : if output_mode == \"replace\" : output_cols = list_of_cols else : output_cols = [( i + \"_scaled\" ) for i in list_of_cols ] print ( \"Before: \" ) idf . select ( list_of_cols ) . describe () . show ( 5 , False ) print ( \"After: \" ) odf . select ( output_cols ) . describe () . show ( 5 , False ) return odf def outlier_categories ( spark, idf, list_of_cols='all', drop_cols=[], coverage=1.0, max_category=50, pre_existing_model=False, model_path='NA', output_mode='replace', print_impact=False) This function replaces less frequently seen values (called as outlier values in the current context) in a categorical column by 'others'. Outlier values can be defined in two ways \u2013 a) Max N categories, where N is used defined value. In this method, top N-1 frequently seen categories are considered and rest are clubbed under single category 'others'. or Alternatively, b) Coverage \u2013 top frequently seen categories are considered till it covers minimum N% of rows and rest lesser seen values are mapped to mapped to others. Even if the Coverage is less, maximum category constraint is given priority. Further, there is a caveat that when multiple categories have same rank. Then, number of categorical values can be more than max_category defined by the user. Parameters spark Spark Session idf Input Dataframe list_of_cols List of categorical columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all categorical columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) coverage Defines the minimum % of rows that will be mapped to actual category name and the rest to be mapped to others and takes value between 0 to 1. Coverage of 0.8 can be interpreted as top frequently seen categories are considered till it covers minimum 80% of rows and rest lesser seen values are mapped to others. (Default value = 1.0) max_category Even if coverage is less, only (max_category - 1) categories will be mapped to actual name and rest to others. Caveat is when multiple categories have same rank, then #categories can be more than max_category. (Default value = 50) pre_existing_model Boolean argument \u2013 True or False. True if the model with the outlier/other values for each attribute exists already to be used, False Otherwise. (Default value = False) model_path If pre_existing_model is True, this argument is path for the pre-saved model. If pre_existing_model is False, this field can be used for saving the model. Default \"NA\" means there is neither pre-existing model nor there is a need to save one. output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with transformed column. \u201cappend\u201d option append transformed column to the input dataset with a postfix \"_outliered\" e.g. column X is appended as X_outliered. (Default value = \"replace\") print_impact True, False This argument is to print before and after unique count of the transformed features (Default value = False) Returns DataFrame Transformed Dataframe Expand source code def outlier_categories ( spark , idf , list_of_cols = \"all\" , drop_cols = [], coverage = 1.0 , max_category = 50 , pre_existing_model = False , model_path = \"NA\" , output_mode = \"replace\" , print_impact = False , ): \"\"\" This function replaces less frequently seen values (called as outlier values in the current context) in a categorical column by 'others'. Outlier values can be defined in two ways \u2013 a) Max N categories, where N is used defined value. In this method, top N-1 frequently seen categories are considered and rest are clubbed under single category 'others'. or Alternatively, b) Coverage \u2013 top frequently seen categories are considered till it covers minimum N% of rows and rest lesser seen values are mapped to mapped to others. Even if the Coverage is less, maximum category constraint is given priority. Further, there is a caveat that when multiple categories have same rank. Then, number of categorical values can be more than max_category defined by the user. Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of categorical columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all categorical columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) coverage Defines the minimum % of rows that will be mapped to actual category name and the rest to be mapped to others and takes value between 0 to 1. Coverage of 0.8 can be interpreted as top frequently seen categories are considered till it covers minimum 80% of rows and rest lesser seen values are mapped to others. (Default value = 1.0) max_category Even if coverage is less, only (max_category - 1) categories will be mapped to actual name and rest to others. Caveat is when multiple categories have same rank, then #categories can be more than max_category. (Default value = 50) pre_existing_model Boolean argument \u2013 True or False. True if the model with the outlier/other values for each attribute exists already to be used, False Otherwise. (Default value = False) model_path If pre_existing_model is True, this argument is path for the pre-saved model. If pre_existing_model is False, this field can be used for saving the model. Default \"NA\" means there is neither pre-existing model nor there is a need to save one. output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with transformed column. \u201cappend\u201d option append transformed column to the input dataset with a postfix \"_outliered\" e.g. column X is appended as X_outliered. (Default value = \"replace\") print_impact True, False This argument is to print before and after unique count of the transformed features (Default value = False) Returns ------- DataFrame Transformed Dataframe \"\"\" cat_cols = attributeType_segregation ( idf )[ 1 ] if list_of_cols == \"all\" : list_of_cols = cat_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in drop_cols ])) if any ( x not in cat_cols for x in list_of_cols ): raise TypeError ( \"Invalid input for Column(s)\" ) if len ( list_of_cols ) == 0 : warnings . warn ( \"No Outlier Categories Computation - No categorical column(s) to transform\" ) return idf if ( coverage <= 0 ) | ( coverage > 1 ): raise TypeError ( \"Invalid input for Coverage Value\" ) if max_category < 2 : raise TypeError ( \"Invalid input for Maximum No. of Categories Allowed\" ) if output_mode not in ( \"replace\" , \"append\" ): raise TypeError ( \"Invalid input for output_mode\" ) if pre_existing_model : df_model = spark . read . csv ( model_path + \"/outlier_categories\" , header = True , inferSchema = True ) else : for index , i in enumerate ( list_of_cols ): window = Window . partitionBy () . orderBy ( F . desc ( \"count_pct\" )) df_cats = ( idf . groupBy ( i ) . count () . dropna () . withColumn ( \"count_pct\" , F . col ( \"count\" ) / F . sum ( \"count\" ) . over ( Window . partitionBy ()), ) . withColumn ( \"rank\" , F . rank () . over ( window )) . withColumn ( \"cumu\" , F . sum ( \"count_pct\" ) . over ( window . rowsBetween ( Window . unboundedPreceding , 0 ) ), ) . withColumn ( \"lag_cumu\" , F . lag ( \"cumu\" ) . over ( window )) . fillna ( 0 ) . where ( ~ (( F . col ( \"cumu\" ) >= coverage ) & ( F . col ( \"lag_cumu\" ) >= coverage ))) . where ( F . col ( \"rank\" ) <= ( max_category - 1 )) . select ( F . lit ( i ) . alias ( \"attribute\" ), F . col ( i ) . alias ( \"parameters\" )) ) if index == 0 : df_model = df_cats else : df_model = df_model . union ( df_cats ) odf = idf for i in list_of_cols : parameters = ( df_model . where ( F . col ( \"attribute\" ) == i ) . select ( \"parameters\" ) . rdd . flatMap ( lambda x : x ) . collect () ) if output_mode == \"replace\" : odf = odf . withColumn ( i , F . when ( ( F . col ( i ) . isin ( parameters )) | ( F . col ( i ) . isNull ()), F . col ( i ) ) . otherwise ( \"others\" ), ) else : odf = odf . withColumn ( i + \"_outliered\" , F . when ( ( F . col ( i ) . isin ( parameters )) | ( F . col ( i ) . isNull ()), F . col ( i ) ) . otherwise ( \"others\" ), ) if ( not pre_existing_model ) & ( model_path != \"NA\" ): df_model . repartition ( 1 ) . write . csv ( model_path + \"/outlier_categories\" , header = True , mode = \"overwrite\" ) if print_impact : if output_mode == \"replace\" : output_cols = list_of_cols else : output_cols = [( i + \"_outliered\" ) for i in list_of_cols ] uniqueCount_computation ( spark , idf , list_of_cols ) . select ( \"attribute\" , F . col ( \"unique_values\" ) . alias ( \"uniqueValues_before\" ) ) . show ( len ( list_of_cols ), False ) uniqueCount_computation ( spark , odf , output_cols ) . select ( \"attribute\" , F . col ( \"unique_values\" ) . alias ( \"uniqueValues_after\" ) ) . show ( len ( list_of_cols ), False ) return odf def z_standardization ( spark, idf, list_of_cols='all', drop_cols=[], pre_existing_model=False, model_path='NA', output_mode='replace', print_impact=False) Standardization is commonly used in data pre-processing process. z_standardization standardizes the selected attributes of an input dataframe by normalizing each attribute to have standard deviation of 1 and mean of 0. For each attribute, the standard deviation (s) and mean (u) are calculated and a sample x will be standardized into ( x-u)/s. If the standard deviation of an attribute is 0, it will be excluded in standardization and a warning will be shown. None values will be kept as None in the output dataframe. Parameters spark Spark Session idf Input Dataframe list_of_cols List of numerical columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all numerical columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) pre_existing_model Boolean argument \u2013 True or False. True if model files (Mean/stddev for each feature) exists already, False Otherwise (Default value = False) model_path If pre_existing_model is True, this argument is path for referring the pre-saved model. If pre_existing_model is False, this argument can be used for saving the model. Default \"NA\" means there is neither pre-existing model nor there is a need to save one. output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with transformed column. \u201cappend\u201d option append transformed column to the input dataset with a postfix \"_scaled\" e.g. column X is appended as X_scaled. (Default value = \"replace\") print_impact True, False (Default value = False) This argument is to print out the before and after descriptive statistics of rescaled columns. Returns DataFrame Rescaled Dataframe Expand source code def z_standardization ( spark , idf , list_of_cols = \"all\" , drop_cols = [], pre_existing_model = False , model_path = \"NA\" , output_mode = \"replace\" , print_impact = False , ): \"\"\" Standardization is commonly used in data pre-processing process. z_standardization standardizes the selected attributes of an input dataframe by normalizing each attribute to have standard deviation of 1 and mean of 0. For each attribute, the standard deviation (s) and mean (u) are calculated and a sample x will be standardized into ( x-u)/s. If the standard deviation of an attribute is 0, it will be excluded in standardization and a warning will be shown. None values will be kept as None in the output dataframe. Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of numerical columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all numerical columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) pre_existing_model Boolean argument \u2013 True or False. True if model files (Mean/stddev for each feature) exists already, False Otherwise (Default value = False) model_path If pre_existing_model is True, this argument is path for referring the pre-saved model. If pre_existing_model is False, this argument can be used for saving the model. Default \"NA\" means there is neither pre-existing model nor there is a need to save one. output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with transformed column. \u201cappend\u201d option append transformed column to the input dataset with a postfix \"_scaled\" e.g. column X is appended as X_scaled. (Default value = \"replace\") print_impact True, False (Default value = False) This argument is to print out the before and after descriptive statistics of rescaled columns. Returns ------- DataFrame Rescaled Dataframe \"\"\" num_cols = attributeType_segregation ( idf )[ 0 ] if list_of_cols == \"all\" : list_of_cols = num_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in drop_cols ])) if any ( x not in num_cols for x in list_of_cols ): raise TypeError ( \"Invalid input for Column(s)\" ) if len ( list_of_cols ) == 0 : warnings . warn ( \"No Standardization Performed - No numerical column(s) to transform\" ) return idf if output_mode not in ( \"replace\" , \"append\" ): raise TypeError ( \"Invalid input for output_mode\" ) parameters = [] excluded_cols = [] if pre_existing_model : df_model = spark . read . parquet ( model_path + \"/z_standardization\" ) for i in list_of_cols : mapped_value = ( df_model . where ( F . col ( \"feature\" ) == i ) . select ( \"parameters\" ) . rdd . flatMap ( lambda x : x ) . collect ()[ 0 ] ) parameters . append ( mapped_value ) else : for i in list_of_cols : mean , stddev = idf . select ( F . mean ( i ), F . stddev ( i )) . first () parameters . append ( [ float ( mean ) if mean else None , float ( stddev ) if stddev else None ] ) if stddev : if round ( stddev , 5 ) == 0.0 : excluded_cols . append ( i ) else : excluded_cols . append ( i ) if len ( excluded_cols ) > 0 : warnings . warn ( \"The following column(s) are excluded from standardization because the standard deviation is zero:\" + str ( excluded_cols ) ) odf = idf for index , i in enumerate ( list_of_cols ): if i not in excluded_cols : modify_col = ( i + \"_scaled\" ) if ( output_mode == \"append\" ) else i odf = odf . withColumn ( modify_col , ( F . col ( i ) - parameters [ index ][ 0 ]) / parameters [ index ][ 1 ] ) if ( not pre_existing_model ) & ( model_path != \"NA\" ): df_model = spark . createDataFrame ( zip ( list_of_cols , parameters ), schema = [ \"feature\" , \"parameters\" ] ) df_model . coalesce ( 1 ) . write . parquet ( model_path + \"/z_standardization\" , mode = \"overwrite\" ) if print_impact : if output_mode == \"replace\" : output_cols = list_of_cols else : output_cols = [ ( i + \"_scaled\" ) for i in list_of_cols if i not in excluded_cols ] print ( \"Before: \" ) idf . select ( list_of_cols ) . describe () . show ( 5 , False ) print ( \"After: \" ) odf . select ( output_cols ) . describe () . show ( 5 , False ) return odf","title":"<code>transformers</code>"},{"location":"api/data_transformer/transformers.html#transformers","text":"The data transformer module supports selected pre-processing & transformation functions, such as binning, encoding, scaling, imputation, to name a few, which are required for statistics generation and quality checks. Functions supported through this modules are listed below: attribute_binning monotonic_binning cat_to_num_unsupervised cat_to_num_supervised z_standardization IQR_standardization normalization imputation_MMM imputation_sklearn imputation_matrixFactorization auto_imputation autoencoder_latentFeatures PCA_latentFeatures feature_transformation boxcox_transformation outlier_categories expression_parser Expand source code # coding=utf-8 \"\"\" The data transformer module supports selected pre-processing & transformation functions, such as binning, encoding, scaling, imputation, to name a few, which are required for statistics generation and quality checks. Functions supported through this modules are listed below: - attribute_binning - monotonic_binning - cat_to_num_unsupervised - cat_to_num_supervised - z_standardization - IQR_standardization - normalization - imputation_MMM - imputation_sklearn - imputation_matrixFactorization - auto_imputation - autoencoder_latentFeatures - PCA_latentFeatures - feature_transformation - boxcox_transformation - outlier_categories - expression_parser \"\"\" import copy import os import pickle import random import subprocess import tempfile import warnings from itertools import chain import numpy as np import pandas as pd import pyspark from packaging import version from scipy import stats if version . parse ( pyspark . __version__ ) < version . parse ( \"3.0.0\" ): from pyspark.ml.feature import OneHotEncoderEstimator as OneHotEncoder else : from pyspark.ml.feature import OneHotEncoder from pyspark.sql import functions as F from pyspark.sql import types as T from pyspark.sql.window import Window from pyspark.mllib.stat import Statistics from pyspark.ml.feature import StringIndexerModel from pyspark.ml.recommendation import ALS from pyspark.ml.evaluation import RegressionEvaluator from pyspark.ml.feature import Imputer , ImputerModel , StringIndexer , IndexToString from pyspark.ml.feature import ( VectorAssembler , MinMaxScaler , MinMaxScalerModel , PCA , PCAModel , ) from pyspark.ml.linalg import DenseVector from anovos.data_analyzer.stats_generator import ( missingCount_computation , uniqueCount_computation , ) from anovos.data_ingest.data_ingest import read_dataset , recast_column from anovos.shared.utils import attributeType_segregation , get_dtype from ..shared.utils import platform_root_path # enable_iterative_imputer is prequisite for importing IterativeImputer # check the following issue for more details https://github.com/scikit-learn/scikit-learn/issues/16833 from sklearn.experimental import enable_iterative_imputer # noqa from sklearn.impute import KNNImputer , IterativeImputer import tensorflow from tensorflow.keras.models import load_model , Model from tensorflow.keras.layers import Dense , Input , BatchNormalization , LeakyReLU def attribute_binning ( spark , idf , list_of_cols = \"all\" , drop_cols = [], method_type = \"equal_range\" , bin_size = 10 , bin_dtype = \"numerical\" , pre_existing_model = False , model_path = \"NA\" , output_mode = \"replace\" , print_impact = False , ): \"\"\" Attribute binning (or discretization) is a method of numerical attribute into discrete (integer or categorical values) using pre-defined number of bins. This data pre-processing technique is used to reduce the effects of minor observation errors. Also, Binning introduces non-linearity and tends to improve the performance of the model. In this function, we are focussing on unsupervised way of binning i.e. without taking the target variable into account - Equal Range Binning, Equal Frequency Binning. In Equal Range method, each bin is of equal size/width and computed as: w = max- min / no. of bins *bins cutoff=[min, min+w,min+2w\u2026..,max-w,max]* whereas in Equal Frequency binning method, bins are created in such a way that each bin has equal no. of rows, though the width of bins may vary from each other. w = 1 / no. of bins *bins cutoff=[min, wthpctile, 2wthpctile\u2026.,max ]* Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of numerical columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all numerical columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) method_type \"equal_frequency\", \"equal_range\". In \"equal_range\" method, each bin is of equal size/width and in \"equal_frequency\", each bin has equal no. of rows, though the width of bins may vary. (Default value = \"equal_range\") bin_size Number of bins. (Default value = 10) bin_dtype \"numerical\", \"categorical\". With \"numerical\" option, original value is replaced with an Integer (1,2,\u2026) and with \"categorical\" option, original replaced with a string describing min and max value allowed in the bin (\"minval-maxval\"). (Default value = \"numerical\") pre_existing_model Boolean argument \u2013 True or False. True if binning model exists already, False Otherwise. (Default value = False) model_path If pre_existing_model is True, this argument is path for referring the pre-saved model. If pre_existing_model is False, this argument can be used for saving the model. Default \"NA\" means there is neither pre-existing model nor there is a need to save one. output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with transformed column. \u201cappend\u201d option append transformed column to the input dataset with a postfix \"_binned\" e.g. column X is appended as X_binned. (Default value = \"replace\") print_impact True, False (Default value = False) This argument is to print the number of categories generated for each attribute (may or may be not same as bin_size) Returns ------- DataFrame Binned Dataframe \"\"\" num_cols = attributeType_segregation ( idf )[ 0 ] if list_of_cols == \"all\" : list_of_cols = num_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in drop_cols ])) if any ( x not in num_cols for x in list_of_cols ): raise TypeError ( \"Invalid input for Column(s)\" ) if len ( list_of_cols ) == 0 : warnings . warn ( \"No Binning Performed - No numerical column(s) to transform\" ) return idf if method_type not in ( \"equal_frequency\" , \"equal_range\" ): raise TypeError ( \"Invalid input for method_type\" ) if bin_size < 2 : raise TypeError ( \"Invalid input for bin_size\" ) if output_mode not in ( \"replace\" , \"append\" ): raise TypeError ( \"Invalid input for output_mode\" ) if pre_existing_model : df_model = spark . read . parquet ( model_path + \"/attribute_binning\" ) bin_cutoffs = [] for i in list_of_cols : mapped_value = ( df_model . where ( F . col ( \"attribute\" ) == i ) . select ( \"parameters\" ) . rdd . flatMap ( lambda x : x ) . collect ()[ 0 ] ) bin_cutoffs . append ( mapped_value ) else : if method_type == \"equal_frequency\" : pctile_width = 1 / bin_size pctile_cutoff = [] for j in range ( 1 , bin_size ): pctile_cutoff . append ( j * pctile_width ) bin_cutoffs = idf . approxQuantile ( list_of_cols , pctile_cutoff , 0.01 ) else : bin_cutoffs = [] for i in list_of_cols : max_val = ( idf . select ( F . col ( i )) . groupBy () . max () . rdd . flatMap ( lambda x : x ) . collect () + [ None ] )[ 0 ] min_val = ( idf . select ( F . col ( i )) . groupBy () . min () . rdd . flatMap ( lambda x : x ) . collect () + [ None ] )[ 0 ] bin_cutoff = [] if max_val is not None : bin_width = ( max_val - min_val ) / bin_size for j in range ( 1 , bin_size ): bin_cutoff . append ( min_val + j * bin_width ) bin_cutoffs . append ( bin_cutoff ) if model_path != \"NA\" : df_model = spark . createDataFrame ( zip ( list_of_cols , bin_cutoffs ), schema = [ \"attribute\" , \"parameters\" ] ) df_model . write . parquet ( model_path + \"/attribute_binning\" , mode = \"overwrite\" ) def bucket_label ( value , index ): if value is None : return None for i in range ( 0 , len ( bin_cutoffs [ index ])): if value <= bin_cutoffs [ index ][ i ]: if bin_dtype == \"numerical\" : return i + 1 else : if i == 0 : return \"<= \" + str ( round ( bin_cutoffs [ index ][ i ], 4 )) else : return ( str ( round ( bin_cutoffs [ index ][ i - 1 ], 4 )) + \"-\" + str ( round ( bin_cutoffs [ index ][ i ], 4 )) ) else : next if bin_dtype == \"numerical\" : return len ( bin_cutoffs [ 0 ]) + 1 else : return \"> \" + str ( round ( bin_cutoffs [ index ][ len ( bin_cutoffs [ 0 ]) - 1 ], 4 )) if bin_dtype == \"numerical\" : f_bucket_label = F . udf ( bucket_label , T . IntegerType ()) else : f_bucket_label = F . udf ( bucket_label , T . StringType ()) odf = idf for idx , i in enumerate ( list_of_cols ): odf = odf . withColumn ( i + \"_binned\" , f_bucket_label ( F . col ( i ), F . lit ( idx ))) if idx % 5 == 0 : odf . persist ( pyspark . StorageLevel . MEMORY_AND_DISK ) . count () if output_mode == \"replace\" : for col in list_of_cols : odf = odf . drop ( col ) . withColumnRenamed ( col + \"_binned\" , col ) if print_impact : if output_mode == \"replace\" : output_cols = list_of_cols else : output_cols = [( i + \"_binned\" ) for i in list_of_cols ] uniqueCount_computation ( spark , odf , output_cols ) . show ( len ( output_cols ), False ) return odf def monotonic_binning ( spark , idf , list_of_cols = \"all\" , drop_cols = [], label_col = \"label\" , event_label = 1 , bin_method = \"equal_range\" , bin_size = 10 , bin_dtype = \"numerical\" , output_mode = \"replace\" , ): \"\"\" This function constitutes supervised way of binning the numerical attribute into discrete (integer or categorical values) attribute. Instead of pre-defined fixed number of bins, number of bins are dynamically computed to ensure the monotonic nature of bins i.e. % event should increase or decrease with the bin. Monotonic nature of bins is evaluated by looking at spearman rank correlation, which should be either +1 or -1, between the bin index and % event. In case, the monotonic nature is not attained, user defined fixed number of bins are used for the binning. Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of numerical columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all numerical columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) label_col Label/Target column (Default value = \"label\") event_label Value of (positive) event (i.e label 1) (Default value = 1) bin_method \"equal_frequency\", \"equal_range\". In \"equal_range\" method, each bin is of equal size/width and in \"equal_frequency\", each bin has equal no. of rows, though the width of bins may vary. (Default value = \"equal_range\") bin_size Default number of bins in case monotonicity is not achieved. bin_dtype \"numerical\", \"categorical\". With \"numerical\" option, original value is replaced with an Integer (1,2,\u2026) and with \"categorical\" option, original replaced with a string describing min and max value allowed in the bin (\"minval-maxval\"). (Default value = \"numerical\") output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with transformed column. \u201cappend\u201d option append transformed column to the input dataset with a postfix \"_binned\" e.g. column X is appended as X_binned. (Default value = \"replace\") Returns ------- DataFrame Binned Dataframe \"\"\" num_cols = attributeType_segregation ( idf )[ 0 ] if list_of_cols == \"all\" : list_of_cols = num_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in ( drop_cols + [ label_col ])]) ) if any ( x not in num_cols for x in list_of_cols ): raise TypeError ( \"Invalid input for Column(s)\" ) attribute_binning ( spark , idf , list_of_cols = \"all\" , drop_cols = [], method_type = \"equal_range\" , bin_size = 10 , pre_existing_model = False , model_path = \"NA\" , output_mode = \"replace\" , print_impact = False , ) odf = idf for col in list_of_cols : n = 20 r = 0 while n > 2 : tmp = ( attribute_binning ( spark , idf , [ col ], drop_cols = [], method_type = bin_method , bin_size = n , output_mode = \"append\" , ) . select ( label_col , col , col + \"_binned\" ) . withColumn ( label_col , F . when ( F . col ( label_col ) == event_label , 1 ) . otherwise ( 0 ) ) . groupBy ( col + \"_binned\" ) . agg ( F . avg ( col ) . alias ( \"mean_val\" ), F . avg ( label_col ) . alias ( \"mean_label\" )) . dropna () ) r , p = stats . spearmanr ( tmp . toPandas ()[[ \"mean_val\" ]], tmp . toPandas ()[[ \"mean_label\" ]] ) if r == 1.0 : odf = attribute_binning ( spark , odf , [ col ], drop_cols = [], method_type = bin_method , bin_size = n , bin_dtype = bin_dtype , output_mode = output_mode , ) break n = n - 1 r = 0 if r < 1.0 : odf = attribute_binning ( spark , odf , [ col ], drop_cols = [], method_type = bin_method , bin_size = bin_size , bin_dtype = bin_dtype , output_mode = output_mode , ) return odf def cat_to_num_unsupervised ( spark , idf , list_of_cols = \"all\" , drop_cols = [], method_type = 1 , index_order = \"frequencyDesc\" , cardinality_threshold = 100 , pre_existing_model = False , model_path = \"NA\" , stats_unique = {}, output_mode = \"replace\" , print_impact = False , ): \"\"\" This is unsupervised method of converting a categorical attribute into numerical attribute(s). This is among the most important transformations required for any modelling exercise, as most of the machine learning algorithms cannot process categorical values. It covers two popular encoding techniques \u2013 label encoding & one-hot encoding. In label encoding, each categorical value is assigned a unique integer based on alphabetical or frequency ordering (both ascending & descending options are available \u2013 can be selected by index_order argument). One of the pitfalls of using this technique is that the model may learn some spurious relationship, which doesn't exist or might not make any logical sense in the real world settings. In one-hot encoding, every unique value in the attribute will be added as a feature in a form of dummy/binary attribute. However, using this method on high cardinality attributes can further aggravate the dimensionality issue. Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of categorical columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all categorical columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) method_type 1 for Label Encoding or 0 for One hot encoding. In label encoding, each categorical value is assigned a unique integer based on alphabetical or frequency ordering (both ascending & descending options are available that can be selected by index_order argument). In one-hot encoding, every unique value in the column will be added in a form of dummy/binary column. (Default value = 1) index_order \"frequencyDesc\", \"frequencyAsc\", \"alphabetDesc\", \"alphabetAsc\". Valid only for Label Encoding method_type. (Default value = \"frequencyDesc\") cardinality_threshold Defines threshold to skip columns with higher cardinality values from encoding (Warning is issued). (Default value = 100) pre_existing_model Boolean argument \u2013 True or False. True if encoding model exists already, False Otherwise. (Default value = False) model_path If pre_existing_model is True, this argument is path for referring the pre-saved model. If pre_existing_model is False, this argument can be used for saving the model. Default \"NA\" means there is neither pre existing model nor there is a need to save one. stats_unique Takes arguments for read_dataset (data_ingest module) function in a dictionary format to read pre-saved statistics on unique value count i.e. if measures_of_cardinality or uniqueCount_computation (data_analyzer.stats_generator module) has been computed & saved before. (Default value = {}) output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with transformed column. \u201cappend\u201d option append transformed column to the input dataset with a postfix \"_index\" for label encoding e.g. column X is appended as X_index, or a postfix \"_{n}\" for one hot encoding, n varies from 0 to unique value count e.g. column X is appended as X_0, X_1, X_2 (n = 3 i.e. no. of unique values for X). (Default value = \"replace\") print_impact True, False (Default value = False) This argument is to print out the change in schema (one hot encoding) or descriptive statistics (label encoding) Returns ------- DataFrame Encoded Dataframe \"\"\" cat_cols = attributeType_segregation ( idf )[ 1 ] if list_of_cols == \"all\" : list_of_cols = cat_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] if any ( x not in cat_cols for x in list_of_cols ): raise TypeError ( \"Invalid input for Column(s)\" ) if method_type not in ( 0 , 1 ): raise TypeError ( \"Invalid input for method_type\" ) if index_order not in ( \"frequencyDesc\" , \"frequencyAsc\" , \"alphabetDesc\" , \"alphabetAsc\" , ): raise TypeError ( \"Invalid input for Encoding Index Order\" ) if output_mode not in ( \"replace\" , \"append\" ): raise TypeError ( \"Invalid input for output_mode\" ) skip_cols = [] if method_type == 0 : if stats_unique == {}: skip_cols = ( uniqueCount_computation ( spark , idf , list_of_cols ) . where ( F . col ( \"unique_values\" ) > cardinality_threshold ) . select ( \"attribute\" ) . rdd . flatMap ( lambda x : x ) . collect () ) else : skip_cols = ( read_dataset ( spark , ** stats_unique ) . where ( F . col ( \"unique_values\" ) > cardinality_threshold ) . select ( \"attribute\" ) . rdd . flatMap ( lambda x : x ) . collect () ) skip_cols = list ( set ([ e for e in skip_cols if e in list_of_cols and e not in drop_cols ]) ) if skip_cols : warnings . warn ( \"Columns dropped from one-hot encoding due to high cardinality: \" + \",\" . join ( skip_cols ) ) list_of_cols = list ( set ([ e for e in list_of_cols if e not in drop_cols + skip_cols ]) ) if len ( list_of_cols ) == 0 : warnings . warn ( \"No Encoding Computation - No categorical column(s) to transform\" ) return idf list_of_cols_vec = [] list_of_cols_idx = [] for i in list_of_cols : list_of_cols_vec . append ( i + \"_vec\" ) list_of_cols_idx . append ( i + \"_index\" ) idf_id = idf . withColumn ( \"tempID\" , F . monotonically_increasing_id ()) idf_indexed = idf_id . select ([ \"tempID\" ] + list_of_cols ) if version . parse ( pyspark . __version__ ) < version . parse ( \"3.0.0\" ): for idx , i in enumerate ( list_of_cols ): if pre_existing_model : indexerModel = StringIndexerModel . load ( model_path + \"/cat_to_num_unsupervised/indexer-model/\" + i ) else : stringIndexer = StringIndexer ( inputCol = i , outputCol = i + \"_index\" , stringOrderType = index_order , handleInvalid = \"keep\" , ) indexerModel = stringIndexer . fit ( idf . select ( i )) if model_path != \"NA\" : indexerModel . write () . overwrite () . save ( model_path + \"/cat_to_num_unsupervised/indexer-model/\" + i ) idf_indexed = indexerModel . transform ( idf_indexed ) if idx % 5 == 0 : idf_indexed . persist ( pyspark . StorageLevel . MEMORY_AND_DISK ) . count () else : if pre_existing_model : indexerModel = StringIndexerModel . load ( model_path + \"/cat_to_num_unsupervised/indexer\" ) else : stringIndexer = StringIndexer ( inputCols = list_of_cols , outputCols = list_of_cols_idx , stringOrderType = index_order , handleInvalid = \"keep\" , ) indexerModel = stringIndexer . fit ( idf_indexed ) if model_path != \"NA\" : indexerModel . write () . overwrite () . save ( model_path + \"/cat_to_num_unsupervised/indexer\" ) idf_indexed = indexerModel . transform ( idf_indexed ) odf_indexed = idf_id . join ( idf_indexed . drop ( * list_of_cols ), \"tempID\" , \"left_outer\" ) . drop ( \"tempID\" ) odf_indexed . persist ( pyspark . StorageLevel . MEMORY_AND_DISK ) . count () if method_type == 0 : if pre_existing_model : encoder = OneHotEncoder . load ( model_path + \"/cat_to_num_unsupervised/encoder\" ) else : encoder = OneHotEncoder ( inputCols = list_of_cols_idx , outputCols = list_of_cols_vec , handleInvalid = \"keep\" , ) if model_path != \"NA\" : encoder . write () . overwrite () . save ( model_path + \"/cat_to_num_unsupervised/encoder\" ) odf_encoded = encoder . fit ( odf_indexed ) . transform ( odf_indexed ) odf = odf_encoded def vector_to_array ( v ): v = DenseVector ( v ) new_array = list ([ int ( x ) for x in v ]) return new_array f_vector_to_array = F . udf ( vector_to_array , T . ArrayType ( T . IntegerType ())) odf_sample = odf . take ( 1 ) for i in list_of_cols : uniq_cats = odf_sample [ 0 ] . asDict ()[ i + \"_vec\" ] . size odf_schema = odf . schema . add ( T . StructField ( \"tmp\" , T . ArrayType ( T . IntegerType ())) ) for j in range ( 0 , uniq_cats ): odf_schema = odf_schema . add ( T . StructField ( i + \"_\" + str ( j ), T . IntegerType ()) ) odf = ( odf . withColumn ( \"tmp\" , f_vector_to_array ( i + \"_vec\" )) . rdd . map ( lambda x : ( * x , * x [ \"tmp\" ])) . toDF ( schema = odf_schema ) ) if output_mode == \"replace\" : odf = odf . drop ( i , i + \"_vec\" , i + \"_index\" , \"tmp\" ) else : odf = odf . drop ( i + \"_vec\" , i + \"_index\" , \"tmp\" ) else : odf = odf_indexed for i in list_of_cols : odf = odf . withColumn ( i + \"_index\" , F . when ( F . col ( i ) . isNull (), None ) . otherwise ( F . col ( i + \"_index\" ) . cast ( T . IntegerType ()) ), ) if output_mode == \"replace\" : for i in list_of_cols : odf = odf . drop ( i ) . withColumnRenamed ( i + \"_index\" , i ) odf = odf . select ( idf . columns ) if print_impact and method_type == 1 : print ( \"Before\" ) idf . describe () . where ( F . col ( \"summary\" ) . isin ( \"count\" , \"min\" , \"max\" )) . show ( 3 , False ) print ( \"After\" ) odf . describe () . where ( F . col ( \"summary\" ) . isin ( \"count\" , \"min\" , \"max\" )) . show ( 3 , False ) if print_impact and method_type == 0 : print ( \"Before\" ) idf . printSchema () print ( \"After\" ) odf . printSchema () return odf def cat_to_num_supervised ( spark , idf , list_of_cols = \"all\" , drop_cols = [], label_col = \"label\" , event_label = 1 , pre_existing_model = False , model_path = \"NA\" , output_mode = \"replace\" , run_type = \"local\" , print_impact = False , ): \"\"\" This is a supervised method to convert a categorical attribute into a numerical attribute. It takes a label/target column to indicate whether the event is positive or negative. For each column, the positive event rate for each categorical value is used as the encoded numerical value. For example, there are 3 distinct values in a categorical attribute X: X1, X2 and X3. Within the input dataframe, there are - 15 positive events and 5 negative events with X==X1; - 10 positive events and 40 negative events with X==X2; - 20 positive events and 20 negative events with X==X3. Thus, value X1 is mapped to 15/(15+5) = 0.75, value X2 is mapped to 10/(10+40) = 0.2 and value X3 is mapped to 20/(20+20) = 0.5. This mapping will be applied to all values in attribute X. This encoding method can be helpful in avoiding creating too many dummy variables which may cause dimensionality issue and it also works with categorical attributes without an order or rank. Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of categorical columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all categorical columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) label_col Label/Target column (Default value = \"label\") event_label Value of (positive) event (i.e label 1) (Default value = 1) pre_existing_model Boolean argument \u2013 True or False. True if model (original and mapped numerical value for each column) exists already, False Otherwise. (Default value = False) model_path If pre_existing_model is True, this argument is path for referring the pre-saved model. If pre_existing_model is False, this argument can be used for saving the model. Default \"NA\" is used to save the model in \"intermediate_data/\" folder for optimization purpose. output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with transformed column. \u201cappend\u201d option append transformed column to the input dataset with a postfix \"_encoded\" e.g. column X is appended as X_encoded. (Default value = \"replace\") run_type \"local\", \"emr\", \"databricks\" (Default value = \"local\") print_impact True, False (Default value = False) This argument is to print out the descriptive statistics of encoded columns. Returns ------- DataFrame Encoded Dataframe \"\"\" cat_cols = attributeType_segregation ( idf )[ 1 ] if list_of_cols == \"all\" : list_of_cols = cat_cols elif isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if ( e not in drop_cols ) & ( e != label_col )]) ) if any ( x not in cat_cols for x in list_of_cols ): raise TypeError ( \"Invalid input for Column(s)\" ) if len ( list_of_cols ) == 0 : warnings . warn ( \"No Categorical Encoding - No categorical column(s) to transform\" ) return idf if label_col not in idf . columns : raise TypeError ( \"Invalid input for Label Column\" ) idf_id = idf . withColumn ( \"tempID\" , F . monotonically_increasing_id ()) odf_partial = idf_id . select ([ \"tempID\" ] + list_of_cols ) for index , i in enumerate ( list_of_cols ): if pre_existing_model : df_tmp = spark . read . csv ( model_path + \"/cat_to_num_supervised/\" + i , header = True , inferSchema = True , ) else : df_tmp = ( idf . withColumn ( label_col , F . when ( F . col ( label_col ) == event_label , \"1\" ) . otherwise ( \"0\" ), ) . groupBy ( i ) . pivot ( label_col ) . count () . fillna ( 0 ) . withColumn ( i + \"_encoded\" , F . round ( F . col ( \"1\" ) / ( F . col ( \"1\" ) + F . col ( \"0\" )), 4 ) ) . drop ( * [ \"1\" , \"0\" ]) ) if run_type in list ( platform_root_path . keys ()): root_path = platform_root_path [ run_type ] else : root_path = \"\" if model_path == \"NA\" : model_path = root_path + \"intermediate_data\" df_tmp . coalesce ( 1 ) . write . csv ( model_path + \"/cat_to_num_supervised/\" + i , header = True , mode = \"overwrite\" , ) df_tmp = spark . read . csv ( model_path + \"/cat_to_num_supervised/\" + i , header = True , inferSchema = True , ) if df_tmp . count () > 1 : odf_partial = odf_partial . join ( df_tmp , i , \"left_outer\" ) else : odf_partial = odf_partial . crossJoin ( df_tmp ) odf = idf_id . join ( odf_partial . drop ( * list_of_cols ), \"tempID\" , \"left_outer\" ) . drop ( \"tempID\" ) if output_mode == \"replace\" : for i in list_of_cols : odf = odf . drop ( i ) . withColumnRenamed ( i + \"_encoded\" , i ) odf = odf . select ( idf . columns ) if print_impact : if output_mode == \"replace\" : output_cols = list_of_cols else : output_cols = [( i + \"_encoded\" ) for i in list_of_cols ] print ( \"Before: \" ) idf . select ( list_of_cols ) . describe () . where ( F . col ( \"summary\" ) . isin ( \"count\" , \"min\" , \"max\" ) ) . show ( 3 , False ) print ( \"After: \" ) odf . select ( output_cols ) . describe () . where ( F . col ( \"summary\" ) . isin ( \"count\" , \"min\" , \"max\" ) ) . show ( 3 , False ) return odf def z_standardization ( spark , idf , list_of_cols = \"all\" , drop_cols = [], pre_existing_model = False , model_path = \"NA\" , output_mode = \"replace\" , print_impact = False , ): \"\"\" Standardization is commonly used in data pre-processing process. z_standardization standardizes the selected attributes of an input dataframe by normalizing each attribute to have standard deviation of 1 and mean of 0. For each attribute, the standard deviation (s) and mean (u) are calculated and a sample x will be standardized into ( x-u)/s. If the standard deviation of an attribute is 0, it will be excluded in standardization and a warning will be shown. None values will be kept as None in the output dataframe. Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of numerical columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all numerical columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) pre_existing_model Boolean argument \u2013 True or False. True if model files (Mean/stddev for each feature) exists already, False Otherwise (Default value = False) model_path If pre_existing_model is True, this argument is path for referring the pre-saved model. If pre_existing_model is False, this argument can be used for saving the model. Default \"NA\" means there is neither pre-existing model nor there is a need to save one. output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with transformed column. \u201cappend\u201d option append transformed column to the input dataset with a postfix \"_scaled\" e.g. column X is appended as X_scaled. (Default value = \"replace\") print_impact True, False (Default value = False) This argument is to print out the before and after descriptive statistics of rescaled columns. Returns ------- DataFrame Rescaled Dataframe \"\"\" num_cols = attributeType_segregation ( idf )[ 0 ] if list_of_cols == \"all\" : list_of_cols = num_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in drop_cols ])) if any ( x not in num_cols for x in list_of_cols ): raise TypeError ( \"Invalid input for Column(s)\" ) if len ( list_of_cols ) == 0 : warnings . warn ( \"No Standardization Performed - No numerical column(s) to transform\" ) return idf if output_mode not in ( \"replace\" , \"append\" ): raise TypeError ( \"Invalid input for output_mode\" ) parameters = [] excluded_cols = [] if pre_existing_model : df_model = spark . read . parquet ( model_path + \"/z_standardization\" ) for i in list_of_cols : mapped_value = ( df_model . where ( F . col ( \"feature\" ) == i ) . select ( \"parameters\" ) . rdd . flatMap ( lambda x : x ) . collect ()[ 0 ] ) parameters . append ( mapped_value ) else : for i in list_of_cols : mean , stddev = idf . select ( F . mean ( i ), F . stddev ( i )) . first () parameters . append ( [ float ( mean ) if mean else None , float ( stddev ) if stddev else None ] ) if stddev : if round ( stddev , 5 ) == 0.0 : excluded_cols . append ( i ) else : excluded_cols . append ( i ) if len ( excluded_cols ) > 0 : warnings . warn ( \"The following column(s) are excluded from standardization because the standard deviation is zero:\" + str ( excluded_cols ) ) odf = idf for index , i in enumerate ( list_of_cols ): if i not in excluded_cols : modify_col = ( i + \"_scaled\" ) if ( output_mode == \"append\" ) else i odf = odf . withColumn ( modify_col , ( F . col ( i ) - parameters [ index ][ 0 ]) / parameters [ index ][ 1 ] ) if ( not pre_existing_model ) & ( model_path != \"NA\" ): df_model = spark . createDataFrame ( zip ( list_of_cols , parameters ), schema = [ \"feature\" , \"parameters\" ] ) df_model . coalesce ( 1 ) . write . parquet ( model_path + \"/z_standardization\" , mode = \"overwrite\" ) if print_impact : if output_mode == \"replace\" : output_cols = list_of_cols else : output_cols = [ ( i + \"_scaled\" ) for i in list_of_cols if i not in excluded_cols ] print ( \"Before: \" ) idf . select ( list_of_cols ) . describe () . show ( 5 , False ) print ( \"After: \" ) odf . select ( output_cols ) . describe () . show ( 5 , False ) return odf def IQR_standardization ( spark , idf , list_of_cols = \"all\" , drop_cols = [], pre_existing_model = False , model_path = \"NA\" , output_mode = \"replace\" , print_impact = False , ): \"\"\" Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of numerical columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all numerical columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) pre_existing_model Boolean argument \u2013 True or False. True if model files (25/50/75 percentile for each feature) exists already, False Otherwise (Default value = False) model_path If pre_existing_model is True, this argument is path for referring the pre-saved model. If pre_existing_model is False, this argument can be used for saving the model. Default \"NA\" means there is neither pre-existing model nor there is a need to save one. output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with transformed column. \u201cappend\u201d option append transformed column to the input dataset with a postfix \"_scaled\" e.g. column X is appended as X_scaled. (Default value = \"replace\") print_impact True, False (Default value = False) This argument is to print out the before and after descriptive statistics of rescaled columns. Returns ------- DataFrame Rescaled Dataframe \"\"\" num_cols = attributeType_segregation ( idf )[ 0 ] if list_of_cols == \"all\" : list_of_cols = num_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in drop_cols ])) if any ( x not in num_cols for x in list_of_cols ): raise TypeError ( \"Invalid input for Column(s)\" ) if len ( list_of_cols ) == 0 : warnings . warn ( \"No Standardization Performed - No numerical column(s) to transform\" ) return idf if output_mode not in ( \"replace\" , \"append\" ): raise TypeError ( \"Invalid input for output_mode\" ) if pre_existing_model : df_model = spark . read . parquet ( model_path + \"/IQR_standardization\" ) parameters = [] for i in list_of_cols : mapped_value = ( df_model . where ( F . col ( \"feature\" ) == i ) . select ( \"parameters\" ) . rdd . flatMap ( lambda x : x ) . collect ()[ 0 ] ) parameters . append ( mapped_value ) else : parameters = idf . approxQuantile ( list_of_cols , [ 0.25 , 0.5 , 0.75 ], 0.01 ) excluded_cols = [] for i , param in zip ( list_of_cols , parameters ): if len ( param ) > 0 : if round ( param [ 0 ], 5 ) == round ( param [ 2 ], 5 ): excluded_cols . append ( i ) else : excluded_cols . append ( i ) if len ( excluded_cols ) > 0 : warnings . warn ( \"The following column(s) are excluded from standardization because the 75th and 25th percentiles are the same:\" + str ( excluded_cols ) ) odf = idf for index , i in enumerate ( list_of_cols ): if i not in excluded_cols : modify_col = ( i + \"_scaled\" ) if ( output_mode == \"append\" ) else i odf = odf . withColumn ( modify_col , ( F . col ( i ) - parameters [ index ][ 1 ]) / ( parameters [ index ][ 2 ] - parameters [ index ][ 0 ]), ) if ( not pre_existing_model ) & ( model_path != \"NA\" ): df_model = spark . createDataFrame ( zip ( list_of_cols , parameters ), schema = [ \"feature\" , \"parameters\" ] ) df_model . coalesce ( 1 ) . write . parquet ( model_path + \"/IQR_standardization\" , mode = \"overwrite\" ) if print_impact : if output_mode == \"replace\" : output_cols = list_of_cols else : output_cols = [ ( i + \"_scaled\" ) for i in list_of_cols if i not in excluded_cols ] print ( \"Before: \" ) idf . select ( list_of_cols ) . describe () . show ( 5 , False ) print ( \"After: \" ) odf . select ( output_cols ) . describe () . show ( 5 , False ) return odf def normalization ( idf , list_of_cols = \"all\" , drop_cols = [], pre_existing_model = False , model_path = \"NA\" , output_mode = \"replace\" , print_impact = False , ): \"\"\" Parameters ---------- idf Input Dataframe list_of_cols List of numerical columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all numerical columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) pre_existing_model Boolean argument \u2013 True or False. True if normalization/scalar model exists already, False Otherwise (Default value = False) model_path If pre_existing_model is True, this argument is path for referring the pre-saved model. If pre_existing_model is False, this argument can be used for saving the model. Default \"NA\" means there is neither pre-existing model nor there is a need to save one. output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with transformed column. \u201cappend\u201d option append transformed column to the input dataset with a postfix \"_scaled\" e.g. column X is appended as X_scaled. (Default value = \"replace\") print_impact True, False (Default value = False) This argument is to print out before and after descriptive statistics of rescaled columns. Returns ------- DataFrame Rescaled Dataframe \"\"\" num_cols = attributeType_segregation ( idf )[ 0 ] if list_of_cols == \"all\" : list_of_cols = num_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in drop_cols ])) if any ( x not in num_cols for x in list_of_cols ): raise TypeError ( \"Invalid input for Column(s)\" ) if len ( list_of_cols ) == 0 : warnings . warn ( \"No Normalization Performed - No numerical column(s) to transform\" ) return idf if output_mode not in ( \"replace\" , \"append\" ): raise TypeError ( \"Invalid input for output_mode\" ) idf_id = idf . withColumn ( \"tempID\" , F . monotonically_increasing_id ()) idf_partial = idf_id . select ([ \"tempID\" ] + list_of_cols ) assembler = VectorAssembler ( inputCols = list_of_cols , outputCol = \"list_of_cols_vector\" , handleInvalid = \"keep\" ) assembled_data = assembler . transform ( idf_partial ) if pre_existing_model : scalerModel = MinMaxScalerModel . load ( model_path + \"/normalization\" ) else : scaler = MinMaxScaler ( inputCol = \"list_of_cols_vector\" , outputCol = \"list_of_cols_scaled\" ) scalerModel = scaler . fit ( assembled_data ) if model_path != \"NA\" : scalerModel . write () . overwrite () . save ( model_path + \"/normalization\" ) scaledData = scalerModel . transform ( assembled_data ) def vector_to_array ( v ): return v . toArray () . tolist () f_vector_to_array = F . udf ( vector_to_array , T . ArrayType ( T . FloatType ())) odf_partial = scaledData . withColumn ( \"list_of_cols_array\" , f_vector_to_array ( \"list_of_cols_scaled\" ) ) . drop ( * [ \"list_of_cols_scaled\" , \"list_of_cols_vector\" ]) odf_schema = odf_partial . schema for i in list_of_cols : odf_schema = odf_schema . add ( T . StructField ( i + \"_scaled\" , T . FloatType ())) odf_partial = ( odf_partial . rdd . map ( lambda x : ( * x , * x [ \"list_of_cols_array\" ])) . toDF ( schema = odf_schema ) . drop ( \"list_of_cols_array\" ) ) odf = idf_id . join ( odf_partial . drop ( * list_of_cols ), \"tempID\" , \"left_outer\" ) . select ( idf . columns + [ ( F . when ( F . isnan ( F . col ( i + \"_scaled\" )), None ) . otherwise ( F . col ( i + \"_scaled\" ) ) ) . alias ( i + \"_scaled\" ) for i in list_of_cols ] ) if output_mode == \"replace\" : for i in list_of_cols : odf = odf . drop ( i ) . withColumnRenamed ( i + \"_scaled\" , i ) odf = odf . select ( idf . columns ) if print_impact : if output_mode == \"replace\" : output_cols = list_of_cols else : output_cols = [( i + \"_scaled\" ) for i in list_of_cols ] print ( \"Before: \" ) idf . select ( list_of_cols ) . describe () . show ( 5 , False ) print ( \"After: \" ) odf . select ( output_cols ) . describe () . show ( 5 , False ) return odf def imputation_MMM ( spark , idf , list_of_cols = \"missing\" , drop_cols = [], method_type = \"median\" , pre_existing_model = False , model_path = \"NA\" , output_mode = \"replace\" , stats_missing = {}, stats_mode = {}, print_impact = False , ): \"\"\" This function handles missing value related issues by substituting null values by the measure of central tendency (mode for categorical features and mean/median for numerical features). For numerical attributes, it leverages [Imputer](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.feature.Imputer .html) functionality of Spark MLlib. Though, Imputer can be used for categorical attributes but this feature is available only in Spark3.x, therefore for categorical features, we compute mode or leverage mode computation from Measures of Central Tendency. Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of columns to impute e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all (non-array) columns for analysis. This is super useful instead of specifying all column names manually. \"missing\" (default) can be passed to include only those columns with missing values. One of the usecases where \"all\" may be preferable over \"missing\" is when the user wants to save the imputation model for the future use e.g. a column may not have missing value in the training dataset but missing values may possibly appear in the prediction dataset. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) method_type \"median\", \"mean\" (valid only for for numerical columns attributes). Mode is only option for categorical columns. (Default value = \"median\") pre_existing_model Boolean argument \u2013 True or False. True if imputation model exists already, False otherwise. (Default value = False) model_path If pre_existing_model is True, this argument is path for referring the pre-saved model. If pre_existing_model is False, this argument can be used for saving the model. Default \"NA\" means there is neither pre-existing model nor there is a need to save one. output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with transformed column. \u201cappend\u201d option append transformed column to the input dataset with a postfix \"_imputed\" e.g. column X is appended as X_imputed. (Default value = \"replace\") stats_missing Takes arguments for read_dataset (data_ingest module) function in a dictionary format to read pre-saved statistics on missing count/pct i.e. if measures_of_counts or missingCount_computation (data_analyzer.stats_generator module) has been computed & saved before. (Default value = {}) stats_mode Takes arguments for read_dataset (data_ingest module) function in a dictionary format to read pre-saved statistics on most frequently seen values i.e. if measures_of_centralTendency or mode_computation (data_analyzer.stats_generator module) has been computed & saved before. (Default value = {}) print_impact True, False (Default value = False) This argument is to print out before and after missing counts of imputed columns. Returns ------- DataFrame Imputed Dataframe \"\"\" if stats_missing == {}: missing_df = missingCount_computation ( spark , idf ) else : missing_df = read_dataset ( spark , ** stats_missing ) . select ( \"attribute\" , \"missing_count\" , \"missing_pct\" ) missing_cols = ( missing_df . where ( F . col ( \"missing_count\" ) > 0 ) . select ( \"attribute\" ) . rdd . flatMap ( lambda x : x ) . collect () ) if str ( pre_existing_model ) . lower () == \"true\" : pre_existing_model = True elif str ( pre_existing_model ) . lower () == \"false\" : pre_existing_model = False else : raise TypeError ( \"Non-Boolean input for pre_existing_model\" ) if ( len ( missing_cols ) == 0 ) & ( not pre_existing_model ) & ( model_path == \"NA\" ): return idf num_cols , cat_cols , other_cols = attributeType_segregation ( idf ) if list_of_cols == \"all\" : list_of_cols = num_cols + cat_cols if list_of_cols == \"missing\" : list_of_cols = [ x for x in missing_cols if x in num_cols + cat_cols ] if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in drop_cols ])) if len ( list_of_cols ) == 0 : warnings . warn ( \"No Imputation performed- No column(s) to impute\" ) return idf if any ( x not in num_cols + cat_cols for x in list_of_cols ): raise TypeError ( \"Invalid input for Column(s)\" ) if method_type not in ( \"mode\" , \"mean\" , \"median\" ): raise TypeError ( \"Invalid input for method_type\" ) if output_mode not in ( \"replace\" , \"append\" ): raise TypeError ( \"Invalid input for output_mode\" ) num_cols , cat_cols , other_cols = attributeType_segregation ( idf . select ( list_of_cols )) odf = idf if len ( num_cols ) > 0 : recast_cols = [] recast_type = [] for i in num_cols : if get_dtype ( idf , i ) not in ( \"float\" , \"double\" ): odf = odf . withColumn ( i , F . col ( i ) . cast ( T . DoubleType ())) recast_cols . append ( i + \"_imputed\" ) recast_type . append ( get_dtype ( idf , i )) # For mode imputation if method_type == \"mode\" : if stats_mode == {}: parameters = [ str ( ( idf . select ( i ) . dropna () . groupby ( i ) . count () . orderBy ( \"count\" , ascending = False ) . first () or [ None ] )[ 0 ] ) for i in num_cols ] else : mode_df = read_dataset ( spark , ** stats_mode ) . replace ( \"None\" , None ) mode_df_cols = list ( mode_df . select ( \"attribute\" ) . toPandas ()[ \"attribute\" ]) parameters = [] for i in num_cols : if i not in mode_df_cols : parameters . append ( str ( ( idf . select ( i ) . dropna () . groupby ( i ) . count () . orderBy ( \"count\" , ascending = False ) . first () or [ None ] )[ 0 ] ) ) else : parameters . append ( mode_df . where ( F . col ( \"attribute\" ) == i ) . select ( \"mode\" ) . rdd . flatMap ( list ) . collect ()[ 0 ] ) for index , i in enumerate ( num_cols ): odf = odf . withColumn ( i + \"_imputed\" , F . when ( F . col ( i ) . isNull (), parameters [ index ]) . otherwise ( F . col ( i )), ) else : # For mean, median imputation # Building new imputer model or uploading the existing model if pre_existing_model : imputerModel = ImputerModel . load ( model_path + \"/imputation_MMM/num_imputer-model\" ) else : imputer = Imputer ( strategy = method_type , inputCols = num_cols , outputCols = [( e + \"_imputed\" ) for e in num_cols ], ) imputerModel = imputer . fit ( odf ) # Applying model # odf = recast_column(imputerModel.transform(odf), recast_cols, recast_type) odf = imputerModel . transform ( odf ) for i , j in zip ( recast_cols , recast_type ): odf = odf . withColumn ( i , F . col ( i ) . cast ( j )) # Saving model if required if ( not pre_existing_model ) & ( model_path != \"NA\" ): imputerModel . write () . overwrite () . save ( model_path + \"/imputation_MMM/num_imputer-model\" ) if len ( cat_cols ) > 0 : if pre_existing_model : df_model = spark . read . csv ( model_path + \"/imputation_MMM/cat_imputer\" , header = True , inferSchema = True , ) parameters = [] for i in cat_cols : mapped_value = ( df_model . where ( F . col ( \"attribute\" ) == i ) . select ( \"parameters\" ) . rdd . flatMap ( lambda x : x ) . collect ()[ 0 ] ) parameters . append ( mapped_value ) else : if stats_mode == {}: parameters = [ str ( ( idf . select ( i ) . dropna () . groupby ( i ) . count () . orderBy ( \"count\" , ascending = False ) . first () or [ None ] )[ 0 ] ) for i in cat_cols ] else : mode_df = read_dataset ( spark , ** stats_mode ) . replace ( \"None\" , None ) parameters = [ mode_df . where ( F . col ( \"attribute\" ) == i ) . select ( \"mode\" ) . rdd . flatMap ( list ) . collect ()[ 0 ] for i in cat_cols ] for index , i in enumerate ( cat_cols ): odf = odf . withColumn ( i + \"_imputed\" , F . when ( F . col ( i ) . isNull (), parameters [ index ]) . otherwise ( F . col ( i )), ) if ( not pre_existing_model ) & ( model_path != \"NA\" ): df_model = spark . createDataFrame ( zip ( cat_cols , parameters ), schema = [ \"attribute\" , \"parameters\" ] ) df_model . repartition ( 1 ) . write . csv ( model_path + \"/imputation_MMM/cat_imputer\" , header = True , mode = \"overwrite\" , ) for i in num_cols + cat_cols : if i not in missing_cols : odf = odf . drop ( i + \"_imputed\" ) elif output_mode == \"replace\" : odf = odf . drop ( i ) . withColumnRenamed ( i + \"_imputed\" , i ) if print_impact : if output_mode == \"replace\" : odf_print = missing_df . select ( \"attribute\" , F . col ( \"missing_count\" ) . alias ( \"missingCount_before\" ) ) . join ( missingCount_computation ( spark , odf , list_of_cols ) . select ( \"attribute\" , F . col ( \"missing_count\" ) . alias ( \"missingCount_after\" ) ), \"attribute\" , \"inner\" , ) else : output_cols = [ ( i + \"_imputed\" ) for i in [ e for e in ( num_cols + cat_cols ) if e in missing_cols ] ] odf_print = missing_df . select ( \"attribute\" , F . col ( \"missing_count\" ) . alias ( \"missingCount_before\" ) ) . join ( missingCount_computation ( spark , odf , output_cols ) . withColumnRenamed ( \"attribute\" , \"attribute_after\" ) . withColumn ( \"attribute\" , F . expr ( \"substring(attribute_after, 1, length(attribute_after)-8)\" ), ) . drop ( \"missing_pct\" ), \"attribute\" , \"inner\" , ) odf_print . show ( len ( list_of_cols ), False ) return odf def imputation_sklearn ( spark , idf , list_of_cols = \"missing\" , drop_cols = [], method_type = \"KNN\" , sample_size = 500000 , pre_existing_model = False , model_path = \"NA\" , output_mode = \"replace\" , stats_missing = {}, run_type = \"local\" , print_impact = False , ): \"\"\" The function \"imputation_sklearn\" leverages sklearn imputer algorithms. Two methods are supported via this function: \u201cKNN\u201d and \u201cregression\u201d. \u201cKNN\u201d option trains a sklearn.impute.KNNImputer which is based on k-Nearest Neighbors algorithm. The missing values of a sample are imputed using the mean of its 5 nearest neighbors in the training set. \u201cregression\u201d option trains a sklearn.impute.IterativeImputer which models attribute to impute as a function of rest of the attributes and imputes using the estimation. Imputation is performed in an iterative way from attributes with fewest number of missing values to most. All the hyperparameters used in the above mentioned imputers are their default values. However, sklearn imputers are not scalable, which might be slow if the size of the input dataframe is large. Thus, an input sample_size (the default value is 500,000) can be set to control the number of samples to be used to train the imputer. If the total number of input dataset exceeds sample_size, the rest of the samples will be imputed using the trained imputer in a scalable manner. This is one of the way to demonstrate how Anovos has been designed as a scalable feature engineering library. Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of columns to impute e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all (non-array) columns for analysis. This is super useful instead of specifying all column names manually. \"missing\" (default) can be passed to include only those columns with missing values. One of the usecases where \"all\" may be preferable over \"missing\" is when the user wants to save the imputation model for the future use e.g. a column may not have missing value in the training dataset but missing values may possibly appear in the prediction dataset. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) method_type \"KNN\", \"regression\". \"KNN\" option trains a sklearn.impute.KNNImputer. \"regression\" option trains a sklearn.impute.IterativeImputer (Default value = \"KNN\") sample_size Maximum rows for training the sklearn imputer (Default value = 500000) pre_existing_model Boolean argument \u2013 True or False. True if imputation model exists already, False otherwise. (Default value = False) model_path If pre_existing_model is True, this argument is path for referring the pre-saved model. If pre_existing_model is False, this argument can be used for saving the model. Default \"NA\" means there is neither pre-existing model nor there is a need to save one. output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with transformed column. \u201cappend\u201d option append transformed column to the input dataset with a postfix \"_imputed\" e.g. column X is appended as X_imputed. (Default value = \"replace\") stats_missing Takes arguments for read_dataset (data_ingest module) function in a dictionary format to read pre-saved statistics on missing count/pct i.e. if measures_of_counts or missingCount_computation (data_analyzer.stats_generator module) has been computed & saved before. (Default value = {}) run_type \"local\", \"emr\", \"databricks\" (Default value = \"local\") print_impact True, False (Default value = False) This argument is to print out before and after missing counts of imputed columns. Returns ------- DataFrame Imputed Dataframe \"\"\" num_cols = attributeType_segregation ( idf )[ 0 ] if stats_missing == {}: missing_df = missingCount_computation ( spark , idf , num_cols ) else : missing_df = ( read_dataset ( spark , ** stats_missing ) . select ( \"attribute\" , \"missing_count\" , \"missing_pct\" ) . where ( F . col ( \"attribute\" ) . isin ( num_cols )) ) empty_cols = ( missing_df . where ( F . col ( \"missing_pct\" ) == 1.0 ) . select ( \"attribute\" ) . rdd . flatMap ( lambda x : x ) . collect () ) if len ( empty_cols ) > 0 : warnings . warn ( \"Following columns dropped from the imputation as all values are null: \" + \",\" . join ( empty_cols ) ) missing_cols = ( missing_df . where ( F . col ( \"missing_count\" ) > 0 ) . where ( F . col ( \"missing_pct\" ) < 1.0 ) . select ( \"attribute\" ) . rdd . flatMap ( lambda x : x ) . collect () ) if list_of_cols == \"all\" : list_of_cols = num_cols if list_of_cols == \"missing\" : list_of_cols = missing_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if ( e not in drop_cols ) & ( e not in empty_cols )]) ) if len ( list_of_cols ) <= 1 : warnings . warn ( \"No Imputation Performed - No Column(s) or Insufficient Column(s) to Impute\" ) return idf if str ( pre_existing_model ) . lower () == \"true\" : pre_existing_model = True elif str ( pre_existing_model ) . lower () == \"false\" : pre_existing_model = False else : raise TypeError ( \"Non-Boolean input for pre_existing_model\" ) if ( ( len ([ e for e in list_of_cols if e in missing_cols ]) == 0 ) & ( not pre_existing_model ) & ( model_path == \"NA\" ) ): warnings . warn ( \"No Imputation Performed - No Column(s) to Impute and No Imputation Model to be saved\" ) return idf if any ( x not in num_cols for x in list_of_cols ): raise TypeError ( \"Invalid input for Column(s)\" ) if method_type not in ( \"KNN\" , \"regression\" ): raise TypeError ( \"Invalid input for method_type\" ) if output_mode not in ( \"replace\" , \"append\" ): raise TypeError ( \"Invalid input for output_mode\" ) if pre_existing_model : if run_type == \"emr\" : bash_cmd = \"aws s3 cp \" + model_path + \"/imputation_sklearn.sav .\" output = subprocess . check_output ([ \"bash\" , \"-c\" , bash_cmd ]) imputer = pickle . load ( open ( \"imputation_sklearn.sav\" , \"rb\" )) else : imputer = pickle . load ( open ( model_path + \"/imputation_sklearn.sav\" , \"rb\" )) idf_rest = idf else : sample_ratio = min ( 1.0 , float ( sample_size ) / idf . count ()) idf_model = idf . sample ( False , sample_ratio , 0 ) idf_pd = idf_model . select ( list_of_cols ) . toPandas () if method_type == \"KNN\" : imputer = KNNImputer ( n_neighbors = 5 , weights = \"uniform\" , metric = \"nan_euclidean\" ) if method_type == \"regression\" : imputer = IterativeImputer () imputer . fit ( idf_pd ) if ( not pre_existing_model ) & ( model_path != \"NA\" ): if run_type == \"emr\" : pickle . dump ( imputer , open ( \"imputation_sklearn.sav\" , \"wb\" )) bash_cmd = ( \"aws s3 cp imputation_sklearn.sav \" + model_path + \"/imputation_sklearn.sav\" ) output = subprocess . check_output ([ \"bash\" , \"-c\" , bash_cmd ]) imputer = pickle . load ( open ( \"imputation_sklearn.sav\" , \"rb\" )) else : local_path = model_path + \"/imputation_sklearn.sav\" os . makedirs ( os . path . dirname ( local_path ), exist_ok = True ) pickle . dump ( imputer , open ( local_path , \"wb\" )) imputer = pickle . load ( open ( model_path + \"/imputation_sklearn.sav\" , \"rb\" ) ) @F . pandas_udf ( returnType = T . ArrayType ( T . DoubleType ())) def prediction ( * cols ): X = pd . concat ( cols , axis = 1 ) return pd . Series ( row . tolist () for row in imputer . transform ( X )) odf = idf . withColumn ( \"features\" , prediction ( * list_of_cols )) odf . persist ( pyspark . StorageLevel . MEMORY_AND_DISK ) . count () odf_schema = odf . schema for i in list_of_cols : odf_schema = odf_schema . add ( T . StructField ( i + \"_imputed\" , T . FloatType ())) odf = ( odf . rdd . map ( lambda x : ( * x , * x [ \"features\" ])) . toDF ( schema = odf_schema ) . drop ( \"features\" ) ) output_cols = [] for i in list_of_cols : if output_mode == \"append\" : if i not in missing_cols : odf = odf . drop ( i + \"_imputed\" ) else : output_cols . append ( i + \"_imputed\" ) else : odf = odf . drop ( i ) . withColumnRenamed ( i + \"_imputed\" , i ) odf = odf . select ( idf . columns + output_cols ) if print_impact : if output_mode == \"replace\" : odf_print = missing_df . select ( \"attribute\" , F . col ( \"missing_count\" ) . alias ( \"missingCount_before\" ) ) . join ( missingCount_computation ( spark , odf , list_of_cols ) . select ( \"attribute\" , F . col ( \"missing_count\" ) . alias ( \"missingCount_after\" ) ), \"attribute\" , \"inner\" , ) else : odf_print = missing_df . select ( \"attribute\" , F . col ( \"missing_count\" ) . alias ( \"missingCount_before\" ) ) . join ( missingCount_computation ( spark , odf , output_cols ) . withColumnRenamed ( \"attribute\" , \"attribute_after\" ) . withColumn ( \"attribute\" , F . expr ( \"substring(attribute_after, 1, length(attribute_after)-8)\" ), ) . drop ( \"missing_pct\" ), \"attribute\" , \"inner\" , ) odf_print . show ( len ( list_of_cols ), False ) return odf def imputation_matrixFactorization ( spark , idf , list_of_cols = \"missing\" , drop_cols = [], id_col = \"\" , output_mode = \"replace\" , stats_missing = {}, print_impact = False , ): \"\"\" imputation_matrixFactorization uses collaborative filtering technique to impute missing values. Collaborative filtering is commonly used in recommender systems to fill the missing user-item entries and PySpark provides an implementation using alternating least squares (ALS) algorithm, which is used in this function. To fit our problem into the ALS model, each attribute is treated as an item and an id column needs to be specified by the user to generate the user-item pairs. In the case, ID column doesn't exist in the dataset and proxu ID column is implicitly generated by the function. Subsequently, all user-item pairs with known values will be used to train the ALS model and the trained model can be used to predict the user-item pairs with missing values. Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of columns to impute e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all (non-array) columns for analysis. This is super useful instead of specifying all column names manually. \"missing\" (default) can be passed to include only those columns with missing values. One of the usecases where \"all\" may be preferable over \"missing\" is when the user wants to save the imputation model for the future use e.g. a column may not have missing value in the training dataset but missing values may possibly appear in the prediction dataset. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) id_col ID column (Default value = \"\") output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with transformed column. \u201cappend\u201d option append transformed column to the input dataset with a postfix \"_imputed\" e.g. column X is appended as X_imputed. (Default value = \"replace\") stats_missing Takes arguments for read_dataset (data_ingest module) function in a dictionary format to read pre-saved statistics on missing count/pct i.e. if measures_of_counts or missingCount_computation (data_analyzer.stats_generator module) has been computed & saved before. (Default value = {}) print_impact True, False (Default value = False) This argument is to print out before and after missing counts of imputed columns. Returns ------- DataFrame Imputed Dataframe \"\"\" num_cols = attributeType_segregation ( idf )[ 0 ] if stats_missing == {}: missing_df = missingCount_computation ( spark , idf , num_cols ) else : missing_df = ( read_dataset ( spark , ** stats_missing ) . select ( \"attribute\" , \"missing_count\" , \"missing_pct\" ) . where ( F . col ( \"attribute\" ) . isin ( num_cols )) ) empty_cols = ( missing_df . where ( F . col ( \"missing_pct\" ) == 1.0 ) . select ( \"attribute\" ) . rdd . flatMap ( lambda x : x ) . collect () ) if len ( empty_cols ) > 0 : warnings . warn ( \"Following columns dropped from the imputation as all values are null: \" + \",\" . join ( empty_cols ) ) missing_cols = ( missing_df . where ( F . col ( \"missing_count\" ) > 0 ) . where ( F . col ( \"missing_pct\" ) < 1.0 ) . select ( \"attribute\" ) . rdd . flatMap ( lambda x : x ) . collect () ) if list_of_cols == \"all\" : list_of_cols = num_cols if list_of_cols == \"missing\" : list_of_cols = missing_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ( [ e for e in list_of_cols if ( e not in drop_cols ) & ( e != id_col ) & ( e not in empty_cols ) ] ) ) if ( len ( list_of_cols ) == 0 ) | ( len ([ e for e in list_of_cols if e in missing_cols ]) == 0 ): warnings . warn ( \"No Imputation Performed - No Column(s) to Impute\" ) return idf if len ( list_of_cols ) == 1 : warnings . warn ( \"No Imputation Performed - Needs more than 1 column for matrix factorization\" ) return idf if any ( x not in num_cols for x in list_of_cols ): raise TypeError ( \"Invalid input for Column(s)\" ) if output_mode not in ( \"replace\" , \"append\" ): raise TypeError ( \"Invalid input for output_mode\" ) remove_id = False if id_col == \"\" : idf = idf . withColumn ( \"id\" , F . monotonically_increasing_id ()) . withColumn ( \"id\" , F . row_number () . over ( Window . orderBy ( \"id\" )) ) id_col = \"id\" remove_id = True key_and_val = F . create_map ( list ( chain . from_iterable ([[ F . lit ( c ), F . col ( c )] for c in list_of_cols ])) ) df_flatten = idf . select ( id_col , F . explode ( key_and_val )) . withColumn ( \"key\" , F . concat ( F . col ( \"key\" ), F . lit ( \"_imputed\" )) ) id_type = get_dtype ( idf , id_col ) if id_type == \"string\" : id_indexer = StringIndexer () . setInputCol ( id_col ) . setOutputCol ( \"IDLabel\" ) id_indexer_model = id_indexer . fit ( df_flatten ) df_flatten = id_indexer_model . transform ( df_flatten ) . drop ( id_col ) else : df_flatten = df_flatten . withColumnRenamed ( id_col , \"IDLabel\" ) indexer = StringIndexer () . setInputCol ( \"key\" ) . setOutputCol ( \"keyLabel\" ) indexer_model = indexer . fit ( df_flatten ) df_encoded = indexer_model . transform ( df_flatten ) . drop ( \"key\" ) df_model = df_encoded . where ( F . col ( \"value\" ) . isNotNull ()) df_test = df_encoded . where ( F . col ( \"value\" ) . isNull ()) if ( df_model . select ( \"IDLabel\" ) . distinct () . count () < df_encoded . select ( \"IDLabel\" ) . distinct () . count () ): warnings . warn ( \"The returned odf may not be fully imputed because values for all list_of_cols are null for some IDs\" ) als = ALS ( maxIter = 20 , regParam = 0.01 , userCol = \"IDLabel\" , itemCol = \"keyLabel\" , ratingCol = \"value\" , coldStartStrategy = \"drop\" , ) model = als . fit ( df_model ) df_pred = ( model . transform ( df_test ) . drop ( \"value\" ) . withColumnRenamed ( \"prediction\" , \"value\" ) ) df_encoded_pred = df_model . union ( df_pred . select ( df_model . columns )) if id_type == \"string\" : IDlabelReverse = IndexToString () . setInputCol ( \"IDLabel\" ) . setOutputCol ( id_col ) df_encoded_pred = IDlabelReverse . transform ( df_encoded_pred ) else : df_encoded_pred = df_encoded_pred . withColumnRenamed ( \"IDLabel\" , id_col ) keylabelReverse = IndexToString () . setInputCol ( \"keyLabel\" ) . setOutputCol ( \"key\" ) odf_imputed = ( keylabelReverse . transform ( df_encoded_pred ) . groupBy ( id_col ) . pivot ( \"key\" ) . agg ( F . first ( \"value\" )) . select ( [ id_col ] + [( i + \"_imputed\" ) for i in list_of_cols if i in missing_cols ] ) ) odf = idf . join ( odf_imputed , id_col , \"left_outer\" ) for i in list_of_cols : if i not in missing_cols : odf = odf . drop ( i + \"_imputed\" ) elif output_mode == \"replace\" : odf = odf . drop ( i ) . withColumnRenamed ( i + \"_imputed\" , i ) if remove_id : odf = odf . drop ( \"id\" ) if print_impact : if output_mode == \"replace\" : odf_print = missing_df . select ( \"attribute\" , F . col ( \"missing_count\" ) . alias ( \"missingCount_before\" ) ) . join ( missingCount_computation ( spark , odf , list_of_cols ) . select ( \"attribute\" , F . col ( \"missing_count\" ) . alias ( \"missingCount_after\" ) ), \"attribute\" , \"inner\" , ) else : output_cols = [ ( i + \"_imputed\" ) for i in [ e for e in list_of_cols if e in missing_cols ] ] odf_print = missing_df . select ( \"attribute\" , F . col ( \"missing_count\" ) . alias ( \"missingCount_before\" ) ) . join ( missingCount_computation ( spark , odf , output_cols ) . withColumnRenamed ( \"attribute\" , \"attribute_after\" ) . withColumn ( \"attribute\" , F . expr ( \"substring(attribute_after, 1, length(attribute_after)-8)\" ), ) . drop ( \"missing_pct\" ), \"attribute\" , \"inner\" , ) odf_print . show ( len ( list_of_cols ), False ) return odf def auto_imputation ( spark , idf , list_of_cols = \"missing\" , drop_cols = [], id_col = \"\" , null_pct = 0.1 , stats_missing = {}, output_mode = \"replace\" , run_type = \"local\" , print_impact = True , ): \"\"\" auto_imputation tests for 5 imputation methods using the other imputation functions provided in this module and returns the one with the best performance. The 5 methods are: (1) imputation_MMM with method_type=\"mean\" (2) imputation_MMM with method_type=\"median\" (3) imputation_sklearn with method_type=\"KNN\" (4) imputation_sklearn with method_type=\"regression\" (5) imputation_matrixFactorization Samples without missing values in attributes to impute are used for testing by removing some % of values and impute them again using the above 5 methods. RMSE/attribute_mean is used as the evaluation metric for each attribute to reduce the effect of unit difference among attributes. The final error of a method is calculated by the sum of ( RMSE/attribute_mean) for all numerical attributes to impute and the method with the least error will be selected. The above testing is only applicable for numerical attributes. If categorical attributes are included, they will be automatically imputed using imputation_MMM. In addition, if there is only one numerical attribute to impute, only method (1) and (2) will be tested because the rest of the methods require more than one column. Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of columns to impute e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all (non-array) columns for analysis. This is super useful instead of specifying all column names manually. \"missing\" (default) can be passed to include only those columns with missing values. One of the usecases where \"all\" may be preferable over \"missing\" is when the user wants to save the imputation model for the future use e.g. a column may not have missing value in the training dataset but missing values may possibly appear in the prediction dataset. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) id_col ID column (Default value = \"\") null_pct proportion of the valid input data to be replaced by None to form the test data (Default value = 0.1) stats_missing Takes arguments for read_dataset (data_ingest module) function in a dictionary format to read pre-saved statistics on missing count/pct i.e. if measures_of_counts or missingCount_computation (data_analyzer.stats_generator module) has been computed & saved before. (Default value = {}) output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with transformed column. \u201cappend\u201d option append transformed column to the input dataset with a postfix \"_imputed\" e.g. column X is appended as X_imputed. (Default value = \"replace\") run_type \"local\", \"emr\", \"databricks\" (Default value = \"local\") print_impact True, False (Default value = False) This argument is to print out before and after missing counts of imputed columns. It also print the name of best performing imputation method along with RMSE details. Returns ------- DataFrame Imputed Dataframe \"\"\" if run_type in list ( platform_root_path . keys ()): root_path = platform_root_path [ run_type ] else : root_path = \"\" if stats_missing == {}: missing_df = missingCount_computation ( spark , idf ) missing_df . write . parquet ( root_path + \"intermediate_data/imputation_comparison/missingCount_computation\" , mode = \"overwrite\" , ) stats_missing = { \"file_path\" : root_path + \"intermediate_data/imputation_comparison/missingCount_computation\" , \"file_type\" : \"parquet\" , } else : missing_df = read_dataset ( spark , ** stats_missing ) . select ( \"attribute\" , \"missing_count\" , \"missing_pct\" ) empty_cols = ( missing_df . where ( F . col ( \"missing_pct\" ) == 1.0 ) . select ( \"attribute\" ) . rdd . flatMap ( lambda x : x ) . collect () ) if len ( empty_cols ) > 0 : warnings . warn ( \"Following columns have all null values: \" + \",\" . join ( empty_cols )) missing_cols = ( missing_df . where ( F . col ( \"missing_count\" ) > 0 ) . select ( \"attribute\" ) . rdd . flatMap ( lambda x : x ) . collect () ) if list_of_cols == \"all\" : list_of_cols = idf . columns if list_of_cols == \"missing\" : list_of_cols = missing_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if ( e not in drop_cols ) & ( e != id_col )]) ) if any ( x not in idf . columns for x in list_of_cols ): raise TypeError ( \"Invalid input for Column(s)\" ) del_cols = [ e for e in list_of_cols if e in empty_cols ] odf_del = idf . drop ( * del_cols ) list_of_cols = [ e for e in list_of_cols if e not in empty_cols ] num_cols , cat_cols , other_cols = attributeType_segregation ( odf_del . select ( list_of_cols ) ) missing_catcols = [ e for e in cat_cols if e in missing_cols ] missing_numcols = [ e for e in num_cols if e in missing_cols ] if missing_catcols : odf_imputed_cat = imputation_MMM ( spark , odf_del , list_of_cols = missing_catcols , stats_missing = stats_missing ) else : odf_imputed_cat = odf_del if len ( missing_numcols ) == 0 : warnings . warn ( \"No Imputation Performed for numerical columns - No Column(s) to Impute\" ) return odf_imputed_cat idf_test = ( odf_imputed_cat . dropna ( subset = missing_numcols ) . withColumn ( \"index\" , F . monotonically_increasing_id ()) . withColumn ( \"index\" , F . row_number () . over ( Window . orderBy ( \"index\" ))) ) null_count = int ( null_pct * idf_test . count ()) idf_null = idf_test for i in missing_numcols : null_index = random . sample ( range ( idf_test . count ()), null_count ) idf_null = idf_null . withColumn ( i , F . when ( F . col ( \"index\" ) . isin ( null_index ), None ) . otherwise ( F . col ( i )) ) idf_null . write . parquet ( root_path + \"intermediate_data/imputation_comparison/test_dataset\" , mode = \"overwrite\" , ) idf_null = spark . read . parquet ( root_path + \"intermediate_data/imputation_comparison/test_dataset\" ) method1 = imputation_MMM ( spark , idf_null , list_of_cols = missing_numcols , method_type = \"mean\" , stats_missing = stats_missing , output_mode = output_mode , ) method2 = imputation_MMM ( spark , idf_null , list_of_cols = missing_numcols , method_type = \"median\" , stats_missing = stats_missing , output_mode = output_mode , ) valid_methods = [ method1 , method2 ] if len ( num_cols ) > 1 : method3 = imputation_sklearn ( spark , idf_null , list_of_cols = num_cols , method_type = \"KNN\" , stats_missing = stats_missing , output_mode = output_mode , ) method4 = imputation_sklearn ( spark , idf_null , list_of_cols = num_cols , method_type = \"regression\" , stats_missing = stats_missing , output_mode = output_mode , ) method5 = imputation_matrixFactorization ( spark , idf_null , list_of_cols = num_cols , id_col = id_col , stats_missing = stats_missing , output_mode = output_mode , ) valid_methods = [ method1 , method2 , method3 , method4 , method5 ] nrmse_all = [] method_all = [ \"MMM-mean\" , \"MMM-median\" , \"KNN\" , \"regression\" , \"matrix_factorization\" ] for index , method in enumerate ( valid_methods ): nrmse = 0 for i in missing_numcols : pred_col = ( i + \"_imputed\" ) if output_mode == \"append\" else i idf_joined = ( idf_test . select ( \"index\" , F . col ( i ) . alias ( \"val\" )) . join ( method . select ( \"index\" , F . col ( pred_col ) . alias ( \"pred\" )), \"index\" , \"left_outer\" , ) . dropna () ) idf_joined = recast_column ( idf = idf_joined , list_of_cols = [ \"val\" , \"pred\" ], list_of_dtypes = [ \"double\" , \"double\" ], ) pred_mean = float ( method . select ( F . mean ( pred_col )) . rdd . flatMap ( lambda x : x ) . collect ()[ 0 ] ) i_nrmse = ( RegressionEvaluator ( metricName = \"rmse\" , labelCol = \"val\" , predictionCol = \"pred\" ) . evaluate ( idf_joined ) ) / abs ( pred_mean ) nrmse += i_nrmse nrmse_all . append ( nrmse ) min_index = nrmse_all . index ( np . min ( nrmse_all )) best_method = method_all [ min_index ] odf = valid_methods [ min_index ] if print_impact : print ( list ( zip ( method_all , nrmse_all ))) print ( \"Best Imputation Method: \" , best_method ) return odf def autoencoder_latentFeatures ( spark , idf , list_of_cols = \"all\" , drop_cols = [], reduction_params = 0.5 , sample_size = 500000 , epochs = 100 , batch_size = 256 , pre_existing_model = False , model_path = \"NA\" , standardization = True , standardization_configs = { \"pre_existing_model\" : False , \"model_path\" : \"NA\" }, imputation = False , imputation_configs = { \"imputation_function\" : \"imputation_MMM\" }, stats_missing = {}, output_mode = \"replace\" , run_type = \"local\" , print_impact = False , ): \"\"\" Many machine learning models suffer from \"the curse of dimensionality\" when the number of features is too large. autoencoder_latentFeatures is able to reduce the dimensionality by compressing input attributes to a smaller number of latent features. To be more specific, it trains a neural network model using TensorFlow library. The neural network contains an encoder and a decoder, where the encoder learns to represent the input using smaller number of latent features controlled by the input *reduction_params* and the decoder learns to reproduce the input using the latent features. In the end, only the encoder will be kept and the latent features generated by the encoder will be added to the output dataframe. However, the neural network model is not trained in a scalable manner, which might not be able to handle large input dataframe. Thus, an input *sample_size* (the default value is 500,000) can be set to control the number of samples to be used to train the model. If the total number of samples exceeds *sample_size*, the rest of the samples will be predicted using the fitted encoder. Standardization is highly recommended if the input attributes are not of the same scale. Otherwise, the model might not converge smoothly. Inputs *standardization* and *standardization_configs* can be set accordingly to perform standardization within the function. In addition, if a sample contains missing values in the model input, the output values for its latent features will all be None. Thus data imputation is also recommended if missing values exist, which can be done within the function by setting inputs *imputation* and *imputation_configs*. Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of numerical columns to encode e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all numerical columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) reduction_params Determines the number of encoded features in the result. If reduction_params < 1, int(reduction_params * <number of columns>) columns will be generated. Else, reduction_params columns will be generated. (Default value = 0.5) sample_size Maximum rows for training the autoencoder model using tensorflow. (Default value = 500000) epochs Number of epochs to train the tensorflow model. (Default value = 100) batch_size Number of samples per gradient update when fitting the tensorflow model. (Default value = 256) pre_existing_model Boolean argument \u2013 True or False. True if model exists already, False Otherwise (Default value = False) model_path If pre_existing_model is True, this argument is path for referring the pre-saved model. If pre_existing_model is False, this argument can be used for saving the model. Default \"NA\" means there is neither pre-existing model nor there is a need to save one. standardization Boolean argument \u2013 True or False. True, if the standardization required. (Default value = True) standardization_configs z_standardization function arguments in dictionary format. (Default value = {\"pre_existing_model\": False) imputation Boolean argument \u2013 True or False. True, if the imputation required. (Default value = False) imputation_configs Takes input in dictionary format. Imputation function name is provided with key \"imputation_name\". optional arguments pertaining to that imputation function can be provided with argument name as key. (Default value = {\"imputation_function\": \"imputation_MMM\"}) stats_missing Takes arguments for read_dataset (data_ingest module) function in a dictionary format to read pre-saved statistics on missing count/pct i.e. if measures_of_counts or missingCount_computation (data_analyzer.stats_generator module) has been computed & saved before. (Default value = {}) output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with transformed columns: latent_<col_index>. \u201cappend\u201d option append transformed columns with format latent_<col_index> to the input dataset, e.g. latent_0, latent_1 will be appended if reduction_params=2. (Default value = \"replace\") run_type \"local\", \"emr\", \"databricks\" (Default value = \"local\") print_impact True, False This argument is to print descriptive statistics of the latest features (Default value = False) Returns ------- DataFrame Dataframe with Latent Features \"\"\" num_cols = attributeType_segregation ( idf )[ 0 ] if list_of_cols == \"all\" : list_of_cols = num_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in drop_cols ])) if any ( x not in num_cols for x in list_of_cols ): raise TypeError ( \"Invalid input for Column(s)\" ) if len ( list_of_cols ) == 0 : warnings . warn ( \"No Latent Features Generated - No Column(s) to Transform\" ) return idf if run_type in list ( platform_root_path . keys ()): root_path = platform_root_path [ run_type ] else : root_path = \"\" if stats_missing == {}: missing_df = missingCount_computation ( spark , idf , list_of_cols ) missing_df . write . parquet ( root_path + \"intermediate_data/autoencoder_latentFeatures/missingCount_computation\" , mode = \"overwrite\" , ) stats_missing = { \"file_path\" : root_path + \"intermediate_data/autoencoder_latentFeatures/missingCount_computation\" , \"file_type\" : \"parquet\" , } else : missing_df = ( read_dataset ( spark , ** stats_missing ) . select ( \"attribute\" , \"missing_count\" , \"missing_pct\" ) . where ( F . col ( \"attribute\" ) . isin ( list_of_cols )) ) empty_cols = ( missing_df . where ( F . col ( \"missing_pct\" ) == 1.0 ) . select ( \"attribute\" ) . rdd . flatMap ( lambda x : x ) . collect () ) if len ( empty_cols ) > 0 : warnings . warn ( \"The following column(s) are excluded from dimensionality reduction as all values are null: \" + \",\" . join ( empty_cols ) ) list_of_cols = [ e for e in list_of_cols if e not in empty_cols ] if standardization : idf_standardized = z_standardization ( spark , idf , list_of_cols = list_of_cols , output_mode = \"append\" , ** standardization_configs , ) list_of_cols_scaled = [ i + \"_scaled\" for i in list_of_cols if ( i + \"_scaled\" ) in idf_standardized . columns ] else : idf_standardized = idf for i in list_of_cols : idf_standardized = idf_standardized . withColumn ( i + \"_scaled\" , F . col ( i )) list_of_cols_scaled = [ i + \"_scaled\" for i in list_of_cols ] if imputation : all_functions = globals () . copy () all_functions . update ( locals ()) f = all_functions . get ( imputation_configs [ \"imputation_function\" ]) args = copy . deepcopy ( imputation_configs ) args . pop ( \"imputation_function\" , None ) missing_df_scaled = ( read_dataset ( spark , ** stats_missing ) . select ( \"attribute\" , \"missing_count\" , \"missing_pct\" ) . withColumn ( \"attribute\" , F . concat ( F . col ( \"attribute\" ), F . lit ( \"_scaled\" ))) ) missing_df_scaled . write . parquet ( root_path + \"intermediate_data/autoencoder_latentFeatures/missingCount_computation_scaled\" , mode = \"overwrite\" , ) stats_missing_scaled = { \"file_path\" : root_path + \"intermediate_data/autoencoder_latentFeatures/missingCount_computation_scaled\" , \"file_type\" : \"parquet\" , } idf_imputed = f ( spark , idf_standardized , list_of_cols_scaled , stats_missing = stats_missing_scaled , ** args , ) else : idf_imputed = idf_standardized . dropna ( subset = list_of_cols_scaled ) n_inputs = len ( list_of_cols_scaled ) if reduction_params < 1 : n_bottleneck = int ( reduction_params * n_inputs ) else : n_bottleneck = int ( reduction_params ) if pre_existing_model : if run_type == \"emr\" : bash_cmd = ( \"aws s3 cp \" + model_path + \"/autoencoders_latentFeatures/encoder.h5 .\" ) output = subprocess . check_output ([ \"bash\" , \"-c\" , bash_cmd ]) bash_cmd = ( \"aws s3 cp \" + model_path + \"/autoencoders_latentFeatures/model.h5 .\" ) output = subprocess . check_output ([ \"bash\" , \"-c\" , bash_cmd ]) encoder = load_model ( \"encoder.h5\" ) model = load_model ( \"model.h5\" ) else : encoder = load_model ( model_path + \"/autoencoders_latentFeatures/encoder.h5\" ) model = load_model ( model_path + \"/autoencoders_latentFeatures/model.h5\" ) else : idf_valid = idf_imputed . select ( list_of_cols_scaled ) idf_model = idf_valid . sample ( False , min ( 1.0 , float ( sample_size ) / idf_valid . count ()), 0 ) idf_train = idf_model . sample ( False , 0.8 , 0 ) idf_test = idf_model . subtract ( idf_train ) X_train = idf_train . toPandas () X_test = idf_test . toPandas () visible = Input ( shape = ( n_inputs ,)) e = Dense ( n_inputs * 2 )( visible ) e = BatchNormalization ()( e ) e = LeakyReLU ()( e ) e = Dense ( n_inputs )( e ) e = BatchNormalization ()( e ) e = LeakyReLU ()( e ) bottleneck = Dense ( n_bottleneck )( e ) d = Dense ( n_inputs )( bottleneck ) d = BatchNormalization ()( d ) d = LeakyReLU ()( d ) d = Dense ( n_inputs * 2 )( d ) d = BatchNormalization ()( d ) d = LeakyReLU ()( d ) output = Dense ( n_inputs , activation = \"linear\" )( d ) model = Model ( inputs = visible , outputs = output ) encoder = Model ( inputs = visible , outputs = bottleneck ) model . compile ( optimizer = \"adam\" , loss = \"mse\" ) history = model . fit ( X_train , X_train , epochs = int ( epochs ), batch_size = int ( batch_size ), verbose = 2 , validation_data = ( X_test , X_test ), ) if ( not pre_existing_model ) & ( model_path != \"NA\" ): if run_type == \"emr\" : encoder . save ( \"encoder.h5\" ) model . save ( \"model.h5\" ) bash_cmd = ( \"aws s3 cp encoder.h5 \" + model_path + \"/autoencoders_latentFeatures/encoder.h5\" ) output = subprocess . check_output ([ \"bash\" , \"-c\" , bash_cmd ]) bash_cmd = ( \"aws s3 cp model.h5 \" + model_path + \"/autoencoders_latentFeatures/model.h5\" ) output = subprocess . check_output ([ \"bash\" , \"-c\" , bash_cmd ]) else : if not os . path . exists ( model_path + \"/autoencoders_latentFeatures/\" ): os . makedirs ( model_path + \"/autoencoders_latentFeatures/\" ) encoder . save ( model_path + \"/autoencoders_latentFeatures/encoder.h5\" ) model . save ( model_path + \"/autoencoders_latentFeatures/model.h5\" ) class ModelWrapperPickable : def __init__ ( self , model ): self . model = model def __getstate__ ( self ): model_str = \"\" with tempfile . NamedTemporaryFile ( suffix = \".hdf5\" , delete = True ) as fd : tensorflow . keras . models . save_model ( self . model , fd . name , overwrite = True ) model_str = fd . read () d = { \"model_str\" : model_str } return d def __setstate__ ( self , state ): with tempfile . NamedTemporaryFile ( suffix = \".hdf5\" , delete = True ) as fd : fd . write ( state [ \"model_str\" ]) fd . flush () self . model = tensorflow . keras . models . load_model ( fd . name ) model_wrapper = ModelWrapperPickable ( encoder ) def compute_output_pandas_udf ( model_wrapper ): @F . pandas_udf ( returnType = T . ArrayType ( T . DoubleType ())) def predict_pandas_udf ( * cols ): X = pd . concat ( cols , axis = 1 ) return pd . Series ( row . tolist () for row in model_wrapper . model . predict ( X )) return predict_pandas_udf odf = idf_imputed . withColumn ( \"predicted_output\" , compute_output_pandas_udf ( model_wrapper )( * list_of_cols_scaled ), ) odf_schema = odf . schema for i in range ( 0 , n_bottleneck ): odf_schema = odf_schema . add ( T . StructField ( \"latent_\" + str ( i ), T . FloatType ())) odf = ( odf . rdd . map ( lambda x : ( * x , * x [ \"predicted_output\" ])) . toDF ( schema = odf_schema ) . drop ( \"predicted_output\" ) ) if output_mode == \"replace\" : odf = odf . drop ( * list_of_cols ) if print_impact : output_cols = [ \"latent_\" + str ( j ) for j in range ( 0 , n_bottleneck )] odf . select ( output_cols ) . describe () . show ( 5 , False ) return odf def PCA_latentFeatures ( spark , idf , list_of_cols = \"all\" , drop_cols = [], explained_variance_cutoff = 0.95 , pre_existing_model = False , model_path = \"NA\" , standardization = True , standardization_configs = { \"pre_existing_model\" : False , \"model_path\" : \"NA\" }, imputation = False , imputation_configs = { \"imputation_function\" : \"imputation_MMM\" }, stats_missing = {}, output_mode = \"replace\" , run_type = \"local\" , print_impact = False , ): \"\"\" Similar to autoencoder_latentFeatures, PCA_latentFeatures also generates latent features which reduces the dimensionality of the input dataframe but through a different technique: Principal Component Analysis (PCA). PCA algorithm produces principal components such that it can describe most of the remaining variance and all the principal components are orthogonal to each other. The final number of generated principal components is controlled by the input *explained_variance_cutoff*. In other words, the number of selected principal components, k, is the minimum value such that top k components (latent features) can explain at least *explained_variance_cutoff* of the total variance. Standardization is highly recommended if the input attributes are not of the same scale. Otherwise the generated latent features might be dominated by the attributes with larger variance. Inputs *standardization* and *standardization_configs* can be set accordingly to perform standardization within the function. In addition, data imputation is also recommended if missing values exist, which can be done within the function by setting inputs *imputation* and *imputation_configs*. Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of numerical columns to encode e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all numerical columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) explained_variance_cutoff Determines the number of latent columns in the output. If N is the smallest integer such that top N latent columns explain more than explained_variance_cutoff variance, these N columns will be selected. (Default value = 0.95) pre_existing_model Boolean argument \u2013 True or False. True if model exists already, False Otherwise (Default value = False) model_path If pre_existing_model is True, this argument is path for referring the pre-saved model. If pre_existing_model is False, this argument can be used for saving the model. Default \"NA\" means there is neither pre-existing model nor there is a need to save one. standardization Boolean argument \u2013 True or False. True, if the standardization required. (Default value = True) standardization_configs z_standardization function arguments in dictionary format. (Default value = {\"pre_existing_model\": False) imputation Boolean argument \u2013 True or False. True, if the imputation required. (Default value = False) imputation_configs Takes input in dictionary format. Imputation function name is provided with key \"imputation_name\". optional arguments pertaining to that imputation function can be provided with argument name as key. (Default value = {\"imputation_function\": \"imputation_MMM\"}) stats_missing Takes arguments for read_dataset (data_ingest module) function in a dictionary format to read pre-saved statistics on missing count/pct i.e. if measures_of_counts or missingCount_computation (data_analyzer.stats_generator module) has been computed & saved before. (Default value = {}) output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with transformed columns: latent_<col_index>. \u201cappend\u201d option append transformed columns with format latent_<col_index> to the input dataset, e.g. latent_0, latent_1. (Default value = \"replace\") run_type \"local\", \"emr\", \"databricks\" (Default value = \"local\") print_impact True, False This argument is to print descriptive statistics of the latest features (Default value = False) Returns ------- DataFrame Dataframe with Latent Features \"\"\" num_cols = attributeType_segregation ( idf )[ 0 ] if list_of_cols == \"all\" : list_of_cols = num_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in drop_cols ])) if any ( x not in num_cols for x in list_of_cols ): raise TypeError ( \"Invalid input for Column(s)\" ) if len ( list_of_cols ) == 0 : warnings . warn ( \"No Latent Features Generated - No Column(s) to Transform\" ) return idf if run_type in list ( platform_root_path . keys ()): root_path = platform_root_path [ run_type ] else : root_path = \"\" if stats_missing == {}: missing_df = missingCount_computation ( spark , idf , list_of_cols ) missing_df . write . parquet ( root_path + \"intermediate_data/PCA_latentFeatures/missingCount_computation\" , mode = \"overwrite\" , ) stats_missing = { \"file_path\" : root_path + \"intermediate_data/PCA_latentFeatures/missingCount_computation\" , \"file_type\" : \"parquet\" , } else : missing_df = ( read_dataset ( spark , ** stats_missing ) . select ( \"attribute\" , \"missing_count\" , \"missing_pct\" ) . where ( F . col ( \"attribute\" ) . isin ( list_of_cols )) ) empty_cols = ( missing_df . where ( F . col ( \"missing_pct\" ) == 1.0 ) . select ( \"attribute\" ) . rdd . flatMap ( lambda x : x ) . collect () ) if len ( empty_cols ) > 0 : warnings . warn ( \"The following column(s) are excluded from dimensionality reduction as all values are null: \" + \",\" . join ( empty_cols ) ) list_of_cols = [ e for e in list_of_cols if e not in empty_cols ] if standardization : idf_standardized = z_standardization ( spark , idf , list_of_cols = list_of_cols , output_mode = \"append\" , ** standardization_configs , ) list_of_cols_scaled = [ i + \"_scaled\" for i in list_of_cols if ( i + \"_scaled\" ) in idf_standardized . columns ] else : idf_standardized = idf for i in list_of_cols : idf_standardized = idf_standardized . withColumn ( i + \"_scaled\" , F . col ( i )) list_of_cols_scaled = [ i + \"_scaled\" for i in list_of_cols ] if imputation : all_functions = globals () . copy () all_functions . update ( locals ()) f = all_functions . get ( imputation_configs [ \"imputation_function\" ]) args = copy . deepcopy ( imputation_configs ) args . pop ( \"imputation_function\" , None ) missing_df_scaled = ( read_dataset ( spark , ** stats_missing ) . select ( \"attribute\" , \"missing_count\" , \"missing_pct\" ) . withColumn ( \"attribute\" , F . concat ( F . col ( \"attribute\" ), F . lit ( \"_scaled\" ))) ) missing_df_scaled . write . parquet ( root_path + \"intermediate_data/PCA_latentFeatures/missingCount_computation_scaled\" , mode = \"overwrite\" , ) stats_missing_scaled = { \"file_path\" : root_path + \"intermediate_data/PCA_latentFeatures/missingCount_computation_scaled\" , \"file_type\" : \"parquet\" , } idf_imputed = f ( spark , idf_standardized , list_of_cols_scaled , stats_missing = stats_missing_scaled , ** args , ) else : idf_imputed = idf_standardized . dropna ( subset = list_of_cols_scaled ) idf_imputed . persist ( pyspark . StorageLevel . MEMORY_AND_DISK ) . count () assembler = VectorAssembler ( inputCols = list_of_cols_scaled , outputCol = \"features\" ) assembled_data = assembler . transform ( idf_imputed ) . drop ( * list_of_cols_scaled ) if pre_existing_model : pca = PCA . load ( model_path + \"/PCA_latentFeatures/pca_path\" ) pcaModel = PCAModel . load ( model_path + \"/PCA_latentFeatures/pcaModel_path\" ) n = pca . getK () else : pca = PCA ( k = len ( list_of_cols_scaled ), inputCol = \"features\" , outputCol = \"features_pca\" ) pcaModel = pca . fit ( assembled_data ) explained_variance = 0 for n in range ( 1 , len ( list_of_cols ) + 1 ): explained_variance += pcaModel . explainedVariance [ n - 1 ] if explained_variance > explained_variance_cutoff : break pca = PCA ( k = n , inputCol = \"features\" , outputCol = \"features_pca\" ) pcaModel = pca . fit ( assembled_data ) if ( not pre_existing_model ) & ( model_path != \"NA\" ): pcaModel . write () . overwrite () . save ( model_path + \"/PCA_latentFeatures/pcaModel_path\" ) pca . write () . overwrite () . save ( model_path + \"/PCA_latentFeatures/pca_path\" ) def vector_to_array ( v ): return v . toArray () . tolist () f_vector_to_array = F . udf ( vector_to_array , T . ArrayType ( T . FloatType ())) odf = ( pcaModel . transform ( assembled_data ) . withColumn ( \"features_pca_array\" , f_vector_to_array ( \"features_pca\" )) . drop ( * [ \"features\" , \"features_pca\" ]) ) odf_schema = odf . schema for i in range ( 0 , n ): odf_schema = odf_schema . add ( T . StructField ( \"latent_\" + str ( i ), T . FloatType ())) odf = ( odf . rdd . map ( lambda x : ( * x , * x [ \"features_pca_array\" ])) . toDF ( schema = odf_schema ) . drop ( \"features_pca_array\" ) . replace ( float ( \"nan\" ), None , subset = [ \"latent_\" + str ( j ) for j in range ( 0 , n )]) ) if output_mode == \"replace\" : odf = odf . drop ( * list_of_cols ) if print_impact : print ( \"Explained Variance: \" , round ( np . sum ( pcaModel . explainedVariance [ 0 : n ]), 4 )) output_cols = [ \"latent_\" + str ( j ) for j in range ( 0 , n )] odf . select ( output_cols ) . describe () . show ( 5 , False ) return odf def feature_transformation ( idf , list_of_cols = \"all\" , drop_cols = [], method_type = \"sqrt\" , N = None , output_mode = \"replace\" , print_impact = False , ): \"\"\" As the name indicates, feature_transformation performs mathematical transformation over selected attributes. The following methods are supported for an input attribute x: ln(x), log10(x), log2(x), e^x, 2^x, 10^x, N^x, square and cube root of x, x^2, x^3, x^N, trigonometric transformations of x (sin, cos, tan, asin, acos, atan), radians, x%N, x!, 1/x, floor and ceiling of x and x rounded to N decimal places. Some transformations only work with positive or non-negative input values such as log and square root and an error will be returned if violated. Parameters ---------- idf Input Dataframe list_of_cols List of numerical columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all numerical columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) method_type \"ln\", \"log10\", \"log2\", \"exp\", \"powOf2\" (2^x), \"powOf10\" (10^x), \"powOfN\" (N^x), \"sqrt\" (square root), \"cbrt\" (cube root), \"sq\" (square), \"cb\" (cube), \"toPowerN\" (x^N), \"sin\", \"cos\", \"tan\", \"asin\", \"acos\", \"atan\", \"radians\", \"remainderDivByN\" (x%N), \"factorial\" (x!), \"mul_inv\" (1/x), \"floor\", \"ceil\", \"roundN\" (round to N decimal places) (Default value = \"sqrt\") N None by default. If method_type is \"powOfN\", \"toPowerN\", \"remainderDivByN\" or \"roundN\", N will be used as the required constant. output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with transformed columns. \u201cappend\u201d option append transformed columns with a postfix (E.g. \"_ln\", \"_powOf<N>\") to the input dataset. (Default value = \"replace\") print_impact True, False This argument is to print before and after descriptive statistics of the transformed features (Default value = False) Returns ------- DataFrame Transformed Dataframe \"\"\" num_cols = attributeType_segregation ( idf )[ 0 ] if list_of_cols == \"all\" : list_of_cols = num_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in drop_cols ])) if ( len ( list_of_cols ) == 0 ) | ( any ( x not in num_cols for x in list_of_cols )): raise TypeError ( \"Invalid input for Column(s)\" ) if method_type not in [ \"ln\" , \"log10\" , \"log2\" , \"exp\" , \"powOf2\" , \"powOf10\" , \"powOfN\" , \"sqrt\" , \"cbrt\" , \"sq\" , \"cb\" , \"toPowerN\" , \"sin\" , \"cos\" , \"tan\" , \"asin\" , \"acos\" , \"atan\" , \"radians\" , \"remainderDivByN\" , \"factorial\" , \"mul_inv\" , \"floor\" , \"ceil\" , \"roundN\" , ]: raise TypeError ( \"Invalid input method_type\" ) num_cols = attributeType_segregation ( idf . select ( list_of_cols ))[ 0 ] list_of_cols = num_cols odf = idf transformation_function = { \"ln\" : F . log , \"log10\" : F . log10 , \"log2\" : F . log2 , \"exp\" : F . exp , \"powOf2\" : ( lambda x : F . pow ( 2.0 , x )), \"powOf10\" : ( lambda x : F . pow ( 10.0 , x )), \"powOfN\" : ( lambda x : F . pow ( N , x )), \"sqrt\" : F . sqrt , \"cbrt\" : F . cbrt , \"sq\" : ( lambda x : x ** 2 ), \"cb\" : ( lambda x : x ** 3 ), \"toPowerN\" : ( lambda x : x ** N ), \"sin\" : F . sin , \"cos\" : F . cos , \"tan\" : F . tan , \"asin\" : F . asin , \"acos\" : F . acos , \"atan\" : F . atan , \"radians\" : F . radians , \"remainderDivByN\" : ( lambda x : x % F . lit ( N )), \"factorial\" : F . factorial , \"mul_inv\" : ( lambda x : F . lit ( 1 ) / x ), \"floor\" : F . floor , \"ceil\" : F . ceil , \"roundN\" : ( lambda x : F . round ( x , N )), } def get_col_name ( i ): if output_mode == \"replace\" : return i else : if method_type in [ \"powOfN\" , \"toPowerN\" , \"remainderDivByN\" , \"roundN\" ]: return i + \"_\" + method_type [: - 1 ] + str ( N ) else : return i + \"_\" + method_type output_cols = [] for i in list_of_cols : modify_col = get_col_name ( i ) odf = odf . withColumn ( modify_col , transformation_function [ method_type ]( F . col ( i ))) output_cols . append ( modify_col ) if print_impact : print ( \"Before:\" ) idf . select ( list_of_cols ) . describe () . show ( 5 , False ) print ( \"After:\" ) odf . select ( output_cols ) . describe () . show ( 5 , False ) return odf def boxcox_transformation ( idf , list_of_cols = \"all\" , drop_cols = [], boxcox_lambda = None , output_mode = \"replace\" , print_impact = False , ): \"\"\" Some machine learning algorithms require the input data to follow normal distributions. Thus, when the input data is too skewed, boxcox_transformation can be used to transform it into a more normal-like distribution. The transformed value of a sample x depends on a coefficient lambda: (1) if lambda = 0, x is transformed into log(x); (2) if lambda !=0, x is transformed into (x^lambda-1)/lambda. The value of lambda can be either specified by the user or automatically selected within the function. If lambda needs to be selected within the function, a range of values (0, 1, -1, 0.5, -0.5, 2, -2, 0.25, -0.25, 3, -3, 4, -4, 5, -5) will be tested and the lambda, which optimizes the KolmogorovSmirnovTest by PySpark with the theoretical distribution being normal, will be used to perform the transformation. Different lambda values can be assigned to different attributes but one attribute can only be assigned one lambda value. Parameters ---------- idf Input Dataframe list_of_cols List of numerical columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all numerical columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) boxcox_lambda Lambda value for box_cox transormation. If boxcox_lambda is not None, it will be directly used for the transformation. It can be a (1) list: each element represents a lambda value for an attribute and the length of the list must be the same as the number of columns to transform. (2) int/float: all attributes will be assigned the same lambda value. Else, search for the best lambda among [1,-1,0.5,-0.5,2,-2,0.25,-0.25,3,-3,4,-4,5,-5] for each column and apply the transformation (Default value = None) output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with transformed columns. \u201cappend\u201d option append transformed columns with a postfix \"_bxcx_<lambda>\" to the input dataset. (Default value = \"replace\") print_impact True, False This argument is to print before and after descriptive statistics of the transformed features (Default value = False) Returns ------- DataFrame Transformed Dataframe \"\"\" num_cols = attributeType_segregation ( idf )[ 0 ] if list_of_cols == \"all\" : list_of_cols = num_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in drop_cols ])) if ( len ( list_of_cols ) == 0 ) | ( any ( x not in num_cols for x in list_of_cols )): raise TypeError ( \"Invalid input for Column(s)\" ) num_cols = attributeType_segregation ( idf . select ( list_of_cols ))[ 0 ] list_of_cols = num_cols odf = idf col_mins = idf . select ([ F . min ( i ) for i in list_of_cols ]) if any ([ i <= 0 for i in col_mins . rdd . flatMap ( lambda x : x ) . collect ()]): col_mins . show ( 1 , False ) raise ValueError ( \"Data must be positive\" ) if boxcox_lambda is not None : if isinstance ( boxcox_lambda , ( list , tuple )): if len ( boxcox_lambda ) != len ( list_of_cols ): raise TypeError ( \"Invalid input for boxcox_lambda\" ) elif not all ([ isinstance ( l , ( float , int )) for l in boxcox_lambda ]): raise TypeError ( \"Invalid input for boxcox_lambda\" ) else : boxcox_lambda_list = list ( boxcox_lambda ) elif isinstance ( boxcox_lambda , ( float , int )): boxcox_lambda_list = [ boxcox_lambda ] * len ( list_of_cols ) else : raise TypeError ( \"Invalid input for boxcox_lambda\" ) else : boxcox_lambda_list = [] for i in list_of_cols : lambdaVal = [ 1 , - 1 , 0.5 , - 0.5 , 2 , - 2 , 0.25 , - 0.25 , 3 , - 3 , 4 , - 4 , 5 , - 5 ] best_pVal = 0 for j in lambdaVal : pVal = Statistics . kolmogorovSmirnovTest ( odf . select ( F . pow ( F . col ( i ), j )) . rdd . flatMap ( lambda x : x ), \"norm\" ) . pValue if pVal > best_pVal : best_pVal = pVal best_lambdaVal = j pVal = Statistics . kolmogorovSmirnovTest ( odf . select ( F . log ( F . col ( i ))) . rdd . flatMap ( lambda x : x ), \"norm\" ) . pValue if pVal > best_pVal : best_pVal = pVal best_lambdaVal = 0 boxcox_lambda_list . append ( best_lambdaVal ) output_cols = [] for i , curr_lambdaVal in zip ( list_of_cols , boxcox_lambda_list ): if curr_lambdaVal != 1 : modify_col = ( ( i + \"_bxcx_\" + str ( curr_lambdaVal )) if output_mode == \"append\" else i ) output_cols . append ( modify_col ) if curr_lambdaVal == 0 : odf = odf . withColumn ( modify_col , F . log ( F . col ( i ))) else : odf = odf . withColumn ( modify_col , F . pow ( F . col ( i ), curr_lambdaVal )) if len ( output_cols ) == 0 : warnings . warn ( \"lambdaVal for all columns are 1 so no transformation is performed and idf is returned\" ) return idf if print_impact : print ( \"Transformed Columns: \" , list_of_cols ) print ( \"Best BoxCox Parameter(s): \" , boxcox_lambda_list ) print ( \"Before:\" ) idf . select ( list_of_cols ) . describe () . unionByName ( idf . select ([ F . skewness ( i ) . alias ( i ) for i in list_of_cols ]) . withColumn ( \"summary\" , F . lit ( \"skewness\" ) ) ) . show ( 6 , False ) print ( \"After:\" ) if output_mode == \"replace\" : odf . select ( list_of_cols ) . describe () . unionByName ( odf . select ([ F . skewness ( i ) . alias ( i ) for i in list_of_cols ]) . withColumn ( \"summary\" , F . lit ( \"skewness\" ) ) ) . show ( 6 , False ) else : output_cols = [( \"`\" + i + \"`\" ) for i in output_cols ] odf . select ( output_cols ) . describe () . unionByName ( odf . select ( [ F . skewness ( i ) . alias ( i [ 1 : - 1 ]) for i in output_cols ] ) . withColumn ( \"summary\" , F . lit ( \"skewness\" )) ) . show ( 6 , False ) return odf def outlier_categories ( spark , idf , list_of_cols = \"all\" , drop_cols = [], coverage = 1.0 , max_category = 50 , pre_existing_model = False , model_path = \"NA\" , output_mode = \"replace\" , print_impact = False , ): \"\"\" This function replaces less frequently seen values (called as outlier values in the current context) in a categorical column by 'others'. Outlier values can be defined in two ways \u2013 a) Max N categories, where N is used defined value. In this method, top N-1 frequently seen categories are considered and rest are clubbed under single category 'others'. or Alternatively, b) Coverage \u2013 top frequently seen categories are considered till it covers minimum N% of rows and rest lesser seen values are mapped to mapped to others. Even if the Coverage is less, maximum category constraint is given priority. Further, there is a caveat that when multiple categories have same rank. Then, number of categorical values can be more than max_category defined by the user. Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of categorical columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all categorical columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) coverage Defines the minimum % of rows that will be mapped to actual category name and the rest to be mapped to others and takes value between 0 to 1. Coverage of 0.8 can be interpreted as top frequently seen categories are considered till it covers minimum 80% of rows and rest lesser seen values are mapped to others. (Default value = 1.0) max_category Even if coverage is less, only (max_category - 1) categories will be mapped to actual name and rest to others. Caveat is when multiple categories have same rank, then #categories can be more than max_category. (Default value = 50) pre_existing_model Boolean argument \u2013 True or False. True if the model with the outlier/other values for each attribute exists already to be used, False Otherwise. (Default value = False) model_path If pre_existing_model is True, this argument is path for the pre-saved model. If pre_existing_model is False, this field can be used for saving the model. Default \"NA\" means there is neither pre-existing model nor there is a need to save one. output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with transformed column. \u201cappend\u201d option append transformed column to the input dataset with a postfix \"_outliered\" e.g. column X is appended as X_outliered. (Default value = \"replace\") print_impact True, False This argument is to print before and after unique count of the transformed features (Default value = False) Returns ------- DataFrame Transformed Dataframe \"\"\" cat_cols = attributeType_segregation ( idf )[ 1 ] if list_of_cols == \"all\" : list_of_cols = cat_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in drop_cols ])) if any ( x not in cat_cols for x in list_of_cols ): raise TypeError ( \"Invalid input for Column(s)\" ) if len ( list_of_cols ) == 0 : warnings . warn ( \"No Outlier Categories Computation - No categorical column(s) to transform\" ) return idf if ( coverage <= 0 ) | ( coverage > 1 ): raise TypeError ( \"Invalid input for Coverage Value\" ) if max_category < 2 : raise TypeError ( \"Invalid input for Maximum No. of Categories Allowed\" ) if output_mode not in ( \"replace\" , \"append\" ): raise TypeError ( \"Invalid input for output_mode\" ) if pre_existing_model : df_model = spark . read . csv ( model_path + \"/outlier_categories\" , header = True , inferSchema = True ) else : for index , i in enumerate ( list_of_cols ): window = Window . partitionBy () . orderBy ( F . desc ( \"count_pct\" )) df_cats = ( idf . groupBy ( i ) . count () . dropna () . withColumn ( \"count_pct\" , F . col ( \"count\" ) / F . sum ( \"count\" ) . over ( Window . partitionBy ()), ) . withColumn ( \"rank\" , F . rank () . over ( window )) . withColumn ( \"cumu\" , F . sum ( \"count_pct\" ) . over ( window . rowsBetween ( Window . unboundedPreceding , 0 ) ), ) . withColumn ( \"lag_cumu\" , F . lag ( \"cumu\" ) . over ( window )) . fillna ( 0 ) . where ( ~ (( F . col ( \"cumu\" ) >= coverage ) & ( F . col ( \"lag_cumu\" ) >= coverage ))) . where ( F . col ( \"rank\" ) <= ( max_category - 1 )) . select ( F . lit ( i ) . alias ( \"attribute\" ), F . col ( i ) . alias ( \"parameters\" )) ) if index == 0 : df_model = df_cats else : df_model = df_model . union ( df_cats ) odf = idf for i in list_of_cols : parameters = ( df_model . where ( F . col ( \"attribute\" ) == i ) . select ( \"parameters\" ) . rdd . flatMap ( lambda x : x ) . collect () ) if output_mode == \"replace\" : odf = odf . withColumn ( i , F . when ( ( F . col ( i ) . isin ( parameters )) | ( F . col ( i ) . isNull ()), F . col ( i ) ) . otherwise ( \"others\" ), ) else : odf = odf . withColumn ( i + \"_outliered\" , F . when ( ( F . col ( i ) . isin ( parameters )) | ( F . col ( i ) . isNull ()), F . col ( i ) ) . otherwise ( \"others\" ), ) if ( not pre_existing_model ) & ( model_path != \"NA\" ): df_model . repartition ( 1 ) . write . csv ( model_path + \"/outlier_categories\" , header = True , mode = \"overwrite\" ) if print_impact : if output_mode == \"replace\" : output_cols = list_of_cols else : output_cols = [( i + \"_outliered\" ) for i in list_of_cols ] uniqueCount_computation ( spark , idf , list_of_cols ) . select ( \"attribute\" , F . col ( \"unique_values\" ) . alias ( \"uniqueValues_before\" ) ) . show ( len ( list_of_cols ), False ) uniqueCount_computation ( spark , odf , output_cols ) . select ( \"attribute\" , F . col ( \"unique_values\" ) . alias ( \"uniqueValues_after\" ) ) . show ( len ( list_of_cols ), False ) return odf def expression_parser ( idf , list_of_expr , postfix = \"\" , print_impact = False ): \"\"\" expression_parser can be used to evaluate a list of SQL expressions and output the result as new features. It is able to handle column names containing special characters such as \u201c.\u201d, \u201c-\u201d, \u201c@\u201d, \u201c^\u201d, etc, by converting them to \u201c_\u201d first before the evaluation and convert them back to the original names before returning the output dataframe. Parameters ---------- idf Input Dataframe list_of_expr List of expressions to evaluate as new features e.g., [\"expr1\",\"expr2\"]. Alternatively, expressions can be specified in a string format, where different expressions are separated by pipe delimiter \u201c|\u201d e.g., \"expr1|expr2\". postfix postfix for new feature name.Naming convention \"f\" + expression_index + postfix e.g. with postfix of \"new\", new added features are named as f0new, f1new etc. (Default value = \"\") print_impact True, False This argument is to print the descriptive statistics of the parsed features (Default value = False) Returns ------- DataFrame Parsed Dataframe \"\"\" if isinstance ( list_of_expr , str ): list_of_expr = [ x . strip () for x in list_of_expr . split ( \"|\" )] special_chars = [ \"&\" , \"$\" , \";\" , \":\" , \",\" , \"*\" , \"#\" , \"@\" , \"?\" , \"%\" , \"!\" , \"^\" , \"(\" , \")\" , \"-\" , \"/\" , \"'\" , \".\" , '\"' , ] rename_cols = [] replace_chars = {} for char in special_chars : for col in idf . columns : if char in col : rename_cols . append ( col ) if col in replace_chars . keys (): ( replace_chars [ col ]) . append ( char ) else : replace_chars [ col ] = [ char ] rename_mapping_to_new , rename_mapping_to_old = {}, {} idf_renamed = idf for col in rename_cols : new_col = col for char in replace_chars [ col ]: new_col = new_col . replace ( char , \"_\" ) rename_mapping_to_old [ new_col ] = col rename_mapping_to_new [ col ] = new_col idf_renamed = idf_renamed . withColumnRenamed ( col , new_col ) list_of_expr_ = [] for expr in list_of_expr : new_expr = expr for col in rename_cols : if col in expr : new_expr = new_expr . replace ( col , rename_mapping_to_new [ col ]) list_of_expr_ . append ( new_expr ) list_of_expr = list_of_expr_ odf = idf_renamed new_cols = [] for index , exp in enumerate ( list_of_expr ): odf = odf . withColumn ( \"f\" + str ( index ) + postfix , F . expr ( exp )) new_cols . append ( \"f\" + str ( index ) + postfix ) for new_col , col in rename_mapping_to_old . items (): odf = odf . withColumnRenamed ( new_col , col ) if print_impact : print ( \"Columns Added: \" , new_cols ) odf . select ( new_cols ) . describe () . show ( 5 , False ) return odf","title":"transformers"},{"location":"api/data_transformer/transformers.html#functions","text":"def IQR_standardization ( spark, idf, list_of_cols='all', drop_cols=[], pre_existing_model=False, model_path='NA', output_mode='replace', print_impact=False)","title":"Functions"},{"location":"api/drift/_index.html","text":"Overview Sub-modules anovos.drift.detector anovos.drift.distances anovos.drift.validations","title":"Overview"},{"location":"api/drift/_index.html#overview","text":"","title":"Overview"},{"location":"api/drift/_index.html#sub-modules","text":"anovos.drift.detector anovos.drift.distances anovos.drift.validations","title":"Sub-modules"},{"location":"api/drift/detector.html","text":"detector Expand source code # coding=utf-8 from __future__ import division , print_function import numpy as np import pandas as pd import pyspark from loguru import logger from pyspark.sql import SparkSession , DataFrame from pyspark.sql import functions as F from pyspark.sql import types as T from scipy.stats import variation import sympy as sp from anovos.data_ingest.data_ingest import concatenate_dataset from anovos.data_transformer.transformers import attribute_binning from anovos.shared.utils import attributeType_segregation from .distances import hellinger , psi , js_divergence , ks from .validations import check_distance_method , check_list_of_columns from ..shared.utils import platform_root_path @check_distance_method @check_list_of_columns def statistics ( spark : SparkSession , idf_target : DataFrame , idf_source : DataFrame , * , list_of_cols : list = \"all\" , drop_cols : list = None , method_type : str = \"PSI\" , bin_method : str = \"equal_range\" , bin_size : int = 10 , threshold : float = 0.1 , pre_existing_source : bool = False , source_path : str = \"NA\" , model_directory : str = \"drift_statistics\" , run_type : str = \"local\" , print_impact : bool = False , ): \"\"\" When the performance of a deployed machine learning model degrades in production, one potential reason is that the data used in training and prediction are not following the same distribution. Data drift mainly includes the following manifestations: - Covariate shift: training and test data follow different distributions. For example, An algorithm predicting income that is trained on younger population but tested on older population. - Prior probability shift: change of prior probability. For example in a spam classification problem, the proportion of spam emails changes from 0.2 in training data to 0.6 in testing data. - Concept shift: the distribution of the target variable changes given fixed input values. For example in the same spam classification problem, emails tagged as spam in training data are more likely to be tagged as non-spam in testing data. In our module, we mainly focus on covariate shift detection. In summary, given 2 datasets, source and target datasets, we would like to quantify the drift of some numerical attributes from source to target datasets. The whole process can be broken down into 2 steps: (1) convert each attribute of interest in source and target datasets into source and target probability distributions. (2) calculate the statistical distance between source and target distributions for each attribute. In the first step, attribute_binning is firstly performed to bin the numerical attributes of the source dataset, which requires two input variables: bin_method and bin_size. The same binning method is applied on the target dataset to align two results. The probability distributions are computed by dividing the frequency of each bin by the total frequency. In the second step, 4 choices of statistical metrics are provided to measure the data drift of an attribute from source to target distribution: Population Stability Index (PSI), Jensen-Shannon Divergence (JSD), Hellinger Distance (HD) and Kolmogorov-Smirnov Distance (KS). They are calculated as below: For two discrete probability distributions *P=(p_1,\u2026,p_k)* and *Q=(q_1,\u2026,q_k),* ![https://raw.githubusercontent.com/anovos/anovos-docs/main/docs/assets/drift_stats_formulae.png](https://raw.githubusercontent.com/anovos/anovos-docs/main/docs/assets/drift_stats_formulae.png) A threshold can be set to flag out drifted attributes. If multiple statistical metrics have been calculated, an attribute will be marked as drifted if any of its statistical metric is larger than the threshold. This function can be used in many scenarios. For example: 1. Attribute level data drift can be analysed together with the attribute importance of a machine learning model. The more important an attribute is, the more attention it needs to be given if drift presents. 2. To analyse data drift over time, one can treat one dataset as the source / baseline dataset and multiple datasets as the target datasets. Drift analysis can be performed between the source dataset and each of the target dataset to quantify the drift over time. Parameters ---------- spark Spark Session idf_target Input Dataframe idf_source Baseline/Source Dataframe. This argument is ignored if pre_existing_source is True. list_of_cols List of columns to check drift e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all (non-array) columns for analysis. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". (Default value = None) method_type \"PSI\", \"JSD\", \"HD\", \"KS\",\"all\". \"all\" can be passed to calculate all drift metrics. One or more methods can be passed in a form of list or string where different metrics are separated by pipe delimiter \u201c|\u201d e.g. [\"PSI\", \"JSD\"] or \"PSI|JSD\". (Default value = \"PSI\") bin_method \"equal_frequency\", \"equal_range\". In \"equal_range\" method, each bin is of equal size/width and in \"equal_frequency\", each bin has equal no. of rows, though the width of bins may vary. (Default value = \"equal_range\") bin_size Number of bins for creating histogram. (Default value = 10) threshold A column is flagged if any drift metric is above the threshold. (Default value = 0.1) pre_existing_source Boolean argument \u2013 True or False. True if the drift_statistics folder (binning model & frequency counts for each attribute) exists already, False Otherwise. (Default value = False) source_path If pre_existing_source is False, this argument can be used for saving the drift_statistics folder. The drift_statistics folder will have attribute_binning (binning model) & frequency_counts sub-folders. If pre_existing_source is True, this argument is path for referring the drift_statistics folder. Default \"NA\" for temporarily saving data in \"intermediate_data/\" folder. (Default value = \"NA\") model_directory If pre_existing_source is False, this argument can be used for saving the drift stats to folder. The default drift statics directory is drift_statistics folder will have attribute_binning If pre_existing_source is True, this argument is model_directory for referring the drift statistics dir. Default \"drift_statistics\" for temporarily saving source dataset attribute_binning folder. (Default value = \"drift_statistics\") run_type \"local\", \"emr\", \"databricks\" (Default value = \"local\") print_impact True, False. (Default value = False) This argument is to print out the drift statistics of all attributes and attributes meeting the threshold. Returns ------- DataFrame [attribute, *metric, flagged] Number of columns will be dependent on method argument. There will be one column for each drift method/metric. \"\"\" drop_cols = drop_cols or [] num_cols = attributeType_segregation ( idf_target . select ( list_of_cols ))[ 0 ] if run_type in list ( platform_root_path . keys ()): root_path = platform_root_path [ run_type ] else : root_path = \"\" if source_path == \"NA\" : source_path = root_path + \"intermediate_data\" if not pre_existing_source : source_bin = attribute_binning ( spark , idf_source , list_of_cols = num_cols , method_type = bin_method , bin_size = bin_size , pre_existing_model = False , model_path = source_path + \"/\" + model_directory , ) source_bin . persist ( pyspark . StorageLevel . MEMORY_AND_DISK ) . count () target_bin = attribute_binning ( spark , idf_target , list_of_cols = num_cols , method_type = bin_method , bin_size = bin_size , pre_existing_model = True , model_path = source_path + \"/\" + model_directory , ) target_bin . persist ( pyspark . StorageLevel . MEMORY_AND_DISK ) . count () result = { \"attribute\" : [], \"flagged\" : []} for method in method_type : result [ method ] = [] for i in list_of_cols : if pre_existing_source : x = spark . read . csv ( source_path + \"/\" + model_directory + \"/frequency_counts/\" + i , header = True , inferSchema = True , ) else : x = ( source_bin . groupBy ( i ) . agg (( F . count ( i ) / idf_source . count ()) . alias ( \"p\" )) . fillna ( - 1 ) ) x . coalesce ( 1 ) . write . csv ( source_path + \"/\" + model_directory + \"/frequency_counts/\" + i , header = True , mode = \"overwrite\" , ) y = ( target_bin . groupBy ( i ) . agg (( F . count ( i ) / idf_target . count ()) . alias ( \"q\" )) . fillna ( - 1 ) ) xy = ( x . join ( y , i , \"full_outer\" ) . fillna ( 0.0001 , subset = [ \"p\" , \"q\" ]) . replace ( 0 , 0.0001 ) . orderBy ( i ) ) p = np . array ( xy . select ( \"p\" ) . rdd . flatMap ( lambda x : x ) . collect ()) q = np . array ( xy . select ( \"q\" ) . rdd . flatMap ( lambda x : x ) . collect ()) result [ \"attribute\" ] . append ( i ) counter = 0 for idx , method in enumerate ( method_type ): drift_function = { \"PSI\" : psi , \"JSD\" : js_divergence , \"HD\" : hellinger , \"KS\" : ks , } metric = float ( round ( drift_function [ method ]( p , q ), 4 )) result [ method ] . append ( metric ) if counter == 0 : if metric > threshold : result [ \"flagged\" ] . append ( 1 ) counter = 1 if ( idx == ( len ( method_type ) - 1 )) & ( counter == 0 ): result [ \"flagged\" ] . append ( 0 ) odf = ( spark . createDataFrame ( pd . DataFrame . from_dict ( result , orient = \"index\" ) . transpose () ) . select ([ \"attribute\" ] + method_type + [ \"flagged\" ]) . orderBy ( F . desc ( \"flagged\" )) ) if print_impact : logger . info ( \"All Attributes:\" ) odf . show ( len ( list_of_cols )) logger . info ( \"Attributes meeting Data Drift threshold:\" ) drift = odf . where ( F . col ( \"flagged\" ) == 1 ) drift . show ( drift . count ()) return odf def stability_index_computation ( spark , * idfs , list_of_cols = \"all\" , drop_cols = [], metric_weightages = { \"mean\" : 0.5 , \"stddev\" : 0.3 , \"kurtosis\" : 0.2 }, existing_metric_path = \"\" , appended_metric_path = \"\" , threshold = 1 , print_impact = False , ): \"\"\" The data stability is represented by a single metric to summarise the stability of an attribute over multiple time periods. For example, given 6 datasets collected in 6 consecutive time periods (D1, D2, \u2026, D6), data stability index of an attribute measures how stable the attribute is from D1 to D6. The major difference between data drift and data stability is that data drift analysis is only based on 2 datasets: source and target. However data stability analysis can be performed on multiple datasets. In addition, the source dataset is not required indicating that the stability index can be directly computed among multiple target datasets by comparing the statistical properties among them. In summary, given N datasets representing different time periods, we would like to measure the stability of some numerical attributes from the first to the N-th dataset. The whole process can be broken down into 2 steps: (1) Choose a few statistical metrics to describe the distribution of each attribute at each time period. (2) Compute attribute level stability by combining the stability of each statistical metric over time periods. In the first step, we choose mean, standard deviation and kurtosis as the statistical metrics in our implementation. Intuitively, they represent different aspects of a distribution: mean measures central tendency, standard deviation measures dispersion and kurtosis measures shape of a distribution. Reasons of selecting those 3 metrics will be explained in a later section. With mean, standard deviation and kurtosis computed for each attribute at each time interval, we can form 3 arrays of size N for each attribute. In the second step, Coefficient of Variation (CV) is used to measure the stability of each metric. CV represents the ratio of the standard deviation to the mean, which is a unitless statistic to compare the relative variation from one array to another. Considering the wide potential range of CV, the absolute value of CV is then mapped to an integer between 0 and 4 according to the table below, where 0 indicates highly unstable and 4 indicates highly stable. We call this integer a metric stability index. | abs(CV) Interval | Metric Stability Index | |------------------|------------------------| | [0, 0.03) | 4 | | [0.03, 0.1) | 3 | | [0.1, 0.2) | 2 | | [0.2, 0.5) | 1 | | [0.5, +inf) | 0 | Finally, the attribute stability index (SI) is a weighted sum of 3 metric stability indexes, where we assign 50% for mean, 30% for standard deviation and 20% for kurtosis by default. The final output is a float between 0 and 4 and an attribute can be classified as one of the following categories: very unstable (0\u2264SI<1), unstable (1\u2264SI<2), marginally stable (2\u2264SI<3), stable (3\u2264SI<3.5) and very stable (3.5\u2264SI\u22644). For example, there are 6 samples of attribute X from T1 to T6. For each sample, we have computed the statistical metrics of X from T1 to T6: | idx | Mean | Standard deviation | Kurtosis | |-----|------|--------------------|----------| | 1 | 11 | 2 | 3.9 | | 2 | 12 | 1 | 4.2 | | 3 | 15 | 3 | 4.0 | | 4 | 10 | 2 | 4.1 | | 5 | 11 | 1 | 4.2 | | 6 | 13 | 0.5 | 4.0 | Then we calculate the Coefficient of Variation for each array: - CV of mean = CV([11, 12, 15, 10, 11, 13]) = 0.136 - CV of standard deviation = CV([2, 1, 3, 2, 1, 0.5]) = 0.529 - CV of kurtosis = CV([3.9, 4.2, 4.0, 4.1, 4.2, 4.0]) = 0.027 Metric stability indexes are then computed by mapping each CV value to an integer accordingly. As a result, metric stability index is 2 for mean, 0 for standard deviation and 4 for kurtosis. Why mean is chosen over median? - Dummy variables which take only the value 0 or 1 are frequently seen in machine learning features. Mean of a dummy variable represents the proportion of value 1 and median of a dummy variable is either 0 or 1 whichever is more frequent. However, CV may not work well when 0 appears in the array or the array contains both positive and negative values. For example, intuitively [0,0,0,0,0,1,0,0,0] is a stable array but its CV is 2.83 which is extremely high, but cv of [0.45,0.44,0.48,0.49,0.42,0.52,0.49,0.47,0.48] is 0.06 which is much more reasonable. Thus we decided to use mean instead of median. Although median is considered as a more robust choice, outlier treatment can be applied prior to data stability analysis to handle this issue. Why kurtosis is chosen over skewness? - Kurtosis is a positive value (note that we use kurtosis instead of excess kurtosis which) but skewness can range from \u2013inf to +inf. Usually, if skewness is between -0.5 and 0.5, the distribution is approximately symmetric. Thus, if the skewness fluctuates around 0, the CV is highly likely to be high or invalid because the mean will be close to 0. Stability index is preferred in the following scenario: - Pairwise drift analysis can be performed between the source dataset and each of the target dataset to quantify the drift over time. However this can be time-consuming especially when the number of target dataset is large. In this case, measuring data stability instead of data drift would be a much faster alternative and the source/baseline dataset is not required as well Troubleshooting - If the attribute stability index appears to be nan, it may due to one of the following reasons: - One metric ( likely to be kurtosis) is nan. For example, the kurtosis of a sample is nan If its standard deviation is 0. - The mean of a metric from the first to the N-th dataset is zero, causing the denominator of CV to be 0. For example, when mean of attribute X is always zero for all datasets, its stability index would be nan. Limitation - Limitation of CV: CV may not work well when 0 appears in the array or the array contains both positive and negative values. Parameters ---------- spark Spark Session idfs Variable number of input dataframes list_of_cols List of numerical columns to check stability e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all numerical columns for analysis. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". (Default value = []) metric_weightages Takes input in dictionary format with keys being the metric name - \"mean\",\"stdev\",\"kurtosis\" and value being the weightage of the metric (between 0 and 1). Sum of all weightages must be 1. (Default value = {\"mean\": 0.5, \"stddev\": 0.3, \"kurtosis\": 0.2}) existing_metric_path This argument is path for referring pre-existing metrics of historical datasets and is of schema [idx, attribute, mean, stdev, kurtosis]. idx is index number of historical datasets assigned in chronological order. (Default value = \"\") appended_metric_path This argument is path for saving input dataframes metrics after appending to the historical datasets' metrics. (Default value = \"\") threshold A column is flagged if the stability index is below the threshold, which varies between 0 to 4. The following criteria can be used to classifiy stability_index (SI): very unstable: 0\u2264SI<1, unstable: 1\u2264SI<2, marginally stable: 2\u2264SI<3, stable: 3\u2264SI<3.5 and very stable: 3.5\u2264SI\u22644. (Default value = 1) print_impact True, False (Default value = False) This argument is to print out the stability metrics of all attributes and potential unstable attributes. Returns ------- DataFrame [attribute, mean_si, stddev_si, kurtosis_si, mean_cv, stddev_cv, kurtosis_cv, stability_index]. *_cv is coefficient of variation for each metric. *_si is stability index for each metric. stability_index is net weighted stability index based on the individual metrics' stability index. \"\"\" num_cols = attributeType_segregation ( idfs [ 0 ])[ 0 ] if list_of_cols == \"all\" : list_of_cols = num_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in drop_cols ])) if any ( x not in num_cols for x in list_of_cols ) | ( len ( list_of_cols ) == 0 ): raise TypeError ( \"Invalid input for Column(s)\" ) if ( round ( metric_weightages . get ( \"mean\" , 0 ) + metric_weightages . get ( \"stddev\" , 0 ) + metric_weightages . get ( \"kurtosis\" , 0 ), 3 , ) != 1 ): raise ValueError ( \"Invalid input for metric weightages. Either metric name is incorrect or sum of metric weightages is not 1.0\" ) if existing_metric_path : existing_metric_df = spark . read . csv ( existing_metric_path , header = True , inferSchema = True ) dfs_count = existing_metric_df . select ( F . max ( F . col ( \"idx\" ))) . first ()[ 0 ] else : schema = T . StructType ( [ T . StructField ( \"idx\" , T . IntegerType (), True ), T . StructField ( \"attribute\" , T . StringType (), True ), T . StructField ( \"mean\" , T . DoubleType (), True ), T . StructField ( \"stddev\" , T . DoubleType (), True ), T . StructField ( \"kurtosis\" , T . DoubleType (), True ), ] ) existing_metric_df = spark . sparkContext . emptyRDD () . toDF ( schema ) dfs_count = 0 metric_ls = [] for idf in idfs : for i in list_of_cols : mean , stddev , kurtosis = idf . select ( F . mean ( i ), F . stddev ( i ), F . kurtosis ( i ) ) . first () metric_ls . append ( [ dfs_count + 1 , i , mean , stddev , kurtosis + 3.0 if kurtosis else None ] ) dfs_count += 1 new_metric_df = spark . createDataFrame ( metric_ls , schema = ( \"idx\" , \"attribute\" , \"mean\" , \"stddev\" , \"kurtosis\" ) ) appended_metric_df = concatenate_dataset ( existing_metric_df , new_metric_df ) if appended_metric_path : appended_metric_df . coalesce ( 1 ) . write . csv ( appended_metric_path , header = True , mode = \"overwrite\" ) result = [] for i in list_of_cols : i_output = [ i ] for metric in [ \"mean\" , \"stddev\" , \"kurtosis\" ]: metric_stats = ( appended_metric_df . where ( F . col ( \"attribute\" ) == i ) . orderBy ( \"idx\" ) . select ( metric ) . fillna ( np . nan ) . rdd . flatMap ( list ) . collect () ) metric_cv = round ( float ( variation ([ a for a in metric_stats ])), 4 ) or None i_output . append ( metric_cv ) result . append ( i_output ) schema = T . StructType ( [ T . StructField ( \"attribute\" , T . StringType (), True ), T . StructField ( \"mean_cv\" , T . FloatType (), True ), T . StructField ( \"stddev_cv\" , T . FloatType (), True ), T . StructField ( \"kurtosis_cv\" , T . FloatType (), True ), ] ) odf = spark . createDataFrame ( result , schema = schema ) def score_cv ( cv , thresholds = [ 0.03 , 0.1 , 0.2 , 0.5 ]): if cv is None : return None else : cv = abs ( cv ) stability_index = [ 4 , 3 , 2 , 1 , 0 ] for i , thresh in enumerate ( thresholds ): if cv < thresh : return stability_index [ i ] return stability_index [ - 1 ] f_score_cv = F . udf ( score_cv , T . IntegerType ()) odf = ( odf . replace ( np . nan , None ) . withColumn ( \"mean_si\" , f_score_cv ( F . col ( \"mean_cv\" ))) . withColumn ( \"stddev_si\" , f_score_cv ( F . col ( \"stddev_cv\" ))) . withColumn ( \"kurtosis_si\" , f_score_cv ( F . col ( \"kurtosis_cv\" ))) . withColumn ( \"stability_index\" , F . round ( ( F . col ( \"mean_si\" ) * metric_weightages . get ( \"mean\" , 0 ) + F . col ( \"stddev_si\" ) * metric_weightages . get ( \"stddev\" , 0 ) + F . col ( \"kurtosis_si\" ) * metric_weightages . get ( \"kurtosis\" , 0 ) ), 4 , ), ) . withColumn ( \"flagged\" , F . when ( ( F . col ( \"stability_index\" ) < threshold ) | ( F . col ( \"stability_index\" ) . isNull ()), 1 , ) . otherwise ( 0 ), ) ) if print_impact : logger . info ( \"All Attributes:\" ) odf . show ( len ( list_of_cols )) logger . info ( \"Potential Unstable Attributes:\" ) unstable = odf . where ( F . col ( \"flagged\" ) == 1 ) unstable . show ( unstable . count ()) return odf def feature_stability_estimation ( spark , attribute_stats , attribute_transformation , metric_weightages = { \"mean\" : 0.5 , \"stddev\" : 0.3 , \"kurtosis\" : 0.2 }, threshold = 1 , print_impact = False , ): \"\"\" This function is able to estimate the stability index of a new feature composed of certain attributes whose stability metrics are known. For example, the new feature F can be expressed as F = g(X1, X2, \u2026, Xn), where X1, X2, \u2026, Xn represent different attributes and g represents the transformation function. The most straightforward way is to generate the new feature for all periods and calculate its stability index. However, it requires reading all historical data again which can be unrealistic for large datasets. Thus, the objective of this function is to estimate feature stability index without reading historical data. One example can be the following scenario: we have attributes A and B, we have their respective stability statistics from T1 to T7. At T7 we realise we need to generate a new feature: A/B, but we don\u2019t have statistics metrics of A/B from T1 to T6 and this is where this function can be applied to generate an estimation without reading datasets from T1 to T6. The estimation can be broken down into 3 steps. 1. Estimate mean and stddev for the new feature based on attribute metrics (no existing resource found to estimate Feature kurtosis). Estimated mean and stddev are generated for each time period using the formula below according to [1]: ![https://raw.githubusercontent.com/anovos/anovos-docs/main/docs/assets/feature_stability_formulae.png](https://raw.githubusercontent.com/anovos/anovos-docs/main/docs/assets/feature_stability_formulae.png) 2. Calculate Coefficient of variation (CV) for estimated feature mean and stddev. Each CV can be then mapped to an integer between 0 and 4 to generate the metric stability index. 3. Similar to the attribute stability index, each metric is assigned a weightage between 0 and 1, where the default values are 50 for mean, 30% for standard deviation and 20% for kurtosis. Because we are unable to generate kurtosis stability index, its minimum and maximum possible values (0 and 4) are used to output a range for global stability index (GSI): * Lower bound of GSI = 0.5\u2217mean stability index + 0.3\u2217stddev stability index + 0.2 \u2217 \ud835\udfce * Upper bound of GSI = 0.5\u2217mean stability index + 0.3\u2217stddev stability index + 0.2 \u2217 \ud835\udfd2 [1] Benaroya, H., Han, S. M., & Nagurka, M. (2005). Probability models in engineering and science (Vol. 192, pp. 168-169). CRC press. Parameters ---------- spark Spark Session attribute_stats Spark dataframe. The intermediate dataframe saved by running function stabilityIndex_computation with schema [idx, attribute, mean, stddev, kurtosis]. It should contain all the attributes used in argument attribute_transformation. attribute_transformation Takes input in dictionary format: each key-value combination represents one new feature. Each key is a string containing all the attributes involved in the new feature seperated by '|'. Each value is the transformation of the attributes in string. For example, {'X|Y|Z': 'X**2+Y/Z', 'A': 'log(A)'} metric_weightages Takes input in dictionary format with keys being the metric name - \"mean\",\"stdev\",\"kurtosis\" and value being the weightage of the metric (between 0 and 1). Sum of all weightages must be 1. (Default value = {\"mean\": 0.5, \"stddev\": 0.3, \"kurtosis\": 0.2}) threshold A column is flagged if the stability index is below the threshold, which varies between 0 and 4. The following criteria can be used to classify stability_index (SI): very unstable: 0\u2264SI<1, unstable: 1\u2264SI<2, marginally stable: 2\u2264SI<3, stable: 3\u2264SI<3.5 and very stable: 3.5\u2264SI\u22644. (Default value = 1) print_impact True, False (Default value = False) This argument is to print out the stability metrics of all newly generated features and potential unstable features. Returns ------- DataFrame [feature_formula, mean_cv, stddev_cv, mean_si, stddev_si, stability_index_lower_bound, stability_index_upper_bound, flagged_lower, flagged_upper]. *_cv is coefficient of variation for each metric. *_si is stability index for each metric. stability_index_lower_bound and stability_index_upper_bound form a range for estimated stability index. flagged_lower and flagged_upper indicate whether the feature is potentially unstable based on the lower and upper bounds for stability index. \"\"\" def stats_estimation ( attributes , transformation , mean , stddev ): attribute_means = list ( zip ( attributes , mean )) first_dev = [] second_dev = [] est_mean = 0 est_var = 0 for attr , s in zip ( attributes , stddev ): first_dev = sp . diff ( transformation , attr ) second_dev = sp . diff ( transformation , attr , 2 ) est_mean += s ** 2 * second_dev . subs ( attribute_means ) / 2 est_var += s ** 2 * ( first_dev . subs ( attribute_means )) ** 2 transformation = sp . parse_expr ( transformation ) est_mean += transformation . subs ( attribute_means ) return [ float ( est_mean ), float ( est_var )] f_stats_estimation = F . udf ( stats_estimation , T . ArrayType ( T . FloatType ())) index = ( attribute_stats . select ( \"idx\" ) . distinct () . orderBy ( \"idx\" ) . rdd . flatMap ( list ) . collect () ) attribute_names = list ( attribute_transformation . keys ()) transformations = list ( attribute_transformation . values ()) feature_metric = [] for attributes , transformation in zip ( attribute_names , transformations ): attributes = [ x . strip () for x in attributes . split ( \"|\" )] for idx in index : attr_mean_list , attr_stddev_list = [], [] for attr in attributes : df_temp = attribute_stats . where ( ( F . col ( \"idx\" ) == idx ) & ( F . col ( \"attribute\" ) == attr ) ) if df_temp . count () == 0 : raise TypeError ( \"Invalid input for attribute_stats: all involved attributes must have available statistics across all time periods (idx)\" ) attr_mean_list . append ( df_temp . select ( \"mean\" ) . rdd . flatMap ( lambda x : x ) . collect ()[ 0 ] ) attr_stddev_list . append ( df_temp . select ( \"stddev\" ) . rdd . flatMap ( lambda x : x ) . collect ()[ 0 ] ) feature_metric . append ( [ idx , transformation , attributes , attr_mean_list , attr_stddev_list ] ) schema = T . StructType ( [ T . StructField ( \"idx\" , T . IntegerType (), True ), T . StructField ( \"transformation\" , T . StringType (), True ), T . StructField ( \"attributes\" , T . ArrayType ( T . StringType ()), True ), T . StructField ( \"attr_mean_list\" , T . ArrayType ( T . FloatType ()), True ), T . StructField ( \"attr_stddev_list\" , T . ArrayType ( T . FloatType ()), True ), ] ) df_feature_metric = ( spark . createDataFrame ( feature_metric , schema = schema ) . withColumn ( \"est_feature_stats\" , f_stats_estimation ( \"attributes\" , \"transformation\" , \"attr_mean_list\" , \"attr_stddev_list\" ), ) . withColumn ( \"est_feature_mean\" , F . col ( \"est_feature_stats\" )[ 0 ]) . withColumn ( \"est_feature_stddev\" , F . sqrt ( F . col ( \"est_feature_stats\" )[ 1 ])) . select ( \"idx\" , \"attributes\" , \"transformation\" , \"est_feature_mean\" , \"est_feature_stddev\" , ) ) output = [] for idx , i in enumerate ( transformations ): i_output = [ i ] for metric in [ \"est_feature_mean\" , \"est_feature_stddev\" ]: metric_stats = ( df_feature_metric . where ( F . col ( \"transformation\" ) == i ) . orderBy ( \"idx\" ) . select ( metric ) . fillna ( np . nan ) . rdd . flatMap ( list ) . collect () ) metric_cv = round ( float ( variation ([ a for a in metric_stats ])), 4 ) or None i_output . append ( metric_cv ) output . append ( i_output ) schema = T . StructType ( [ T . StructField ( \"feature_formula\" , T . StringType (), True ), T . StructField ( \"mean_cv\" , T . FloatType (), True ), T . StructField ( \"stddev_cv\" , T . FloatType (), True ), ] ) odf = spark . createDataFrame ( output , schema = schema ) def score_cv ( cv , thresholds = [ 0.03 , 0.1 , 0.2 , 0.5 ]): if cv is None : return None else : cv = abs ( cv ) stability_index = [ 4 , 3 , 2 , 1 , 0 ] for i , thresh in enumerate ( thresholds ): if cv < thresh : return stability_index [ i ] return stability_index [ - 1 ] f_score_cv = F . udf ( score_cv , T . IntegerType ()) odf = ( odf . replace ( np . nan , None ) . withColumn ( \"mean_si\" , f_score_cv ( F . col ( \"mean_cv\" ))) . withColumn ( \"stddev_si\" , f_score_cv ( F . col ( \"stddev_cv\" ))) . withColumn ( \"stability_index_lower_bound\" , F . round ( F . col ( \"mean_si\" ) * metric_weightages . get ( \"mean\" , 0 ) + F . col ( \"stddev_si\" ) * metric_weightages . get ( \"stddev\" , 0 ), 4 , ), ) . withColumn ( \"stability_index_upper_bound\" , F . round ( F . col ( \"stability_index_lower_bound\" ) + 4 * metric_weightages . get ( \"kurtosis\" , 0 ), 4 , ), ) . withColumn ( \"flagged_lower\" , F . when ( ( F . col ( \"stability_index_lower_bound\" ) < threshold ) | ( F . col ( \"stability_index_lower_bound\" ) . isNull ()), 1 , ) . otherwise ( 0 ), ) . withColumn ( \"flagged_upper\" , F . when ( ( F . col ( \"stability_index_upper_bound\" ) < threshold ) | ( F . col ( \"stability_index_upper_bound\" ) . isNull ()), 1 , ) . otherwise ( 0 ), ) ) if print_impact : logger . info ( \"All Features:\" ) odf . show ( len ( attribute_names ), False ) logger . info ( \"Potential Unstable Features Identified by Both Lower and Upper Bounds:\" ) unstable = odf . where ( F . col ( \"flagged_upper\" ) == 1 ) unstable . show ( unstable . count ()) return odf Functions def feature_stability_estimation ( spark, attribute_stats, attribute_transformation, metric_weightages={'mean': 0.5, 'stddev': 0.3, 'kurtosis': 0.2}, threshold=1, print_impact=False) This function is able to estimate the stability index of a new feature composed of certain attributes whose stability metrics are known. For example, the new feature F can be expressed as F = g(X1, X2, \u2026, Xn), where X1, X2, \u2026, Xn represent different attributes and g represents the transformation function. The most straightforward way is to generate the new feature for all periods and calculate its stability index. However, it requires reading all historical data again which can be unrealistic for large datasets. Thus, the objective of this function is to estimate feature stability index without reading historical data. One example can be the following scenario: we have attributes A and B, we have their respective stability statistics from T1 to T7. At T7 we realise we need to generate a new feature: A/B, but we don\u2019t have statistics metrics of A/B from T1 to T6 and this is where this function can be applied to generate an estimation without reading datasets from T1 to T6. The estimation can be broken down into 3 steps. 1. Estimate mean and stddev for the new feature based on attribute metrics (no existing resource found to estimate Feature kurtosis). Estimated mean and stddev are generated for each time period using the formula below according to [1]: 2. Calculate Coefficient of variation (CV) for estimated feature mean and stddev. Each CV can be then mapped to an integer between 0 and 4 to generate the metric stability index. 3. Similar to the attribute stability index, each metric is assigned a weightage between 0 and 1, where the default values are 50 for mean, 30% for standard deviation and 20% for kurtosis. Because we are unable to generate kurtosis stability index, its minimum and maximum possible values (0 and 4) are used to output a range for global stability index (GSI): * Lower bound of GSI = 0.5\u2217mean stability index + 0.3\u2217stddev stability index + 0.2 \u2217 \ud835\udfce * Upper bound of GSI = 0.5\u2217mean stability index + 0.3\u2217stddev stability index + 0.2 \u2217 \ud835\udfd2 [1] Benaroya, H., Han, S. M., & Nagurka, M. (2005). Probability models in engineering and science (Vol. 192, pp. 168-169). CRC press. Parameters spark Spark Session attribute_stats Spark dataframe. The intermediate dataframe saved by running function stabilityIndex_computation with schema [idx, attribute, mean, stddev, kurtosis]. It should contain all the attributes used in argument attribute_transformation. attribute_transformation Takes input in dictionary format: each key-value combination represents one new feature. Each key is a string containing all the attributes involved in the new feature seperated by '|'. Each value is the transformation of the attributes in string. For example, {'X|Y|Z': 'X**2+Y/Z', 'A': 'log(A)'} metric_weightages Takes input in dictionary format with keys being the metric name - \"mean\",\"stdev\",\"kurtosis\" and value being the weightage of the metric (between 0 and 1). Sum of all weightages must be 1. (Default value = {\"mean\": 0.5, \"stddev\": 0.3, \"kurtosis\": 0.2}) threshold A column is flagged if the stability index is below the threshold, which varies between 0 and 4. The following criteria can be used to classify stability_index (SI): very unstable: 0\u2264SI<1, unstable: 1\u2264SI<2, marginally stable: 2\u2264SI<3, stable: 3\u2264SI<3.5 and very stable: 3.5\u2264SI\u22644. (Default value = 1) print_impact True, False (Default value = False) This argument is to print out the stability metrics of all newly generated features and potential unstable features. Returns DataFrame [feature_formula, mean_cv, stddev_cv, mean_si, stddev_si, stability_index_lower_bound, stability_index_upper_bound, flagged_lower, flagged_upper]. _cv is coefficient of variation for each metric. _si is stability index for each metric. stability_index_lower_bound and stability_index_upper_bound form a range for estimated stability index. flagged_lower and flagged_upper indicate whether the feature is potentially unstable based on the lower and upper bounds for stability index. Expand source code def feature_stability_estimation ( spark , attribute_stats , attribute_transformation , metric_weightages = { \"mean\" : 0.5 , \"stddev\" : 0.3 , \"kurtosis\" : 0.2 }, threshold = 1 , print_impact = False , ): \"\"\" This function is able to estimate the stability index of a new feature composed of certain attributes whose stability metrics are known. For example, the new feature F can be expressed as F = g(X1, X2, \u2026, Xn), where X1, X2, \u2026, Xn represent different attributes and g represents the transformation function. The most straightforward way is to generate the new feature for all periods and calculate its stability index. However, it requires reading all historical data again which can be unrealistic for large datasets. Thus, the objective of this function is to estimate feature stability index without reading historical data. One example can be the following scenario: we have attributes A and B, we have their respective stability statistics from T1 to T7. At T7 we realise we need to generate a new feature: A/B, but we don\u2019t have statistics metrics of A/B from T1 to T6 and this is where this function can be applied to generate an estimation without reading datasets from T1 to T6. The estimation can be broken down into 3 steps. 1. Estimate mean and stddev for the new feature based on attribute metrics (no existing resource found to estimate Feature kurtosis). Estimated mean and stddev are generated for each time period using the formula below according to [1]: ![https://raw.githubusercontent.com/anovos/anovos-docs/main/docs/assets/feature_stability_formulae.png](https://raw.githubusercontent.com/anovos/anovos-docs/main/docs/assets/feature_stability_formulae.png) 2. Calculate Coefficient of variation (CV) for estimated feature mean and stddev. Each CV can be then mapped to an integer between 0 and 4 to generate the metric stability index. 3. Similar to the attribute stability index, each metric is assigned a weightage between 0 and 1, where the default values are 50 for mean, 30% for standard deviation and 20% for kurtosis. Because we are unable to generate kurtosis stability index, its minimum and maximum possible values (0 and 4) are used to output a range for global stability index (GSI): * Lower bound of GSI = 0.5\u2217mean stability index + 0.3\u2217stddev stability index + 0.2 \u2217 \ud835\udfce * Upper bound of GSI = 0.5\u2217mean stability index + 0.3\u2217stddev stability index + 0.2 \u2217 \ud835\udfd2 [1] Benaroya, H., Han, S. M., & Nagurka, M. (2005). Probability models in engineering and science (Vol. 192, pp. 168-169). CRC press. Parameters ---------- spark Spark Session attribute_stats Spark dataframe. The intermediate dataframe saved by running function stabilityIndex_computation with schema [idx, attribute, mean, stddev, kurtosis]. It should contain all the attributes used in argument attribute_transformation. attribute_transformation Takes input in dictionary format: each key-value combination represents one new feature. Each key is a string containing all the attributes involved in the new feature seperated by '|'. Each value is the transformation of the attributes in string. For example, {'X|Y|Z': 'X**2+Y/Z', 'A': 'log(A)'} metric_weightages Takes input in dictionary format with keys being the metric name - \"mean\",\"stdev\",\"kurtosis\" and value being the weightage of the metric (between 0 and 1). Sum of all weightages must be 1. (Default value = {\"mean\": 0.5, \"stddev\": 0.3, \"kurtosis\": 0.2}) threshold A column is flagged if the stability index is below the threshold, which varies between 0 and 4. The following criteria can be used to classify stability_index (SI): very unstable: 0\u2264SI<1, unstable: 1\u2264SI<2, marginally stable: 2\u2264SI<3, stable: 3\u2264SI<3.5 and very stable: 3.5\u2264SI\u22644. (Default value = 1) print_impact True, False (Default value = False) This argument is to print out the stability metrics of all newly generated features and potential unstable features. Returns ------- DataFrame [feature_formula, mean_cv, stddev_cv, mean_si, stddev_si, stability_index_lower_bound, stability_index_upper_bound, flagged_lower, flagged_upper]. *_cv is coefficient of variation for each metric. *_si is stability index for each metric. stability_index_lower_bound and stability_index_upper_bound form a range for estimated stability index. flagged_lower and flagged_upper indicate whether the feature is potentially unstable based on the lower and upper bounds for stability index. \"\"\" def stats_estimation ( attributes , transformation , mean , stddev ): attribute_means = list ( zip ( attributes , mean )) first_dev = [] second_dev = [] est_mean = 0 est_var = 0 for attr , s in zip ( attributes , stddev ): first_dev = sp . diff ( transformation , attr ) second_dev = sp . diff ( transformation , attr , 2 ) est_mean += s ** 2 * second_dev . subs ( attribute_means ) / 2 est_var += s ** 2 * ( first_dev . subs ( attribute_means )) ** 2 transformation = sp . parse_expr ( transformation ) est_mean += transformation . subs ( attribute_means ) return [ float ( est_mean ), float ( est_var )] f_stats_estimation = F . udf ( stats_estimation , T . ArrayType ( T . FloatType ())) index = ( attribute_stats . select ( \"idx\" ) . distinct () . orderBy ( \"idx\" ) . rdd . flatMap ( list ) . collect () ) attribute_names = list ( attribute_transformation . keys ()) transformations = list ( attribute_transformation . values ()) feature_metric = [] for attributes , transformation in zip ( attribute_names , transformations ): attributes = [ x . strip () for x in attributes . split ( \"|\" )] for idx in index : attr_mean_list , attr_stddev_list = [], [] for attr in attributes : df_temp = attribute_stats . where ( ( F . col ( \"idx\" ) == idx ) & ( F . col ( \"attribute\" ) == attr ) ) if df_temp . count () == 0 : raise TypeError ( \"Invalid input for attribute_stats: all involved attributes must have available statistics across all time periods (idx)\" ) attr_mean_list . append ( df_temp . select ( \"mean\" ) . rdd . flatMap ( lambda x : x ) . collect ()[ 0 ] ) attr_stddev_list . append ( df_temp . select ( \"stddev\" ) . rdd . flatMap ( lambda x : x ) . collect ()[ 0 ] ) feature_metric . append ( [ idx , transformation , attributes , attr_mean_list , attr_stddev_list ] ) schema = T . StructType ( [ T . StructField ( \"idx\" , T . IntegerType (), True ), T . StructField ( \"transformation\" , T . StringType (), True ), T . StructField ( \"attributes\" , T . ArrayType ( T . StringType ()), True ), T . StructField ( \"attr_mean_list\" , T . ArrayType ( T . FloatType ()), True ), T . StructField ( \"attr_stddev_list\" , T . ArrayType ( T . FloatType ()), True ), ] ) df_feature_metric = ( spark . createDataFrame ( feature_metric , schema = schema ) . withColumn ( \"est_feature_stats\" , f_stats_estimation ( \"attributes\" , \"transformation\" , \"attr_mean_list\" , \"attr_stddev_list\" ), ) . withColumn ( \"est_feature_mean\" , F . col ( \"est_feature_stats\" )[ 0 ]) . withColumn ( \"est_feature_stddev\" , F . sqrt ( F . col ( \"est_feature_stats\" )[ 1 ])) . select ( \"idx\" , \"attributes\" , \"transformation\" , \"est_feature_mean\" , \"est_feature_stddev\" , ) ) output = [] for idx , i in enumerate ( transformations ): i_output = [ i ] for metric in [ \"est_feature_mean\" , \"est_feature_stddev\" ]: metric_stats = ( df_feature_metric . where ( F . col ( \"transformation\" ) == i ) . orderBy ( \"idx\" ) . select ( metric ) . fillna ( np . nan ) . rdd . flatMap ( list ) . collect () ) metric_cv = round ( float ( variation ([ a for a in metric_stats ])), 4 ) or None i_output . append ( metric_cv ) output . append ( i_output ) schema = T . StructType ( [ T . StructField ( \"feature_formula\" , T . StringType (), True ), T . StructField ( \"mean_cv\" , T . FloatType (), True ), T . StructField ( \"stddev_cv\" , T . FloatType (), True ), ] ) odf = spark . createDataFrame ( output , schema = schema ) def score_cv ( cv , thresholds = [ 0.03 , 0.1 , 0.2 , 0.5 ]): if cv is None : return None else : cv = abs ( cv ) stability_index = [ 4 , 3 , 2 , 1 , 0 ] for i , thresh in enumerate ( thresholds ): if cv < thresh : return stability_index [ i ] return stability_index [ - 1 ] f_score_cv = F . udf ( score_cv , T . IntegerType ()) odf = ( odf . replace ( np . nan , None ) . withColumn ( \"mean_si\" , f_score_cv ( F . col ( \"mean_cv\" ))) . withColumn ( \"stddev_si\" , f_score_cv ( F . col ( \"stddev_cv\" ))) . withColumn ( \"stability_index_lower_bound\" , F . round ( F . col ( \"mean_si\" ) * metric_weightages . get ( \"mean\" , 0 ) + F . col ( \"stddev_si\" ) * metric_weightages . get ( \"stddev\" , 0 ), 4 , ), ) . withColumn ( \"stability_index_upper_bound\" , F . round ( F . col ( \"stability_index_lower_bound\" ) + 4 * metric_weightages . get ( \"kurtosis\" , 0 ), 4 , ), ) . withColumn ( \"flagged_lower\" , F . when ( ( F . col ( \"stability_index_lower_bound\" ) < threshold ) | ( F . col ( \"stability_index_lower_bound\" ) . isNull ()), 1 , ) . otherwise ( 0 ), ) . withColumn ( \"flagged_upper\" , F . when ( ( F . col ( \"stability_index_upper_bound\" ) < threshold ) | ( F . col ( \"stability_index_upper_bound\" ) . isNull ()), 1 , ) . otherwise ( 0 ), ) ) if print_impact : logger . info ( \"All Features:\" ) odf . show ( len ( attribute_names ), False ) logger . info ( \"Potential Unstable Features Identified by Both Lower and Upper Bounds:\" ) unstable = odf . where ( F . col ( \"flagged_upper\" ) == 1 ) unstable . show ( unstable . count ()) return odf def stability_index_computation ( spark, *idfs, list_of_cols='all', drop_cols=[], metric_weightages={'mean': 0.5, 'stddev': 0.3, 'kurtosis': 0.2}, existing_metric_path='', appended_metric_path='', threshold=1, print_impact=False) The data stability is represented by a single metric to summarise the stability of an attribute over multiple time periods. For example, given 6 datasets collected in 6 consecutive time periods (D1, D2, \u2026, D6), data stability index of an attribute measures how stable the attribute is from D1 to D6. The major difference between data drift and data stability is that data drift analysis is only based on 2 datasets: source and target. However data stability analysis can be performed on multiple datasets. In addition, the source dataset is not required indicating that the stability index can be directly computed among multiple target datasets by comparing the statistical properties among them. In summary, given N datasets representing different time periods, we would like to measure the stability of some numerical attributes from the first to the N-th dataset. The whole process can be broken down into 2 steps: (1) Choose a few statistical metrics to describe the distribution of each attribute at each time period. (2) Compute attribute level stability by combining the stability of each statistical metric over time periods. In the first step, we choose mean, standard deviation and kurtosis as the statistical metrics in our implementation. Intuitively, they represent different aspects of a distribution: mean measures central tendency, standard deviation measures dispersion and kurtosis measures shape of a distribution. Reasons of selecting those 3 metrics will be explained in a later section. With mean, standard deviation and kurtosis computed for each attribute at each time interval, we can form 3 arrays of size N for each attribute. In the second step, Coefficient of Variation (CV) is used to measure the stability of each metric. CV represents the ratio of the standard deviation to the mean, which is a unitless statistic to compare the relative variation from one array to another. Considering the wide potential range of CV, the absolute value of CV is then mapped to an integer between 0 and 4 according to the table below, where 0 indicates highly unstable and 4 indicates highly stable. We call this integer a metric stability index. abs(CV) Interval Metric Stability Index [0, 0.03) 4 [0.03, 0.1) 3 [0.1, 0.2) 2 [0.2, 0.5) 1 [0.5, +inf) 0 Finally, the attribute stability index (SI) is a weighted sum of 3 metric stability indexes, where we assign 50% for mean, 30% for standard deviation and 20% for kurtosis by default. The final output is a float between 0 and 4 and an attribute can be classified as one of the following categories: very unstable (0\u2264SI<1), unstable (1\u2264SI<2), marginally stable (2\u2264SI<3), stable (3\u2264SI<3.5) and very stable (3.5\u2264SI\u22644). For example, there are 6 samples of attribute X from T1 to T6. For each sample, we have computed the statistical metrics of X from T1 to T6: idx Mean Standard deviation Kurtosis 1 11 2 3.9 2 12 1 4.2 3 15 3 4.0 4 10 2 4.1 5 11 1 4.2 6 13 0.5 4.0 Then we calculate the Coefficient of Variation for each array: CV of mean = CV([11, 12, 15, 10, 11, 13]) = 0.136 CV of standard deviation = CV([2, 1, 3, 2, 1, 0.5]) = 0.529 CV of kurtosis = CV([3.9, 4.2, 4.0, 4.1, 4.2, 4.0]) = 0.027 Metric stability indexes are then computed by mapping each CV value to an integer accordingly. As a result, metric stability index is 2 for mean, 0 for standard deviation and 4 for kurtosis. Why mean is chosen over median? Dummy variables which take only the value 0 or 1 are frequently seen in machine learning features. Mean of a dummy variable represents the proportion of value 1 and median of a dummy variable is either 0 or 1 whichever is more frequent. However, CV may not work well when 0 appears in the array or the array contains both positive and negative values. For example, intuitively [0,0,0,0,0,1,0,0,0] is a stable array but its CV is 2.83 which is extremely high, but cv of [0.45,0.44,0.48,0.49,0.42,0.52,0.49,0.47,0.48] is 0.06 which is much more reasonable. Thus we decided to use mean instead of median. Although median is considered as a more robust choice, outlier treatment can be applied prior to data stability analysis to handle this issue. Why kurtosis is chosen over skewness? Kurtosis is a positive value (note that we use kurtosis instead of excess kurtosis which) but skewness can range from \u2013inf to +inf. Usually, if skewness is between -0.5 and 0.5, the distribution is approximately symmetric. Thus, if the skewness fluctuates around 0, the CV is highly likely to be high or invalid because the mean will be close to 0. Stability index is preferred in the following scenario: Pairwise drift analysis can be performed between the source dataset and each of the target dataset to quantify the drift over time. However this can be time-consuming especially when the number of target dataset is large. In this case, measuring data stability instead of data drift would be a much faster alternative and the source/baseline dataset is not required as well Troubleshooting If the attribute stability index appears to be nan, it may due to one of the following reasons: - One metric ( likely to be kurtosis) is nan. For example, the kurtosis of a sample is nan If its standard deviation is 0. - The mean of a metric from the first to the N-th dataset is zero, causing the denominator of CV to be 0. For example, when mean of attribute X is always zero for all datasets, its stability index would be nan. Limitation Limitation of CV: CV may not work well when 0 appears in the array or the array contains both positive and negative values. Parameters spark Spark Session idfs Variable number of input dataframes list_of_cols List of numerical columns to check stability e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all numerical columns for analysis. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". (Default value = []) metric_weightages Takes input in dictionary format with keys being the metric name - \"mean\",\"stdev\",\"kurtosis\" and value being the weightage of the metric (between 0 and 1). Sum of all weightages must be 1. (Default value = {\"mean\": 0.5, \"stddev\": 0.3, \"kurtosis\": 0.2}) existing_metric_path This argument is path for referring pre-existing metrics of historical datasets and is of schema [idx, attribute, mean, stdev, kurtosis]. idx is index number of historical datasets assigned in chronological order. (Default value = \"\") appended_metric_path This argument is path for saving input dataframes metrics after appending to the historical datasets' metrics. (Default value = \"\") threshold A column is flagged if the stability index is below the threshold, which varies between 0 to 4. The following criteria can be used to classifiy stability_index (SI): very unstable: 0\u2264SI<1, unstable: 1\u2264SI<2, marginally stable: 2\u2264SI<3, stable: 3\u2264SI<3.5 and very stable: 3.5\u2264SI\u22644. (Default value = 1) print_impact True, False (Default value = False) This argument is to print out the stability metrics of all attributes and potential unstable attributes. Returns DataFrame [attribute, mean_si, stddev_si, kurtosis_si, mean_cv, stddev_cv, kurtosis_cv, stability_index]. _cv is coefficient of variation for each metric. _si is stability index for each metric. stability_index is net weighted stability index based on the individual metrics' stability index. Expand source code def stability_index_computation ( spark , * idfs , list_of_cols = \"all\" , drop_cols = [], metric_weightages = { \"mean\" : 0.5 , \"stddev\" : 0.3 , \"kurtosis\" : 0.2 }, existing_metric_path = \"\" , appended_metric_path = \"\" , threshold = 1 , print_impact = False , ): \"\"\" The data stability is represented by a single metric to summarise the stability of an attribute over multiple time periods. For example, given 6 datasets collected in 6 consecutive time periods (D1, D2, \u2026, D6), data stability index of an attribute measures how stable the attribute is from D1 to D6. The major difference between data drift and data stability is that data drift analysis is only based on 2 datasets: source and target. However data stability analysis can be performed on multiple datasets. In addition, the source dataset is not required indicating that the stability index can be directly computed among multiple target datasets by comparing the statistical properties among them. In summary, given N datasets representing different time periods, we would like to measure the stability of some numerical attributes from the first to the N-th dataset. The whole process can be broken down into 2 steps: (1) Choose a few statistical metrics to describe the distribution of each attribute at each time period. (2) Compute attribute level stability by combining the stability of each statistical metric over time periods. In the first step, we choose mean, standard deviation and kurtosis as the statistical metrics in our implementation. Intuitively, they represent different aspects of a distribution: mean measures central tendency, standard deviation measures dispersion and kurtosis measures shape of a distribution. Reasons of selecting those 3 metrics will be explained in a later section. With mean, standard deviation and kurtosis computed for each attribute at each time interval, we can form 3 arrays of size N for each attribute. In the second step, Coefficient of Variation (CV) is used to measure the stability of each metric. CV represents the ratio of the standard deviation to the mean, which is a unitless statistic to compare the relative variation from one array to another. Considering the wide potential range of CV, the absolute value of CV is then mapped to an integer between 0 and 4 according to the table below, where 0 indicates highly unstable and 4 indicates highly stable. We call this integer a metric stability index. | abs(CV) Interval | Metric Stability Index | |------------------|------------------------| | [0, 0.03) | 4 | | [0.03, 0.1) | 3 | | [0.1, 0.2) | 2 | | [0.2, 0.5) | 1 | | [0.5, +inf) | 0 | Finally, the attribute stability index (SI) is a weighted sum of 3 metric stability indexes, where we assign 50% for mean, 30% for standard deviation and 20% for kurtosis by default. The final output is a float between 0 and 4 and an attribute can be classified as one of the following categories: very unstable (0\u2264SI<1), unstable (1\u2264SI<2), marginally stable (2\u2264SI<3), stable (3\u2264SI<3.5) and very stable (3.5\u2264SI\u22644). For example, there are 6 samples of attribute X from T1 to T6. For each sample, we have computed the statistical metrics of X from T1 to T6: | idx | Mean | Standard deviation | Kurtosis | |-----|------|--------------------|----------| | 1 | 11 | 2 | 3.9 | | 2 | 12 | 1 | 4.2 | | 3 | 15 | 3 | 4.0 | | 4 | 10 | 2 | 4.1 | | 5 | 11 | 1 | 4.2 | | 6 | 13 | 0.5 | 4.0 | Then we calculate the Coefficient of Variation for each array: - CV of mean = CV([11, 12, 15, 10, 11, 13]) = 0.136 - CV of standard deviation = CV([2, 1, 3, 2, 1, 0.5]) = 0.529 - CV of kurtosis = CV([3.9, 4.2, 4.0, 4.1, 4.2, 4.0]) = 0.027 Metric stability indexes are then computed by mapping each CV value to an integer accordingly. As a result, metric stability index is 2 for mean, 0 for standard deviation and 4 for kurtosis. Why mean is chosen over median? - Dummy variables which take only the value 0 or 1 are frequently seen in machine learning features. Mean of a dummy variable represents the proportion of value 1 and median of a dummy variable is either 0 or 1 whichever is more frequent. However, CV may not work well when 0 appears in the array or the array contains both positive and negative values. For example, intuitively [0,0,0,0,0,1,0,0,0] is a stable array but its CV is 2.83 which is extremely high, but cv of [0.45,0.44,0.48,0.49,0.42,0.52,0.49,0.47,0.48] is 0.06 which is much more reasonable. Thus we decided to use mean instead of median. Although median is considered as a more robust choice, outlier treatment can be applied prior to data stability analysis to handle this issue. Why kurtosis is chosen over skewness? - Kurtosis is a positive value (note that we use kurtosis instead of excess kurtosis which) but skewness can range from \u2013inf to +inf. Usually, if skewness is between -0.5 and 0.5, the distribution is approximately symmetric. Thus, if the skewness fluctuates around 0, the CV is highly likely to be high or invalid because the mean will be close to 0. Stability index is preferred in the following scenario: - Pairwise drift analysis can be performed between the source dataset and each of the target dataset to quantify the drift over time. However this can be time-consuming especially when the number of target dataset is large. In this case, measuring data stability instead of data drift would be a much faster alternative and the source/baseline dataset is not required as well Troubleshooting - If the attribute stability index appears to be nan, it may due to one of the following reasons: - One metric ( likely to be kurtosis) is nan. For example, the kurtosis of a sample is nan If its standard deviation is 0. - The mean of a metric from the first to the N-th dataset is zero, causing the denominator of CV to be 0. For example, when mean of attribute X is always zero for all datasets, its stability index would be nan. Limitation - Limitation of CV: CV may not work well when 0 appears in the array or the array contains both positive and negative values. Parameters ---------- spark Spark Session idfs Variable number of input dataframes list_of_cols List of numerical columns to check stability e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all numerical columns for analysis. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". (Default value = []) metric_weightages Takes input in dictionary format with keys being the metric name - \"mean\",\"stdev\",\"kurtosis\" and value being the weightage of the metric (between 0 and 1). Sum of all weightages must be 1. (Default value = {\"mean\": 0.5, \"stddev\": 0.3, \"kurtosis\": 0.2}) existing_metric_path This argument is path for referring pre-existing metrics of historical datasets and is of schema [idx, attribute, mean, stdev, kurtosis]. idx is index number of historical datasets assigned in chronological order. (Default value = \"\") appended_metric_path This argument is path for saving input dataframes metrics after appending to the historical datasets' metrics. (Default value = \"\") threshold A column is flagged if the stability index is below the threshold, which varies between 0 to 4. The following criteria can be used to classifiy stability_index (SI): very unstable: 0\u2264SI<1, unstable: 1\u2264SI<2, marginally stable: 2\u2264SI<3, stable: 3\u2264SI<3.5 and very stable: 3.5\u2264SI\u22644. (Default value = 1) print_impact True, False (Default value = False) This argument is to print out the stability metrics of all attributes and potential unstable attributes. Returns ------- DataFrame [attribute, mean_si, stddev_si, kurtosis_si, mean_cv, stddev_cv, kurtosis_cv, stability_index]. *_cv is coefficient of variation for each metric. *_si is stability index for each metric. stability_index is net weighted stability index based on the individual metrics' stability index. \"\"\" num_cols = attributeType_segregation ( idfs [ 0 ])[ 0 ] if list_of_cols == \"all\" : list_of_cols = num_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in drop_cols ])) if any ( x not in num_cols for x in list_of_cols ) | ( len ( list_of_cols ) == 0 ): raise TypeError ( \"Invalid input for Column(s)\" ) if ( round ( metric_weightages . get ( \"mean\" , 0 ) + metric_weightages . get ( \"stddev\" , 0 ) + metric_weightages . get ( \"kurtosis\" , 0 ), 3 , ) != 1 ): raise ValueError ( \"Invalid input for metric weightages. Either metric name is incorrect or sum of metric weightages is not 1.0\" ) if existing_metric_path : existing_metric_df = spark . read . csv ( existing_metric_path , header = True , inferSchema = True ) dfs_count = existing_metric_df . select ( F . max ( F . col ( \"idx\" ))) . first ()[ 0 ] else : schema = T . StructType ( [ T . StructField ( \"idx\" , T . IntegerType (), True ), T . StructField ( \"attribute\" , T . StringType (), True ), T . StructField ( \"mean\" , T . DoubleType (), True ), T . StructField ( \"stddev\" , T . DoubleType (), True ), T . StructField ( \"kurtosis\" , T . DoubleType (), True ), ] ) existing_metric_df = spark . sparkContext . emptyRDD () . toDF ( schema ) dfs_count = 0 metric_ls = [] for idf in idfs : for i in list_of_cols : mean , stddev , kurtosis = idf . select ( F . mean ( i ), F . stddev ( i ), F . kurtosis ( i ) ) . first () metric_ls . append ( [ dfs_count + 1 , i , mean , stddev , kurtosis + 3.0 if kurtosis else None ] ) dfs_count += 1 new_metric_df = spark . createDataFrame ( metric_ls , schema = ( \"idx\" , \"attribute\" , \"mean\" , \"stddev\" , \"kurtosis\" ) ) appended_metric_df = concatenate_dataset ( existing_metric_df , new_metric_df ) if appended_metric_path : appended_metric_df . coalesce ( 1 ) . write . csv ( appended_metric_path , header = True , mode = \"overwrite\" ) result = [] for i in list_of_cols : i_output = [ i ] for metric in [ \"mean\" , \"stddev\" , \"kurtosis\" ]: metric_stats = ( appended_metric_df . where ( F . col ( \"attribute\" ) == i ) . orderBy ( \"idx\" ) . select ( metric ) . fillna ( np . nan ) . rdd . flatMap ( list ) . collect () ) metric_cv = round ( float ( variation ([ a for a in metric_stats ])), 4 ) or None i_output . append ( metric_cv ) result . append ( i_output ) schema = T . StructType ( [ T . StructField ( \"attribute\" , T . StringType (), True ), T . StructField ( \"mean_cv\" , T . FloatType (), True ), T . StructField ( \"stddev_cv\" , T . FloatType (), True ), T . StructField ( \"kurtosis_cv\" , T . FloatType (), True ), ] ) odf = spark . createDataFrame ( result , schema = schema ) def score_cv ( cv , thresholds = [ 0.03 , 0.1 , 0.2 , 0.5 ]): if cv is None : return None else : cv = abs ( cv ) stability_index = [ 4 , 3 , 2 , 1 , 0 ] for i , thresh in enumerate ( thresholds ): if cv < thresh : return stability_index [ i ] return stability_index [ - 1 ] f_score_cv = F . udf ( score_cv , T . IntegerType ()) odf = ( odf . replace ( np . nan , None ) . withColumn ( \"mean_si\" , f_score_cv ( F . col ( \"mean_cv\" ))) . withColumn ( \"stddev_si\" , f_score_cv ( F . col ( \"stddev_cv\" ))) . withColumn ( \"kurtosis_si\" , f_score_cv ( F . col ( \"kurtosis_cv\" ))) . withColumn ( \"stability_index\" , F . round ( ( F . col ( \"mean_si\" ) * metric_weightages . get ( \"mean\" , 0 ) + F . col ( \"stddev_si\" ) * metric_weightages . get ( \"stddev\" , 0 ) + F . col ( \"kurtosis_si\" ) * metric_weightages . get ( \"kurtosis\" , 0 ) ), 4 , ), ) . withColumn ( \"flagged\" , F . when ( ( F . col ( \"stability_index\" ) < threshold ) | ( F . col ( \"stability_index\" ) . isNull ()), 1 , ) . otherwise ( 0 ), ) ) if print_impact : logger . info ( \"All Attributes:\" ) odf . show ( len ( list_of_cols )) logger . info ( \"Potential Unstable Attributes:\" ) unstable = odf . where ( F . col ( \"flagged\" ) == 1 ) unstable . show ( unstable . count ()) return odf def statistics ( spark: pyspark.sql.session.SparkSession, idf_target: pyspark.sql.dataframe.DataFrame, idf_source: pyspark.sql.dataframe.DataFrame, *, list_of_cols: list = 'all', drop_cols: list = None, method_type: str = 'PSI', bin_method: str = 'equal_range', bin_size: int = 10, threshold: float = 0.1, pre_existing_source: bool = False, source_path: str = 'NA', model_directory: str = 'drift_statistics', run_type: str = 'local', print_impact: bool = False) When the performance of a deployed machine learning model degrades in production, one potential reason is that the data used in training and prediction are not following the same distribution. Data drift mainly includes the following manifestations: Covariate shift: training and test data follow different distributions. For example, An algorithm predicting income that is trained on younger population but tested on older population. Prior probability shift: change of prior probability. For example in a spam classification problem, the proportion of spam emails changes from 0.2 in training data to 0.6 in testing data. Concept shift: the distribution of the target variable changes given fixed input values. For example in the same spam classification problem, emails tagged as spam in training data are more likely to be tagged as non-spam in testing data. In our module, we mainly focus on covariate shift detection. In summary, given 2 datasets, source and target datasets, we would like to quantify the drift of some numerical attributes from source to target datasets. The whole process can be broken down into 2 steps: (1) convert each attribute of interest in source and target datasets into source and target probability distributions. (2) calculate the statistical distance between source and target distributions for each attribute. In the first step, attribute_binning is firstly performed to bin the numerical attributes of the source dataset, which requires two input variables: bin_method and bin_size. The same binning method is applied on the target dataset to align two results. The probability distributions are computed by dividing the frequency of each bin by the total frequency. In the second step, 4 choices of statistical metrics are provided to measure the data drift of an attribute from source to target distribution: Population Stability Index (PSI), Jensen-Shannon Divergence (JSD), Hellinger Distance (HD) and Kolmogorov-Smirnov Distance (KS). They are calculated as below: For two discrete probability distributions P=(p_1,\u2026,p_k) and Q=(q_1,\u2026,q_k), A threshold can be set to flag out drifted attributes. If multiple statistical metrics have been calculated, an attribute will be marked as drifted if any of its statistical metric is larger than the threshold. This function can be used in many scenarios. For example: Attribute level data drift can be analysed together with the attribute importance of a machine learning model. The more important an attribute is, the more attention it needs to be given if drift presents. To analyse data drift over time, one can treat one dataset as the source / baseline dataset and multiple datasets as the target datasets. Drift analysis can be performed between the source dataset and each of the target dataset to quantify the drift over time. Parameters spark Spark Session idf_target Input Dataframe idf_source Baseline/Source Dataframe. This argument is ignored if pre_existing_source is True. list_of_cols List of columns to check drift e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all (non-array) columns for analysis. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". (Default value = None) method_type \"PSI\", \"JSD\", \"HD\", \"KS\",\"all\". \"all\" can be passed to calculate all drift metrics. One or more methods can be passed in a form of list or string where different metrics are separated by pipe delimiter \u201c|\u201d e.g. [\"PSI\", \"JSD\"] or \"PSI|JSD\". (Default value = \"PSI\") bin_method \"equal_frequency\", \"equal_range\". In \"equal_range\" method, each bin is of equal size/width and in \"equal_frequency\", each bin has equal no. of rows, though the width of bins may vary. (Default value = \"equal_range\") bin_size Number of bins for creating histogram. (Default value = 10) threshold A column is flagged if any drift metric is above the threshold. (Default value = 0.1) pre_existing_source Boolean argument \u2013 True or False. True if the drift_statistics folder (binning model & frequency counts for each attribute) exists already, False Otherwise. (Default value = False) source_path If pre_existing_source is False, this argument can be used for saving the drift_statistics folder. The drift_statistics folder will have attribute_binning (binning model) & frequency_counts sub-folders. If pre_existing_source is True, this argument is path for referring the drift_statistics folder. Default \"NA\" for temporarily saving data in \"intermediate_data/\" folder. (Default value = \"NA\") model_directory If pre_existing_source is False, this argument can be used for saving the drift stats to folder. The default drift statics directory is drift_statistics folder will have attribute_binning If pre_existing_source is True, this argument is model_directory for referring the drift statistics dir. Default \"drift_statistics\" for temporarily saving source dataset attribute_binning folder. (Default value = \"drift_statistics\") run_type \"local\", \"emr\", \"databricks\" (Default value = \"local\") print_impact True, False. (Default value = False) This argument is to print out the drift statistics of all attributes and attributes meeting the threshold. Returns DataFrame [attribute, *metric, flagged] Number of columns will be dependent on method argument. There will be one column for each drift method/metric. Expand source code @check_distance_method @check_list_of_columns def statistics ( spark : SparkSession , idf_target : DataFrame , idf_source : DataFrame , * , list_of_cols : list = \"all\" , drop_cols : list = None , method_type : str = \"PSI\" , bin_method : str = \"equal_range\" , bin_size : int = 10 , threshold : float = 0.1 , pre_existing_source : bool = False , source_path : str = \"NA\" , model_directory : str = \"drift_statistics\" , run_type : str = \"local\" , print_impact : bool = False , ): \"\"\" When the performance of a deployed machine learning model degrades in production, one potential reason is that the data used in training and prediction are not following the same distribution. Data drift mainly includes the following manifestations: - Covariate shift: training and test data follow different distributions. For example, An algorithm predicting income that is trained on younger population but tested on older population. - Prior probability shift: change of prior probability. For example in a spam classification problem, the proportion of spam emails changes from 0.2 in training data to 0.6 in testing data. - Concept shift: the distribution of the target variable changes given fixed input values. For example in the same spam classification problem, emails tagged as spam in training data are more likely to be tagged as non-spam in testing data. In our module, we mainly focus on covariate shift detection. In summary, given 2 datasets, source and target datasets, we would like to quantify the drift of some numerical attributes from source to target datasets. The whole process can be broken down into 2 steps: (1) convert each attribute of interest in source and target datasets into source and target probability distributions. (2) calculate the statistical distance between source and target distributions for each attribute. In the first step, attribute_binning is firstly performed to bin the numerical attributes of the source dataset, which requires two input variables: bin_method and bin_size. The same binning method is applied on the target dataset to align two results. The probability distributions are computed by dividing the frequency of each bin by the total frequency. In the second step, 4 choices of statistical metrics are provided to measure the data drift of an attribute from source to target distribution: Population Stability Index (PSI), Jensen-Shannon Divergence (JSD), Hellinger Distance (HD) and Kolmogorov-Smirnov Distance (KS). They are calculated as below: For two discrete probability distributions *P=(p_1,\u2026,p_k)* and *Q=(q_1,\u2026,q_k),* ![https://raw.githubusercontent.com/anovos/anovos-docs/main/docs/assets/drift_stats_formulae.png](https://raw.githubusercontent.com/anovos/anovos-docs/main/docs/assets/drift_stats_formulae.png) A threshold can be set to flag out drifted attributes. If multiple statistical metrics have been calculated, an attribute will be marked as drifted if any of its statistical metric is larger than the threshold. This function can be used in many scenarios. For example: 1. Attribute level data drift can be analysed together with the attribute importance of a machine learning model. The more important an attribute is, the more attention it needs to be given if drift presents. 2. To analyse data drift over time, one can treat one dataset as the source / baseline dataset and multiple datasets as the target datasets. Drift analysis can be performed between the source dataset and each of the target dataset to quantify the drift over time. Parameters ---------- spark Spark Session idf_target Input Dataframe idf_source Baseline/Source Dataframe. This argument is ignored if pre_existing_source is True. list_of_cols List of columns to check drift e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all (non-array) columns for analysis. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". (Default value = None) method_type \"PSI\", \"JSD\", \"HD\", \"KS\",\"all\". \"all\" can be passed to calculate all drift metrics. One or more methods can be passed in a form of list or string where different metrics are separated by pipe delimiter \u201c|\u201d e.g. [\"PSI\", \"JSD\"] or \"PSI|JSD\". (Default value = \"PSI\") bin_method \"equal_frequency\", \"equal_range\". In \"equal_range\" method, each bin is of equal size/width and in \"equal_frequency\", each bin has equal no. of rows, though the width of bins may vary. (Default value = \"equal_range\") bin_size Number of bins for creating histogram. (Default value = 10) threshold A column is flagged if any drift metric is above the threshold. (Default value = 0.1) pre_existing_source Boolean argument \u2013 True or False. True if the drift_statistics folder (binning model & frequency counts for each attribute) exists already, False Otherwise. (Default value = False) source_path If pre_existing_source is False, this argument can be used for saving the drift_statistics folder. The drift_statistics folder will have attribute_binning (binning model) & frequency_counts sub-folders. If pre_existing_source is True, this argument is path for referring the drift_statistics folder. Default \"NA\" for temporarily saving data in \"intermediate_data/\" folder. (Default value = \"NA\") model_directory If pre_existing_source is False, this argument can be used for saving the drift stats to folder. The default drift statics directory is drift_statistics folder will have attribute_binning If pre_existing_source is True, this argument is model_directory for referring the drift statistics dir. Default \"drift_statistics\" for temporarily saving source dataset attribute_binning folder. (Default value = \"drift_statistics\") run_type \"local\", \"emr\", \"databricks\" (Default value = \"local\") print_impact True, False. (Default value = False) This argument is to print out the drift statistics of all attributes and attributes meeting the threshold. Returns ------- DataFrame [attribute, *metric, flagged] Number of columns will be dependent on method argument. There will be one column for each drift method/metric. \"\"\" drop_cols = drop_cols or [] num_cols = attributeType_segregation ( idf_target . select ( list_of_cols ))[ 0 ] if run_type in list ( platform_root_path . keys ()): root_path = platform_root_path [ run_type ] else : root_path = \"\" if source_path == \"NA\" : source_path = root_path + \"intermediate_data\" if not pre_existing_source : source_bin = attribute_binning ( spark , idf_source , list_of_cols = num_cols , method_type = bin_method , bin_size = bin_size , pre_existing_model = False , model_path = source_path + \"/\" + model_directory , ) source_bin . persist ( pyspark . StorageLevel . MEMORY_AND_DISK ) . count () target_bin = attribute_binning ( spark , idf_target , list_of_cols = num_cols , method_type = bin_method , bin_size = bin_size , pre_existing_model = True , model_path = source_path + \"/\" + model_directory , ) target_bin . persist ( pyspark . StorageLevel . MEMORY_AND_DISK ) . count () result = { \"attribute\" : [], \"flagged\" : []} for method in method_type : result [ method ] = [] for i in list_of_cols : if pre_existing_source : x = spark . read . csv ( source_path + \"/\" + model_directory + \"/frequency_counts/\" + i , header = True , inferSchema = True , ) else : x = ( source_bin . groupBy ( i ) . agg (( F . count ( i ) / idf_source . count ()) . alias ( \"p\" )) . fillna ( - 1 ) ) x . coalesce ( 1 ) . write . csv ( source_path + \"/\" + model_directory + \"/frequency_counts/\" + i , header = True , mode = \"overwrite\" , ) y = ( target_bin . groupBy ( i ) . agg (( F . count ( i ) / idf_target . count ()) . alias ( \"q\" )) . fillna ( - 1 ) ) xy = ( x . join ( y , i , \"full_outer\" ) . fillna ( 0.0001 , subset = [ \"p\" , \"q\" ]) . replace ( 0 , 0.0001 ) . orderBy ( i ) ) p = np . array ( xy . select ( \"p\" ) . rdd . flatMap ( lambda x : x ) . collect ()) q = np . array ( xy . select ( \"q\" ) . rdd . flatMap ( lambda x : x ) . collect ()) result [ \"attribute\" ] . append ( i ) counter = 0 for idx , method in enumerate ( method_type ): drift_function = { \"PSI\" : psi , \"JSD\" : js_divergence , \"HD\" : hellinger , \"KS\" : ks , } metric = float ( round ( drift_function [ method ]( p , q ), 4 )) result [ method ] . append ( metric ) if counter == 0 : if metric > threshold : result [ \"flagged\" ] . append ( 1 ) counter = 1 if ( idx == ( len ( method_type ) - 1 )) & ( counter == 0 ): result [ \"flagged\" ] . append ( 0 ) odf = ( spark . createDataFrame ( pd . DataFrame . from_dict ( result , orient = \"index\" ) . transpose () ) . select ([ \"attribute\" ] + method_type + [ \"flagged\" ]) . orderBy ( F . desc ( \"flagged\" )) ) if print_impact : logger . info ( \"All Attributes:\" ) odf . show ( len ( list_of_cols )) logger . info ( \"Attributes meeting Data Drift threshold:\" ) drift = odf . where ( F . col ( \"flagged\" ) == 1 ) drift . show ( drift . count ()) return odf","title":"<code>detector</code>"},{"location":"api/drift/detector.html#detector","text":"Expand source code # coding=utf-8 from __future__ import division , print_function import numpy as np import pandas as pd import pyspark from loguru import logger from pyspark.sql import SparkSession , DataFrame from pyspark.sql import functions as F from pyspark.sql import types as T from scipy.stats import variation import sympy as sp from anovos.data_ingest.data_ingest import concatenate_dataset from anovos.data_transformer.transformers import attribute_binning from anovos.shared.utils import attributeType_segregation from .distances import hellinger , psi , js_divergence , ks from .validations import check_distance_method , check_list_of_columns from ..shared.utils import platform_root_path @check_distance_method @check_list_of_columns def statistics ( spark : SparkSession , idf_target : DataFrame , idf_source : DataFrame , * , list_of_cols : list = \"all\" , drop_cols : list = None , method_type : str = \"PSI\" , bin_method : str = \"equal_range\" , bin_size : int = 10 , threshold : float = 0.1 , pre_existing_source : bool = False , source_path : str = \"NA\" , model_directory : str = \"drift_statistics\" , run_type : str = \"local\" , print_impact : bool = False , ): \"\"\" When the performance of a deployed machine learning model degrades in production, one potential reason is that the data used in training and prediction are not following the same distribution. Data drift mainly includes the following manifestations: - Covariate shift: training and test data follow different distributions. For example, An algorithm predicting income that is trained on younger population but tested on older population. - Prior probability shift: change of prior probability. For example in a spam classification problem, the proportion of spam emails changes from 0.2 in training data to 0.6 in testing data. - Concept shift: the distribution of the target variable changes given fixed input values. For example in the same spam classification problem, emails tagged as spam in training data are more likely to be tagged as non-spam in testing data. In our module, we mainly focus on covariate shift detection. In summary, given 2 datasets, source and target datasets, we would like to quantify the drift of some numerical attributes from source to target datasets. The whole process can be broken down into 2 steps: (1) convert each attribute of interest in source and target datasets into source and target probability distributions. (2) calculate the statistical distance between source and target distributions for each attribute. In the first step, attribute_binning is firstly performed to bin the numerical attributes of the source dataset, which requires two input variables: bin_method and bin_size. The same binning method is applied on the target dataset to align two results. The probability distributions are computed by dividing the frequency of each bin by the total frequency. In the second step, 4 choices of statistical metrics are provided to measure the data drift of an attribute from source to target distribution: Population Stability Index (PSI), Jensen-Shannon Divergence (JSD), Hellinger Distance (HD) and Kolmogorov-Smirnov Distance (KS). They are calculated as below: For two discrete probability distributions *P=(p_1,\u2026,p_k)* and *Q=(q_1,\u2026,q_k),* ![https://raw.githubusercontent.com/anovos/anovos-docs/main/docs/assets/drift_stats_formulae.png](https://raw.githubusercontent.com/anovos/anovos-docs/main/docs/assets/drift_stats_formulae.png) A threshold can be set to flag out drifted attributes. If multiple statistical metrics have been calculated, an attribute will be marked as drifted if any of its statistical metric is larger than the threshold. This function can be used in many scenarios. For example: 1. Attribute level data drift can be analysed together with the attribute importance of a machine learning model. The more important an attribute is, the more attention it needs to be given if drift presents. 2. To analyse data drift over time, one can treat one dataset as the source / baseline dataset and multiple datasets as the target datasets. Drift analysis can be performed between the source dataset and each of the target dataset to quantify the drift over time. Parameters ---------- spark Spark Session idf_target Input Dataframe idf_source Baseline/Source Dataframe. This argument is ignored if pre_existing_source is True. list_of_cols List of columns to check drift e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all (non-array) columns for analysis. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". (Default value = None) method_type \"PSI\", \"JSD\", \"HD\", \"KS\",\"all\". \"all\" can be passed to calculate all drift metrics. One or more methods can be passed in a form of list or string where different metrics are separated by pipe delimiter \u201c|\u201d e.g. [\"PSI\", \"JSD\"] or \"PSI|JSD\". (Default value = \"PSI\") bin_method \"equal_frequency\", \"equal_range\". In \"equal_range\" method, each bin is of equal size/width and in \"equal_frequency\", each bin has equal no. of rows, though the width of bins may vary. (Default value = \"equal_range\") bin_size Number of bins for creating histogram. (Default value = 10) threshold A column is flagged if any drift metric is above the threshold. (Default value = 0.1) pre_existing_source Boolean argument \u2013 True or False. True if the drift_statistics folder (binning model & frequency counts for each attribute) exists already, False Otherwise. (Default value = False) source_path If pre_existing_source is False, this argument can be used for saving the drift_statistics folder. The drift_statistics folder will have attribute_binning (binning model) & frequency_counts sub-folders. If pre_existing_source is True, this argument is path for referring the drift_statistics folder. Default \"NA\" for temporarily saving data in \"intermediate_data/\" folder. (Default value = \"NA\") model_directory If pre_existing_source is False, this argument can be used for saving the drift stats to folder. The default drift statics directory is drift_statistics folder will have attribute_binning If pre_existing_source is True, this argument is model_directory for referring the drift statistics dir. Default \"drift_statistics\" for temporarily saving source dataset attribute_binning folder. (Default value = \"drift_statistics\") run_type \"local\", \"emr\", \"databricks\" (Default value = \"local\") print_impact True, False. (Default value = False) This argument is to print out the drift statistics of all attributes and attributes meeting the threshold. Returns ------- DataFrame [attribute, *metric, flagged] Number of columns will be dependent on method argument. There will be one column for each drift method/metric. \"\"\" drop_cols = drop_cols or [] num_cols = attributeType_segregation ( idf_target . select ( list_of_cols ))[ 0 ] if run_type in list ( platform_root_path . keys ()): root_path = platform_root_path [ run_type ] else : root_path = \"\" if source_path == \"NA\" : source_path = root_path + \"intermediate_data\" if not pre_existing_source : source_bin = attribute_binning ( spark , idf_source , list_of_cols = num_cols , method_type = bin_method , bin_size = bin_size , pre_existing_model = False , model_path = source_path + \"/\" + model_directory , ) source_bin . persist ( pyspark . StorageLevel . MEMORY_AND_DISK ) . count () target_bin = attribute_binning ( spark , idf_target , list_of_cols = num_cols , method_type = bin_method , bin_size = bin_size , pre_existing_model = True , model_path = source_path + \"/\" + model_directory , ) target_bin . persist ( pyspark . StorageLevel . MEMORY_AND_DISK ) . count () result = { \"attribute\" : [], \"flagged\" : []} for method in method_type : result [ method ] = [] for i in list_of_cols : if pre_existing_source : x = spark . read . csv ( source_path + \"/\" + model_directory + \"/frequency_counts/\" + i , header = True , inferSchema = True , ) else : x = ( source_bin . groupBy ( i ) . agg (( F . count ( i ) / idf_source . count ()) . alias ( \"p\" )) . fillna ( - 1 ) ) x . coalesce ( 1 ) . write . csv ( source_path + \"/\" + model_directory + \"/frequency_counts/\" + i , header = True , mode = \"overwrite\" , ) y = ( target_bin . groupBy ( i ) . agg (( F . count ( i ) / idf_target . count ()) . alias ( \"q\" )) . fillna ( - 1 ) ) xy = ( x . join ( y , i , \"full_outer\" ) . fillna ( 0.0001 , subset = [ \"p\" , \"q\" ]) . replace ( 0 , 0.0001 ) . orderBy ( i ) ) p = np . array ( xy . select ( \"p\" ) . rdd . flatMap ( lambda x : x ) . collect ()) q = np . array ( xy . select ( \"q\" ) . rdd . flatMap ( lambda x : x ) . collect ()) result [ \"attribute\" ] . append ( i ) counter = 0 for idx , method in enumerate ( method_type ): drift_function = { \"PSI\" : psi , \"JSD\" : js_divergence , \"HD\" : hellinger , \"KS\" : ks , } metric = float ( round ( drift_function [ method ]( p , q ), 4 )) result [ method ] . append ( metric ) if counter == 0 : if metric > threshold : result [ \"flagged\" ] . append ( 1 ) counter = 1 if ( idx == ( len ( method_type ) - 1 )) & ( counter == 0 ): result [ \"flagged\" ] . append ( 0 ) odf = ( spark . createDataFrame ( pd . DataFrame . from_dict ( result , orient = \"index\" ) . transpose () ) . select ([ \"attribute\" ] + method_type + [ \"flagged\" ]) . orderBy ( F . desc ( \"flagged\" )) ) if print_impact : logger . info ( \"All Attributes:\" ) odf . show ( len ( list_of_cols )) logger . info ( \"Attributes meeting Data Drift threshold:\" ) drift = odf . where ( F . col ( \"flagged\" ) == 1 ) drift . show ( drift . count ()) return odf def stability_index_computation ( spark , * idfs , list_of_cols = \"all\" , drop_cols = [], metric_weightages = { \"mean\" : 0.5 , \"stddev\" : 0.3 , \"kurtosis\" : 0.2 }, existing_metric_path = \"\" , appended_metric_path = \"\" , threshold = 1 , print_impact = False , ): \"\"\" The data stability is represented by a single metric to summarise the stability of an attribute over multiple time periods. For example, given 6 datasets collected in 6 consecutive time periods (D1, D2, \u2026, D6), data stability index of an attribute measures how stable the attribute is from D1 to D6. The major difference between data drift and data stability is that data drift analysis is only based on 2 datasets: source and target. However data stability analysis can be performed on multiple datasets. In addition, the source dataset is not required indicating that the stability index can be directly computed among multiple target datasets by comparing the statistical properties among them. In summary, given N datasets representing different time periods, we would like to measure the stability of some numerical attributes from the first to the N-th dataset. The whole process can be broken down into 2 steps: (1) Choose a few statistical metrics to describe the distribution of each attribute at each time period. (2) Compute attribute level stability by combining the stability of each statistical metric over time periods. In the first step, we choose mean, standard deviation and kurtosis as the statistical metrics in our implementation. Intuitively, they represent different aspects of a distribution: mean measures central tendency, standard deviation measures dispersion and kurtosis measures shape of a distribution. Reasons of selecting those 3 metrics will be explained in a later section. With mean, standard deviation and kurtosis computed for each attribute at each time interval, we can form 3 arrays of size N for each attribute. In the second step, Coefficient of Variation (CV) is used to measure the stability of each metric. CV represents the ratio of the standard deviation to the mean, which is a unitless statistic to compare the relative variation from one array to another. Considering the wide potential range of CV, the absolute value of CV is then mapped to an integer between 0 and 4 according to the table below, where 0 indicates highly unstable and 4 indicates highly stable. We call this integer a metric stability index. | abs(CV) Interval | Metric Stability Index | |------------------|------------------------| | [0, 0.03) | 4 | | [0.03, 0.1) | 3 | | [0.1, 0.2) | 2 | | [0.2, 0.5) | 1 | | [0.5, +inf) | 0 | Finally, the attribute stability index (SI) is a weighted sum of 3 metric stability indexes, where we assign 50% for mean, 30% for standard deviation and 20% for kurtosis by default. The final output is a float between 0 and 4 and an attribute can be classified as one of the following categories: very unstable (0\u2264SI<1), unstable (1\u2264SI<2), marginally stable (2\u2264SI<3), stable (3\u2264SI<3.5) and very stable (3.5\u2264SI\u22644). For example, there are 6 samples of attribute X from T1 to T6. For each sample, we have computed the statistical metrics of X from T1 to T6: | idx | Mean | Standard deviation | Kurtosis | |-----|------|--------------------|----------| | 1 | 11 | 2 | 3.9 | | 2 | 12 | 1 | 4.2 | | 3 | 15 | 3 | 4.0 | | 4 | 10 | 2 | 4.1 | | 5 | 11 | 1 | 4.2 | | 6 | 13 | 0.5 | 4.0 | Then we calculate the Coefficient of Variation for each array: - CV of mean = CV([11, 12, 15, 10, 11, 13]) = 0.136 - CV of standard deviation = CV([2, 1, 3, 2, 1, 0.5]) = 0.529 - CV of kurtosis = CV([3.9, 4.2, 4.0, 4.1, 4.2, 4.0]) = 0.027 Metric stability indexes are then computed by mapping each CV value to an integer accordingly. As a result, metric stability index is 2 for mean, 0 for standard deviation and 4 for kurtosis. Why mean is chosen over median? - Dummy variables which take only the value 0 or 1 are frequently seen in machine learning features. Mean of a dummy variable represents the proportion of value 1 and median of a dummy variable is either 0 or 1 whichever is more frequent. However, CV may not work well when 0 appears in the array or the array contains both positive and negative values. For example, intuitively [0,0,0,0,0,1,0,0,0] is a stable array but its CV is 2.83 which is extremely high, but cv of [0.45,0.44,0.48,0.49,0.42,0.52,0.49,0.47,0.48] is 0.06 which is much more reasonable. Thus we decided to use mean instead of median. Although median is considered as a more robust choice, outlier treatment can be applied prior to data stability analysis to handle this issue. Why kurtosis is chosen over skewness? - Kurtosis is a positive value (note that we use kurtosis instead of excess kurtosis which) but skewness can range from \u2013inf to +inf. Usually, if skewness is between -0.5 and 0.5, the distribution is approximately symmetric. Thus, if the skewness fluctuates around 0, the CV is highly likely to be high or invalid because the mean will be close to 0. Stability index is preferred in the following scenario: - Pairwise drift analysis can be performed between the source dataset and each of the target dataset to quantify the drift over time. However this can be time-consuming especially when the number of target dataset is large. In this case, measuring data stability instead of data drift would be a much faster alternative and the source/baseline dataset is not required as well Troubleshooting - If the attribute stability index appears to be nan, it may due to one of the following reasons: - One metric ( likely to be kurtosis) is nan. For example, the kurtosis of a sample is nan If its standard deviation is 0. - The mean of a metric from the first to the N-th dataset is zero, causing the denominator of CV to be 0. For example, when mean of attribute X is always zero for all datasets, its stability index would be nan. Limitation - Limitation of CV: CV may not work well when 0 appears in the array or the array contains both positive and negative values. Parameters ---------- spark Spark Session idfs Variable number of input dataframes list_of_cols List of numerical columns to check stability e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all numerical columns for analysis. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". (Default value = []) metric_weightages Takes input in dictionary format with keys being the metric name - \"mean\",\"stdev\",\"kurtosis\" and value being the weightage of the metric (between 0 and 1). Sum of all weightages must be 1. (Default value = {\"mean\": 0.5, \"stddev\": 0.3, \"kurtosis\": 0.2}) existing_metric_path This argument is path for referring pre-existing metrics of historical datasets and is of schema [idx, attribute, mean, stdev, kurtosis]. idx is index number of historical datasets assigned in chronological order. (Default value = \"\") appended_metric_path This argument is path for saving input dataframes metrics after appending to the historical datasets' metrics. (Default value = \"\") threshold A column is flagged if the stability index is below the threshold, which varies between 0 to 4. The following criteria can be used to classifiy stability_index (SI): very unstable: 0\u2264SI<1, unstable: 1\u2264SI<2, marginally stable: 2\u2264SI<3, stable: 3\u2264SI<3.5 and very stable: 3.5\u2264SI\u22644. (Default value = 1) print_impact True, False (Default value = False) This argument is to print out the stability metrics of all attributes and potential unstable attributes. Returns ------- DataFrame [attribute, mean_si, stddev_si, kurtosis_si, mean_cv, stddev_cv, kurtosis_cv, stability_index]. *_cv is coefficient of variation for each metric. *_si is stability index for each metric. stability_index is net weighted stability index based on the individual metrics' stability index. \"\"\" num_cols = attributeType_segregation ( idfs [ 0 ])[ 0 ] if list_of_cols == \"all\" : list_of_cols = num_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in drop_cols ])) if any ( x not in num_cols for x in list_of_cols ) | ( len ( list_of_cols ) == 0 ): raise TypeError ( \"Invalid input for Column(s)\" ) if ( round ( metric_weightages . get ( \"mean\" , 0 ) + metric_weightages . get ( \"stddev\" , 0 ) + metric_weightages . get ( \"kurtosis\" , 0 ), 3 , ) != 1 ): raise ValueError ( \"Invalid input for metric weightages. Either metric name is incorrect or sum of metric weightages is not 1.0\" ) if existing_metric_path : existing_metric_df = spark . read . csv ( existing_metric_path , header = True , inferSchema = True ) dfs_count = existing_metric_df . select ( F . max ( F . col ( \"idx\" ))) . first ()[ 0 ] else : schema = T . StructType ( [ T . StructField ( \"idx\" , T . IntegerType (), True ), T . StructField ( \"attribute\" , T . StringType (), True ), T . StructField ( \"mean\" , T . DoubleType (), True ), T . StructField ( \"stddev\" , T . DoubleType (), True ), T . StructField ( \"kurtosis\" , T . DoubleType (), True ), ] ) existing_metric_df = spark . sparkContext . emptyRDD () . toDF ( schema ) dfs_count = 0 metric_ls = [] for idf in idfs : for i in list_of_cols : mean , stddev , kurtosis = idf . select ( F . mean ( i ), F . stddev ( i ), F . kurtosis ( i ) ) . first () metric_ls . append ( [ dfs_count + 1 , i , mean , stddev , kurtosis + 3.0 if kurtosis else None ] ) dfs_count += 1 new_metric_df = spark . createDataFrame ( metric_ls , schema = ( \"idx\" , \"attribute\" , \"mean\" , \"stddev\" , \"kurtosis\" ) ) appended_metric_df = concatenate_dataset ( existing_metric_df , new_metric_df ) if appended_metric_path : appended_metric_df . coalesce ( 1 ) . write . csv ( appended_metric_path , header = True , mode = \"overwrite\" ) result = [] for i in list_of_cols : i_output = [ i ] for metric in [ \"mean\" , \"stddev\" , \"kurtosis\" ]: metric_stats = ( appended_metric_df . where ( F . col ( \"attribute\" ) == i ) . orderBy ( \"idx\" ) . select ( metric ) . fillna ( np . nan ) . rdd . flatMap ( list ) . collect () ) metric_cv = round ( float ( variation ([ a for a in metric_stats ])), 4 ) or None i_output . append ( metric_cv ) result . append ( i_output ) schema = T . StructType ( [ T . StructField ( \"attribute\" , T . StringType (), True ), T . StructField ( \"mean_cv\" , T . FloatType (), True ), T . StructField ( \"stddev_cv\" , T . FloatType (), True ), T . StructField ( \"kurtosis_cv\" , T . FloatType (), True ), ] ) odf = spark . createDataFrame ( result , schema = schema ) def score_cv ( cv , thresholds = [ 0.03 , 0.1 , 0.2 , 0.5 ]): if cv is None : return None else : cv = abs ( cv ) stability_index = [ 4 , 3 , 2 , 1 , 0 ] for i , thresh in enumerate ( thresholds ): if cv < thresh : return stability_index [ i ] return stability_index [ - 1 ] f_score_cv = F . udf ( score_cv , T . IntegerType ()) odf = ( odf . replace ( np . nan , None ) . withColumn ( \"mean_si\" , f_score_cv ( F . col ( \"mean_cv\" ))) . withColumn ( \"stddev_si\" , f_score_cv ( F . col ( \"stddev_cv\" ))) . withColumn ( \"kurtosis_si\" , f_score_cv ( F . col ( \"kurtosis_cv\" ))) . withColumn ( \"stability_index\" , F . round ( ( F . col ( \"mean_si\" ) * metric_weightages . get ( \"mean\" , 0 ) + F . col ( \"stddev_si\" ) * metric_weightages . get ( \"stddev\" , 0 ) + F . col ( \"kurtosis_si\" ) * metric_weightages . get ( \"kurtosis\" , 0 ) ), 4 , ), ) . withColumn ( \"flagged\" , F . when ( ( F . col ( \"stability_index\" ) < threshold ) | ( F . col ( \"stability_index\" ) . isNull ()), 1 , ) . otherwise ( 0 ), ) ) if print_impact : logger . info ( \"All Attributes:\" ) odf . show ( len ( list_of_cols )) logger . info ( \"Potential Unstable Attributes:\" ) unstable = odf . where ( F . col ( \"flagged\" ) == 1 ) unstable . show ( unstable . count ()) return odf def feature_stability_estimation ( spark , attribute_stats , attribute_transformation , metric_weightages = { \"mean\" : 0.5 , \"stddev\" : 0.3 , \"kurtosis\" : 0.2 }, threshold = 1 , print_impact = False , ): \"\"\" This function is able to estimate the stability index of a new feature composed of certain attributes whose stability metrics are known. For example, the new feature F can be expressed as F = g(X1, X2, \u2026, Xn), where X1, X2, \u2026, Xn represent different attributes and g represents the transformation function. The most straightforward way is to generate the new feature for all periods and calculate its stability index. However, it requires reading all historical data again which can be unrealistic for large datasets. Thus, the objective of this function is to estimate feature stability index without reading historical data. One example can be the following scenario: we have attributes A and B, we have their respective stability statistics from T1 to T7. At T7 we realise we need to generate a new feature: A/B, but we don\u2019t have statistics metrics of A/B from T1 to T6 and this is where this function can be applied to generate an estimation without reading datasets from T1 to T6. The estimation can be broken down into 3 steps. 1. Estimate mean and stddev for the new feature based on attribute metrics (no existing resource found to estimate Feature kurtosis). Estimated mean and stddev are generated for each time period using the formula below according to [1]: ![https://raw.githubusercontent.com/anovos/anovos-docs/main/docs/assets/feature_stability_formulae.png](https://raw.githubusercontent.com/anovos/anovos-docs/main/docs/assets/feature_stability_formulae.png) 2. Calculate Coefficient of variation (CV) for estimated feature mean and stddev. Each CV can be then mapped to an integer between 0 and 4 to generate the metric stability index. 3. Similar to the attribute stability index, each metric is assigned a weightage between 0 and 1, where the default values are 50 for mean, 30% for standard deviation and 20% for kurtosis. Because we are unable to generate kurtosis stability index, its minimum and maximum possible values (0 and 4) are used to output a range for global stability index (GSI): * Lower bound of GSI = 0.5\u2217mean stability index + 0.3\u2217stddev stability index + 0.2 \u2217 \ud835\udfce * Upper bound of GSI = 0.5\u2217mean stability index + 0.3\u2217stddev stability index + 0.2 \u2217 \ud835\udfd2 [1] Benaroya, H., Han, S. M., & Nagurka, M. (2005). Probability models in engineering and science (Vol. 192, pp. 168-169). CRC press. Parameters ---------- spark Spark Session attribute_stats Spark dataframe. The intermediate dataframe saved by running function stabilityIndex_computation with schema [idx, attribute, mean, stddev, kurtosis]. It should contain all the attributes used in argument attribute_transformation. attribute_transformation Takes input in dictionary format: each key-value combination represents one new feature. Each key is a string containing all the attributes involved in the new feature seperated by '|'. Each value is the transformation of the attributes in string. For example, {'X|Y|Z': 'X**2+Y/Z', 'A': 'log(A)'} metric_weightages Takes input in dictionary format with keys being the metric name - \"mean\",\"stdev\",\"kurtosis\" and value being the weightage of the metric (between 0 and 1). Sum of all weightages must be 1. (Default value = {\"mean\": 0.5, \"stddev\": 0.3, \"kurtosis\": 0.2}) threshold A column is flagged if the stability index is below the threshold, which varies between 0 and 4. The following criteria can be used to classify stability_index (SI): very unstable: 0\u2264SI<1, unstable: 1\u2264SI<2, marginally stable: 2\u2264SI<3, stable: 3\u2264SI<3.5 and very stable: 3.5\u2264SI\u22644. (Default value = 1) print_impact True, False (Default value = False) This argument is to print out the stability metrics of all newly generated features and potential unstable features. Returns ------- DataFrame [feature_formula, mean_cv, stddev_cv, mean_si, stddev_si, stability_index_lower_bound, stability_index_upper_bound, flagged_lower, flagged_upper]. *_cv is coefficient of variation for each metric. *_si is stability index for each metric. stability_index_lower_bound and stability_index_upper_bound form a range for estimated stability index. flagged_lower and flagged_upper indicate whether the feature is potentially unstable based on the lower and upper bounds for stability index. \"\"\" def stats_estimation ( attributes , transformation , mean , stddev ): attribute_means = list ( zip ( attributes , mean )) first_dev = [] second_dev = [] est_mean = 0 est_var = 0 for attr , s in zip ( attributes , stddev ): first_dev = sp . diff ( transformation , attr ) second_dev = sp . diff ( transformation , attr , 2 ) est_mean += s ** 2 * second_dev . subs ( attribute_means ) / 2 est_var += s ** 2 * ( first_dev . subs ( attribute_means )) ** 2 transformation = sp . parse_expr ( transformation ) est_mean += transformation . subs ( attribute_means ) return [ float ( est_mean ), float ( est_var )] f_stats_estimation = F . udf ( stats_estimation , T . ArrayType ( T . FloatType ())) index = ( attribute_stats . select ( \"idx\" ) . distinct () . orderBy ( \"idx\" ) . rdd . flatMap ( list ) . collect () ) attribute_names = list ( attribute_transformation . keys ()) transformations = list ( attribute_transformation . values ()) feature_metric = [] for attributes , transformation in zip ( attribute_names , transformations ): attributes = [ x . strip () for x in attributes . split ( \"|\" )] for idx in index : attr_mean_list , attr_stddev_list = [], [] for attr in attributes : df_temp = attribute_stats . where ( ( F . col ( \"idx\" ) == idx ) & ( F . col ( \"attribute\" ) == attr ) ) if df_temp . count () == 0 : raise TypeError ( \"Invalid input for attribute_stats: all involved attributes must have available statistics across all time periods (idx)\" ) attr_mean_list . append ( df_temp . select ( \"mean\" ) . rdd . flatMap ( lambda x : x ) . collect ()[ 0 ] ) attr_stddev_list . append ( df_temp . select ( \"stddev\" ) . rdd . flatMap ( lambda x : x ) . collect ()[ 0 ] ) feature_metric . append ( [ idx , transformation , attributes , attr_mean_list , attr_stddev_list ] ) schema = T . StructType ( [ T . StructField ( \"idx\" , T . IntegerType (), True ), T . StructField ( \"transformation\" , T . StringType (), True ), T . StructField ( \"attributes\" , T . ArrayType ( T . StringType ()), True ), T . StructField ( \"attr_mean_list\" , T . ArrayType ( T . FloatType ()), True ), T . StructField ( \"attr_stddev_list\" , T . ArrayType ( T . FloatType ()), True ), ] ) df_feature_metric = ( spark . createDataFrame ( feature_metric , schema = schema ) . withColumn ( \"est_feature_stats\" , f_stats_estimation ( \"attributes\" , \"transformation\" , \"attr_mean_list\" , \"attr_stddev_list\" ), ) . withColumn ( \"est_feature_mean\" , F . col ( \"est_feature_stats\" )[ 0 ]) . withColumn ( \"est_feature_stddev\" , F . sqrt ( F . col ( \"est_feature_stats\" )[ 1 ])) . select ( \"idx\" , \"attributes\" , \"transformation\" , \"est_feature_mean\" , \"est_feature_stddev\" , ) ) output = [] for idx , i in enumerate ( transformations ): i_output = [ i ] for metric in [ \"est_feature_mean\" , \"est_feature_stddev\" ]: metric_stats = ( df_feature_metric . where ( F . col ( \"transformation\" ) == i ) . orderBy ( \"idx\" ) . select ( metric ) . fillna ( np . nan ) . rdd . flatMap ( list ) . collect () ) metric_cv = round ( float ( variation ([ a for a in metric_stats ])), 4 ) or None i_output . append ( metric_cv ) output . append ( i_output ) schema = T . StructType ( [ T . StructField ( \"feature_formula\" , T . StringType (), True ), T . StructField ( \"mean_cv\" , T . FloatType (), True ), T . StructField ( \"stddev_cv\" , T . FloatType (), True ), ] ) odf = spark . createDataFrame ( output , schema = schema ) def score_cv ( cv , thresholds = [ 0.03 , 0.1 , 0.2 , 0.5 ]): if cv is None : return None else : cv = abs ( cv ) stability_index = [ 4 , 3 , 2 , 1 , 0 ] for i , thresh in enumerate ( thresholds ): if cv < thresh : return stability_index [ i ] return stability_index [ - 1 ] f_score_cv = F . udf ( score_cv , T . IntegerType ()) odf = ( odf . replace ( np . nan , None ) . withColumn ( \"mean_si\" , f_score_cv ( F . col ( \"mean_cv\" ))) . withColumn ( \"stddev_si\" , f_score_cv ( F . col ( \"stddev_cv\" ))) . withColumn ( \"stability_index_lower_bound\" , F . round ( F . col ( \"mean_si\" ) * metric_weightages . get ( \"mean\" , 0 ) + F . col ( \"stddev_si\" ) * metric_weightages . get ( \"stddev\" , 0 ), 4 , ), ) . withColumn ( \"stability_index_upper_bound\" , F . round ( F . col ( \"stability_index_lower_bound\" ) + 4 * metric_weightages . get ( \"kurtosis\" , 0 ), 4 , ), ) . withColumn ( \"flagged_lower\" , F . when ( ( F . col ( \"stability_index_lower_bound\" ) < threshold ) | ( F . col ( \"stability_index_lower_bound\" ) . isNull ()), 1 , ) . otherwise ( 0 ), ) . withColumn ( \"flagged_upper\" , F . when ( ( F . col ( \"stability_index_upper_bound\" ) < threshold ) | ( F . col ( \"stability_index_upper_bound\" ) . isNull ()), 1 , ) . otherwise ( 0 ), ) ) if print_impact : logger . info ( \"All Features:\" ) odf . show ( len ( attribute_names ), False ) logger . info ( \"Potential Unstable Features Identified by Both Lower and Upper Bounds:\" ) unstable = odf . where ( F . col ( \"flagged_upper\" ) == 1 ) unstable . show ( unstable . count ()) return odf","title":"detector"},{"location":"api/drift/detector.html#functions","text":"def feature_stability_estimation ( spark, attribute_stats, attribute_transformation, metric_weightages={'mean': 0.5, 'stddev': 0.3, 'kurtosis': 0.2}, threshold=1, print_impact=False) This function is able to estimate the stability index of a new feature composed of certain attributes whose stability metrics are known. For example, the new feature F can be expressed as F = g(X1, X2, \u2026, Xn), where X1, X2, \u2026, Xn represent different attributes and g represents the transformation function. The most straightforward way is to generate the new feature for all periods and calculate its stability index. However, it requires reading all historical data again which can be unrealistic for large datasets. Thus, the objective of this function is to estimate feature stability index without reading historical data. One example can be the following scenario: we have attributes A and B, we have their respective stability statistics from T1 to T7. At T7 we realise we need to generate a new feature: A/B, but we don\u2019t have statistics metrics of A/B from T1 to T6 and this is where this function can be applied to generate an estimation without reading datasets from T1 to T6. The estimation can be broken down into 3 steps. 1. Estimate mean and stddev for the new feature based on attribute metrics (no existing resource found to estimate Feature kurtosis). Estimated mean and stddev are generated for each time period using the formula below according to [1]: 2. Calculate Coefficient of variation (CV) for estimated feature mean and stddev. Each CV can be then mapped to an integer between 0 and 4 to generate the metric stability index. 3. Similar to the attribute stability index, each metric is assigned a weightage between 0 and 1, where the default values are 50 for mean, 30% for standard deviation and 20% for kurtosis. Because we are unable to generate kurtosis stability index, its minimum and maximum possible values (0 and 4) are used to output a range for global stability index (GSI): * Lower bound of GSI = 0.5\u2217mean stability index + 0.3\u2217stddev stability index + 0.2 \u2217 \ud835\udfce * Upper bound of GSI = 0.5\u2217mean stability index + 0.3\u2217stddev stability index + 0.2 \u2217 \ud835\udfd2 [1] Benaroya, H., Han, S. M., & Nagurka, M. (2005). Probability models in engineering and science (Vol. 192, pp. 168-169). CRC press.","title":"Functions"},{"location":"api/drift/distances.html","text":"distances Expand source code import math import numpy as np def hellinger ( p , q ): return math . sqrt ( np . sum (( np . sqrt ( p ) - np . sqrt ( q )) ** 2 ) / 2 ) def psi ( p , q ): return np . sum (( p - q ) * np . log ( p / q )) def kl_divergence ( p , q ): kl = np . sum ( p * np . log ( p / q )) return kl def js_divergence ( p , q ): m = ( p + q ) / 2 pm = kl_divergence ( p , m ) qm = kl_divergence ( q , m ) jsd = ( pm + qm ) / 2 return jsd def ks ( p , q ): return np . max ( np . abs ( np . cumsum ( p ) - np . cumsum ( q ))) Functions def hellinger ( p, q) Expand source code def hellinger ( p , q ): return math . sqrt ( np . sum (( np . sqrt ( p ) - np . sqrt ( q )) ** 2 ) / 2 ) def js_divergence ( p, q) Expand source code def js_divergence ( p , q ): m = ( p + q ) / 2 pm = kl_divergence ( p , m ) qm = kl_divergence ( q , m ) jsd = ( pm + qm ) / 2 return jsd def kl_divergence ( p, q) Expand source code def kl_divergence ( p , q ): kl = np . sum ( p * np . log ( p / q )) return kl def ks ( p, q) Expand source code def ks ( p , q ): return np . max ( np . abs ( np . cumsum ( p ) - np . cumsum ( q ))) def psi ( p, q) Expand source code def psi ( p , q ): return np . sum (( p - q ) * np . log ( p / q ))","title":"<code>distances</code>"},{"location":"api/drift/distances.html#distances","text":"Expand source code import math import numpy as np def hellinger ( p , q ): return math . sqrt ( np . sum (( np . sqrt ( p ) - np . sqrt ( q )) ** 2 ) / 2 ) def psi ( p , q ): return np . sum (( p - q ) * np . log ( p / q )) def kl_divergence ( p , q ): kl = np . sum ( p * np . log ( p / q )) return kl def js_divergence ( p , q ): m = ( p + q ) / 2 pm = kl_divergence ( p , m ) qm = kl_divergence ( q , m ) jsd = ( pm + qm ) / 2 return jsd def ks ( p , q ): return np . max ( np . abs ( np . cumsum ( p ) - np . cumsum ( q )))","title":"distances"},{"location":"api/drift/distances.html#functions","text":"def hellinger ( p, q) Expand source code def hellinger ( p , q ): return math . sqrt ( np . sum (( np . sqrt ( p ) - np . sqrt ( q )) ** 2 ) / 2 ) def js_divergence ( p, q) Expand source code def js_divergence ( p , q ): m = ( p + q ) / 2 pm = kl_divergence ( p , m ) qm = kl_divergence ( q , m ) jsd = ( pm + qm ) / 2 return jsd def kl_divergence ( p, q) Expand source code def kl_divergence ( p , q ): kl = np . sum ( p * np . log ( p / q )) return kl def ks ( p, q) Expand source code def ks ( p , q ): return np . max ( np . abs ( np . cumsum ( p ) - np . cumsum ( q ))) def psi ( p, q) Expand source code def psi ( p , q ): return np . sum (( p - q ) * np . log ( p / q ))","title":"Functions"},{"location":"api/drift/validations.html","text":"validations Expand source code from functools import wraps , partial from loguru import logger from anovos.shared.utils import attributeType_segregation def check_list_of_columns ( func = None , columns = \"list_of_cols\" , target_idx : int = 1 , target : str = \"idf_target\" , drop = \"drop_cols\" , ): if func is None : return partial ( check_list_of_columns , columns = columns , target = target , drop = drop ) @wraps ( func ) def validate ( * args , ** kwargs ): logger . debug ( \"check the list of columns\" ) idf_target = kwargs . get ( target , \"\" ) or args [ target_idx ] cols_raw = kwargs . get ( columns , \"all\" ) if isinstance ( cols_raw , str ): if cols_raw == \"all\" : num_cols , cat_cols , other_cols = attributeType_segregation ( idf_target ) cols = num_cols + cat_cols else : cols = [ x . strip () for x in cols_raw . split ( \"|\" )] elif isinstance ( cols_raw , list ): cols = cols_raw else : raise TypeError ( f \"' { columns } ' must be either a string or a list of strings.\" f \" Received { type ( cols_raw ) } .\" ) drops_raw = kwargs . get ( drop , []) if isinstance ( drops_raw , str ): drops = [ x . strip () for x in drops_raw . split ( \"|\" )] elif isinstance ( drops_raw , list ): drops = drops_raw else : raise TypeError ( f \"' { drop } ' must be either a string or a list of strings. \" f \"Received { type ( drops_raw ) } .\" ) final_cols = list ( set ( e for e in cols if e not in drops )) if not final_cols : raise ValueError ( f \"Empty set of columns is given. Columns to select: { cols } , columns to drop: { drops } .\" ) if any ( x not in idf_target . columns for x in final_cols ): raise ValueError ( f \"Not all columns are in the input dataframe. \" f \"Missing columns: { set ( final_cols ) - set ( idf_target . columns ) } \" ) kwargs [ columns ] = final_cols kwargs [ drop ] = [] return func ( * args , ** kwargs ) return validate def check_distance_method ( func = None , param = \"method_type\" ): if func is None : return partial ( check_distance_method , param = param ) @wraps ( func ) def validate ( * args , ** kwargs ): dist_distance_methods = kwargs . get ( param , \"PSI\" ) if isinstance ( dist_distance_methods , str ): if dist_distance_methods == \"all\" : dist_distance_methods = [ \"PSI\" , \"JSD\" , \"HD\" , \"KS\" ] else : dist_distance_methods = [ x . strip () for x in dist_distance_methods . split ( \"|\" ) ] if any ( x not in ( \"PSI\" , \"JSD\" , \"HD\" , \"KS\" ) for x in dist_distance_methods ): raise TypeError ( f \"Invalid input for { param } \" ) kwargs [ param ] = dist_distance_methods return func ( * args , ** kwargs ) return validate Functions def check_distance_method ( func=None, param='method_type') Expand source code def check_distance_method ( func = None , param = \"method_type\" ): if func is None : return partial ( check_distance_method , param = param ) @wraps ( func ) def validate ( * args , ** kwargs ): dist_distance_methods = kwargs . get ( param , \"PSI\" ) if isinstance ( dist_distance_methods , str ): if dist_distance_methods == \"all\" : dist_distance_methods = [ \"PSI\" , \"JSD\" , \"HD\" , \"KS\" ] else : dist_distance_methods = [ x . strip () for x in dist_distance_methods . split ( \"|\" ) ] if any ( x not in ( \"PSI\" , \"JSD\" , \"HD\" , \"KS\" ) for x in dist_distance_methods ): raise TypeError ( f \"Invalid input for { param } \" ) kwargs [ param ] = dist_distance_methods return func ( * args , ** kwargs ) return validate def check_list_of_columns ( func=None, columns='list_of_cols', target_idx: int = 1, target: str = 'idf_target', drop='drop_cols') Expand source code def check_list_of_columns ( func = None , columns = \"list_of_cols\" , target_idx : int = 1 , target : str = \"idf_target\" , drop = \"drop_cols\" , ): if func is None : return partial ( check_list_of_columns , columns = columns , target = target , drop = drop ) @wraps ( func ) def validate ( * args , ** kwargs ): logger . debug ( \"check the list of columns\" ) idf_target = kwargs . get ( target , \"\" ) or args [ target_idx ] cols_raw = kwargs . get ( columns , \"all\" ) if isinstance ( cols_raw , str ): if cols_raw == \"all\" : num_cols , cat_cols , other_cols = attributeType_segregation ( idf_target ) cols = num_cols + cat_cols else : cols = [ x . strip () for x in cols_raw . split ( \"|\" )] elif isinstance ( cols_raw , list ): cols = cols_raw else : raise TypeError ( f \"' { columns } ' must be either a string or a list of strings.\" f \" Received { type ( cols_raw ) } .\" ) drops_raw = kwargs . get ( drop , []) if isinstance ( drops_raw , str ): drops = [ x . strip () for x in drops_raw . split ( \"|\" )] elif isinstance ( drops_raw , list ): drops = drops_raw else : raise TypeError ( f \"' { drop } ' must be either a string or a list of strings. \" f \"Received { type ( drops_raw ) } .\" ) final_cols = list ( set ( e for e in cols if e not in drops )) if not final_cols : raise ValueError ( f \"Empty set of columns is given. Columns to select: { cols } , columns to drop: { drops } .\" ) if any ( x not in idf_target . columns for x in final_cols ): raise ValueError ( f \"Not all columns are in the input dataframe. \" f \"Missing columns: { set ( final_cols ) - set ( idf_target . columns ) } \" ) kwargs [ columns ] = final_cols kwargs [ drop ] = [] return func ( * args , ** kwargs ) return validate","title":"<code>validations</code>"},{"location":"api/drift/validations.html#validations","text":"Expand source code from functools import wraps , partial from loguru import logger from anovos.shared.utils import attributeType_segregation def check_list_of_columns ( func = None , columns = \"list_of_cols\" , target_idx : int = 1 , target : str = \"idf_target\" , drop = \"drop_cols\" , ): if func is None : return partial ( check_list_of_columns , columns = columns , target = target , drop = drop ) @wraps ( func ) def validate ( * args , ** kwargs ): logger . debug ( \"check the list of columns\" ) idf_target = kwargs . get ( target , \"\" ) or args [ target_idx ] cols_raw = kwargs . get ( columns , \"all\" ) if isinstance ( cols_raw , str ): if cols_raw == \"all\" : num_cols , cat_cols , other_cols = attributeType_segregation ( idf_target ) cols = num_cols + cat_cols else : cols = [ x . strip () for x in cols_raw . split ( \"|\" )] elif isinstance ( cols_raw , list ): cols = cols_raw else : raise TypeError ( f \"' { columns } ' must be either a string or a list of strings.\" f \" Received { type ( cols_raw ) } .\" ) drops_raw = kwargs . get ( drop , []) if isinstance ( drops_raw , str ): drops = [ x . strip () for x in drops_raw . split ( \"|\" )] elif isinstance ( drops_raw , list ): drops = drops_raw else : raise TypeError ( f \"' { drop } ' must be either a string or a list of strings. \" f \"Received { type ( drops_raw ) } .\" ) final_cols = list ( set ( e for e in cols if e not in drops )) if not final_cols : raise ValueError ( f \"Empty set of columns is given. Columns to select: { cols } , columns to drop: { drops } .\" ) if any ( x not in idf_target . columns for x in final_cols ): raise ValueError ( f \"Not all columns are in the input dataframe. \" f \"Missing columns: { set ( final_cols ) - set ( idf_target . columns ) } \" ) kwargs [ columns ] = final_cols kwargs [ drop ] = [] return func ( * args , ** kwargs ) return validate def check_distance_method ( func = None , param = \"method_type\" ): if func is None : return partial ( check_distance_method , param = param ) @wraps ( func ) def validate ( * args , ** kwargs ): dist_distance_methods = kwargs . get ( param , \"PSI\" ) if isinstance ( dist_distance_methods , str ): if dist_distance_methods == \"all\" : dist_distance_methods = [ \"PSI\" , \"JSD\" , \"HD\" , \"KS\" ] else : dist_distance_methods = [ x . strip () for x in dist_distance_methods . split ( \"|\" ) ] if any ( x not in ( \"PSI\" , \"JSD\" , \"HD\" , \"KS\" ) for x in dist_distance_methods ): raise TypeError ( f \"Invalid input for { param } \" ) kwargs [ param ] = dist_distance_methods return func ( * args , ** kwargs ) return validate","title":"validations"},{"location":"api/drift/validations.html#functions","text":"def check_distance_method ( func=None, param='method_type') Expand source code def check_distance_method ( func = None , param = \"method_type\" ): if func is None : return partial ( check_distance_method , param = param ) @wraps ( func ) def validate ( * args , ** kwargs ): dist_distance_methods = kwargs . get ( param , \"PSI\" ) if isinstance ( dist_distance_methods , str ): if dist_distance_methods == \"all\" : dist_distance_methods = [ \"PSI\" , \"JSD\" , \"HD\" , \"KS\" ] else : dist_distance_methods = [ x . strip () for x in dist_distance_methods . split ( \"|\" ) ] if any ( x not in ( \"PSI\" , \"JSD\" , \"HD\" , \"KS\" ) for x in dist_distance_methods ): raise TypeError ( f \"Invalid input for { param } \" ) kwargs [ param ] = dist_distance_methods return func ( * args , ** kwargs ) return validate def check_list_of_columns ( func=None, columns='list_of_cols', target_idx: int = 1, target: str = 'idf_target', drop='drop_cols') Expand source code def check_list_of_columns ( func = None , columns = \"list_of_cols\" , target_idx : int = 1 , target : str = \"idf_target\" , drop = \"drop_cols\" , ): if func is None : return partial ( check_list_of_columns , columns = columns , target = target , drop = drop ) @wraps ( func ) def validate ( * args , ** kwargs ): logger . debug ( \"check the list of columns\" ) idf_target = kwargs . get ( target , \"\" ) or args [ target_idx ] cols_raw = kwargs . get ( columns , \"all\" ) if isinstance ( cols_raw , str ): if cols_raw == \"all\" : num_cols , cat_cols , other_cols = attributeType_segregation ( idf_target ) cols = num_cols + cat_cols else : cols = [ x . strip () for x in cols_raw . split ( \"|\" )] elif isinstance ( cols_raw , list ): cols = cols_raw else : raise TypeError ( f \"' { columns } ' must be either a string or a list of strings.\" f \" Received { type ( cols_raw ) } .\" ) drops_raw = kwargs . get ( drop , []) if isinstance ( drops_raw , str ): drops = [ x . strip () for x in drops_raw . split ( \"|\" )] elif isinstance ( drops_raw , list ): drops = drops_raw else : raise TypeError ( f \"' { drop } ' must be either a string or a list of strings. \" f \"Received { type ( drops_raw ) } .\" ) final_cols = list ( set ( e for e in cols if e not in drops )) if not final_cols : raise ValueError ( f \"Empty set of columns is given. Columns to select: { cols } , columns to drop: { drops } .\" ) if any ( x not in idf_target . columns for x in final_cols ): raise ValueError ( f \"Not all columns are in the input dataframe. \" f \"Missing columns: { set ( final_cols ) - set ( idf_target . columns ) } \" ) kwargs [ columns ] = final_cols kwargs [ drop ] = [] return func ( * args , ** kwargs ) return validate","title":"Functions"},{"location":"api/feature_recommender/_index.html","text":"Overview Sub-modules anovos.feature_recommender.featrec_init anovos.feature_recommender.feature_exploration Feature explorer helps list down the potential features from our corpus based on user defined industry or/and use case. anovos.feature_recommender.feature_recommendation Feature recommender recommends features based on ingested data dictionary by the user.","title":"Overview"},{"location":"api/feature_recommender/_index.html#overview","text":"","title":"Overview"},{"location":"api/feature_recommender/_index.html#sub-modules","text":"anovos.feature_recommender.featrec_init anovos.feature_recommender.feature_exploration Feature explorer helps list down the potential features from our corpus based on user defined industry or/and use case. anovos.feature_recommender.feature_recommendation Feature recommender recommends features based on ingested data dictionary by the user.","title":"Sub-modules"},{"location":"api/feature_recommender/featrec_init.html","text":"featrec_init Expand source code import os import copy from re import finditer import pandas as pd from sentence_transformers import SentenceTransformer from torch.hub import _get_torch_home import site def detect_model_path (): \"\"\" Returns ------- Local Feature Explorer and Recommender semantic model path (if the model is pre-downloaded) \"\"\" transformers_path = os . getenv ( \"SENTENCE_TRANSFORMERS_HOME\" ) if transformers_path is None : try : torch_home = _get_torch_home () except ImportError : torch_home = os . path . expanduser ( os . getenv ( \"TORCH_HOME\" , os . path . join ( os . getenv ( \"XDG_CACHE_HOME\" , \"~/.cache\" ), \"torch\" ), ) ) transformers_path = os . path . join ( torch_home , \"sentence_transformers\" ) model_path = os . path . join ( transformers_path , \"sentence-transformers_all-mpnet-base-v2\" ) return model_path def model_download (): print ( \"Starting the Semantic Model download\" ) SentenceTransformer ( \"all-mpnet-base-v2\" ) print ( \"Model downloading finished\" ) class _TransformerModel : def __init__ ( self ): self . _model = None @property def model ( self ) -> SentenceTransformer : if self . _model is None : model_path = detect_model_path () if os . path . exists ( model_path ): self . _model = SentenceTransformer ( model_path ) else : raise FileNotFoundError ( \"Model has not been downloaded. Please use model_download() function to download the model first\" ) return self . _model model_fer = _TransformerModel () def init_input_fer (): \"\"\" Returns ------- Loading the Feature Explorer and Recommender (FER) Input DataFrame (FER corpus) \"\"\" local_path = os . path . join ( os . path . dirname ( os . path . abspath ( __file__ )), \"data\" , \"flatten_fr_db.csv\" ) if os . path . exists ( local_path ): input_path_fer = local_path else : site_path = site . getsitepackages ()[ 0 ] input_path_fer = os . path . join ( site_path , \"anovos/feature_recommender/data/flatten_fr_db.csv\" ) df_input_fer = pd . read_csv ( input_path_fer ) return df_input_fer def get_column_name ( df ): \"\"\" Parameters ---------- df Input DataFrame Returns ------- feature_name_column Column name of Feature Name in the input DataFrame (string) feature_desc_column Column name of Feature Description in the input DataFrame (string) industry_column Column name of Industry in the input DataFrame (string) usecase_column Column name of Usecase in the input DataFrame (string) \"\"\" feature_name_column = str ( df . columns . tolist ()[ 0 ]) feature_desc_column = str ( df . columns . tolist ()[ 1 ]) industry_column = str ( df . columns . tolist ()[ 2 ]) usecase_column = str ( df . columns . tolist ()[ 3 ]) return ( feature_name_column , feature_desc_column , industry_column , usecase_column , ) def camel_case_split ( input ): \"\"\" Parameters ---------- input Input (string) which requires cleaning Returns ------- \"\"\" processed_input = \"\" matches = finditer ( \".+?(?:(?<=[a-z])(?=[A-Z])|(?<=[A-Z])(?=[A-Z][a-z])|$)\" , input ) for m in matches : processed_input += str ( m . group ( 0 )) + str ( \" \" ) return processed_input def recommendation_data_prep ( df , name_column , desc_column ): \"\"\" Parameters ---------- df Input DataFrame name_column Column name of Input DataFrame attribute/ feature name (string) desc_column Column name of Input DataFrame attribute/ feature description (string) Returns ------- list_corpus List of prepared data for Feature Recommender functions return df_prep Processed DataFrame for Feature Recommender functions \"\"\" if not isinstance ( df , pd . DataFrame ): raise TypeError ( \"Invalid input for df\" ) if name_column not in df . columns and name_column is not None : raise TypeError ( \"Invalid input for name_column\" ) if desc_column not in df . columns and desc_column is not None : raise TypeError ( \"Invalid input for desc_column\" ) if name_column is None and desc_column is None : raise TypeError ( \"Need at least one input for either name_column or desc_column\" ) df_prep = copy . deepcopy ( df ) if name_column is None : df_prep [ desc_column ] = df_prep [ desc_column ] . astype ( str ) df_prep_com = df_prep [ desc_column ] elif desc_column is None : df_prep [ name_column ] = df_prep [ name_column ] . astype ( str ) df_prep_com = df_prep [ name_column ] else : df_prep [ name_column ] = df_prep [ name_column ] . str . replace ( \"_\" , \" \" ) df_prep [ name_column ] = df_prep [ name_column ] . astype ( str ) df_prep [ desc_column ] = df_prep [ desc_column ] . astype ( str ) df_prep_com = df_prep [[ name_column , desc_column ]] . agg ( \" \" . join , axis = 1 ) df_prep_com = df_prep_com . replace ({ \"[^A-Za-z0-9 ]+\" : \" \" }, regex = True ) for i in range ( len ( df_prep_com )): df_prep_com [ i ] = df_prep_com [ i ] . strip () df_prep_com [ i ] = camel_case_split ( df_prep_com [ i ]) list_corpus = df_prep_com . to_list () return list_corpus , df_prep def feature_exploration_prep (): \"\"\" Returns ------- df_input_fer DataFrame used in Feature Exploration functions \"\"\" df_input_fer = init_input_fer () df_input_fer = df_input_fer . rename ( columns = lambda x : x . strip () . replace ( \" \" , \"_\" )) return df_input_fer def feature_recommendation_prep (): \"\"\" Returns ------- list_train_fer List of prepared data for Feature Recommendation functions df_red_fer DataFrame used in Feature Recommendation functions list_embedding_train_fer List of embedding tensor for Feature Recommendation functions \"\"\" df_input_fer = init_input_fer () ( feature_name_column , feature_desc_column , industry_column , usecase_column , ) = get_column_name ( df_input_fer ) df_groupby_fer = ( df_input_fer . groupby ([ feature_name_column , feature_desc_column ]) . agg ( { industry_column : lambda x : \", \" . join ( set ( x . dropna ())), usecase_column : lambda x : \", \" . join ( set ( x . dropna ())), } ) . reset_index () ) list_train_fer , df_rec_fer = recommendation_data_prep ( df_groupby_fer , feature_name_column , feature_name_column ) return list_train_fer , df_rec_fer class EmbeddingsTrainFer : def __init__ ( self , list_train_fer ): self . list_train_fer = list_train_fer self . _embeddings = None @property def get ( self ): if self . _embeddings is None : self . _embeddings = model_fer . model . encode ( self . list_train_fer , convert_to_tensor = True ) return self . _embeddings Functions def camel_case_split ( input) Parameters input Input (string) which requires cleaning Returns Expand source code def camel_case_split ( input ): \"\"\" Parameters ---------- input Input (string) which requires cleaning Returns ------- \"\"\" processed_input = \"\" matches = finditer ( \".+?(?:(?<=[a-z])(?=[A-Z])|(?<=[A-Z])(?=[A-Z][a-z])|$)\" , input ) for m in matches : processed_input += str ( m . group ( 0 )) + str ( \" \" ) return processed_input def detect_model_path ( ) Returns Local Feature Explorer and Recommender semantic model path (if the model is pre-downloaded) Expand source code def detect_model_path (): \"\"\" Returns ------- Local Feature Explorer and Recommender semantic model path (if the model is pre-downloaded) \"\"\" transformers_path = os . getenv ( \"SENTENCE_TRANSFORMERS_HOME\" ) if transformers_path is None : try : torch_home = _get_torch_home () except ImportError : torch_home = os . path . expanduser ( os . getenv ( \"TORCH_HOME\" , os . path . join ( os . getenv ( \"XDG_CACHE_HOME\" , \"~/.cache\" ), \"torch\" ), ) ) transformers_path = os . path . join ( torch_home , \"sentence_transformers\" ) model_path = os . path . join ( transformers_path , \"sentence-transformers_all-mpnet-base-v2\" ) return model_path def feature_exploration_prep ( ) Returns df_input_fer DataFrame used in Feature Exploration functions Expand source code def feature_exploration_prep (): \"\"\" Returns ------- df_input_fer DataFrame used in Feature Exploration functions \"\"\" df_input_fer = init_input_fer () df_input_fer = df_input_fer . rename ( columns = lambda x : x . strip () . replace ( \" \" , \"_\" )) return df_input_fer def feature_recommendation_prep ( ) Returns list_train_fer List of prepared data for Feature Recommendation functions df_red_fer DataFrame used in Feature Recommendation functions list_embedding_train_fer List of embedding tensor for Feature Recommendation functions Expand source code def feature_recommendation_prep (): \"\"\" Returns ------- list_train_fer List of prepared data for Feature Recommendation functions df_red_fer DataFrame used in Feature Recommendation functions list_embedding_train_fer List of embedding tensor for Feature Recommendation functions \"\"\" df_input_fer = init_input_fer () ( feature_name_column , feature_desc_column , industry_column , usecase_column , ) = get_column_name ( df_input_fer ) df_groupby_fer = ( df_input_fer . groupby ([ feature_name_column , feature_desc_column ]) . agg ( { industry_column : lambda x : \", \" . join ( set ( x . dropna ())), usecase_column : lambda x : \", \" . join ( set ( x . dropna ())), } ) . reset_index () ) list_train_fer , df_rec_fer = recommendation_data_prep ( df_groupby_fer , feature_name_column , feature_name_column ) return list_train_fer , df_rec_fer def get_column_name ( df) Parameters df Input DataFrame Returns feature_name_column Column name of Feature Name in the input DataFrame (string) feature_desc_column Column name of Feature Description in the input DataFrame (string) industry_column Column name of Industry in the input DataFrame (string) usecase_column Column name of Usecase in the input DataFrame (string) Expand source code def get_column_name ( df ): \"\"\" Parameters ---------- df Input DataFrame Returns ------- feature_name_column Column name of Feature Name in the input DataFrame (string) feature_desc_column Column name of Feature Description in the input DataFrame (string) industry_column Column name of Industry in the input DataFrame (string) usecase_column Column name of Usecase in the input DataFrame (string) \"\"\" feature_name_column = str ( df . columns . tolist ()[ 0 ]) feature_desc_column = str ( df . columns . tolist ()[ 1 ]) industry_column = str ( df . columns . tolist ()[ 2 ]) usecase_column = str ( df . columns . tolist ()[ 3 ]) return ( feature_name_column , feature_desc_column , industry_column , usecase_column , ) def init_input_fer ( ) Returns Loading the Feature Explorer and Recommender (FER) Input DataFrame (FER corpus) Expand source code def init_input_fer (): \"\"\" Returns ------- Loading the Feature Explorer and Recommender (FER) Input DataFrame (FER corpus) \"\"\" local_path = os . path . join ( os . path . dirname ( os . path . abspath ( __file__ )), \"data\" , \"flatten_fr_db.csv\" ) if os . path . exists ( local_path ): input_path_fer = local_path else : site_path = site . getsitepackages ()[ 0 ] input_path_fer = os . path . join ( site_path , \"anovos/feature_recommender/data/flatten_fr_db.csv\" ) df_input_fer = pd . read_csv ( input_path_fer ) return df_input_fer def model_download ( ) Expand source code def model_download (): print ( \"Starting the Semantic Model download\" ) SentenceTransformer ( \"all-mpnet-base-v2\" ) print ( \"Model downloading finished\" ) def recommendation_data_prep ( df, name_column, desc_column) Parameters df Input DataFrame name_column Column name of Input DataFrame attribute/ feature name (string) desc_column Column name of Input DataFrame attribute/ feature description (string) Returns list_corpus List of prepared data for Feature Recommender functions return df_prep Processed DataFrame for Feature Recommender functions Expand source code def recommendation_data_prep ( df , name_column , desc_column ): \"\"\" Parameters ---------- df Input DataFrame name_column Column name of Input DataFrame attribute/ feature name (string) desc_column Column name of Input DataFrame attribute/ feature description (string) Returns ------- list_corpus List of prepared data for Feature Recommender functions return df_prep Processed DataFrame for Feature Recommender functions \"\"\" if not isinstance ( df , pd . DataFrame ): raise TypeError ( \"Invalid input for df\" ) if name_column not in df . columns and name_column is not None : raise TypeError ( \"Invalid input for name_column\" ) if desc_column not in df . columns and desc_column is not None : raise TypeError ( \"Invalid input for desc_column\" ) if name_column is None and desc_column is None : raise TypeError ( \"Need at least one input for either name_column or desc_column\" ) df_prep = copy . deepcopy ( df ) if name_column is None : df_prep [ desc_column ] = df_prep [ desc_column ] . astype ( str ) df_prep_com = df_prep [ desc_column ] elif desc_column is None : df_prep [ name_column ] = df_prep [ name_column ] . astype ( str ) df_prep_com = df_prep [ name_column ] else : df_prep [ name_column ] = df_prep [ name_column ] . str . replace ( \"_\" , \" \" ) df_prep [ name_column ] = df_prep [ name_column ] . astype ( str ) df_prep [ desc_column ] = df_prep [ desc_column ] . astype ( str ) df_prep_com = df_prep [[ name_column , desc_column ]] . agg ( \" \" . join , axis = 1 ) df_prep_com = df_prep_com . replace ({ \"[^A-Za-z0-9 ]+\" : \" \" }, regex = True ) for i in range ( len ( df_prep_com )): df_prep_com [ i ] = df_prep_com [ i ] . strip () df_prep_com [ i ] = camel_case_split ( df_prep_com [ i ]) list_corpus = df_prep_com . to_list () return list_corpus , df_prep Classes class EmbeddingsTrainFer ( list_train_fer) Expand source code class EmbeddingsTrainFer : def __init__ ( self , list_train_fer ): self . list_train_fer = list_train_fer self . _embeddings = None @property def get ( self ): if self . _embeddings is None : self . _embeddings = model_fer . model . encode ( self . list_train_fer , convert_to_tensor = True ) return self . _embeddings Instance variables var get Expand source code @property def get ( self ): if self . _embeddings is None : self . _embeddings = model_fer . model . encode ( self . list_train_fer , convert_to_tensor = True ) return self . _embeddings","title":"<code>featrec_init</code>"},{"location":"api/feature_recommender/featrec_init.html#featrec_init","text":"Expand source code import os import copy from re import finditer import pandas as pd from sentence_transformers import SentenceTransformer from torch.hub import _get_torch_home import site def detect_model_path (): \"\"\" Returns ------- Local Feature Explorer and Recommender semantic model path (if the model is pre-downloaded) \"\"\" transformers_path = os . getenv ( \"SENTENCE_TRANSFORMERS_HOME\" ) if transformers_path is None : try : torch_home = _get_torch_home () except ImportError : torch_home = os . path . expanduser ( os . getenv ( \"TORCH_HOME\" , os . path . join ( os . getenv ( \"XDG_CACHE_HOME\" , \"~/.cache\" ), \"torch\" ), ) ) transformers_path = os . path . join ( torch_home , \"sentence_transformers\" ) model_path = os . path . join ( transformers_path , \"sentence-transformers_all-mpnet-base-v2\" ) return model_path def model_download (): print ( \"Starting the Semantic Model download\" ) SentenceTransformer ( \"all-mpnet-base-v2\" ) print ( \"Model downloading finished\" ) class _TransformerModel : def __init__ ( self ): self . _model = None @property def model ( self ) -> SentenceTransformer : if self . _model is None : model_path = detect_model_path () if os . path . exists ( model_path ): self . _model = SentenceTransformer ( model_path ) else : raise FileNotFoundError ( \"Model has not been downloaded. Please use model_download() function to download the model first\" ) return self . _model model_fer = _TransformerModel () def init_input_fer (): \"\"\" Returns ------- Loading the Feature Explorer and Recommender (FER) Input DataFrame (FER corpus) \"\"\" local_path = os . path . join ( os . path . dirname ( os . path . abspath ( __file__ )), \"data\" , \"flatten_fr_db.csv\" ) if os . path . exists ( local_path ): input_path_fer = local_path else : site_path = site . getsitepackages ()[ 0 ] input_path_fer = os . path . join ( site_path , \"anovos/feature_recommender/data/flatten_fr_db.csv\" ) df_input_fer = pd . read_csv ( input_path_fer ) return df_input_fer def get_column_name ( df ): \"\"\" Parameters ---------- df Input DataFrame Returns ------- feature_name_column Column name of Feature Name in the input DataFrame (string) feature_desc_column Column name of Feature Description in the input DataFrame (string) industry_column Column name of Industry in the input DataFrame (string) usecase_column Column name of Usecase in the input DataFrame (string) \"\"\" feature_name_column = str ( df . columns . tolist ()[ 0 ]) feature_desc_column = str ( df . columns . tolist ()[ 1 ]) industry_column = str ( df . columns . tolist ()[ 2 ]) usecase_column = str ( df . columns . tolist ()[ 3 ]) return ( feature_name_column , feature_desc_column , industry_column , usecase_column , ) def camel_case_split ( input ): \"\"\" Parameters ---------- input Input (string) which requires cleaning Returns ------- \"\"\" processed_input = \"\" matches = finditer ( \".+?(?:(?<=[a-z])(?=[A-Z])|(?<=[A-Z])(?=[A-Z][a-z])|$)\" , input ) for m in matches : processed_input += str ( m . group ( 0 )) + str ( \" \" ) return processed_input def recommendation_data_prep ( df , name_column , desc_column ): \"\"\" Parameters ---------- df Input DataFrame name_column Column name of Input DataFrame attribute/ feature name (string) desc_column Column name of Input DataFrame attribute/ feature description (string) Returns ------- list_corpus List of prepared data for Feature Recommender functions return df_prep Processed DataFrame for Feature Recommender functions \"\"\" if not isinstance ( df , pd . DataFrame ): raise TypeError ( \"Invalid input for df\" ) if name_column not in df . columns and name_column is not None : raise TypeError ( \"Invalid input for name_column\" ) if desc_column not in df . columns and desc_column is not None : raise TypeError ( \"Invalid input for desc_column\" ) if name_column is None and desc_column is None : raise TypeError ( \"Need at least one input for either name_column or desc_column\" ) df_prep = copy . deepcopy ( df ) if name_column is None : df_prep [ desc_column ] = df_prep [ desc_column ] . astype ( str ) df_prep_com = df_prep [ desc_column ] elif desc_column is None : df_prep [ name_column ] = df_prep [ name_column ] . astype ( str ) df_prep_com = df_prep [ name_column ] else : df_prep [ name_column ] = df_prep [ name_column ] . str . replace ( \"_\" , \" \" ) df_prep [ name_column ] = df_prep [ name_column ] . astype ( str ) df_prep [ desc_column ] = df_prep [ desc_column ] . astype ( str ) df_prep_com = df_prep [[ name_column , desc_column ]] . agg ( \" \" . join , axis = 1 ) df_prep_com = df_prep_com . replace ({ \"[^A-Za-z0-9 ]+\" : \" \" }, regex = True ) for i in range ( len ( df_prep_com )): df_prep_com [ i ] = df_prep_com [ i ] . strip () df_prep_com [ i ] = camel_case_split ( df_prep_com [ i ]) list_corpus = df_prep_com . to_list () return list_corpus , df_prep def feature_exploration_prep (): \"\"\" Returns ------- df_input_fer DataFrame used in Feature Exploration functions \"\"\" df_input_fer = init_input_fer () df_input_fer = df_input_fer . rename ( columns = lambda x : x . strip () . replace ( \" \" , \"_\" )) return df_input_fer def feature_recommendation_prep (): \"\"\" Returns ------- list_train_fer List of prepared data for Feature Recommendation functions df_red_fer DataFrame used in Feature Recommendation functions list_embedding_train_fer List of embedding tensor for Feature Recommendation functions \"\"\" df_input_fer = init_input_fer () ( feature_name_column , feature_desc_column , industry_column , usecase_column , ) = get_column_name ( df_input_fer ) df_groupby_fer = ( df_input_fer . groupby ([ feature_name_column , feature_desc_column ]) . agg ( { industry_column : lambda x : \", \" . join ( set ( x . dropna ())), usecase_column : lambda x : \", \" . join ( set ( x . dropna ())), } ) . reset_index () ) list_train_fer , df_rec_fer = recommendation_data_prep ( df_groupby_fer , feature_name_column , feature_name_column ) return list_train_fer , df_rec_fer class EmbeddingsTrainFer : def __init__ ( self , list_train_fer ): self . list_train_fer = list_train_fer self . _embeddings = None @property def get ( self ): if self . _embeddings is None : self . _embeddings = model_fer . model . encode ( self . list_train_fer , convert_to_tensor = True ) return self . _embeddings","title":"featrec_init"},{"location":"api/feature_recommender/featrec_init.html#functions","text":"def camel_case_split ( input)","title":"Functions"},{"location":"api/feature_recommender/featrec_init.html#_1","text":"Classes class EmbeddingsTrainFer ( list_train_fer) Expand source code class EmbeddingsTrainFer : def __init__ ( self , list_train_fer ): self . list_train_fer = list_train_fer self . _embeddings = None @property def get ( self ): if self . _embeddings is None : self . _embeddings = model_fer . model . encode ( self . list_train_fer , convert_to_tensor = True ) return self . _embeddings","title":""},{"location":"api/feature_recommender/feature_exploration.html","text":"feature_exploration Feature explorer helps list down the potential features from our corpus based on user defined industry or/and use case. Expand source code \"\"\"Feature explorer helps list down the potential features from our corpus based on user defined industry or/and use case. \"\"\" from anovos.feature_recommender.featrec_init import ( feature_exploration_prep , get_column_name , model_fer , ) from sentence_transformers import util import pandas as pd import numpy as np df_input_fer = feature_exploration_prep () ( feature_name_column , feature_desc_column , industry_column , usecase_column , ) = get_column_name ( df_input_fer ) def list_all_industry (): \"\"\" Lists down all the Industries that are supported in Feature Recommender module. Returns ------- DataFrame of all the supported industries as part of feature exploration/recommendation \"\"\" odf_uni = df_input_fer . iloc [:, 2 ] . unique () odf = pd . DataFrame ( odf_uni , columns = [ \"Industry\" ]) return odf def list_all_usecase (): \"\"\" Lists down all the Use cases that are supported in Feature Recommender module. Returns ------- DataFrame of all the supported usecases as part of feature exploration/recommendation \"\"\" odf_uni = df_input_fer . iloc [:, 3 ] . unique () odf = pd . DataFrame ( odf_uni , columns = [ \"Usecase\" ]) return odf def list_all_pair (): \"\"\" Lists down all the Industry/Use case pairs that are supported in Feature Recommender module. Returns ------- DataFrame of all the supported Industry/Usecase pairs as part of feature exploration/recommendation \"\"\" odf = df_input_fer . iloc [:, [ 2 , 3 ]] . drop_duplicates ( keep = \"last\" , ignore_index = True ) return odf def process_usecase ( usecase : str , semantic : bool ): \"\"\" Parameters ---------- usecase : str Input usecase semantic : bool Whether the input needs to go through semantic similarity or not. Default is True. Returns ------- \"\"\" if type ( semantic ) != bool : raise TypeError ( \"Invalid input for semantic\" ) if type ( usecase ) != str : raise TypeError ( \"Invalid input for usecase\" ) usecase = usecase . lower () . strip () usecase = usecase . replace ( \"[^A-Za-z0-9 ]+\" , \" \" ) all_usecase = list_all_usecase ()[ \"Usecase\" ] . to_list () if semantic and usecase not in all_usecase : all_usecase_embeddings = model_fer . model . encode ( all_usecase , convert_to_tensor = True ) usecase_embeddings = model_fer . model . encode ( usecase , convert_to_tensor = True ) cos_scores = util . pytorch_cos_sim ( usecase_embeddings , all_usecase_embeddings )[ 0 ] first_match_index = int ( np . argpartition ( - cos_scores , 0 )[ 0 ]) processed_usecase = all_usecase [ first_match_index ] print ( \"Given input Usecase is not available. Showing the most semantically relevant Usecase result: \" , processed_usecase , ) else : processed_usecase = usecase return processed_usecase def process_industry ( industry : str , semantic : bool ): \"\"\" Parameters ---------- industry : str Input industry semantic : bool Whether the input needs to go through semantic similarity or not. Default is True. Returns ------- \"\"\" if type ( semantic ) != bool : raise TypeError ( \"Invalid input for semantic\" ) if type ( industry ) != str : raise TypeError ( \"Invalid input for industry\" ) industry = industry . lower () . strip () industry = industry . replace ( \"[^A-Za-z0-9 ]+\" , \" \" ) all_industry = list_all_industry ()[ \"Industry\" ] . to_list () if semantic and industry not in all_industry : all_industry_embeddings = model_fer . model . encode ( all_industry , convert_to_tensor = True ) industry_embeddings = model_fer . model . encode ( industry , convert_to_tensor = True ) cos_scores = util . pytorch_cos_sim ( industry_embeddings , all_industry_embeddings )[ 0 ] first_match_index = int ( np . argpartition ( - cos_scores , 0 )[ 0 ]) processed_industry = all_industry [ first_match_index ] print ( \"Given input Industry is not available. Showing the most semantically relevant Industry result: \" , processed_industry , ) else : processed_industry = industry return processed_industry def list_usecase_by_industry ( industry , semantic = True ): \"\"\" Lists down all the Use cases that are supported in Feature Recommender Package based on the Input Industry. Parameters ---------- industry : str Input industry semantic : bool Input semantic - Whether the input needs to go through semantic similarity or not. Default is True. Returns ------- \"\"\" industry = process_industry ( industry , semantic ) odf = pd . DataFrame ( df_input_fer . loc [ df_input_fer . iloc [:, 2 ] == industry ] . iloc [:, 3 ]) odf = odf . drop_duplicates ( keep = \"last\" , ignore_index = True ) return odf def list_industry_by_usecase ( usecase , semantic = True ): \"\"\" Lists down all the Use cases that are supported in Feature Recommender Package based on the Input Industry. Parameters ---------- usecase : str Input usecase semantic : bool Input semantic - Whether the input needs to go through semantic similarity or not. Default is True. Returns ------- \"\"\" usecase = process_usecase ( usecase , semantic ) odf = pd . DataFrame ( df_input_fer . loc [ df_input_fer . iloc [:, 3 ] == usecase ] . iloc [:, 2 ]) odf = odf . drop_duplicates ( keep = \"last\" , ignore_index = True ) return odf def list_feature_by_industry ( industry , num_of_feat = 100 , semantic = True ): \"\"\" Lists down all the Features that are available in Feature Recommender Package based on the Input Industry. Parameters ---------- industry : str Input industry num_of_feat : int Number of features to be displayed in the output. Value can be either integer, or 'all' - display all features matched with the input. Default is 100. semantic : bool Input semantic - Whether the input needs to go through semantic similarity or not. Default is True. Returns ------- DataFrame Columns are: - Feature Name: Name of the suggested Feature - Feature Description: Description of the suggested Feature - Industry: Industry name of the suggested Feature - Usecase: Usecase name of the suggested Feature - Source: Source of the suggested Feature The list of features is sorted by the Usecases' Feature Popularity to the Input Industry. \"\"\" if type ( num_of_feat ) != int or num_of_feat < 0 : if num_of_feat != \"all\" : raise TypeError ( \"Invalid input for num_of_feat\" ) industry = process_industry ( industry , semantic ) odf = df_input_fer . loc [ df_input_fer . iloc [:, 2 ] == industry ] . drop_duplicates ( keep = \"last\" , ignore_index = True ) if len ( odf ) > 0 : odf [ \"count\" ] = odf . groupby ( usecase_column )[ usecase_column ] . transform ( \"count\" ) odf . sort_values ( \"count\" , inplace = True , ascending = False ) odf = odf . drop ( \"count\" , axis = 1 ) if num_of_feat != \"all\" : odf = odf . head ( num_of_feat ) . reset_index ( drop = True ) else : odf = odf . reset_index ( drop = True ) return odf def list_feature_by_usecase ( usecase , num_of_feat = 100 , semantic = True ): \"\"\" Lists down all the Features that are available in Feature Recommender Package based on the Input Usecase. Parameters ---------- usecase : str Input usecase num_of_feat : int Number of features to be displayed in the output. Value can be either integer, or 'all' - display all features matched with the input. Default is 100. semantic : bool Input semantic - Whether the input needs to go through semantic similarity or not. Default is True. Returns ------- DataFrame Columns are: - Feature Name: Name of the suggested Feature - Feature Description: Description of the suggested Feature - Industry: Industry name of the suggested Feature - Usecase: Usecase name of the suggested Feature - Source: Source of the suggested Feature The list of features is sorted by the Industries' Feature Popularity to the Input Usecase. \"\"\" if type ( num_of_feat ) != int or num_of_feat < 0 : if num_of_feat != \"all\" : raise TypeError ( \"Invalid input for num_of_feat\" ) usecase = process_usecase ( usecase , semantic ) odf = df_input_fer . loc [ df_input_fer . iloc [:, 3 ] == usecase ] . drop_duplicates ( keep = \"last\" , ignore_index = True ) if len ( odf ) > 0 : odf [ \"count\" ] = odf . groupby ( industry_column )[ industry_column ] . transform ( \"count\" ) odf . sort_values ( \"count\" , inplace = True , ascending = False ) odf = odf . drop ( \"count\" , axis = 1 ) if num_of_feat != \"all\" : odf = odf . head ( num_of_feat ) . reset_index ( drop = True ) else : odf = odf . reset_index ( drop = True ) return odf def list_feature_by_pair ( industry , usecase , num_of_feat = 100 , semantic = True ): \"\"\" Lists down all the Features that are available in Feature Recommender Package based on the Input Industry/Usecase pair Parameters ---------- industry Input industry (string) usecase Input usecase (string) num_of_feat Number of features to be displayed in the output. Value can be either integer, or 'all' - display all features matched with the input. Default is 100. semantic Input semantic (boolean) - Whether the input needs to go through semantic similarity or not. Default is True. Returns ------- DataFrame Columns are: - Feature Name: Name of the suggested Feature - Feature Description: Description of the suggested Feature - Industry: Industry name of the suggested Feature - Usecase: Usecase name of the suggested Feature - Source: Source of the suggested Feature \"\"\" if type ( num_of_feat ) != int or num_of_feat < 0 : if num_of_feat != \"all\" : raise TypeError ( \"Invalid input for num_of_feat\" ) industry = process_industry ( industry , semantic ) usecase = process_usecase ( usecase , semantic ) if num_of_feat != \"all\" : odf = ( df_input_fer . loc [ ( df_input_fer . iloc [:, 2 ] == industry ) & ( df_input_fer . iloc [:, 3 ] == usecase ) ] . drop_duplicates ( keep = \"last\" , ignore_index = True ) . head ( num_of_feat ) ) else : odf = df_input_fer . loc [ ( df_input_fer . iloc [:, 2 ] == industry ) & ( df_input_fer . iloc [:, 3 ] == usecase ) ] . drop_duplicates ( keep = \"last\" , ignore_index = True ) return odf Functions def list_all_industry ( ) Lists down all the Industries that are supported in Feature Recommender module. Returns DataFrame of all the supported industries as part of feature exploration/recommendation Expand source code def list_all_industry (): \"\"\" Lists down all the Industries that are supported in Feature Recommender module. Returns ------- DataFrame of all the supported industries as part of feature exploration/recommendation \"\"\" odf_uni = df_input_fer . iloc [:, 2 ] . unique () odf = pd . DataFrame ( odf_uni , columns = [ \"Industry\" ]) return odf def list_all_pair ( ) Lists down all the Industry/Use case pairs that are supported in Feature Recommender module. Returns DataFrame of all the supported Industry/Usecase pairs as part of feature exploration/recommendation Expand source code def list_all_pair (): \"\"\" Lists down all the Industry/Use case pairs that are supported in Feature Recommender module. Returns ------- DataFrame of all the supported Industry/Usecase pairs as part of feature exploration/recommendation \"\"\" odf = df_input_fer . iloc [:, [ 2 , 3 ]] . drop_duplicates ( keep = \"last\" , ignore_index = True ) return odf def list_all_usecase ( ) Lists down all the Use cases that are supported in Feature Recommender module. Returns DataFrame of all the supported usecases as part of feature exploration/recommendation Expand source code def list_all_usecase (): \"\"\" Lists down all the Use cases that are supported in Feature Recommender module. Returns ------- DataFrame of all the supported usecases as part of feature exploration/recommendation \"\"\" odf_uni = df_input_fer . iloc [:, 3 ] . unique () odf = pd . DataFrame ( odf_uni , columns = [ \"Usecase\" ]) return odf def list_feature_by_industry ( industry, num_of_feat=100, semantic=True) Lists down all the Features that are available in Feature Recommender Package based on the Input Industry. Parameters industry :\u2002 str Input industry num_of_feat :\u2002 int Number of features to be displayed in the output. Value can be either integer, or 'all' - display all features matched with the input. Default is 100. semantic :\u2002 bool Input semantic - Whether the input needs to go through semantic similarity or not. Default is True. Returns DataFrame Columns are: - Feature Name: Name of the suggested Feature - Feature Description: Description of the suggested Feature - Industry: Industry name of the suggested Feature - Usecase: Usecase name of the suggested Feature - Source: Source of the suggested Feature The list of features is sorted by the Usecases' Feature Popularity to the Input Industry. Expand source code def list_feature_by_industry ( industry , num_of_feat = 100 , semantic = True ): \"\"\" Lists down all the Features that are available in Feature Recommender Package based on the Input Industry. Parameters ---------- industry : str Input industry num_of_feat : int Number of features to be displayed in the output. Value can be either integer, or 'all' - display all features matched with the input. Default is 100. semantic : bool Input semantic - Whether the input needs to go through semantic similarity or not. Default is True. Returns ------- DataFrame Columns are: - Feature Name: Name of the suggested Feature - Feature Description: Description of the suggested Feature - Industry: Industry name of the suggested Feature - Usecase: Usecase name of the suggested Feature - Source: Source of the suggested Feature The list of features is sorted by the Usecases' Feature Popularity to the Input Industry. \"\"\" if type ( num_of_feat ) != int or num_of_feat < 0 : if num_of_feat != \"all\" : raise TypeError ( \"Invalid input for num_of_feat\" ) industry = process_industry ( industry , semantic ) odf = df_input_fer . loc [ df_input_fer . iloc [:, 2 ] == industry ] . drop_duplicates ( keep = \"last\" , ignore_index = True ) if len ( odf ) > 0 : odf [ \"count\" ] = odf . groupby ( usecase_column )[ usecase_column ] . transform ( \"count\" ) odf . sort_values ( \"count\" , inplace = True , ascending = False ) odf = odf . drop ( \"count\" , axis = 1 ) if num_of_feat != \"all\" : odf = odf . head ( num_of_feat ) . reset_index ( drop = True ) else : odf = odf . reset_index ( drop = True ) return odf def list_feature_by_pair ( industry, usecase, num_of_feat=100, semantic=True) Lists down all the Features that are available in Feature Recommender Package based on the Input Industry/Usecase pair Parameters industry Input industry (string) usecase Input usecase (string) num_of_feat Number of features to be displayed in the output. Value can be either integer, or 'all' - display all features matched with the input. Default is 100. semantic Input semantic (boolean) - Whether the input needs to go through semantic similarity or not. Default is True. Returns DataFrame Columns are: Feature Name: Name of the suggested Feature Feature Description: Description of the suggested Feature Industry: Industry name of the suggested Feature Usecase: Usecase name of the suggested Feature Source: Source of the suggested Feature Expand source code def list_feature_by_pair ( industry , usecase , num_of_feat = 100 , semantic = True ): \"\"\" Lists down all the Features that are available in Feature Recommender Package based on the Input Industry/Usecase pair Parameters ---------- industry Input industry (string) usecase Input usecase (string) num_of_feat Number of features to be displayed in the output. Value can be either integer, or 'all' - display all features matched with the input. Default is 100. semantic Input semantic (boolean) - Whether the input needs to go through semantic similarity or not. Default is True. Returns ------- DataFrame Columns are: - Feature Name: Name of the suggested Feature - Feature Description: Description of the suggested Feature - Industry: Industry name of the suggested Feature - Usecase: Usecase name of the suggested Feature - Source: Source of the suggested Feature \"\"\" if type ( num_of_feat ) != int or num_of_feat < 0 : if num_of_feat != \"all\" : raise TypeError ( \"Invalid input for num_of_feat\" ) industry = process_industry ( industry , semantic ) usecase = process_usecase ( usecase , semantic ) if num_of_feat != \"all\" : odf = ( df_input_fer . loc [ ( df_input_fer . iloc [:, 2 ] == industry ) & ( df_input_fer . iloc [:, 3 ] == usecase ) ] . drop_duplicates ( keep = \"last\" , ignore_index = True ) . head ( num_of_feat ) ) else : odf = df_input_fer . loc [ ( df_input_fer . iloc [:, 2 ] == industry ) & ( df_input_fer . iloc [:, 3 ] == usecase ) ] . drop_duplicates ( keep = \"last\" , ignore_index = True ) return odf def list_feature_by_usecase ( usecase, num_of_feat=100, semantic=True) Lists down all the Features that are available in Feature Recommender Package based on the Input Usecase. Parameters usecase :\u2002 str Input usecase num_of_feat :\u2002 int Number of features to be displayed in the output. Value can be either integer, or 'all' - display all features matched with the input. Default is 100. semantic :\u2002 bool Input semantic - Whether the input needs to go through semantic similarity or not. Default is True. Returns DataFrame Columns are: Feature Name: Name of the suggested Feature Feature Description: Description of the suggested Feature Industry: Industry name of the suggested Feature Usecase: Usecase name of the suggested Feature Source: Source of the suggested Feature The list of features is sorted by the Industries' Feature Popularity to the Input Usecase. Expand source code def list_feature_by_usecase ( usecase , num_of_feat = 100 , semantic = True ): \"\"\" Lists down all the Features that are available in Feature Recommender Package based on the Input Usecase. Parameters ---------- usecase : str Input usecase num_of_feat : int Number of features to be displayed in the output. Value can be either integer, or 'all' - display all features matched with the input. Default is 100. semantic : bool Input semantic - Whether the input needs to go through semantic similarity or not. Default is True. Returns ------- DataFrame Columns are: - Feature Name: Name of the suggested Feature - Feature Description: Description of the suggested Feature - Industry: Industry name of the suggested Feature - Usecase: Usecase name of the suggested Feature - Source: Source of the suggested Feature The list of features is sorted by the Industries' Feature Popularity to the Input Usecase. \"\"\" if type ( num_of_feat ) != int or num_of_feat < 0 : if num_of_feat != \"all\" : raise TypeError ( \"Invalid input for num_of_feat\" ) usecase = process_usecase ( usecase , semantic ) odf = df_input_fer . loc [ df_input_fer . iloc [:, 3 ] == usecase ] . drop_duplicates ( keep = \"last\" , ignore_index = True ) if len ( odf ) > 0 : odf [ \"count\" ] = odf . groupby ( industry_column )[ industry_column ] . transform ( \"count\" ) odf . sort_values ( \"count\" , inplace = True , ascending = False ) odf = odf . drop ( \"count\" , axis = 1 ) if num_of_feat != \"all\" : odf = odf . head ( num_of_feat ) . reset_index ( drop = True ) else : odf = odf . reset_index ( drop = True ) return odf def list_industry_by_usecase ( usecase, semantic=True) Lists down all the Use cases that are supported in Feature Recommender Package based on the Input Industry. Parameters usecase :\u2002 str Input usecase semantic :\u2002 bool Input semantic - Whether the input needs to go through semantic similarity or not. Default is True. Returns Expand source code def list_industry_by_usecase ( usecase , semantic = True ): \"\"\" Lists down all the Use cases that are supported in Feature Recommender Package based on the Input Industry. Parameters ---------- usecase : str Input usecase semantic : bool Input semantic - Whether the input needs to go through semantic similarity or not. Default is True. Returns ------- \"\"\" usecase = process_usecase ( usecase , semantic ) odf = pd . DataFrame ( df_input_fer . loc [ df_input_fer . iloc [:, 3 ] == usecase ] . iloc [:, 2 ]) odf = odf . drop_duplicates ( keep = \"last\" , ignore_index = True ) return odf def list_usecase_by_industry ( industry, semantic=True) Lists down all the Use cases that are supported in Feature Recommender Package based on the Input Industry. Parameters industry :\u2002 str Input industry semantic :\u2002 bool Input semantic - Whether the input needs to go through semantic similarity or not. Default is True. Returns Expand source code def list_usecase_by_industry ( industry , semantic = True ): \"\"\" Lists down all the Use cases that are supported in Feature Recommender Package based on the Input Industry. Parameters ---------- industry : str Input industry semantic : bool Input semantic - Whether the input needs to go through semantic similarity or not. Default is True. Returns ------- \"\"\" industry = process_industry ( industry , semantic ) odf = pd . DataFrame ( df_input_fer . loc [ df_input_fer . iloc [:, 2 ] == industry ] . iloc [:, 3 ]) odf = odf . drop_duplicates ( keep = \"last\" , ignore_index = True ) return odf def process_industry ( industry: str, semantic: bool) Parameters industry :\u2002 str Input industry semantic :\u2002 bool Whether the input needs to go through semantic similarity or not. Default is True. Returns Expand source code def process_industry ( industry : str , semantic : bool ): \"\"\" Parameters ---------- industry : str Input industry semantic : bool Whether the input needs to go through semantic similarity or not. Default is True. Returns ------- \"\"\" if type ( semantic ) != bool : raise TypeError ( \"Invalid input for semantic\" ) if type ( industry ) != str : raise TypeError ( \"Invalid input for industry\" ) industry = industry . lower () . strip () industry = industry . replace ( \"[^A-Za-z0-9 ]+\" , \" \" ) all_industry = list_all_industry ()[ \"Industry\" ] . to_list () if semantic and industry not in all_industry : all_industry_embeddings = model_fer . model . encode ( all_industry , convert_to_tensor = True ) industry_embeddings = model_fer . model . encode ( industry , convert_to_tensor = True ) cos_scores = util . pytorch_cos_sim ( industry_embeddings , all_industry_embeddings )[ 0 ] first_match_index = int ( np . argpartition ( - cos_scores , 0 )[ 0 ]) processed_industry = all_industry [ first_match_index ] print ( \"Given input Industry is not available. Showing the most semantically relevant Industry result: \" , processed_industry , ) else : processed_industry = industry return processed_industry def process_usecase ( usecase: str, semantic: bool) Parameters usecase :\u2002 str Input usecase semantic :\u2002 bool Whether the input needs to go through semantic similarity or not. Default is True. Returns Expand source code def process_usecase ( usecase : str , semantic : bool ): \"\"\" Parameters ---------- usecase : str Input usecase semantic : bool Whether the input needs to go through semantic similarity or not. Default is True. Returns ------- \"\"\" if type ( semantic ) != bool : raise TypeError ( \"Invalid input for semantic\" ) if type ( usecase ) != str : raise TypeError ( \"Invalid input for usecase\" ) usecase = usecase . lower () . strip () usecase = usecase . replace ( \"[^A-Za-z0-9 ]+\" , \" \" ) all_usecase = list_all_usecase ()[ \"Usecase\" ] . to_list () if semantic and usecase not in all_usecase : all_usecase_embeddings = model_fer . model . encode ( all_usecase , convert_to_tensor = True ) usecase_embeddings = model_fer . model . encode ( usecase , convert_to_tensor = True ) cos_scores = util . pytorch_cos_sim ( usecase_embeddings , all_usecase_embeddings )[ 0 ] first_match_index = int ( np . argpartition ( - cos_scores , 0 )[ 0 ]) processed_usecase = all_usecase [ first_match_index ] print ( \"Given input Usecase is not available. Showing the most semantically relevant Usecase result: \" , processed_usecase , ) else : processed_usecase = usecase return processed_usecase","title":"<code>feature_exploration</code>"},{"location":"api/feature_recommender/feature_exploration.html#feature_exploration","text":"Feature explorer helps list down the potential features from our corpus based on user defined industry or/and use case. Expand source code \"\"\"Feature explorer helps list down the potential features from our corpus based on user defined industry or/and use case. \"\"\" from anovos.feature_recommender.featrec_init import ( feature_exploration_prep , get_column_name , model_fer , ) from sentence_transformers import util import pandas as pd import numpy as np df_input_fer = feature_exploration_prep () ( feature_name_column , feature_desc_column , industry_column , usecase_column , ) = get_column_name ( df_input_fer ) def list_all_industry (): \"\"\" Lists down all the Industries that are supported in Feature Recommender module. Returns ------- DataFrame of all the supported industries as part of feature exploration/recommendation \"\"\" odf_uni = df_input_fer . iloc [:, 2 ] . unique () odf = pd . DataFrame ( odf_uni , columns = [ \"Industry\" ]) return odf def list_all_usecase (): \"\"\" Lists down all the Use cases that are supported in Feature Recommender module. Returns ------- DataFrame of all the supported usecases as part of feature exploration/recommendation \"\"\" odf_uni = df_input_fer . iloc [:, 3 ] . unique () odf = pd . DataFrame ( odf_uni , columns = [ \"Usecase\" ]) return odf def list_all_pair (): \"\"\" Lists down all the Industry/Use case pairs that are supported in Feature Recommender module. Returns ------- DataFrame of all the supported Industry/Usecase pairs as part of feature exploration/recommendation \"\"\" odf = df_input_fer . iloc [:, [ 2 , 3 ]] . drop_duplicates ( keep = \"last\" , ignore_index = True ) return odf def process_usecase ( usecase : str , semantic : bool ): \"\"\" Parameters ---------- usecase : str Input usecase semantic : bool Whether the input needs to go through semantic similarity or not. Default is True. Returns ------- \"\"\" if type ( semantic ) != bool : raise TypeError ( \"Invalid input for semantic\" ) if type ( usecase ) != str : raise TypeError ( \"Invalid input for usecase\" ) usecase = usecase . lower () . strip () usecase = usecase . replace ( \"[^A-Za-z0-9 ]+\" , \" \" ) all_usecase = list_all_usecase ()[ \"Usecase\" ] . to_list () if semantic and usecase not in all_usecase : all_usecase_embeddings = model_fer . model . encode ( all_usecase , convert_to_tensor = True ) usecase_embeddings = model_fer . model . encode ( usecase , convert_to_tensor = True ) cos_scores = util . pytorch_cos_sim ( usecase_embeddings , all_usecase_embeddings )[ 0 ] first_match_index = int ( np . argpartition ( - cos_scores , 0 )[ 0 ]) processed_usecase = all_usecase [ first_match_index ] print ( \"Given input Usecase is not available. Showing the most semantically relevant Usecase result: \" , processed_usecase , ) else : processed_usecase = usecase return processed_usecase def process_industry ( industry : str , semantic : bool ): \"\"\" Parameters ---------- industry : str Input industry semantic : bool Whether the input needs to go through semantic similarity or not. Default is True. Returns ------- \"\"\" if type ( semantic ) != bool : raise TypeError ( \"Invalid input for semantic\" ) if type ( industry ) != str : raise TypeError ( \"Invalid input for industry\" ) industry = industry . lower () . strip () industry = industry . replace ( \"[^A-Za-z0-9 ]+\" , \" \" ) all_industry = list_all_industry ()[ \"Industry\" ] . to_list () if semantic and industry not in all_industry : all_industry_embeddings = model_fer . model . encode ( all_industry , convert_to_tensor = True ) industry_embeddings = model_fer . model . encode ( industry , convert_to_tensor = True ) cos_scores = util . pytorch_cos_sim ( industry_embeddings , all_industry_embeddings )[ 0 ] first_match_index = int ( np . argpartition ( - cos_scores , 0 )[ 0 ]) processed_industry = all_industry [ first_match_index ] print ( \"Given input Industry is not available. Showing the most semantically relevant Industry result: \" , processed_industry , ) else : processed_industry = industry return processed_industry def list_usecase_by_industry ( industry , semantic = True ): \"\"\" Lists down all the Use cases that are supported in Feature Recommender Package based on the Input Industry. Parameters ---------- industry : str Input industry semantic : bool Input semantic - Whether the input needs to go through semantic similarity or not. Default is True. Returns ------- \"\"\" industry = process_industry ( industry , semantic ) odf = pd . DataFrame ( df_input_fer . loc [ df_input_fer . iloc [:, 2 ] == industry ] . iloc [:, 3 ]) odf = odf . drop_duplicates ( keep = \"last\" , ignore_index = True ) return odf def list_industry_by_usecase ( usecase , semantic = True ): \"\"\" Lists down all the Use cases that are supported in Feature Recommender Package based on the Input Industry. Parameters ---------- usecase : str Input usecase semantic : bool Input semantic - Whether the input needs to go through semantic similarity or not. Default is True. Returns ------- \"\"\" usecase = process_usecase ( usecase , semantic ) odf = pd . DataFrame ( df_input_fer . loc [ df_input_fer . iloc [:, 3 ] == usecase ] . iloc [:, 2 ]) odf = odf . drop_duplicates ( keep = \"last\" , ignore_index = True ) return odf def list_feature_by_industry ( industry , num_of_feat = 100 , semantic = True ): \"\"\" Lists down all the Features that are available in Feature Recommender Package based on the Input Industry. Parameters ---------- industry : str Input industry num_of_feat : int Number of features to be displayed in the output. Value can be either integer, or 'all' - display all features matched with the input. Default is 100. semantic : bool Input semantic - Whether the input needs to go through semantic similarity or not. Default is True. Returns ------- DataFrame Columns are: - Feature Name: Name of the suggested Feature - Feature Description: Description of the suggested Feature - Industry: Industry name of the suggested Feature - Usecase: Usecase name of the suggested Feature - Source: Source of the suggested Feature The list of features is sorted by the Usecases' Feature Popularity to the Input Industry. \"\"\" if type ( num_of_feat ) != int or num_of_feat < 0 : if num_of_feat != \"all\" : raise TypeError ( \"Invalid input for num_of_feat\" ) industry = process_industry ( industry , semantic ) odf = df_input_fer . loc [ df_input_fer . iloc [:, 2 ] == industry ] . drop_duplicates ( keep = \"last\" , ignore_index = True ) if len ( odf ) > 0 : odf [ \"count\" ] = odf . groupby ( usecase_column )[ usecase_column ] . transform ( \"count\" ) odf . sort_values ( \"count\" , inplace = True , ascending = False ) odf = odf . drop ( \"count\" , axis = 1 ) if num_of_feat != \"all\" : odf = odf . head ( num_of_feat ) . reset_index ( drop = True ) else : odf = odf . reset_index ( drop = True ) return odf def list_feature_by_usecase ( usecase , num_of_feat = 100 , semantic = True ): \"\"\" Lists down all the Features that are available in Feature Recommender Package based on the Input Usecase. Parameters ---------- usecase : str Input usecase num_of_feat : int Number of features to be displayed in the output. Value can be either integer, or 'all' - display all features matched with the input. Default is 100. semantic : bool Input semantic - Whether the input needs to go through semantic similarity or not. Default is True. Returns ------- DataFrame Columns are: - Feature Name: Name of the suggested Feature - Feature Description: Description of the suggested Feature - Industry: Industry name of the suggested Feature - Usecase: Usecase name of the suggested Feature - Source: Source of the suggested Feature The list of features is sorted by the Industries' Feature Popularity to the Input Usecase. \"\"\" if type ( num_of_feat ) != int or num_of_feat < 0 : if num_of_feat != \"all\" : raise TypeError ( \"Invalid input for num_of_feat\" ) usecase = process_usecase ( usecase , semantic ) odf = df_input_fer . loc [ df_input_fer . iloc [:, 3 ] == usecase ] . drop_duplicates ( keep = \"last\" , ignore_index = True ) if len ( odf ) > 0 : odf [ \"count\" ] = odf . groupby ( industry_column )[ industry_column ] . transform ( \"count\" ) odf . sort_values ( \"count\" , inplace = True , ascending = False ) odf = odf . drop ( \"count\" , axis = 1 ) if num_of_feat != \"all\" : odf = odf . head ( num_of_feat ) . reset_index ( drop = True ) else : odf = odf . reset_index ( drop = True ) return odf def list_feature_by_pair ( industry , usecase , num_of_feat = 100 , semantic = True ): \"\"\" Lists down all the Features that are available in Feature Recommender Package based on the Input Industry/Usecase pair Parameters ---------- industry Input industry (string) usecase Input usecase (string) num_of_feat Number of features to be displayed in the output. Value can be either integer, or 'all' - display all features matched with the input. Default is 100. semantic Input semantic (boolean) - Whether the input needs to go through semantic similarity or not. Default is True. Returns ------- DataFrame Columns are: - Feature Name: Name of the suggested Feature - Feature Description: Description of the suggested Feature - Industry: Industry name of the suggested Feature - Usecase: Usecase name of the suggested Feature - Source: Source of the suggested Feature \"\"\" if type ( num_of_feat ) != int or num_of_feat < 0 : if num_of_feat != \"all\" : raise TypeError ( \"Invalid input for num_of_feat\" ) industry = process_industry ( industry , semantic ) usecase = process_usecase ( usecase , semantic ) if num_of_feat != \"all\" : odf = ( df_input_fer . loc [ ( df_input_fer . iloc [:, 2 ] == industry ) & ( df_input_fer . iloc [:, 3 ] == usecase ) ] . drop_duplicates ( keep = \"last\" , ignore_index = True ) . head ( num_of_feat ) ) else : odf = df_input_fer . loc [ ( df_input_fer . iloc [:, 2 ] == industry ) & ( df_input_fer . iloc [:, 3 ] == usecase ) ] . drop_duplicates ( keep = \"last\" , ignore_index = True ) return odf","title":"feature_exploration"},{"location":"api/feature_recommender/feature_exploration.html#functions","text":"def list_all_industry ( ) Lists down all the Industries that are supported in Feature Recommender module.","title":"Functions"},{"location":"api/feature_recommender/feature_recommendation.html","text":"feature_recommendation Feature recommender recommends features based on ingested data dictionary by the user. Expand source code \"\"\"Feature recommender recommends features based on ingested data dictionary by the user.\"\"\" import copy import random import re import numpy as np import pandas as pd import plotly.graph_objects as go from sentence_transformers import util from anovos.feature_recommender.featrec_init import ( recommendation_data_prep , model_fer , camel_case_split , feature_recommendation_prep , get_column_name , EmbeddingsTrainFer , ) from anovos.feature_recommender.feature_exploration import ( list_usecase_by_industry , process_industry , process_usecase , ) list_train_fer , df_rec_fer = feature_recommendation_prep () list_embedding_train_fer = EmbeddingsTrainFer ( list_train_fer ) ( feature_name_column , feature_desc_column , industry_column , usecase_column , ) = get_column_name ( df_rec_fer ) def feature_recommendation ( df , name_column = None , desc_column = None , suggested_industry = \"all\" , suggested_usecase = \"all\" , semantic = True , top_n = 2 , threshold = 0.3 , ): \"\"\"Recommends features to users based on their input attributes, and their goal industry and/or use case Parameters ---------- df : DataFrame Input DataFrame - Users' Data dictionary. It is expected to consist of attribute name and/or attribute description name_column : str Input, column name of Attribute Name in Input DataFrame. Default is None. desc_column : str Input, column name of Attribute Description in Input DataFrame. Default is None. suggested_industry : str Input, Industry of interest to the user (if any) to be filtered out. Default is 'all', meaning all Industries available. suggested_usecase : str Input, Usecase of interest to the user (if any) to be filtered out. Default is 'all', meaning all Usecases available. semantic : bool Input semantic - Whether the input needs to go through semantic similarity or not. Default is True. top_n : int Number of features displayed. Default is 2 threshold : float Input threshold value. Default is 0.3 Returns ------- DataFrame Columns are: - Input Attribute Name: Name of the input Attribute - Input Attribute Description: Description of the input Attribute - Recommended Feature Name: Name of the recommended Feature - Recommended Feature Description: Description of the recommended Feature - Feature Similarity Score: Semantic similarity score between input Attribute and recommended Feature - Industry: Industry name of the recommended Feature - Usecase: Usecase name of the recommended Feature - Source: Source of the recommended Feature \"\"\" if not isinstance ( df , pd . DataFrame ): raise TypeError ( \"Invalid input for df\" ) if type ( top_n ) != int or top_n < 0 : raise TypeError ( \"Invalid input for top_n\" ) if top_n > len ( list_train_fer ): raise TypeError ( \"top_n value is too large\" ) if type ( threshold ) != float : raise TypeError ( \"Invalid input for threshold\" ) if threshold < 0 or threshold > 1 : raise TypeError ( \"Invalid input for threshold. Threshold value is between 0 and 1\" ) list_user , df_user = recommendation_data_prep ( df , name_column , desc_column ) if suggested_industry != \"all\" and suggested_industry == \"all\" : suggested_industry = process_industry ( suggested_industry , semantic ) df_rec_fr = df_rec_fer [ df_rec_fer . iloc [:, 2 ] . str . contains ( suggested_industry )] list_keep = list ( df_rec_fr . index ) list_embedding_train_fr = [ list_embedding_train_fer . get . tolist ()[ x ] for x in list_keep ] df_rec_fr = df_rec_fr . reset_index ( drop = True ) elif suggested_usecase != \"all\" and suggested_industry == \"all\" : suggested_usecase = process_usecase ( suggested_usecase , semantic ) df_rec_fr = df_rec_fer [ df_rec_fer . iloc [:, 3 ] . str . contains ( suggested_usecase )] list_keep = list ( df_rec_fr . index ) list_embedding_train_fr = [ list_embedding_train_fer . get . tolist ()[ x ] for x in list_keep ] df_rec_fr = df_rec_fr . reset_index ( drop = True ) elif suggested_usecase != \"all\" and suggested_industry != \"all\" : suggested_industry = process_industry ( suggested_industry , semantic ) suggested_usecase = process_usecase ( suggested_usecase , semantic ) df_rec_fr = df_rec_fer [ df_rec_fer . iloc [:, 2 ] . str . contains ( suggested_industry ) & df_rec_fer . iloc [:, 3 ] . str . contains ( suggested_usecase ) ] if len ( df_rec_fr ) > 0 : list_keep = list ( df_rec_fr . index ) list_embedding_train_fr = [ list_embedding_train_fer . get . tolist ()[ x ] for x in list_keep ] df_rec_fr = df_rec_fr . reset_index ( drop = True ) else : df_out = pd . DataFrame ( columns = [ \"Input_Attribute_Name\" , \"Input_Attribute_Description\" , \"Recommended_Feature_Name\" , \"Recommended_Feature_Description\" , \"Feature_Similarity_Score\" , \"Industry\" , \"Usecase\" , ] ) print ( \"Industry/Usecase pair does not exist.\" ) return df_out else : df_rec_fr = df_rec_fer list_embedding_train_fr = list_embedding_train_fer . get if name_column is None : df_out = pd . DataFrame ( columns = [ \"Input_Attribute_Description\" , \"Recommended_Feature_Name\" , \"Recommended_Feature_Description\" , \"Feature_Similarity_Score\" , \"Industry\" , \"Usecase\" , ] ) elif desc_column is None : df_out = pd . DataFrame ( columns = [ \"Input_Attribute_Name\" , \"Recommended_Feature_Name\" , \"Recommended_Feature_Description\" , \"Feature_Similarity_Score\" , \"Industry\" , \"Usecase\" , ] ) else : df_out = pd . DataFrame ( columns = [ \"Input_Attribute_Name\" , \"Input_Attribute_Description\" , \"Recommended_Feature_Name\" , \"Recommended_Feature_Description\" , \"Feature_Similarity_Score\" , \"Industry\" , \"Usecase\" , ] ) list_embedding_user = model_fer . model . encode ( list_user , convert_to_tensor = True ) for i , feature in enumerate ( list_user ): cos_scores = util . pytorch_cos_sim ( list_embedding_user , list_embedding_train_fr )[ i ] top_results = np . argpartition ( - cos_scores , range ( top_n ))[ 0 : top_n ] for idx in top_results [ 0 : top_n ]: single_score = \" %.4f \" % ( cos_scores [ idx ]) if name_column is None : if float ( single_score ) >= threshold : df_append = pd . DataFrame ( [ [ df_user [ desc_column ] . iloc [ i ], df_rec_fr [ feature_name_column ] . iloc [ int ( idx )], df_rec_fr [ feature_desc_column ] . iloc [ int ( idx )], \" %.4f \" % ( cos_scores [ idx ]), df_rec_fr [ industry_column ] . iloc [ int ( idx )], df_rec_fr [ usecase_column ] . iloc [ int ( idx )], ] ], columns = [ \"Input_Attribute_Description\" , \"Recommended_Feature_Name\" , \"Recommended_Feature_Description\" , \"Feature_Similarity_Score\" , \"Industry\" , \"Usecase\" , ], ) else : df_append = pd . DataFrame ( [ [ df_user [ desc_column ] . iloc [ i ], \"N/A\" , \"N/A\" , \"N/A\" , \"N/A\" , \"N/A\" , ] ], columns = [ \"Input_Attribute_Description\" , \"Recommended_Feature_Name\" , \"Recommended_Feature_Description\" , \"Feature_Similarity_Score\" , \"Industry\" , \"Usecase\" , ], ) elif desc_column is None : if float ( single_score ) >= threshold : df_append = pd . DataFrame ( [ [ df_user [ name_column ] . iloc [ i ], df_rec_fr [ feature_name_column ] . iloc [ int ( idx )], df_rec_fr [ feature_desc_column ] . iloc [ int ( idx )], \" %.4f \" % ( cos_scores [ idx ]), df_rec_fr [ industry_column ] . iloc [ int ( idx )], df_rec_fr [ usecase_column ] . iloc [ int ( idx )], ] ], columns = [ \"Input_Attribute_Name\" , \"Recommended_Feature_Name\" , \"Recommended_Feature_Description\" , \"Feature_Similarity_Score\" , \"Industry\" , \"Usecase\" , ], ) else : df_append = pd . DataFrame ( [ [ df_user [ name_column ] . iloc [ i ], \"N/A\" , \"N/A\" , \"N/A\" , \"N/A\" , \"N/A\" , ] ], columns = [ \"Input_Attribute_Name\" , \"Recommended_Feature_Name\" , \"Recommended_Feature_Description\" , \"Feature_Similarity_Score\" , \"Industry\" , \"Usecase\" , ], ) else : if float ( single_score ) >= threshold : df_append = pd . DataFrame ( [ [ df_user [ name_column ] . iloc [ i ], df_user [ desc_column ] . iloc [ i ], df_rec_fr [ feature_name_column ] . iloc [ int ( idx )], df_rec_fr [ feature_desc_column ] . iloc [ int ( idx )], \" %.4f \" % ( cos_scores [ idx ]), df_rec_fr [ industry_column ] . iloc [ int ( idx )], df_rec_fr [ usecase_column ] . iloc [ int ( idx )], ] ], columns = [ \"Input_Attribute_Name\" , \"Input_Attribute_Description\" , \"Recommended_Feature_Name\" , \"Recommended_Feature_Description\" , \"Feature_Similarity_Score\" , \"Industry\" , \"Usecase\" , ], ) else : df_append = pd . DataFrame ( [ [ df_user [ name_column ] . iloc [ i ], df_user [ desc_column ] . iloc [ i ], \"N/A\" , \"N/A\" , \"N/A\" , \"N/A\" , \"N/A\" , ] ], columns = [ \"Input_Attribute_Name\" , \"Input_Attribute_Description\" , \"Recommended_Feature_Name\" , \"Recommended_Feature_Description\" , \"Feature_Similarity_Score\" , \"Industry\" , \"Usecase\" , ], ) df_out = pd . concat ( [ df_out , df_append ], ignore_index = True , axis = 0 , join = \"outer\" ) return df_out def find_attr_by_relevance ( df , building_corpus , name_column = None , desc_column = None , threshold = 0.3 ): \"\"\"Provide a comprehensive mapping method from users&#39; input attributes to their own feature corpus, and therefore, help with the process of creating features in cold-start problems Parameters ---------- df : DataFrame Input DataFrame - Users' Data dictionary. It is expected to consist of attribute name and/or attribute description building_corpus : list Input Feature Description name_column : str Input, column name of Attribute Name in Input DataFrame. Default is None. desc_column : str Input, column name of Attribute Description in Input DataFrame. Default is None. threshold : float Input threshold value Default is 0.3 Returns ------- DataFrame Columns are: - Input Feature Desc: Description of the input Feature - Recommended Input Attribute Name: Name of the recommended Feature - Recommended Input Attribute Description: Description of the recommended Feature - Input Attribute Similarity Score: Semantic similarity score between input Attribute and recommended Feature \"\"\" if not isinstance ( df , pd . DataFrame ): raise TypeError ( \"Invalid input for df\" ) if type ( building_corpus ) != list : raise TypeError ( \"Invalid input for building_corpus\" ) if type ( threshold ) != float : raise TypeError ( \"Invalid input for building_corpus\" ) if threshold < 0 or threshold > 1 : raise TypeError ( \"Invalid input for threshold. Threshold value is between 0 and 1\" ) for i in range ( len ( building_corpus )): if type ( building_corpus [ i ]) != str : raise TypeError ( \"Invalid input inside building_corpus:\" , building_corpus [ i ]) building_corpus [ i ] = re . sub ( \"[^A-Za-z0-9]+\" , \" \" , building_corpus [ i ]) building_corpus [ i ] = camel_case_split ( building_corpus [ i ]) building_corpus [ i ] = building_corpus [ i ] . lower () . strip () if name_column is None : df_out = pd . DataFrame ( columns = [ \"Input_Feature_Description\" , \"Recommended_Input_Attribute_Description\" , \"Input_Attribute_Similarity_Score\" , ] ) elif desc_column is None : df_out = pd . DataFrame ( columns = [ \"Input_Feature_Description\" , \"Recommended_Input_Attribute_Name\" , \"Input_Attribute_Similarity_Score\" , ] ) else : df_out = pd . DataFrame ( columns = [ \"Input_Feature_Description\" , \"Recommended_Input_Attribute_Name\" , \"Recommended_Input_Attribute_Description\" , \"Input_Attribute_Similarity_Score\" , ] ) list_user , df_user = recommendation_data_prep ( df , name_column , desc_column ) list_embedding_user = model_fer . model . encode ( list_user , convert_to_tensor = True ) list_embedding_building = model_fer . model . encode ( building_corpus , convert_to_tensor = True ) for i , feature in enumerate ( building_corpus ): if name_column is None : df_append = pd . DataFrame ( columns = [ \"Input_Feature_Description\" , \"Recommended_Input_Attribute_Description\" , \"Input_Attribute_Similarity_Score\" , ] ) elif desc_column is None : df_append = pd . DataFrame ( columns = [ \"Input_Feature_Description\" , \"Recommended_Input_Attribute_Name\" , \"Input_Attribute_Similarity_Score\" , ] ) else : df_append = pd . DataFrame ( columns = [ \"Input_Feature_Description\" , \"Recommended_Input_Attribute_Name\" , \"Recommended_Input_Attribute_Description\" , \"Input_Attribute_Similarity_Score\" , ] ) cos_scores = util . pytorch_cos_sim ( list_embedding_building , list_embedding_user )[ i ] top_results = np . argpartition ( - cos_scores , range ( len ( list_user )))[ 0 : len ( list_user ) ] for idx in top_results [ 0 : len ( list_user )]: single_score = \" %.4f \" % ( cos_scores [ idx ]) if float ( single_score ) >= threshold : if name_column is None : df_append . loc [ len ( df_append . index )] = [ feature , df_user [ desc_column ] . iloc [ int ( idx )], single_score , ] elif desc_column is None : df_append . loc [ len ( df_append . index )] = [ feature , df_user [ name_column ] . iloc [ int ( idx )], single_score , ] else : df_append . loc [ len ( df_append . index )] = [ feature , df_user [ name_column ] . iloc [ int ( idx )], df_user [ desc_column ] . iloc [ int ( idx )], single_score , ] if len ( df_append ) == 0 : if name_column is None : df_append . loc [ len ( df_append . index )] = [ feature , \"N/A\" , \"N/A\" ] elif desc_column is None : df_append . loc [ len ( df_append . index )] = [ feature , \"N/A\" , \"N/A\" ] else : df_append . loc [ len ( df_append . index )] = [ feature , \"N/A\" , \"N/A\" , \"N/A\" ] df_out = pd . concat ([ df_out , df_append ], ignore_index = True , axis = 0 , join = \"outer\" ) return df_out def sankey_visualization ( df , industry_included = False , usecase_included = False ): \"\"\"Visualize Feature Recommendation functions through Sankey plots Parameters ---------- df : DataFrame Input DataFrame. This DataFrame needs to be output of feature_recommendation or find_attr_by_relevance, or in the same format. industry_included : bool Whether the plot needs to include industry mapping or not. Default is False usecase_included : bool Whether the plot needs to include usecase mapping or not. Default is False Returns ------- A `plotly` graph object. \"\"\" fr_proper_col_list = [ \"Recommended_Feature_Name\" , \"Recommended_Feature_Description\" , \"Feature_Similarity_Score\" , \"Industry\" , \"Usecase\" , ] attr_proper_col_list = [ \"Input_Feature_Description\" , \"Input_Attribute_Similarity_Score\" , ] if not isinstance ( df , pd . DataFrame ): raise TypeError ( \"Invalid input for df\" ) if not all ( x in list ( df . columns ) for x in fr_proper_col_list ) and not all ( x in list ( df . columns ) for x in attr_proper_col_list ): raise TypeError ( \"df is not output DataFrame of Feature Recommendation functions\" ) if type ( industry_included ) != bool : raise TypeError ( \"Invalid input for industry_included\" ) if type ( usecase_included ) != bool : raise TypeError ( \"Invalid input for usecase_included\" ) if \"Feature_Similarity_Score\" in df . columns : if \"Input_Attribute_Name\" in df . columns : name_source = \"Input_Attribute_Name\" else : name_source = \"Input_Attribute_Description\" name_target = \"Recommended_Feature_Name\" name_score = \"Feature_Similarity_Score\" else : name_source = \"Input_Feature_Description\" if \"Recommended_Input_Attribute_Name\" in df . columns : name_target = \"Recommended_Input_Attribute_Name\" else : name_target = \"Recommended_Input_Attribute_Description\" name_score = \"Input_Attribute_Similarity_Score\" if industry_included or usecase_included : print ( \"Input is find_attr_by_relevance output DataFrame. There is no suggested Industry and/or Usecase.\" ) industry_included = False usecase_included = False industry_target = \"Industry\" usecase_target = \"Usecase\" df_iter = copy . deepcopy ( df ) for i in range ( len ( df_iter )): if str ( df_iter [ name_score ][ i ]) == \"N/A\" : df = df . drop ([ i ]) df = df . reset_index ( drop = True ) source = [] target = [] value = [] if not industry_included and not usecase_included : source_list = df [ name_source ] . unique () . tolist () target_list = df [ name_target ] . unique () . tolist () label = source_list + target_list for i in range ( len ( df )): source . append ( label . index ( str ( df [ name_source ][ i ]))) target . append ( label . index ( str ( df [ name_target ][ i ]))) value . append ( float ( df [ name_score ][ i ])) elif not industry_included and usecase_included : source_list = df [ name_source ] . unique () . tolist () target_list = df [ name_target ] . unique () . tolist () raw_usecase_list = df [ usecase_target ] . unique () . tolist () usecase_list = [] for i , item in enumerate ( raw_usecase_list ): if \", \" in raw_usecase_list [ i ]: raw_usecase_list [ i ] = raw_usecase_list [ i ] . split ( \", \" ) for j , sub_item in enumerate ( raw_usecase_list [ i ]): usecase_list . append ( sub_item ) else : usecase_list . append ( item ) label = source_list + target_list + usecase_list for i in range ( len ( df )): source . append ( label . index ( str ( df [ name_source ][ i ]))) target . append ( label . index ( str ( df [ name_target ][ i ]))) value . append ( float ( df [ name_score ][ i ])) temp_list = df [ usecase_target ][ i ] . split ( \", \" ) for k , item in enumerate ( temp_list ): source . append ( label . index ( str ( df [ name_target ][ i ]))) target . append ( label . index ( str ( item ))) value . append ( float ( 1 )) elif industry_included and not usecase_included : source_list = df [ name_source ] . unique () . tolist () target_list = df [ name_target ] . unique () . tolist () raw_industry_list = df [ industry_target ] . unique () . tolist () industry_list = [] for i , item in enumerate ( raw_industry_list ): if \", \" in raw_industry_list [ i ]: raw_industry_list [ i ] = raw_industry_list [ i ] . split ( \", \" ) for j , sub_item in enumerate ( raw_industry_list [ i ]): industry_list . append ( sub_item ) else : industry_list . append ( item ) label = source_list + target_list + industry_list for i in range ( len ( df )): source . append ( label . index ( str ( df [ name_source ][ i ]))) target . append ( label . index ( str ( df [ name_target ][ i ]))) value . append ( float ( df [ name_score ][ i ])) temp_list = df [ industry_target ][ i ] . split ( \", \" ) for k , item in enumerate ( temp_list ): source . append ( label . index ( str ( df [ name_target ][ i ]))) target . append ( label . index ( str ( item ))) value . append ( float ( 1 )) else : source_list = df [ name_source ] . unique () . tolist () target_list = df [ name_target ] . unique () . tolist () raw_industry_list = df [ industry_target ] . unique () . tolist () raw_usecase_list = df [ usecase_target ] . unique () . tolist () industry_list = [] for i , item in enumerate ( raw_industry_list ): if \", \" in raw_industry_list [ i ]: raw_industry_list [ i ] = raw_industry_list [ i ] . split ( \", \" ) for j , sub_item in enumerate ( raw_industry_list [ i ]): industry_list . append ( sub_item ) else : industry_list . append ( item ) usecase_list = [] for i , item in enumerate ( raw_usecase_list ): if \", \" in raw_usecase_list [ i ]: raw_usecase_list [ i ] = raw_usecase_list [ i ] . split ( \", \" ) for j , sub_item in enumerate ( raw_usecase_list [ i ]): usecase_list . append ( sub_item ) else : usecase_list . append ( item ) label = source_list + target_list + industry_list + usecase_list for i in range ( len ( df )): source . append ( label . index ( str ( df [ name_source ][ i ]))) target . append ( label . index ( str ( df [ name_target ][ i ]))) value . append ( float ( df [ name_score ][ i ])) temp_list_industry = df [ industry_target ][ i ] . split ( \", \" ) temp_list_usecase = df [ usecase_target ][ i ] . split ( \", \" ) for k , item_industry in enumerate ( temp_list_industry ): source . append ( label . index ( str ( df [ name_target ][ i ]))) target . append ( label . index ( str ( item_industry ))) value . append ( float ( 1 )) for j , item_usecase in enumerate ( temp_list_usecase ): if ( item_usecase in list_usecase_by_industry ( item_industry )[ usecase_column ] . tolist () ): source . append ( label . index ( str ( item_industry ))) target . append ( label . index ( str ( item_usecase ))) value . append ( float ( 1 )) line_color = [ \"#\" + \"\" . join ([ random . choice ( \"0123456789ABCDEF\" ) for j in range ( 6 )]) for k in range ( len ( value )) ] label_color = [ \"#\" + \"\" . join ([ random . choice ( \"0123456789ABCDEF\" ) for e in range ( 6 )]) for f in range ( len ( label )) ] fig = go . Figure ( data = [ go . Sankey ( node = dict ( pad = 15 , thickness = 20 , line = dict ( color = line_color , width = 0.5 ), label = label , color = label_color , ), link = dict ( source = source , target = target , value = value ), ) ] ) fig . update_layout ( title_text = \"Feature Recommendation Sankey Visualization\" , font_size = 10 ) return fig Functions def feature_recommendation ( df, name_column=None, desc_column=None, suggested_industry='all', suggested_usecase='all', semantic=True, top_n=2, threshold=0.3) Recommends features to users based on their input attributes, and their goal industry and/or use case Parameters df :\u2002 DataFrame Input DataFrame - Users' Data dictionary. It is expected to consist of attribute name and/or attribute description name_column :\u2002 str Input, column name of Attribute Name in Input DataFrame. Default is None. desc_column :\u2002 str Input, column name of Attribute Description in Input DataFrame. Default is None. suggested_industry :\u2002 str Input, Industry of interest to the user (if any) to be filtered out. Default is 'all', meaning all Industries available. suggested_usecase :\u2002 str Input, Usecase of interest to the user (if any) to be filtered out. Default is 'all', meaning all Usecases available. semantic :\u2002 bool Input semantic - Whether the input needs to go through semantic similarity or not. Default is True. top_n :\u2002 int Number of features displayed. Default is 2 threshold :\u2002 float Input threshold value. Default is 0.3 Returns DataFrame Columns are: Input Attribute Name: Name of the input Attribute Input Attribute Description: Description of the input Attribute Recommended Feature Name: Name of the recommended Feature Recommended Feature Description: Description of the recommended Feature Feature Similarity Score: Semantic similarity score between input Attribute and recommended Feature Industry: Industry name of the recommended Feature Usecase: Usecase name of the recommended Feature Source: Source of the recommended Feature Expand source code def feature_recommendation ( df , name_column = None , desc_column = None , suggested_industry = \"all\" , suggested_usecase = \"all\" , semantic = True , top_n = 2 , threshold = 0.3 , ): \"\"\"Recommends features to users based on their input attributes, and their goal industry and/or use case Parameters ---------- df : DataFrame Input DataFrame - Users' Data dictionary. It is expected to consist of attribute name and/or attribute description name_column : str Input, column name of Attribute Name in Input DataFrame. Default is None. desc_column : str Input, column name of Attribute Description in Input DataFrame. Default is None. suggested_industry : str Input, Industry of interest to the user (if any) to be filtered out. Default is 'all', meaning all Industries available. suggested_usecase : str Input, Usecase of interest to the user (if any) to be filtered out. Default is 'all', meaning all Usecases available. semantic : bool Input semantic - Whether the input needs to go through semantic similarity or not. Default is True. top_n : int Number of features displayed. Default is 2 threshold : float Input threshold value. Default is 0.3 Returns ------- DataFrame Columns are: - Input Attribute Name: Name of the input Attribute - Input Attribute Description: Description of the input Attribute - Recommended Feature Name: Name of the recommended Feature - Recommended Feature Description: Description of the recommended Feature - Feature Similarity Score: Semantic similarity score between input Attribute and recommended Feature - Industry: Industry name of the recommended Feature - Usecase: Usecase name of the recommended Feature - Source: Source of the recommended Feature \"\"\" if not isinstance ( df , pd . DataFrame ): raise TypeError ( \"Invalid input for df\" ) if type ( top_n ) != int or top_n < 0 : raise TypeError ( \"Invalid input for top_n\" ) if top_n > len ( list_train_fer ): raise TypeError ( \"top_n value is too large\" ) if type ( threshold ) != float : raise TypeError ( \"Invalid input for threshold\" ) if threshold < 0 or threshold > 1 : raise TypeError ( \"Invalid input for threshold. Threshold value is between 0 and 1\" ) list_user , df_user = recommendation_data_prep ( df , name_column , desc_column ) if suggested_industry != \"all\" and suggested_industry == \"all\" : suggested_industry = process_industry ( suggested_industry , semantic ) df_rec_fr = df_rec_fer [ df_rec_fer . iloc [:, 2 ] . str . contains ( suggested_industry )] list_keep = list ( df_rec_fr . index ) list_embedding_train_fr = [ list_embedding_train_fer . get . tolist ()[ x ] for x in list_keep ] df_rec_fr = df_rec_fr . reset_index ( drop = True ) elif suggested_usecase != \"all\" and suggested_industry == \"all\" : suggested_usecase = process_usecase ( suggested_usecase , semantic ) df_rec_fr = df_rec_fer [ df_rec_fer . iloc [:, 3 ] . str . contains ( suggested_usecase )] list_keep = list ( df_rec_fr . index ) list_embedding_train_fr = [ list_embedding_train_fer . get . tolist ()[ x ] for x in list_keep ] df_rec_fr = df_rec_fr . reset_index ( drop = True ) elif suggested_usecase != \"all\" and suggested_industry != \"all\" : suggested_industry = process_industry ( suggested_industry , semantic ) suggested_usecase = process_usecase ( suggested_usecase , semantic ) df_rec_fr = df_rec_fer [ df_rec_fer . iloc [:, 2 ] . str . contains ( suggested_industry ) & df_rec_fer . iloc [:, 3 ] . str . contains ( suggested_usecase ) ] if len ( df_rec_fr ) > 0 : list_keep = list ( df_rec_fr . index ) list_embedding_train_fr = [ list_embedding_train_fer . get . tolist ()[ x ] for x in list_keep ] df_rec_fr = df_rec_fr . reset_index ( drop = True ) else : df_out = pd . DataFrame ( columns = [ \"Input_Attribute_Name\" , \"Input_Attribute_Description\" , \"Recommended_Feature_Name\" , \"Recommended_Feature_Description\" , \"Feature_Similarity_Score\" , \"Industry\" , \"Usecase\" , ] ) print ( \"Industry/Usecase pair does not exist.\" ) return df_out else : df_rec_fr = df_rec_fer list_embedding_train_fr = list_embedding_train_fer . get if name_column is None : df_out = pd . DataFrame ( columns = [ \"Input_Attribute_Description\" , \"Recommended_Feature_Name\" , \"Recommended_Feature_Description\" , \"Feature_Similarity_Score\" , \"Industry\" , \"Usecase\" , ] ) elif desc_column is None : df_out = pd . DataFrame ( columns = [ \"Input_Attribute_Name\" , \"Recommended_Feature_Name\" , \"Recommended_Feature_Description\" , \"Feature_Similarity_Score\" , \"Industry\" , \"Usecase\" , ] ) else : df_out = pd . DataFrame ( columns = [ \"Input_Attribute_Name\" , \"Input_Attribute_Description\" , \"Recommended_Feature_Name\" , \"Recommended_Feature_Description\" , \"Feature_Similarity_Score\" , \"Industry\" , \"Usecase\" , ] ) list_embedding_user = model_fer . model . encode ( list_user , convert_to_tensor = True ) for i , feature in enumerate ( list_user ): cos_scores = util . pytorch_cos_sim ( list_embedding_user , list_embedding_train_fr )[ i ] top_results = np . argpartition ( - cos_scores , range ( top_n ))[ 0 : top_n ] for idx in top_results [ 0 : top_n ]: single_score = \" %.4f \" % ( cos_scores [ idx ]) if name_column is None : if float ( single_score ) >= threshold : df_append = pd . DataFrame ( [ [ df_user [ desc_column ] . iloc [ i ], df_rec_fr [ feature_name_column ] . iloc [ int ( idx )], df_rec_fr [ feature_desc_column ] . iloc [ int ( idx )], \" %.4f \" % ( cos_scores [ idx ]), df_rec_fr [ industry_column ] . iloc [ int ( idx )], df_rec_fr [ usecase_column ] . iloc [ int ( idx )], ] ], columns = [ \"Input_Attribute_Description\" , \"Recommended_Feature_Name\" , \"Recommended_Feature_Description\" , \"Feature_Similarity_Score\" , \"Industry\" , \"Usecase\" , ], ) else : df_append = pd . DataFrame ( [ [ df_user [ desc_column ] . iloc [ i ], \"N/A\" , \"N/A\" , \"N/A\" , \"N/A\" , \"N/A\" , ] ], columns = [ \"Input_Attribute_Description\" , \"Recommended_Feature_Name\" , \"Recommended_Feature_Description\" , \"Feature_Similarity_Score\" , \"Industry\" , \"Usecase\" , ], ) elif desc_column is None : if float ( single_score ) >= threshold : df_append = pd . DataFrame ( [ [ df_user [ name_column ] . iloc [ i ], df_rec_fr [ feature_name_column ] . iloc [ int ( idx )], df_rec_fr [ feature_desc_column ] . iloc [ int ( idx )], \" %.4f \" % ( cos_scores [ idx ]), df_rec_fr [ industry_column ] . iloc [ int ( idx )], df_rec_fr [ usecase_column ] . iloc [ int ( idx )], ] ], columns = [ \"Input_Attribute_Name\" , \"Recommended_Feature_Name\" , \"Recommended_Feature_Description\" , \"Feature_Similarity_Score\" , \"Industry\" , \"Usecase\" , ], ) else : df_append = pd . DataFrame ( [ [ df_user [ name_column ] . iloc [ i ], \"N/A\" , \"N/A\" , \"N/A\" , \"N/A\" , \"N/A\" , ] ], columns = [ \"Input_Attribute_Name\" , \"Recommended_Feature_Name\" , \"Recommended_Feature_Description\" , \"Feature_Similarity_Score\" , \"Industry\" , \"Usecase\" , ], ) else : if float ( single_score ) >= threshold : df_append = pd . DataFrame ( [ [ df_user [ name_column ] . iloc [ i ], df_user [ desc_column ] . iloc [ i ], df_rec_fr [ feature_name_column ] . iloc [ int ( idx )], df_rec_fr [ feature_desc_column ] . iloc [ int ( idx )], \" %.4f \" % ( cos_scores [ idx ]), df_rec_fr [ industry_column ] . iloc [ int ( idx )], df_rec_fr [ usecase_column ] . iloc [ int ( idx )], ] ], columns = [ \"Input_Attribute_Name\" , \"Input_Attribute_Description\" , \"Recommended_Feature_Name\" , \"Recommended_Feature_Description\" , \"Feature_Similarity_Score\" , \"Industry\" , \"Usecase\" , ], ) else : df_append = pd . DataFrame ( [ [ df_user [ name_column ] . iloc [ i ], df_user [ desc_column ] . iloc [ i ], \"N/A\" , \"N/A\" , \"N/A\" , \"N/A\" , \"N/A\" , ] ], columns = [ \"Input_Attribute_Name\" , \"Input_Attribute_Description\" , \"Recommended_Feature_Name\" , \"Recommended_Feature_Description\" , \"Feature_Similarity_Score\" , \"Industry\" , \"Usecase\" , ], ) df_out = pd . concat ( [ df_out , df_append ], ignore_index = True , axis = 0 , join = \"outer\" ) return df_out def find_attr_by_relevance ( df, building_corpus, name_column=None, desc_column=None, threshold=0.3) Provide a comprehensive mapping method from users' input attributes to their own feature corpus, and therefore, help with the process of creating features in cold-start problems Parameters df :\u2002 DataFrame Input DataFrame - Users' Data dictionary. It is expected to consist of attribute name and/or attribute description building_corpus :\u2002 list Input Feature Description name_column :\u2002 str Input, column name of Attribute Name in Input DataFrame. Default is None. desc_column :\u2002 str Input, column name of Attribute Description in Input DataFrame. Default is None. threshold :\u2002 float Input threshold value Default is 0.3 Returns DataFrame Columns are: Input Feature Desc: Description of the input Feature Recommended Input Attribute Name: Name of the recommended Feature Recommended Input Attribute Description: Description of the recommended Feature Input Attribute Similarity Score: Semantic similarity score between input Attribute and recommended Feature Expand source code def find_attr_by_relevance ( df , building_corpus , name_column = None , desc_column = None , threshold = 0.3 ): \"\"\"Provide a comprehensive mapping method from users&#39; input attributes to their own feature corpus, and therefore, help with the process of creating features in cold-start problems Parameters ---------- df : DataFrame Input DataFrame - Users' Data dictionary. It is expected to consist of attribute name and/or attribute description building_corpus : list Input Feature Description name_column : str Input, column name of Attribute Name in Input DataFrame. Default is None. desc_column : str Input, column name of Attribute Description in Input DataFrame. Default is None. threshold : float Input threshold value Default is 0.3 Returns ------- DataFrame Columns are: - Input Feature Desc: Description of the input Feature - Recommended Input Attribute Name: Name of the recommended Feature - Recommended Input Attribute Description: Description of the recommended Feature - Input Attribute Similarity Score: Semantic similarity score between input Attribute and recommended Feature \"\"\" if not isinstance ( df , pd . DataFrame ): raise TypeError ( \"Invalid input for df\" ) if type ( building_corpus ) != list : raise TypeError ( \"Invalid input for building_corpus\" ) if type ( threshold ) != float : raise TypeError ( \"Invalid input for building_corpus\" ) if threshold < 0 or threshold > 1 : raise TypeError ( \"Invalid input for threshold. Threshold value is between 0 and 1\" ) for i in range ( len ( building_corpus )): if type ( building_corpus [ i ]) != str : raise TypeError ( \"Invalid input inside building_corpus:\" , building_corpus [ i ]) building_corpus [ i ] = re . sub ( \"[^A-Za-z0-9]+\" , \" \" , building_corpus [ i ]) building_corpus [ i ] = camel_case_split ( building_corpus [ i ]) building_corpus [ i ] = building_corpus [ i ] . lower () . strip () if name_column is None : df_out = pd . DataFrame ( columns = [ \"Input_Feature_Description\" , \"Recommended_Input_Attribute_Description\" , \"Input_Attribute_Similarity_Score\" , ] ) elif desc_column is None : df_out = pd . DataFrame ( columns = [ \"Input_Feature_Description\" , \"Recommended_Input_Attribute_Name\" , \"Input_Attribute_Similarity_Score\" , ] ) else : df_out = pd . DataFrame ( columns = [ \"Input_Feature_Description\" , \"Recommended_Input_Attribute_Name\" , \"Recommended_Input_Attribute_Description\" , \"Input_Attribute_Similarity_Score\" , ] ) list_user , df_user = recommendation_data_prep ( df , name_column , desc_column ) list_embedding_user = model_fer . model . encode ( list_user , convert_to_tensor = True ) list_embedding_building = model_fer . model . encode ( building_corpus , convert_to_tensor = True ) for i , feature in enumerate ( building_corpus ): if name_column is None : df_append = pd . DataFrame ( columns = [ \"Input_Feature_Description\" , \"Recommended_Input_Attribute_Description\" , \"Input_Attribute_Similarity_Score\" , ] ) elif desc_column is None : df_append = pd . DataFrame ( columns = [ \"Input_Feature_Description\" , \"Recommended_Input_Attribute_Name\" , \"Input_Attribute_Similarity_Score\" , ] ) else : df_append = pd . DataFrame ( columns = [ \"Input_Feature_Description\" , \"Recommended_Input_Attribute_Name\" , \"Recommended_Input_Attribute_Description\" , \"Input_Attribute_Similarity_Score\" , ] ) cos_scores = util . pytorch_cos_sim ( list_embedding_building , list_embedding_user )[ i ] top_results = np . argpartition ( - cos_scores , range ( len ( list_user )))[ 0 : len ( list_user ) ] for idx in top_results [ 0 : len ( list_user )]: single_score = \" %.4f \" % ( cos_scores [ idx ]) if float ( single_score ) >= threshold : if name_column is None : df_append . loc [ len ( df_append . index )] = [ feature , df_user [ desc_column ] . iloc [ int ( idx )], single_score , ] elif desc_column is None : df_append . loc [ len ( df_append . index )] = [ feature , df_user [ name_column ] . iloc [ int ( idx )], single_score , ] else : df_append . loc [ len ( df_append . index )] = [ feature , df_user [ name_column ] . iloc [ int ( idx )], df_user [ desc_column ] . iloc [ int ( idx )], single_score , ] if len ( df_append ) == 0 : if name_column is None : df_append . loc [ len ( df_append . index )] = [ feature , \"N/A\" , \"N/A\" ] elif desc_column is None : df_append . loc [ len ( df_append . index )] = [ feature , \"N/A\" , \"N/A\" ] else : df_append . loc [ len ( df_append . index )] = [ feature , \"N/A\" , \"N/A\" , \"N/A\" ] df_out = pd . concat ([ df_out , df_append ], ignore_index = True , axis = 0 , join = \"outer\" ) return df_out def sankey_visualization ( df, industry_included=False, usecase_included=False) Visualize Feature Recommendation functions through Sankey plots Parameters df :\u2002 DataFrame Input DataFrame. This DataFrame needs to be output of feature_recommendation or find_attr_by_relevance, or in the same format. industry_included :\u2002 bool Whether the plot needs to include industry mapping or not. Default is False usecase_included :\u2002 bool Whether the plot needs to include usecase mapping or not. Default is False Returns A plotly graph object. Expand source code def sankey_visualization ( df , industry_included = False , usecase_included = False ): \"\"\"Visualize Feature Recommendation functions through Sankey plots Parameters ---------- df : DataFrame Input DataFrame. This DataFrame needs to be output of feature_recommendation or find_attr_by_relevance, or in the same format. industry_included : bool Whether the plot needs to include industry mapping or not. Default is False usecase_included : bool Whether the plot needs to include usecase mapping or not. Default is False Returns ------- A `plotly` graph object. \"\"\" fr_proper_col_list = [ \"Recommended_Feature_Name\" , \"Recommended_Feature_Description\" , \"Feature_Similarity_Score\" , \"Industry\" , \"Usecase\" , ] attr_proper_col_list = [ \"Input_Feature_Description\" , \"Input_Attribute_Similarity_Score\" , ] if not isinstance ( df , pd . DataFrame ): raise TypeError ( \"Invalid input for df\" ) if not all ( x in list ( df . columns ) for x in fr_proper_col_list ) and not all ( x in list ( df . columns ) for x in attr_proper_col_list ): raise TypeError ( \"df is not output DataFrame of Feature Recommendation functions\" ) if type ( industry_included ) != bool : raise TypeError ( \"Invalid input for industry_included\" ) if type ( usecase_included ) != bool : raise TypeError ( \"Invalid input for usecase_included\" ) if \"Feature_Similarity_Score\" in df . columns : if \"Input_Attribute_Name\" in df . columns : name_source = \"Input_Attribute_Name\" else : name_source = \"Input_Attribute_Description\" name_target = \"Recommended_Feature_Name\" name_score = \"Feature_Similarity_Score\" else : name_source = \"Input_Feature_Description\" if \"Recommended_Input_Attribute_Name\" in df . columns : name_target = \"Recommended_Input_Attribute_Name\" else : name_target = \"Recommended_Input_Attribute_Description\" name_score = \"Input_Attribute_Similarity_Score\" if industry_included or usecase_included : print ( \"Input is find_attr_by_relevance output DataFrame. There is no suggested Industry and/or Usecase.\" ) industry_included = False usecase_included = False industry_target = \"Industry\" usecase_target = \"Usecase\" df_iter = copy . deepcopy ( df ) for i in range ( len ( df_iter )): if str ( df_iter [ name_score ][ i ]) == \"N/A\" : df = df . drop ([ i ]) df = df . reset_index ( drop = True ) source = [] target = [] value = [] if not industry_included and not usecase_included : source_list = df [ name_source ] . unique () . tolist () target_list = df [ name_target ] . unique () . tolist () label = source_list + target_list for i in range ( len ( df )): source . append ( label . index ( str ( df [ name_source ][ i ]))) target . append ( label . index ( str ( df [ name_target ][ i ]))) value . append ( float ( df [ name_score ][ i ])) elif not industry_included and usecase_included : source_list = df [ name_source ] . unique () . tolist () target_list = df [ name_target ] . unique () . tolist () raw_usecase_list = df [ usecase_target ] . unique () . tolist () usecase_list = [] for i , item in enumerate ( raw_usecase_list ): if \", \" in raw_usecase_list [ i ]: raw_usecase_list [ i ] = raw_usecase_list [ i ] . split ( \", \" ) for j , sub_item in enumerate ( raw_usecase_list [ i ]): usecase_list . append ( sub_item ) else : usecase_list . append ( item ) label = source_list + target_list + usecase_list for i in range ( len ( df )): source . append ( label . index ( str ( df [ name_source ][ i ]))) target . append ( label . index ( str ( df [ name_target ][ i ]))) value . append ( float ( df [ name_score ][ i ])) temp_list = df [ usecase_target ][ i ] . split ( \", \" ) for k , item in enumerate ( temp_list ): source . append ( label . index ( str ( df [ name_target ][ i ]))) target . append ( label . index ( str ( item ))) value . append ( float ( 1 )) elif industry_included and not usecase_included : source_list = df [ name_source ] . unique () . tolist () target_list = df [ name_target ] . unique () . tolist () raw_industry_list = df [ industry_target ] . unique () . tolist () industry_list = [] for i , item in enumerate ( raw_industry_list ): if \", \" in raw_industry_list [ i ]: raw_industry_list [ i ] = raw_industry_list [ i ] . split ( \", \" ) for j , sub_item in enumerate ( raw_industry_list [ i ]): industry_list . append ( sub_item ) else : industry_list . append ( item ) label = source_list + target_list + industry_list for i in range ( len ( df )): source . append ( label . index ( str ( df [ name_source ][ i ]))) target . append ( label . index ( str ( df [ name_target ][ i ]))) value . append ( float ( df [ name_score ][ i ])) temp_list = df [ industry_target ][ i ] . split ( \", \" ) for k , item in enumerate ( temp_list ): source . append ( label . index ( str ( df [ name_target ][ i ]))) target . append ( label . index ( str ( item ))) value . append ( float ( 1 )) else : source_list = df [ name_source ] . unique () . tolist () target_list = df [ name_target ] . unique () . tolist () raw_industry_list = df [ industry_target ] . unique () . tolist () raw_usecase_list = df [ usecase_target ] . unique () . tolist () industry_list = [] for i , item in enumerate ( raw_industry_list ): if \", \" in raw_industry_list [ i ]: raw_industry_list [ i ] = raw_industry_list [ i ] . split ( \", \" ) for j , sub_item in enumerate ( raw_industry_list [ i ]): industry_list . append ( sub_item ) else : industry_list . append ( item ) usecase_list = [] for i , item in enumerate ( raw_usecase_list ): if \", \" in raw_usecase_list [ i ]: raw_usecase_list [ i ] = raw_usecase_list [ i ] . split ( \", \" ) for j , sub_item in enumerate ( raw_usecase_list [ i ]): usecase_list . append ( sub_item ) else : usecase_list . append ( item ) label = source_list + target_list + industry_list + usecase_list for i in range ( len ( df )): source . append ( label . index ( str ( df [ name_source ][ i ]))) target . append ( label . index ( str ( df [ name_target ][ i ]))) value . append ( float ( df [ name_score ][ i ])) temp_list_industry = df [ industry_target ][ i ] . split ( \", \" ) temp_list_usecase = df [ usecase_target ][ i ] . split ( \", \" ) for k , item_industry in enumerate ( temp_list_industry ): source . append ( label . index ( str ( df [ name_target ][ i ]))) target . append ( label . index ( str ( item_industry ))) value . append ( float ( 1 )) for j , item_usecase in enumerate ( temp_list_usecase ): if ( item_usecase in list_usecase_by_industry ( item_industry )[ usecase_column ] . tolist () ): source . append ( label . index ( str ( item_industry ))) target . append ( label . index ( str ( item_usecase ))) value . append ( float ( 1 )) line_color = [ \"#\" + \"\" . join ([ random . choice ( \"0123456789ABCDEF\" ) for j in range ( 6 )]) for k in range ( len ( value )) ] label_color = [ \"#\" + \"\" . join ([ random . choice ( \"0123456789ABCDEF\" ) for e in range ( 6 )]) for f in range ( len ( label )) ] fig = go . Figure ( data = [ go . Sankey ( node = dict ( pad = 15 , thickness = 20 , line = dict ( color = line_color , width = 0.5 ), label = label , color = label_color , ), link = dict ( source = source , target = target , value = value ), ) ] ) fig . update_layout ( title_text = \"Feature Recommendation Sankey Visualization\" , font_size = 10 ) return fig","title":"<code>feature_recommendation</code>"},{"location":"api/feature_recommender/feature_recommendation.html#feature_recommendation","text":"Feature recommender recommends features based on ingested data dictionary by the user. Expand source code \"\"\"Feature recommender recommends features based on ingested data dictionary by the user.\"\"\" import copy import random import re import numpy as np import pandas as pd import plotly.graph_objects as go from sentence_transformers import util from anovos.feature_recommender.featrec_init import ( recommendation_data_prep , model_fer , camel_case_split , feature_recommendation_prep , get_column_name , EmbeddingsTrainFer , ) from anovos.feature_recommender.feature_exploration import ( list_usecase_by_industry , process_industry , process_usecase , ) list_train_fer , df_rec_fer = feature_recommendation_prep () list_embedding_train_fer = EmbeddingsTrainFer ( list_train_fer ) ( feature_name_column , feature_desc_column , industry_column , usecase_column , ) = get_column_name ( df_rec_fer ) def feature_recommendation ( df , name_column = None , desc_column = None , suggested_industry = \"all\" , suggested_usecase = \"all\" , semantic = True , top_n = 2 , threshold = 0.3 , ): \"\"\"Recommends features to users based on their input attributes, and their goal industry and/or use case Parameters ---------- df : DataFrame Input DataFrame - Users' Data dictionary. It is expected to consist of attribute name and/or attribute description name_column : str Input, column name of Attribute Name in Input DataFrame. Default is None. desc_column : str Input, column name of Attribute Description in Input DataFrame. Default is None. suggested_industry : str Input, Industry of interest to the user (if any) to be filtered out. Default is 'all', meaning all Industries available. suggested_usecase : str Input, Usecase of interest to the user (if any) to be filtered out. Default is 'all', meaning all Usecases available. semantic : bool Input semantic - Whether the input needs to go through semantic similarity or not. Default is True. top_n : int Number of features displayed. Default is 2 threshold : float Input threshold value. Default is 0.3 Returns ------- DataFrame Columns are: - Input Attribute Name: Name of the input Attribute - Input Attribute Description: Description of the input Attribute - Recommended Feature Name: Name of the recommended Feature - Recommended Feature Description: Description of the recommended Feature - Feature Similarity Score: Semantic similarity score between input Attribute and recommended Feature - Industry: Industry name of the recommended Feature - Usecase: Usecase name of the recommended Feature - Source: Source of the recommended Feature \"\"\" if not isinstance ( df , pd . DataFrame ): raise TypeError ( \"Invalid input for df\" ) if type ( top_n ) != int or top_n < 0 : raise TypeError ( \"Invalid input for top_n\" ) if top_n > len ( list_train_fer ): raise TypeError ( \"top_n value is too large\" ) if type ( threshold ) != float : raise TypeError ( \"Invalid input for threshold\" ) if threshold < 0 or threshold > 1 : raise TypeError ( \"Invalid input for threshold. Threshold value is between 0 and 1\" ) list_user , df_user = recommendation_data_prep ( df , name_column , desc_column ) if suggested_industry != \"all\" and suggested_industry == \"all\" : suggested_industry = process_industry ( suggested_industry , semantic ) df_rec_fr = df_rec_fer [ df_rec_fer . iloc [:, 2 ] . str . contains ( suggested_industry )] list_keep = list ( df_rec_fr . index ) list_embedding_train_fr = [ list_embedding_train_fer . get . tolist ()[ x ] for x in list_keep ] df_rec_fr = df_rec_fr . reset_index ( drop = True ) elif suggested_usecase != \"all\" and suggested_industry == \"all\" : suggested_usecase = process_usecase ( suggested_usecase , semantic ) df_rec_fr = df_rec_fer [ df_rec_fer . iloc [:, 3 ] . str . contains ( suggested_usecase )] list_keep = list ( df_rec_fr . index ) list_embedding_train_fr = [ list_embedding_train_fer . get . tolist ()[ x ] for x in list_keep ] df_rec_fr = df_rec_fr . reset_index ( drop = True ) elif suggested_usecase != \"all\" and suggested_industry != \"all\" : suggested_industry = process_industry ( suggested_industry , semantic ) suggested_usecase = process_usecase ( suggested_usecase , semantic ) df_rec_fr = df_rec_fer [ df_rec_fer . iloc [:, 2 ] . str . contains ( suggested_industry ) & df_rec_fer . iloc [:, 3 ] . str . contains ( suggested_usecase ) ] if len ( df_rec_fr ) > 0 : list_keep = list ( df_rec_fr . index ) list_embedding_train_fr = [ list_embedding_train_fer . get . tolist ()[ x ] for x in list_keep ] df_rec_fr = df_rec_fr . reset_index ( drop = True ) else : df_out = pd . DataFrame ( columns = [ \"Input_Attribute_Name\" , \"Input_Attribute_Description\" , \"Recommended_Feature_Name\" , \"Recommended_Feature_Description\" , \"Feature_Similarity_Score\" , \"Industry\" , \"Usecase\" , ] ) print ( \"Industry/Usecase pair does not exist.\" ) return df_out else : df_rec_fr = df_rec_fer list_embedding_train_fr = list_embedding_train_fer . get if name_column is None : df_out = pd . DataFrame ( columns = [ \"Input_Attribute_Description\" , \"Recommended_Feature_Name\" , \"Recommended_Feature_Description\" , \"Feature_Similarity_Score\" , \"Industry\" , \"Usecase\" , ] ) elif desc_column is None : df_out = pd . DataFrame ( columns = [ \"Input_Attribute_Name\" , \"Recommended_Feature_Name\" , \"Recommended_Feature_Description\" , \"Feature_Similarity_Score\" , \"Industry\" , \"Usecase\" , ] ) else : df_out = pd . DataFrame ( columns = [ \"Input_Attribute_Name\" , \"Input_Attribute_Description\" , \"Recommended_Feature_Name\" , \"Recommended_Feature_Description\" , \"Feature_Similarity_Score\" , \"Industry\" , \"Usecase\" , ] ) list_embedding_user = model_fer . model . encode ( list_user , convert_to_tensor = True ) for i , feature in enumerate ( list_user ): cos_scores = util . pytorch_cos_sim ( list_embedding_user , list_embedding_train_fr )[ i ] top_results = np . argpartition ( - cos_scores , range ( top_n ))[ 0 : top_n ] for idx in top_results [ 0 : top_n ]: single_score = \" %.4f \" % ( cos_scores [ idx ]) if name_column is None : if float ( single_score ) >= threshold : df_append = pd . DataFrame ( [ [ df_user [ desc_column ] . iloc [ i ], df_rec_fr [ feature_name_column ] . iloc [ int ( idx )], df_rec_fr [ feature_desc_column ] . iloc [ int ( idx )], \" %.4f \" % ( cos_scores [ idx ]), df_rec_fr [ industry_column ] . iloc [ int ( idx )], df_rec_fr [ usecase_column ] . iloc [ int ( idx )], ] ], columns = [ \"Input_Attribute_Description\" , \"Recommended_Feature_Name\" , \"Recommended_Feature_Description\" , \"Feature_Similarity_Score\" , \"Industry\" , \"Usecase\" , ], ) else : df_append = pd . DataFrame ( [ [ df_user [ desc_column ] . iloc [ i ], \"N/A\" , \"N/A\" , \"N/A\" , \"N/A\" , \"N/A\" , ] ], columns = [ \"Input_Attribute_Description\" , \"Recommended_Feature_Name\" , \"Recommended_Feature_Description\" , \"Feature_Similarity_Score\" , \"Industry\" , \"Usecase\" , ], ) elif desc_column is None : if float ( single_score ) >= threshold : df_append = pd . DataFrame ( [ [ df_user [ name_column ] . iloc [ i ], df_rec_fr [ feature_name_column ] . iloc [ int ( idx )], df_rec_fr [ feature_desc_column ] . iloc [ int ( idx )], \" %.4f \" % ( cos_scores [ idx ]), df_rec_fr [ industry_column ] . iloc [ int ( idx )], df_rec_fr [ usecase_column ] . iloc [ int ( idx )], ] ], columns = [ \"Input_Attribute_Name\" , \"Recommended_Feature_Name\" , \"Recommended_Feature_Description\" , \"Feature_Similarity_Score\" , \"Industry\" , \"Usecase\" , ], ) else : df_append = pd . DataFrame ( [ [ df_user [ name_column ] . iloc [ i ], \"N/A\" , \"N/A\" , \"N/A\" , \"N/A\" , \"N/A\" , ] ], columns = [ \"Input_Attribute_Name\" , \"Recommended_Feature_Name\" , \"Recommended_Feature_Description\" , \"Feature_Similarity_Score\" , \"Industry\" , \"Usecase\" , ], ) else : if float ( single_score ) >= threshold : df_append = pd . DataFrame ( [ [ df_user [ name_column ] . iloc [ i ], df_user [ desc_column ] . iloc [ i ], df_rec_fr [ feature_name_column ] . iloc [ int ( idx )], df_rec_fr [ feature_desc_column ] . iloc [ int ( idx )], \" %.4f \" % ( cos_scores [ idx ]), df_rec_fr [ industry_column ] . iloc [ int ( idx )], df_rec_fr [ usecase_column ] . iloc [ int ( idx )], ] ], columns = [ \"Input_Attribute_Name\" , \"Input_Attribute_Description\" , \"Recommended_Feature_Name\" , \"Recommended_Feature_Description\" , \"Feature_Similarity_Score\" , \"Industry\" , \"Usecase\" , ], ) else : df_append = pd . DataFrame ( [ [ df_user [ name_column ] . iloc [ i ], df_user [ desc_column ] . iloc [ i ], \"N/A\" , \"N/A\" , \"N/A\" , \"N/A\" , \"N/A\" , ] ], columns = [ \"Input_Attribute_Name\" , \"Input_Attribute_Description\" , \"Recommended_Feature_Name\" , \"Recommended_Feature_Description\" , \"Feature_Similarity_Score\" , \"Industry\" , \"Usecase\" , ], ) df_out = pd . concat ( [ df_out , df_append ], ignore_index = True , axis = 0 , join = \"outer\" ) return df_out def find_attr_by_relevance ( df , building_corpus , name_column = None , desc_column = None , threshold = 0.3 ): \"\"\"Provide a comprehensive mapping method from users&#39; input attributes to their own feature corpus, and therefore, help with the process of creating features in cold-start problems Parameters ---------- df : DataFrame Input DataFrame - Users' Data dictionary. It is expected to consist of attribute name and/or attribute description building_corpus : list Input Feature Description name_column : str Input, column name of Attribute Name in Input DataFrame. Default is None. desc_column : str Input, column name of Attribute Description in Input DataFrame. Default is None. threshold : float Input threshold value Default is 0.3 Returns ------- DataFrame Columns are: - Input Feature Desc: Description of the input Feature - Recommended Input Attribute Name: Name of the recommended Feature - Recommended Input Attribute Description: Description of the recommended Feature - Input Attribute Similarity Score: Semantic similarity score between input Attribute and recommended Feature \"\"\" if not isinstance ( df , pd . DataFrame ): raise TypeError ( \"Invalid input for df\" ) if type ( building_corpus ) != list : raise TypeError ( \"Invalid input for building_corpus\" ) if type ( threshold ) != float : raise TypeError ( \"Invalid input for building_corpus\" ) if threshold < 0 or threshold > 1 : raise TypeError ( \"Invalid input for threshold. Threshold value is between 0 and 1\" ) for i in range ( len ( building_corpus )): if type ( building_corpus [ i ]) != str : raise TypeError ( \"Invalid input inside building_corpus:\" , building_corpus [ i ]) building_corpus [ i ] = re . sub ( \"[^A-Za-z0-9]+\" , \" \" , building_corpus [ i ]) building_corpus [ i ] = camel_case_split ( building_corpus [ i ]) building_corpus [ i ] = building_corpus [ i ] . lower () . strip () if name_column is None : df_out = pd . DataFrame ( columns = [ \"Input_Feature_Description\" , \"Recommended_Input_Attribute_Description\" , \"Input_Attribute_Similarity_Score\" , ] ) elif desc_column is None : df_out = pd . DataFrame ( columns = [ \"Input_Feature_Description\" , \"Recommended_Input_Attribute_Name\" , \"Input_Attribute_Similarity_Score\" , ] ) else : df_out = pd . DataFrame ( columns = [ \"Input_Feature_Description\" , \"Recommended_Input_Attribute_Name\" , \"Recommended_Input_Attribute_Description\" , \"Input_Attribute_Similarity_Score\" , ] ) list_user , df_user = recommendation_data_prep ( df , name_column , desc_column ) list_embedding_user = model_fer . model . encode ( list_user , convert_to_tensor = True ) list_embedding_building = model_fer . model . encode ( building_corpus , convert_to_tensor = True ) for i , feature in enumerate ( building_corpus ): if name_column is None : df_append = pd . DataFrame ( columns = [ \"Input_Feature_Description\" , \"Recommended_Input_Attribute_Description\" , \"Input_Attribute_Similarity_Score\" , ] ) elif desc_column is None : df_append = pd . DataFrame ( columns = [ \"Input_Feature_Description\" , \"Recommended_Input_Attribute_Name\" , \"Input_Attribute_Similarity_Score\" , ] ) else : df_append = pd . DataFrame ( columns = [ \"Input_Feature_Description\" , \"Recommended_Input_Attribute_Name\" , \"Recommended_Input_Attribute_Description\" , \"Input_Attribute_Similarity_Score\" , ] ) cos_scores = util . pytorch_cos_sim ( list_embedding_building , list_embedding_user )[ i ] top_results = np . argpartition ( - cos_scores , range ( len ( list_user )))[ 0 : len ( list_user ) ] for idx in top_results [ 0 : len ( list_user )]: single_score = \" %.4f \" % ( cos_scores [ idx ]) if float ( single_score ) >= threshold : if name_column is None : df_append . loc [ len ( df_append . index )] = [ feature , df_user [ desc_column ] . iloc [ int ( idx )], single_score , ] elif desc_column is None : df_append . loc [ len ( df_append . index )] = [ feature , df_user [ name_column ] . iloc [ int ( idx )], single_score , ] else : df_append . loc [ len ( df_append . index )] = [ feature , df_user [ name_column ] . iloc [ int ( idx )], df_user [ desc_column ] . iloc [ int ( idx )], single_score , ] if len ( df_append ) == 0 : if name_column is None : df_append . loc [ len ( df_append . index )] = [ feature , \"N/A\" , \"N/A\" ] elif desc_column is None : df_append . loc [ len ( df_append . index )] = [ feature , \"N/A\" , \"N/A\" ] else : df_append . loc [ len ( df_append . index )] = [ feature , \"N/A\" , \"N/A\" , \"N/A\" ] df_out = pd . concat ([ df_out , df_append ], ignore_index = True , axis = 0 , join = \"outer\" ) return df_out def sankey_visualization ( df , industry_included = False , usecase_included = False ): \"\"\"Visualize Feature Recommendation functions through Sankey plots Parameters ---------- df : DataFrame Input DataFrame. This DataFrame needs to be output of feature_recommendation or find_attr_by_relevance, or in the same format. industry_included : bool Whether the plot needs to include industry mapping or not. Default is False usecase_included : bool Whether the plot needs to include usecase mapping or not. Default is False Returns ------- A `plotly` graph object. \"\"\" fr_proper_col_list = [ \"Recommended_Feature_Name\" , \"Recommended_Feature_Description\" , \"Feature_Similarity_Score\" , \"Industry\" , \"Usecase\" , ] attr_proper_col_list = [ \"Input_Feature_Description\" , \"Input_Attribute_Similarity_Score\" , ] if not isinstance ( df , pd . DataFrame ): raise TypeError ( \"Invalid input for df\" ) if not all ( x in list ( df . columns ) for x in fr_proper_col_list ) and not all ( x in list ( df . columns ) for x in attr_proper_col_list ): raise TypeError ( \"df is not output DataFrame of Feature Recommendation functions\" ) if type ( industry_included ) != bool : raise TypeError ( \"Invalid input for industry_included\" ) if type ( usecase_included ) != bool : raise TypeError ( \"Invalid input for usecase_included\" ) if \"Feature_Similarity_Score\" in df . columns : if \"Input_Attribute_Name\" in df . columns : name_source = \"Input_Attribute_Name\" else : name_source = \"Input_Attribute_Description\" name_target = \"Recommended_Feature_Name\" name_score = \"Feature_Similarity_Score\" else : name_source = \"Input_Feature_Description\" if \"Recommended_Input_Attribute_Name\" in df . columns : name_target = \"Recommended_Input_Attribute_Name\" else : name_target = \"Recommended_Input_Attribute_Description\" name_score = \"Input_Attribute_Similarity_Score\" if industry_included or usecase_included : print ( \"Input is find_attr_by_relevance output DataFrame. There is no suggested Industry and/or Usecase.\" ) industry_included = False usecase_included = False industry_target = \"Industry\" usecase_target = \"Usecase\" df_iter = copy . deepcopy ( df ) for i in range ( len ( df_iter )): if str ( df_iter [ name_score ][ i ]) == \"N/A\" : df = df . drop ([ i ]) df = df . reset_index ( drop = True ) source = [] target = [] value = [] if not industry_included and not usecase_included : source_list = df [ name_source ] . unique () . tolist () target_list = df [ name_target ] . unique () . tolist () label = source_list + target_list for i in range ( len ( df )): source . append ( label . index ( str ( df [ name_source ][ i ]))) target . append ( label . index ( str ( df [ name_target ][ i ]))) value . append ( float ( df [ name_score ][ i ])) elif not industry_included and usecase_included : source_list = df [ name_source ] . unique () . tolist () target_list = df [ name_target ] . unique () . tolist () raw_usecase_list = df [ usecase_target ] . unique () . tolist () usecase_list = [] for i , item in enumerate ( raw_usecase_list ): if \", \" in raw_usecase_list [ i ]: raw_usecase_list [ i ] = raw_usecase_list [ i ] . split ( \", \" ) for j , sub_item in enumerate ( raw_usecase_list [ i ]): usecase_list . append ( sub_item ) else : usecase_list . append ( item ) label = source_list + target_list + usecase_list for i in range ( len ( df )): source . append ( label . index ( str ( df [ name_source ][ i ]))) target . append ( label . index ( str ( df [ name_target ][ i ]))) value . append ( float ( df [ name_score ][ i ])) temp_list = df [ usecase_target ][ i ] . split ( \", \" ) for k , item in enumerate ( temp_list ): source . append ( label . index ( str ( df [ name_target ][ i ]))) target . append ( label . index ( str ( item ))) value . append ( float ( 1 )) elif industry_included and not usecase_included : source_list = df [ name_source ] . unique () . tolist () target_list = df [ name_target ] . unique () . tolist () raw_industry_list = df [ industry_target ] . unique () . tolist () industry_list = [] for i , item in enumerate ( raw_industry_list ): if \", \" in raw_industry_list [ i ]: raw_industry_list [ i ] = raw_industry_list [ i ] . split ( \", \" ) for j , sub_item in enumerate ( raw_industry_list [ i ]): industry_list . append ( sub_item ) else : industry_list . append ( item ) label = source_list + target_list + industry_list for i in range ( len ( df )): source . append ( label . index ( str ( df [ name_source ][ i ]))) target . append ( label . index ( str ( df [ name_target ][ i ]))) value . append ( float ( df [ name_score ][ i ])) temp_list = df [ industry_target ][ i ] . split ( \", \" ) for k , item in enumerate ( temp_list ): source . append ( label . index ( str ( df [ name_target ][ i ]))) target . append ( label . index ( str ( item ))) value . append ( float ( 1 )) else : source_list = df [ name_source ] . unique () . tolist () target_list = df [ name_target ] . unique () . tolist () raw_industry_list = df [ industry_target ] . unique () . tolist () raw_usecase_list = df [ usecase_target ] . unique () . tolist () industry_list = [] for i , item in enumerate ( raw_industry_list ): if \", \" in raw_industry_list [ i ]: raw_industry_list [ i ] = raw_industry_list [ i ] . split ( \", \" ) for j , sub_item in enumerate ( raw_industry_list [ i ]): industry_list . append ( sub_item ) else : industry_list . append ( item ) usecase_list = [] for i , item in enumerate ( raw_usecase_list ): if \", \" in raw_usecase_list [ i ]: raw_usecase_list [ i ] = raw_usecase_list [ i ] . split ( \", \" ) for j , sub_item in enumerate ( raw_usecase_list [ i ]): usecase_list . append ( sub_item ) else : usecase_list . append ( item ) label = source_list + target_list + industry_list + usecase_list for i in range ( len ( df )): source . append ( label . index ( str ( df [ name_source ][ i ]))) target . append ( label . index ( str ( df [ name_target ][ i ]))) value . append ( float ( df [ name_score ][ i ])) temp_list_industry = df [ industry_target ][ i ] . split ( \", \" ) temp_list_usecase = df [ usecase_target ][ i ] . split ( \", \" ) for k , item_industry in enumerate ( temp_list_industry ): source . append ( label . index ( str ( df [ name_target ][ i ]))) target . append ( label . index ( str ( item_industry ))) value . append ( float ( 1 )) for j , item_usecase in enumerate ( temp_list_usecase ): if ( item_usecase in list_usecase_by_industry ( item_industry )[ usecase_column ] . tolist () ): source . append ( label . index ( str ( item_industry ))) target . append ( label . index ( str ( item_usecase ))) value . append ( float ( 1 )) line_color = [ \"#\" + \"\" . join ([ random . choice ( \"0123456789ABCDEF\" ) for j in range ( 6 )]) for k in range ( len ( value )) ] label_color = [ \"#\" + \"\" . join ([ random . choice ( \"0123456789ABCDEF\" ) for e in range ( 6 )]) for f in range ( len ( label )) ] fig = go . Figure ( data = [ go . Sankey ( node = dict ( pad = 15 , thickness = 20 , line = dict ( color = line_color , width = 0.5 ), label = label , color = label_color , ), link = dict ( source = source , target = target , value = value ), ) ] ) fig . update_layout ( title_text = \"Feature Recommendation Sankey Visualization\" , font_size = 10 ) return fig","title":"feature_recommendation"},{"location":"api/feature_recommender/feature_recommendation.html#functions","text":"def feature_recommendation ( df, name_column=None, desc_column=None, suggested_industry='all', suggested_usecase='all', semantic=True, top_n=2, threshold=0.3) Recommends features to users based on their input attributes, and their goal industry and/or use case","title":"Functions"},{"location":"api/shared/_index.html","text":"Overview Sub-modules anovos.shared.spark anovos.shared.utils","title":"Overview"},{"location":"api/shared/_index.html#overview","text":"","title":"Overview"},{"location":"api/shared/_index.html#sub-modules","text":"anovos.shared.spark anovos.shared.utils","title":"Sub-modules"},{"location":"api/shared/spark.html","text":"spark Expand source code import __main__ from os import environ import findspark from loguru import logger from packaging import version findspark . init () import pyspark from pyspark.sql import SQLContext , SparkSession if version . parse ( pyspark . __version__ ) < version . parse ( \"3.0.0\" ): SPARK_JARS_PACKAGES = [ \"io.github.histogrammar:histogrammar_2.11:1.0.20\" , \"io.github.histogrammar:histogrammar-sparksql_2.11:1.0.20\" , \"org.apache.spark:spark-avro_2.11:\" + str ( pyspark . __version__ ), ] else : SPARK_JARS_PACKAGES = [ \"io.github.histogrammar:histogrammar_2.12:1.0.20\" , \"io.github.histogrammar:histogrammar-sparksql_2.12:1.0.20\" , \"org.apache.spark:spark-avro_2.12:\" + str ( pyspark . __version__ ), ] def init_spark ( app_name = \"anovos\" , master = \"local[*]\" , jars_packages = None , py_files = None , spark_config = None , ): \"\"\" Parameters ---------- app_name Name of Spark app. (Default value = \"anovos\") master Cluster connection details Defaults to local[*] which means to run Spark locally with as many worker threads as logical cores on the machine. jars_packages List of Spark JAR package names. (Default value = None) py_files List of files to send to Spark cluster (master and workers). (Default value = None) spark_config Dictionary of config key-value pairs. (Default value = None) Returns ------- \"\"\" logger . info ( f \"Getting spark session, context and sql context app_name: { app_name } \" ) # detect execution environment flag_repl = not ( hasattr ( __main__ , \"__file__\" )) flag_debug = \"DEBUG\" in environ . keys () if not ( flag_repl or flag_debug ): spark_builder = SparkSession . builder . appName ( app_name ) else : spark_builder = SparkSession . builder . master ( master ) . appName ( app_name ) if jars_packages is not None and jars_packages : spark_jars_packages = \",\" . join ( list ( jars_packages )) spark_builder . config ( \"spark.jars.packages\" , spark_jars_packages ) if py_files is not None and py_files : spark_files = \",\" . join ( list ( py_files )) spark_builder . config ( \"spark.files\" , spark_files ) if spark_config is not None and spark_config : for key , val in spark_config . items (): spark_builder . config ( key , val ) _spark = spark_builder . getOrCreate () _spark_context = _spark . sparkContext _sql_context = SQLContext ( _spark_context ) return _spark , _spark_context , _sql_context configs = { \"app_name\" : \"Anovos_pipeline\" , \"jars_packages\" : SPARK_JARS_PACKAGES , \"py_files\" : [], \"spark_config\" : { \"spark.sql.session.timeZone\" : \"GMT\" , \"spark.python.profile\" : \"false\" , \"spark.yarn.appMasterEnv.ARROW_PRE_0_15_IPC_FORMAT\" : \"1\" , \"spark.executorEnv.ARROW_PRE_0_15_IPC_FORMAT\" : \"1\" , \"spark.sql.session.timeZone\" : \"GMT\" , \"spark.python.profile\" : \"false\" , }, } spark , sc , sqlContext = init_spark ( ** configs ) Functions def init_spark ( app_name='anovos', master='local[*]', jars_packages=None, py_files=None, spark_config=None) Parameters app_name Name of Spark app. (Default value = \"anovos\") master Cluster connection details Defaults to local[*] which means to run Spark locally with as many worker threads as logical cores on the machine. jars_packages List of Spark JAR package names. (Default value = None) py_files List of files to send to Spark cluster (master and workers). (Default value = None) spark_config Dictionary of config key-value pairs. (Default value = None) Returns Expand source code def init_spark ( app_name = \"anovos\" , master = \"local[*]\" , jars_packages = None , py_files = None , spark_config = None , ): \"\"\" Parameters ---------- app_name Name of Spark app. (Default value = \"anovos\") master Cluster connection details Defaults to local[*] which means to run Spark locally with as many worker threads as logical cores on the machine. jars_packages List of Spark JAR package names. (Default value = None) py_files List of files to send to Spark cluster (master and workers). (Default value = None) spark_config Dictionary of config key-value pairs. (Default value = None) Returns ------- \"\"\" logger . info ( f \"Getting spark session, context and sql context app_name: { app_name } \" ) # detect execution environment flag_repl = not ( hasattr ( __main__ , \"__file__\" )) flag_debug = \"DEBUG\" in environ . keys () if not ( flag_repl or flag_debug ): spark_builder = SparkSession . builder . appName ( app_name ) else : spark_builder = SparkSession . builder . master ( master ) . appName ( app_name ) if jars_packages is not None and jars_packages : spark_jars_packages = \",\" . join ( list ( jars_packages )) spark_builder . config ( \"spark.jars.packages\" , spark_jars_packages ) if py_files is not None and py_files : spark_files = \",\" . join ( list ( py_files )) spark_builder . config ( \"spark.files\" , spark_files ) if spark_config is not None and spark_config : for key , val in spark_config . items (): spark_builder . config ( key , val ) _spark = spark_builder . getOrCreate () _spark_context = _spark . sparkContext _sql_context = SQLContext ( _spark_context ) return _spark , _spark_context , _sql_context","title":"<code>spark</code>"},{"location":"api/shared/spark.html#spark","text":"Expand source code import __main__ from os import environ import findspark from loguru import logger from packaging import version findspark . init () import pyspark from pyspark.sql import SQLContext , SparkSession if version . parse ( pyspark . __version__ ) < version . parse ( \"3.0.0\" ): SPARK_JARS_PACKAGES = [ \"io.github.histogrammar:histogrammar_2.11:1.0.20\" , \"io.github.histogrammar:histogrammar-sparksql_2.11:1.0.20\" , \"org.apache.spark:spark-avro_2.11:\" + str ( pyspark . __version__ ), ] else : SPARK_JARS_PACKAGES = [ \"io.github.histogrammar:histogrammar_2.12:1.0.20\" , \"io.github.histogrammar:histogrammar-sparksql_2.12:1.0.20\" , \"org.apache.spark:spark-avro_2.12:\" + str ( pyspark . __version__ ), ] def init_spark ( app_name = \"anovos\" , master = \"local[*]\" , jars_packages = None , py_files = None , spark_config = None , ): \"\"\" Parameters ---------- app_name Name of Spark app. (Default value = \"anovos\") master Cluster connection details Defaults to local[*] which means to run Spark locally with as many worker threads as logical cores on the machine. jars_packages List of Spark JAR package names. (Default value = None) py_files List of files to send to Spark cluster (master and workers). (Default value = None) spark_config Dictionary of config key-value pairs. (Default value = None) Returns ------- \"\"\" logger . info ( f \"Getting spark session, context and sql context app_name: { app_name } \" ) # detect execution environment flag_repl = not ( hasattr ( __main__ , \"__file__\" )) flag_debug = \"DEBUG\" in environ . keys () if not ( flag_repl or flag_debug ): spark_builder = SparkSession . builder . appName ( app_name ) else : spark_builder = SparkSession . builder . master ( master ) . appName ( app_name ) if jars_packages is not None and jars_packages : spark_jars_packages = \",\" . join ( list ( jars_packages )) spark_builder . config ( \"spark.jars.packages\" , spark_jars_packages ) if py_files is not None and py_files : spark_files = \",\" . join ( list ( py_files )) spark_builder . config ( \"spark.files\" , spark_files ) if spark_config is not None and spark_config : for key , val in spark_config . items (): spark_builder . config ( key , val ) _spark = spark_builder . getOrCreate () _spark_context = _spark . sparkContext _sql_context = SQLContext ( _spark_context ) return _spark , _spark_context , _sql_context configs = { \"app_name\" : \"Anovos_pipeline\" , \"jars_packages\" : SPARK_JARS_PACKAGES , \"py_files\" : [], \"spark_config\" : { \"spark.sql.session.timeZone\" : \"GMT\" , \"spark.python.profile\" : \"false\" , \"spark.yarn.appMasterEnv.ARROW_PRE_0_15_IPC_FORMAT\" : \"1\" , \"spark.executorEnv.ARROW_PRE_0_15_IPC_FORMAT\" : \"1\" , \"spark.sql.session.timeZone\" : \"GMT\" , \"spark.python.profile\" : \"false\" , }, } spark , sc , sqlContext = init_spark ( ** configs )","title":"spark"},{"location":"api/shared/spark.html#functions","text":"def init_spark ( app_name='anovos', master='local[*]', jars_packages=None, py_files=None, spark_config=None)","title":"Functions"},{"location":"api/shared/utils.html","text":"utils Expand source code from itertools import chain from pyspark.sql import functions as F platform_root_path = { \"databricks\" : \"dbfs:/\" } def flatten_dataframe ( idf , fixed_cols ): \"\"\" Parameters ---------- idf Input Dataframe fixed_cols All columns except in this list will be melted/unpivoted Returns ------- \"\"\" valid_cols = [ e for e in idf . columns if e not in fixed_cols ] key_and_val = F . create_map ( list ( chain . from_iterable ([[ F . lit ( c ), F . col ( c )] for c in valid_cols ])) ) odf = idf . select ( * fixed_cols , F . explode ( key_and_val )) return odf def transpose_dataframe ( idf , fixed_col ): \"\"\" Parameters ---------- idf Input Dataframe fixed_col Values in this column will be converted into columns as header. Ideally all values should be unique Returns ------- \"\"\" idf_flatten = flatten_dataframe ( idf , fixed_cols = [ fixed_col ]) odf = idf_flatten . groupBy ( \"key\" ) . pivot ( fixed_col ) . agg ( F . first ( \"value\" )) return odf def attributeType_segregation ( idf ): \"\"\" Parameters ---------- idf Input Dataframe Returns ------- \"\"\" cat_cols = [] num_cols = [] other_cols = [] for i in idf . dtypes : if i [ 1 ] == \"string\" : cat_cols . append ( i [ 0 ]) elif ( i [ 1 ] in ( \"double\" , \"int\" , \"bigint\" , \"float\" , \"long\" )) | ( i [ 1 ] . startswith ( \"decimal\" ) ): num_cols . append ( i [ 0 ]) else : other_cols . append ( i [ 0 ]) return num_cols , cat_cols , other_cols def get_dtype ( idf , col ): \"\"\" Parameters ---------- idf Input Dataframe col Column Name for datatype detection Returns ------- \"\"\" return [ dtype for name , dtype in idf . dtypes if name == col ][ 0 ] def ends_with ( string , end_str = \"/\" ): \"\"\" Parameters ---------- string \"s3:mw-bucket\" end_str return: \"s3:mw-bucket/\" (Default value = \"/\") Returns ------- \"\"\" string = str ( string ) if string . endswith ( end_str ): return string return string + end_str def pairwise_reduce ( op , x ): \"\"\" Parameters ---------- op Operation x Input list Returns ------- \"\"\" while len ( x ) > 1 : v = [ op ( i , j ) for i , j in zip ( x [:: 2 ], x [ 1 :: 2 ])] if len ( x ) > 1 and len ( x ) % 2 == 1 : v [ - 1 ] = op ( v [ - 1 ], x [ - 1 ]) x = v return x [ 0 ] def output_to_local ( output_path ): \"\"\" Parameters ---------- output_path : input_path. e.g. dbfs:/sample_path Returns ------- type path after removing \":\" and appending \"/\" . e.g. /dbfs/sample_path \"\"\" punctuations = \":\" for x in output_path : if x in punctuations : local_path = output_path . replace ( x , \"\" ) local_path = \"/\" + local_path return local_path Functions def attributeType_segregation ( idf) Parameters idf Input Dataframe Returns Expand source code def attributeType_segregation ( idf ): \"\"\" Parameters ---------- idf Input Dataframe Returns ------- \"\"\" cat_cols = [] num_cols = [] other_cols = [] for i in idf . dtypes : if i [ 1 ] == \"string\" : cat_cols . append ( i [ 0 ]) elif ( i [ 1 ] in ( \"double\" , \"int\" , \"bigint\" , \"float\" , \"long\" )) | ( i [ 1 ] . startswith ( \"decimal\" ) ): num_cols . append ( i [ 0 ]) else : other_cols . append ( i [ 0 ]) return num_cols , cat_cols , other_cols def ends_with ( string, end_str='/') Parameters string \"s3:mw-bucket\" end_str return: \"s3:mw-bucket/\" (Default value = \"/\") Returns Expand source code def ends_with ( string , end_str = \"/\" ): \"\"\" Parameters ---------- string \"s3:mw-bucket\" end_str return: \"s3:mw-bucket/\" (Default value = \"/\") Returns ------- \"\"\" string = str ( string ) if string . endswith ( end_str ): return string return string + end_str def flatten_dataframe ( idf, fixed_cols) Parameters idf Input Dataframe fixed_cols All columns except in this list will be melted/unpivoted Returns Expand source code def flatten_dataframe ( idf , fixed_cols ): \"\"\" Parameters ---------- idf Input Dataframe fixed_cols All columns except in this list will be melted/unpivoted Returns ------- \"\"\" valid_cols = [ e for e in idf . columns if e not in fixed_cols ] key_and_val = F . create_map ( list ( chain . from_iterable ([[ F . lit ( c ), F . col ( c )] for c in valid_cols ])) ) odf = idf . select ( * fixed_cols , F . explode ( key_and_val )) return odf def get_dtype ( idf, col) Parameters idf Input Dataframe col Column Name for datatype detection Returns Expand source code def get_dtype ( idf , col ): \"\"\" Parameters ---------- idf Input Dataframe col Column Name for datatype detection Returns ------- \"\"\" return [ dtype for name , dtype in idf . dtypes if name == col ][ 0 ] def output_to_local ( output_path) Parameters output_path : input_path. e.g. dbfs:/sample_path Returns type path after removing \":\" and appending \"/\" . e.g. /dbfs/sample_path Expand source code def output_to_local ( output_path ): \"\"\" Parameters ---------- output_path : input_path. e.g. dbfs:/sample_path Returns ------- type path after removing \":\" and appending \"/\" . e.g. /dbfs/sample_path \"\"\" punctuations = \":\" for x in output_path : if x in punctuations : local_path = output_path . replace ( x , \"\" ) local_path = \"/\" + local_path return local_path def pairwise_reduce ( op, x) Parameters op Operation x Input list Returns Expand source code def pairwise_reduce ( op , x ): \"\"\" Parameters ---------- op Operation x Input list Returns ------- \"\"\" while len ( x ) > 1 : v = [ op ( i , j ) for i , j in zip ( x [:: 2 ], x [ 1 :: 2 ])] if len ( x ) > 1 and len ( x ) % 2 == 1 : v [ - 1 ] = op ( v [ - 1 ], x [ - 1 ]) x = v return x [ 0 ] def transpose_dataframe ( idf, fixed_col) Parameters idf Input Dataframe fixed_col Values in this column will be converted into columns as header. Ideally all values should be unique Returns Expand source code def transpose_dataframe ( idf , fixed_col ): \"\"\" Parameters ---------- idf Input Dataframe fixed_col Values in this column will be converted into columns as header. Ideally all values should be unique Returns ------- \"\"\" idf_flatten = flatten_dataframe ( idf , fixed_cols = [ fixed_col ]) odf = idf_flatten . groupBy ( \"key\" ) . pivot ( fixed_col ) . agg ( F . first ( \"value\" )) return odf","title":"<code>utils</code>"},{"location":"api/shared/utils.html#utils","text":"Expand source code from itertools import chain from pyspark.sql import functions as F platform_root_path = { \"databricks\" : \"dbfs:/\" } def flatten_dataframe ( idf , fixed_cols ): \"\"\" Parameters ---------- idf Input Dataframe fixed_cols All columns except in this list will be melted/unpivoted Returns ------- \"\"\" valid_cols = [ e for e in idf . columns if e not in fixed_cols ] key_and_val = F . create_map ( list ( chain . from_iterable ([[ F . lit ( c ), F . col ( c )] for c in valid_cols ])) ) odf = idf . select ( * fixed_cols , F . explode ( key_and_val )) return odf def transpose_dataframe ( idf , fixed_col ): \"\"\" Parameters ---------- idf Input Dataframe fixed_col Values in this column will be converted into columns as header. Ideally all values should be unique Returns ------- \"\"\" idf_flatten = flatten_dataframe ( idf , fixed_cols = [ fixed_col ]) odf = idf_flatten . groupBy ( \"key\" ) . pivot ( fixed_col ) . agg ( F . first ( \"value\" )) return odf def attributeType_segregation ( idf ): \"\"\" Parameters ---------- idf Input Dataframe Returns ------- \"\"\" cat_cols = [] num_cols = [] other_cols = [] for i in idf . dtypes : if i [ 1 ] == \"string\" : cat_cols . append ( i [ 0 ]) elif ( i [ 1 ] in ( \"double\" , \"int\" , \"bigint\" , \"float\" , \"long\" )) | ( i [ 1 ] . startswith ( \"decimal\" ) ): num_cols . append ( i [ 0 ]) else : other_cols . append ( i [ 0 ]) return num_cols , cat_cols , other_cols def get_dtype ( idf , col ): \"\"\" Parameters ---------- idf Input Dataframe col Column Name for datatype detection Returns ------- \"\"\" return [ dtype for name , dtype in idf . dtypes if name == col ][ 0 ] def ends_with ( string , end_str = \"/\" ): \"\"\" Parameters ---------- string \"s3:mw-bucket\" end_str return: \"s3:mw-bucket/\" (Default value = \"/\") Returns ------- \"\"\" string = str ( string ) if string . endswith ( end_str ): return string return string + end_str def pairwise_reduce ( op , x ): \"\"\" Parameters ---------- op Operation x Input list Returns ------- \"\"\" while len ( x ) > 1 : v = [ op ( i , j ) for i , j in zip ( x [:: 2 ], x [ 1 :: 2 ])] if len ( x ) > 1 and len ( x ) % 2 == 1 : v [ - 1 ] = op ( v [ - 1 ], x [ - 1 ]) x = v return x [ 0 ] def output_to_local ( output_path ): \"\"\" Parameters ---------- output_path : input_path. e.g. dbfs:/sample_path Returns ------- type path after removing \":\" and appending \"/\" . e.g. /dbfs/sample_path \"\"\" punctuations = \":\" for x in output_path : if x in punctuations : local_path = output_path . replace ( x , \"\" ) local_path = \"/\" + local_path return local_path","title":"utils"},{"location":"api/shared/utils.html#functions","text":"def attributeType_segregation ( idf)","title":"Functions"},{"location":"community/code-of-conduct.html","text":"Anovos Code of Conduct Our Pledge We as members, contributors, and leaders pledge to make participation in our community a harassment-free experience for everyone, regardless of age, body size, visible or invisible disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, caste, color, religion, or sexual identity and orientation. We pledge to act and interact in ways that contribute to an open, welcoming, diverse, inclusive, and healthy community. Our Standards Examples of behavior that contributes to a positive environment for our community include: Demonstrating empathy and kindness toward other people Being respectful of differing opinions, viewpoints, and experiences Giving and gracefully accepting constructive feedback Accepting responsibility and apologizing to those affected by our mistakes, and learning from the experience Focusing on what is best not just for us as individuals, but for the overall community Examples of unacceptable behavior include: The use of sexualized language or imagery, and sexual attention or advances of any kind Trolling, insulting or derogatory comments, and personal or political attacks Public or private harassment Publishing others' private information, such as a physical or email address, without their explicit permission Other conduct which could reasonably be considered inappropriate in a professional setting Enforcement Responsibilities Community leaders are responsible for clarifying and enforcing our standards of acceptable behavior and will take appropriate and fair corrective action in response to any behavior that they deem inappropriate, threatening, offensive, or harmful. Community leaders have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, and will communicate reasons for moderation decisions when appropriate. Scope This Code of Conduct applies within all community spaces, and also applies when an individual is officially representing the community in public spaces. Examples of representing our community include using an official e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event. Enforcement Instances of abusive, harassing, or otherwise unacceptable behavior may be reported to the community leaders responsible for enforcement at coc@anovos.ai. All complaints will be reviewed and investigated promptly and fairly. All community leaders are obligated to respect the privacy and security of the reporter of any incident. Enforcement Guidelines Community leaders will follow these Community Impact Guidelines in determining the consequences for any action they deem in violation of this Code of Conduct: 1. Correction Community Impact : Use of inappropriate language or other behavior deemed unprofessional or unwelcome in the community. Consequence : A private, written warning from community leaders, providing clarity around the nature of the violation and an explanation of why the behavior was inappropriate. A public apology may be requested. 2. Warning Community Impact : A violation through a single incident or series of actions. Consequence : A warning with consequences for continued behavior. No interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, for a specified period of time. This includes avoiding interactions in community spaces as well as external channels like social media. Violating these terms may lead to a temporary or permanent ban. 3. Temporary Ban Community Impact : A serious violation of community standards, including sustained inappropriate behavior. Consequence : A temporary ban from any sort of interaction or public communication with the community for a specified period of time. No public or private interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, is allowed during this period. Violating these terms may lead to a permanent ban. 4. Permanent Ban Community Impact : Demonstrating a pattern of violation of community standards, including sustained inappropriate behavior, harassment of an individual, or aggression toward or disparagement of classes of individuals. Consequence : A permanent ban from any sort of public interaction within the community. Attribution This Code of Conduct is adapted from the Contributor Covenant , version 2.1, available at https://www.contributor-covenant.org/version/2/1/code_of_conduct.html . Community Impact Guidelines were inspired by Mozilla's code of conduct enforcement ladder . For answers to common questions about this code of conduct, see the FAQ at https://www.contributor-covenant.org/faq . Translations are available at https://www.contributor-covenant.org/translations .","title":"Code of Conduct"},{"location":"community/code-of-conduct.html#anovos-code-of-conduct","text":"","title":"Anovos Code of Conduct"},{"location":"community/code-of-conduct.html#our-pledge","text":"We as members, contributors, and leaders pledge to make participation in our community a harassment-free experience for everyone, regardless of age, body size, visible or invisible disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, caste, color, religion, or sexual identity and orientation. We pledge to act and interact in ways that contribute to an open, welcoming, diverse, inclusive, and healthy community.","title":"Our Pledge"},{"location":"community/code-of-conduct.html#our-standards","text":"Examples of behavior that contributes to a positive environment for our community include: Demonstrating empathy and kindness toward other people Being respectful of differing opinions, viewpoints, and experiences Giving and gracefully accepting constructive feedback Accepting responsibility and apologizing to those affected by our mistakes, and learning from the experience Focusing on what is best not just for us as individuals, but for the overall community Examples of unacceptable behavior include: The use of sexualized language or imagery, and sexual attention or advances of any kind Trolling, insulting or derogatory comments, and personal or political attacks Public or private harassment Publishing others' private information, such as a physical or email address, without their explicit permission Other conduct which could reasonably be considered inappropriate in a professional setting","title":"Our Standards"},{"location":"community/code-of-conduct.html#enforcement-responsibilities","text":"Community leaders are responsible for clarifying and enforcing our standards of acceptable behavior and will take appropriate and fair corrective action in response to any behavior that they deem inappropriate, threatening, offensive, or harmful. Community leaders have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, and will communicate reasons for moderation decisions when appropriate.","title":"Enforcement Responsibilities"},{"location":"community/code-of-conduct.html#scope","text":"This Code of Conduct applies within all community spaces, and also applies when an individual is officially representing the community in public spaces. Examples of representing our community include using an official e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event.","title":"Scope"},{"location":"community/code-of-conduct.html#enforcement","text":"Instances of abusive, harassing, or otherwise unacceptable behavior may be reported to the community leaders responsible for enforcement at coc@anovos.ai. All complaints will be reviewed and investigated promptly and fairly. All community leaders are obligated to respect the privacy and security of the reporter of any incident.","title":"Enforcement"},{"location":"community/code-of-conduct.html#enforcement-guidelines","text":"Community leaders will follow these Community Impact Guidelines in determining the consequences for any action they deem in violation of this Code of Conduct: 1. Correction Community Impact : Use of inappropriate language or other behavior deemed unprofessional or unwelcome in the community. Consequence : A private, written warning from community leaders, providing clarity around the nature of the violation and an explanation of why the behavior was inappropriate. A public apology may be requested. 2. Warning Community Impact : A violation through a single incident or series of actions. Consequence : A warning with consequences for continued behavior. No interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, for a specified period of time. This includes avoiding interactions in community spaces as well as external channels like social media. Violating these terms may lead to a temporary or permanent ban. 3. Temporary Ban Community Impact : A serious violation of community standards, including sustained inappropriate behavior. Consequence : A temporary ban from any sort of interaction or public communication with the community for a specified period of time. No public or private interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, is allowed during this period. Violating these terms may lead to a permanent ban. 4. Permanent Ban Community Impact : Demonstrating a pattern of violation of community standards, including sustained inappropriate behavior, harassment of an individual, or aggression toward or disparagement of classes of individuals. Consequence : A permanent ban from any sort of public interaction within the community.","title":"Enforcement Guidelines"},{"location":"community/code-of-conduct.html#attribution","text":"This Code of Conduct is adapted from the Contributor Covenant , version 2.1, available at https://www.contributor-covenant.org/version/2/1/code_of_conduct.html . Community Impact Guidelines were inspired by Mozilla's code of conduct enforcement ladder . For answers to common questions about this code of conduct, see the FAQ at https://www.contributor-covenant.org/faq . Translations are available at https://www.contributor-covenant.org/translations .","title":"Attribution"},{"location":"community/communication.html","text":"Anovos Communication Channels There are different ways and channels that you can use as a community member or contributor to ask questions, meet fellow Anovos users, and collaborate on improving the library. \ud83d\udcac Slack The Anovos community has a channel on the Feature Engineers Slack Workspace called #anovos . Feel free to join us there! It's the perfect place get in touch with the community as well as the maintainers of Anovos . \ud83d\udea7 GitHub Issues If you encounter a bug or have a feature request, please file an issue on GitHub . We'll make sure to get back to you in time. \u2709 Contributor Mailing List If you want to follow discussions about the development of Anovos and perhaps would like to become a contributor, please sign up for our Mailing List . Don't worry: On the list, the focus is 100% on ongoing and future development, you won't be spammed with general questions or support requests. \ud83d\udc65 Monthly Contributor Meeting We have a monthly meeting to sync ongoing work and discuss feature requests. You will automatically receive an invitation when joining the contributor mailing list. For each meeting, an agenda is shared ahead of time via Google Docs. Feel free to add any items you would like to discuss.","title":"Communication"},{"location":"community/communication.html#anovos-communication-channels","text":"There are different ways and channels that you can use as a community member or contributor to ask questions, meet fellow Anovos users, and collaborate on improving the library.","title":"Anovos Communication Channels"},{"location":"community/communication.html#slack","text":"The Anovos community has a channel on the Feature Engineers Slack Workspace called #anovos . Feel free to join us there! It's the perfect place get in touch with the community as well as the maintainers of Anovos .","title":"\ud83d\udcac Slack"},{"location":"community/communication.html#github-issues","text":"If you encounter a bug or have a feature request, please file an issue on GitHub . We'll make sure to get back to you in time.","title":"\ud83d\udea7 GitHub Issues"},{"location":"community/communication.html#contributor-mailing-list","text":"If you want to follow discussions about the development of Anovos and perhaps would like to become a contributor, please sign up for our Mailing List . Don't worry: On the list, the focus is 100% on ongoing and future development, you won't be spammed with general questions or support requests.","title":"\u2709 Contributor Mailing List"},{"location":"community/communication.html#monthly-contributor-meeting","text":"We have a monthly meeting to sync ongoing work and discuss feature requests. You will automatically receive an invitation when joining the contributor mailing list. For each meeting, an agenda is shared ahead of time via Google Docs. Feel free to add any items you would like to discuss.","title":"\ud83d\udc65 Monthly Contributor Meeting"},{"location":"community/contributing.html","text":"Contributing to Anovos We'd love to have you join us in making Anovos the number one choice for ML feature engineering! \ud83d\ude80 Getting Started with Anovos Development Anovos is an open source project that brings automation to the feature engineering process. To get Anovos up and running on your local machine, follow the Getting Started Guide . The Anovos GitHub Organization contains all the repositories, sample data, and notebooks you need to start working on improvements to the library. \ud83d\udee0 How to Get Involved First of all: We appreciate each and every contribution, and we'd love to get your input! Contributions to Anovos can take many different shapes and forms, for example: Reporting a bug Discussing the current state of the code Submitting a fix Proposing new features Testing out new features Contributing to the docs Giving talks about Anovos at meetups, conferences, and webinars If you're interested in contributing but don't quite know where to start, please don't hesitate to reach out to the maintainers . \u2328 Contributing Code to Anovos Pull requests are the best way to propose changes to the codebase. We follow the GitHub Flow pattern, and everything happens through pull requests. We welcome pull requests by community contributors at all times. To make it simple, please follow these steps to contribute: Fork the repo you're updating and create your branch from main. If you've added code, add the corresponding tests. If you've changed APIs, update the documentation. Ensure that the test suite passes. Issue that pull request! \u2139 Any contributions you make will be under the Apache Software License 2.0. See the License page for more information. \ud83d\udcdd Conventions for Commit Messages Help reviewers know what you're contributing by writing good commit messages. The first line of the commit message is the subject; this should be followed by a blank line and then a message describing the intent and purpose of the commit. We based these guidelines on a post by Chris Beams . When you commit, you are accepting our DCO: Developer Certificate of Origin Version 1.1 Copyright (C) 2004, 2006 The Linux Foundation and its contributors. 1 Letterman Drive Suite D4700 San Francisco, CA, 94129 Everyone is permitted to copy and distribute verbatim copies of this license document, but changing it is not allowed. Developer's Certificate of Origin 1.1 By making a contribution to this project, I certify that: (a) The contribution was created in whole or in part by me and I have the right to submit it under the open source license indicated in the file; or (b) The contribution is based upon previous work that, to the best of my knowledge, is covered under an appropriate open source license and I have the right under that license to submit that work with modifications, whether created in whole or in part by me, under the same open source license (unless I am permitted to submit under a different license), as indicated in the file; or (c) The contribution was provided directly to me by some other person who certified (a), (b) or (c) and I have not modified it. (d) I understand and agree that this project and the contribution are public and that a record of the contribution (including all personal information I submit with it, including my sign-off) is maintained indefinitely and may be redistributed consistent with this project or the open source license(s) involved. When you run git commit make sure you sign-off the commit by typing git commit --signoff or git commit -s The commit subject-line should start with an uppercase letter The commit subject-line should not exceed 72 characters in length The commit subject-line should not end with punctuation (., etc) Note: please do not use the GitHub suggestions feature, since it will not allow your commits to be signed-off. When giving a commit body, be sure to: Leave a blank line after the subject-line Make sure all lines are wrapped to 72 characters Here's an example that would be accepted: Add luke to the contributors' _index.md file We need to add luke to the contributors' _index.md file as a contributor. Signed-off-by: Hans <hans@anovos.ai> Some invalid examples: (feat) Add page about X to documentation This example does not follow the convention by adding a custom scheme of (feat) Update the documentation for page X so including fixing A, B, C and D and F. This example will be truncated in the GitHub UI and via git log --oneline If you would like to amend your commit, follow this guide: Git: Rewriting History","title":"Contributing"},{"location":"community/contributing.html#contributing-to-anovos","text":"We'd love to have you join us in making Anovos the number one choice for ML feature engineering!","title":"Contributing to Anovos"},{"location":"community/contributing.html#getting-started-with-anovos-development","text":"Anovos is an open source project that brings automation to the feature engineering process. To get Anovos up and running on your local machine, follow the Getting Started Guide . The Anovos GitHub Organization contains all the repositories, sample data, and notebooks you need to start working on improvements to the library.","title":"\ud83d\ude80 Getting Started with Anovos Development"},{"location":"community/contributing.html#how-to-get-involved","text":"First of all: We appreciate each and every contribution, and we'd love to get your input! Contributions to Anovos can take many different shapes and forms, for example: Reporting a bug Discussing the current state of the code Submitting a fix Proposing new features Testing out new features Contributing to the docs Giving talks about Anovos at meetups, conferences, and webinars If you're interested in contributing but don't quite know where to start, please don't hesitate to reach out to the maintainers .","title":"\ud83d\udee0 How to Get Involved"},{"location":"community/contributing.html#contributing-code-to-anovos","text":"Pull requests are the best way to propose changes to the codebase. We follow the GitHub Flow pattern, and everything happens through pull requests. We welcome pull requests by community contributors at all times. To make it simple, please follow these steps to contribute: Fork the repo you're updating and create your branch from main. If you've added code, add the corresponding tests. If you've changed APIs, update the documentation. Ensure that the test suite passes. Issue that pull request! \u2139 Any contributions you make will be under the Apache Software License 2.0. See the License page for more information.","title":"\u2328 Contributing Code to Anovos"},{"location":"community/contributing.html#conventions-for-commit-messages","text":"Help reviewers know what you're contributing by writing good commit messages. The first line of the commit message is the subject; this should be followed by a blank line and then a message describing the intent and purpose of the commit. We based these guidelines on a post by Chris Beams . When you commit, you are accepting our DCO: Developer Certificate of Origin Version 1.1 Copyright (C) 2004, 2006 The Linux Foundation and its contributors. 1 Letterman Drive Suite D4700 San Francisco, CA, 94129 Everyone is permitted to copy and distribute verbatim copies of this license document, but changing it is not allowed. Developer's Certificate of Origin 1.1 By making a contribution to this project, I certify that: (a) The contribution was created in whole or in part by me and I have the right to submit it under the open source license indicated in the file; or (b) The contribution is based upon previous work that, to the best of my knowledge, is covered under an appropriate open source license and I have the right under that license to submit that work with modifications, whether created in whole or in part by me, under the same open source license (unless I am permitted to submit under a different license), as indicated in the file; or (c) The contribution was provided directly to me by some other person who certified (a), (b) or (c) and I have not modified it. (d) I understand and agree that this project and the contribution are public and that a record of the contribution (including all personal information I submit with it, including my sign-off) is maintained indefinitely and may be redistributed consistent with this project or the open source license(s) involved. When you run git commit make sure you sign-off the commit by typing git commit --signoff or git commit -s The commit subject-line should start with an uppercase letter The commit subject-line should not exceed 72 characters in length The commit subject-line should not end with punctuation (., etc) Note: please do not use the GitHub suggestions feature, since it will not allow your commits to be signed-off. When giving a commit body, be sure to: Leave a blank line after the subject-line Make sure all lines are wrapped to 72 characters Here's an example that would be accepted: Add luke to the contributors' _index.md file We need to add luke to the contributors' _index.md file as a contributor. Signed-off-by: Hans <hans@anovos.ai> Some invalid examples: (feat) Add page about X to documentation This example does not follow the convention by adding a custom scheme of (feat) Update the documentation for page X so including fixing A, B, C and D and F. This example will be truncated in the GitHub UI and via git log --oneline If you would like to amend your commit, follow this guide: Git: Rewriting History","title":"\ud83d\udcdd Conventions for Commit Messages"},{"location":"using-anovos/config_file.html","text":"Configuring Workloads Anovos workloads can be described by a YAML configuration file. Such a configuration file defines: the input dataset(s) the analyses and transformations to be performed on the data the output files and dataset(s) the reports to be generated Defining workloads this way allows users to make full use of Anovos capabilities while maintaining an easy-to-grasp overview. Since each configuration file fully describes one workload, these files can be shared, versioned, and run across different compute environments. In the following, we'll describe in detail each of the sections in an Anovos configuration file. If you'd rather see a full example right away, have a look at this example . Note that each section of the configuration file maps to a module of Anovos . You'll find links to the respective sections of the API Documentation that provide much more detailed information on each modules' capabilities than we can squeeze into this guide. \ud83d\udcd1 input_dataset This configuration block describes how the input dataset is loaded and prepared using the data_ingest.data_ingest module. Each Anovos configuration file must contain exactly one input_dataset block. Note that the subsequent operations are performed in the order given here: First, columns are deleted, then selected, then renamed, and then recast. read_dataset \ud83d\udd0e Corresponds to data_ingest.read_dataset file_path : The file (or directory) path to read the input dataset from. It can be a local path, an \ud83d\udcd6 S3 path (when running on AWS), a path to a file resource on Google Colab (see \ud83d\udcd6 this tutorial for an overview), or a path on the \ud83d\udcd6 Databricks File System (when running on Azure). file_type : The file format of the input data. Currently, Anovos supports CSV ( csv ), Parquet ( parquet ), and Avro ( avro ). (Please note that if you're using Avro data sources, you need to add the external package org.apache.spark:spark-avro when submitting the Spark job.) file_configs (optional): Options to pass to the respective Spark file reader, e.g., delimiters, schemas, headers. In the case of a CSV file, this might look like: file_configs : delimiter : \", \" header : True inferSchema : True For more information on available configuration options, see the following external documentation: Read CSV files Read Parquet files Read Avro files delete_column \ud83d\udd0e Corresponds to data_ingest.delete_column List of column names (list of strings or string of column names separated by | ) to be deleted from the loaded input data. \ud83e\udd13 Example: delete_column : [ 'unnecessary' , 'obsolete' , 'outdated' ] select_column \ud83d\udd0e Corresponds to data_ingest.select_column List of column names (list of strings or string of column names separated by | ) to be selected for further processing. \ud83e\udd13 Example: select_column : [ 'feature1' , 'feature2' , 'feature3' , 'label' ] rename_column \ud83d\udd0e Corresponds to data_ingest.rename_column list_of_cols : List of the names of columns (list of strings or string of column names separated by | ) to be renamed. list_of_newcols : The new column names. The first element in list_of_cols will be renamed to the first name in list_of_newcols and so on. \ud83e\udd13 Example: rename_column : list_of_cols : [ 'very_long_column_name' , 'price' ] list_of_newcols : [ 'short_name' , 'label' ] This will rename the column very_long_column_name to short_name and the column price to label . recast_column \ud83d\udd0e Corresponds to data_ingest.recast_column list_of_cols : List of the names of columns (list of strings or string of column names separated by | ) to be cast to a different type. list_of_dtypes : The new datatypes. The first element in list_of_cols will be recast to the first type in list_of_dtypes and so on. See \ud83d\udcd6 the Spark documentation for a list of valid datatypes. Note that this field is case-insensitive. \ud83e\udd13 Example: recast_column : list_of_cols : [ 'price' , 'quantity' ] list_of_dtypes : [ 'double' , 'int' ] \ud83d\udcd1 concatenate_dataset \ud83d\udd0e Corresponds to data_ingest.concatenate_dataset This configuration block describes how to combine multiple loaded dataframes into a single one. method There are two different methods to concatenate dataframes: index : Concatenate by column index, i.e., the first column of the first dataframe is matched with the first column of the second dataframe and so forth. name : Concatenate by column name, i.e., columns of the same name are matched. Note that in both cases, the first dataframe will define both the names and the order of the columns in the final dataframe. If the subsequent dataframes have too few columns ( index ) or are missing named columns (`name\u00b4) for the concatenation to proceed, an error will be raised. \ud83e\udd13 Example: method : name dataset1 read_dataset \ud83d\udd0e Corresponds to data_ingest.read_dataset file_path : The file (or directory) path to read the other concatenating input dataset from. It can be a local path, an \ud83d\udcd6 S3 path (when running on AWS), a path to a file resource on Google Colab (see \ud83d\udcd6 this tutorial for an overview), or a path on the \ud83d\udcd6 Databricks File System (when running on Azure). file_type : The file format of the other concatenating input data. Currently, Anovos supports CSV ( csv ), Parquet ( parquet ), and Avro ( avro ). (Please note that if you're using Avro data sources, you need to add the external package org.apache.spark:spark-avro when submitting the Spark job.) file_configs (optional): Options to pass to the respective Spark file reader, e.g., delimiters, schemas, headers. delete_column \ud83d\udd0e Corresponds to data_ingest.delete_column List of column names (list of strings or string of column names separated by | ) to be deleted from the loaded input data. select_column \ud83d\udd0e Corresponds to data_ingest.select_column List of column names (list of strings or string of column names separated by | ) to be selected for further processing. rename_column \ud83d\udd0e Corresponds to data_ingest.rename_column list_of_cols : List of the names of columns (list of strings or string of column names separated by | ) to be renamed. list_of_newcols : The new column names. The first element in list_of_cols will be renamed to the first name in list_of_newcols and so on. recast_column \ud83d\udd0e Corresponds to data_ingest.recast_column list_of_cols : List of the names of columns (list of strings or string of column names separated by | ) to be cast to a different type. list_of_dtypes : The new datatypes. The first element in list_of_cols will be recast to the first type in list_of_dtypes and so on. See \ud83d\udcd6 the Spark documentation for a list of valid datatypes. Note that this field is case-insensitive. dataset2 , dataset3 , ... Additional datasets are configured in the same manner as dataset1 . \ud83d\udcd1 join_dataset \ud83d\udd0e Corresponds to data_ingest.join_dataset This configuration block describes how multiple dataframes are joined into a single one. join_cols The key of the column(s) to join on. In the case that the key consists of multiple columns, they can be passed as a list of strings or a single string where the column names are separated by | . \ud83e\udd13 Example: join_cols : id_column join_type The type of join to perform: inner , full , left , right , left_semi , or left_anti . For a general introduction to joins, see \ud83d\udcd6 this tutorial . \ud83e\udd13 Example: join_type : inner dataset1 read_dataset \ud83d\udd0e Corresponds to data_ingest.read_dataset file_path : The file (or directory) path to read the other joining input dataset from. It can be a local path, an \ud83d\udcd6 S3 path (when running on AWS), a path to a file resource on Google Colab (see \ud83d\udcd6 this tutorial for an overview), or a path on the \ud83d\udcd6 Databricks File System (when running on Azure). file_type : The file format of the other joining input data. Currently, Anovos supports CSV ( csv ), Parquet ( parquet ), and Avro ( avro ). (Please note that if you're using Avro data sources, you need to add the external package org.apache.spark:spark-avro when submitting the Spark job.) file_configs (optional): Options to pass to the respective Spark file reader, e.g., delimiters, schemas, headers. delete_column \ud83d\udd0e Corresponds to data_ingest.delete_column List of column names (list of strings or string of column names separated by | ) to be deleted from the loaded input data. select_column \ud83d\udd0e Corresponds to data_ingest.select_column List of column names (list of strings or string of column names separated by | ) to be selected for further processing. rename_column \ud83d\udd0e Corresponds to data_ingest.rename_column list_of_cols : List of the names of columns (list of strings or string of column names separated by | ) to be renamed. list_of_newcols : The new column names. The first element in list_of_cols will be renamed to the first name in list_of_newcols and so on. recast_column \ud83d\udd0e Corresponds to data_ingest.recast_column list_of_cols : List of the names of columns (list of strings or string of column names separated by | ) to be cast to a different type. list_of_dtypes : The new datatypes. The first element in list_of_cols will be recast to the first type in list_of_dtypes and so on. See \ud83d\udcd6 the Spark documentation for a list of valid datatypes. Note that this field is case-insensitive. dataset2 , dataset3 , ... Additional datasets are configured in the same manner as dataset1 . \ud83d\udcd1 timeseries_analyzer \ud83d\udd0e Corresponds to data_analyzer.ts_analyzer Configuration for the time series analyzer. auto_detection : Can be set to True or False . If True , it attempts to automatically infer the date/timestamp format in the input dataset. id_col : Name of the ID column in the input dataset. tz_offset : The timezone offset of the timestamps in the input dataset. Can be set to either local , gmt , or utc . The default setting is local . inspection : Can be set to True or False . If True , the time series elements undergo an inspection. analysis_level : Can be set to daily , weekly , or hourly . The default setting is daily . If set to daily , the daily view is populated. If set to hourly , the view is shown at a day part level. If set to weekly , the display it per individual weekdays (1-7) as captured. max_days : Maximum number of days up to which the data will be aggregated. If the dataset contains a timestamp/date field with very high number of unique dates (e.g., 20 years worth of daily data), this option can be used to reduce the timespan that is analyzed. \ud83e\udd13 Example: timeseries_analyzer : auto_detection : True id_col : 'id_column' tz_offset : 'local' inspection : True analysis_level : 'daily' max_days : 3600 \ud83d\udcd1 anovos_basic_report \ud83d\udd0e Corresponds to data_report.basic_report_generation The basic report consists of a summary of the outputs of the stats_generator , quality_checker , and association evaluator See the \ud83d\udcd6 documentation for data reports for more details. The basic report can be customized using the following options: basic_report If True , a basic report is generated after completion of the data_analyzer modules. If False , no report is generated. Nevertheless, all the computed statistics and metrics will be available in the final report. report_args id_col : The name of the ID column in the input dataset. label_col : The name of the label or target column in the input dataset. event_lable : The value of the event (label 1 / true ) in the label column. output_path : Path where the basic report is saved. It can be a local path, an \ud83d\udcd6 S3 path (when running on AWS), a path to a file resource on Google Colab (see \ud83d\udcd6 this tutorial for an overview), or a path on the \ud83d\udcd6 Databricks File System (when running on Azure). \ud83e\udd13 Example: report_args : id_col : id_column label_col : label_col event_label : 'class1' output_path : report_stats \ud83d\udcd1 stats_generator \ud83d\udd0e Corresponds to data_analyzer.stats_generator This module generates descriptive statistics of the ingested data. Descriptive statistics are split into different metric types. Each function corresponds to one metric type. metric List of metrics to calculate for the input dataset. Available options are: \ud83d\udcd6 global_summary \ud83d\udcd6 measures_of_count \ud83d\udcd6 measures_of_centralTendency \ud83d\udcd6 measures_of_cardinality \ud83d\udcd6 measures_of_dispersion \ud83d\udcd6 measures_of_percentiles \ud83d\udcd6 measures_of_shape \ud83e\udd13 Example: metric : [ 'global_summary' , 'measures_of_counts' , 'measures_of_cardinality' , 'measures_of_dispersion' ] metric_args list_of_cols : List of column names (list of strings or string of column names separated by | ) to compute the metrics for. Alternatively, if set to \"all\" , all columns are included. drop_cols : List of column names (list of strings or string of column names separated by | ) to exclude from metrics computation. This option is especially useful if list_of_cols is set to \"all\" , as it allows computing metrics for all except a few columns without having to specify a potentially very long list of column names to include. \ud83e\udd13 Example: metric_args : list_of_cols : all drop_cols : [ 'id_column' ] \ud83d\udcd1 quality_checker \ud83d\udd0e Corresponds to data_analyzer.quality_checker This module assesses the data quality along different dimensions. Quality metrics are computed at both the row and column level. Further, the module includes appropriate treatment options to fix several common quality issues. duplicate_detection \ud83d\udd0e Corresponds to quality_checker.duplicate_detection list_of_cols : List of column names (list of strings or string of column names separated by | ) to consider when searching for duplicates. Alternatively, if set to \"all\" , all columns are included. drop_cols : List of column names (list of strings or string of column names separated by | ) to be excluded from duplicate detection. treatment : If False , duplicates are detected and reported. If True , duplicate rows are removed from the input dataset. \ud83e\udd13 Example: duplicate_detection : list_of_cols : all drop_cols : [ 'id_column' ] treatment : True nullRows_detection \ud83d\udd0e Corresponds to quality_checker.nullRows_detection list_of_cols : List of column names (list of strings or string of column names separated by | ) to consider during null rows detection. Alternatively, if set to \"all\" , all columns are included. drop_cols : List of column names (list of strings or string of column names separated by | ) to exclude from null rows detection. treatment : If False , null rows are detected and reported. If True , rows where more than treatment_threshold columns are null are removed from the input dataset. treatment_threshold : It takes a value between 0 and 1 (default 0.8 ) that specifies which fraction of columns has to be null for a row to be considered a null row. If the threshold is 0 , rows with any missing value will be flagged as null . If the threshold is 1 , only rows where all values are missing will be flagged as null . \ud83e\udd13 Example: nullRows_detection : list_of_cols : all drop_cols : [] treatment : True treatment_threshold : 0.75 invalidEntries_detection \ud83d\udd0e Corresponds to quality_checker.invalidEntries_detection list_of_cols : List of column names (list of strings or string of column names separated by | ) to be considered during invalid entries' detection. Alternatively, if set to \"all\" , all columns are included. drop_cols : List of column names (list of strings or string of column names separated by | ) to exclude from invalid entries' detection. treatment : If False , invalid entries are detected and reported. If True , invalid entries are replaced with null . output_mode : Can be either \"replace\" or \"append\" . If set to \"replace\" , the original columns will be replaced with the treated columns. If set to \"append\" , the original columns will be kept and the treated columns will be appended to the dataset. The appended columns will be named as the original column with a suffix \"_cleaned\" (e.g., the column \"cost_of_living_cleaned\" corresponds to the original column \"cost_of_living\" ). \ud83e\udd13 Example: invalidEntries_detection : list_of_cols : all drop_cols : [ 'id_column' ] treatment : True output_mode : replace IDness_detection \ud83d\udd0e Corresponds to quality_checker.IDness_detection list_of_cols : List of column names (list of strings or string of column names separated by | ) to be considered for IDness detection. Alternatively, if set to \"all\" , all columns are included. drop_cols : List of column names (list of strings or string of column names separated by | ) to exclude from IDness detection. treatment : If False , columns with high IDness are detected and reported. If True , columns with an IDness above treatment_threshold are removed. treatment_threshold : A value between 0 and 1 (default 1.0 ). \ud83e\udd13 Example: IDness_detection : list_of_cols : all drop_cols : [ 'id_column' ] treatment : True treatment_threshold : 0.9 biasedness_detection \ud83d\udd0e Corresponds to quality_checker.biasedness_detection list_of_cols : List of column names (list of strings or string of column names separated by | ) to be considered for biasedness detection. Alternatively, if set to \"all\" , all columns are included. drop_cols : List of column names (list of strings or string of column names separated by | ) to exclude from biasedness detection. treatment : If False , columns with high IDness are detected and reported. If True , columns with a bias above treatment_threshold are removed. treatment_threshold : A value between 0 and 1 (default 1.0 ). \ud83e\udd13 Example: biasedness_detection : list_of_cols : all drop_cols : [ 'label_col' ] treatment : True treatment_threshold : 0.98 outlier_detection \ud83d\udd0e Corresponds to quality_checker.outlier_detection list_of_cols : List of column names (list of strings or string of column names separated by | ) to be considered for outlier detection. Alternatively, if set to \"all\" , all columns are included. \u26a0 Note that any column that contains just a single value or only null values is not subjected to outlier detection even if it is selected under this argument. drop_cols : List of column names (list of strings or string of column names separated by | ) to exclude from outlier detection. detection_side : Whether outliers should be detected on the \"upper\" , the \"lower\" , or \"both\" sides. detection_configs : A map that defines the input parameters for different outlier detection methods. Possible keys are: pctile_lower (default 0.05 ) pctile_upper (default 0.95 ) stdev_lower (default 3.0 ) stdev_upper (default 3.0 ) IQR_lower (default 1.5 ) IQR_upper (default 1.5 ) min_validation (default 2 ) For details, see \ud83d\udcd6 the outlier_detection API documentation treatment : If False , outliers are detected and reported. If True , outliers are treated with the specified treatment_method . treatment_method : Specifies how outliers are treated. Possible options are \"null_replacement\" , \"row_removal\" , \"value_replacement\" . pre_existing_model : If True , the file specified under model_path with lower/upper bounds is loaded. If no such file exists, set to False (the default). model_path : The path to the file with lower/upper bounds. It can be a local path, an \ud83d\udcd6 S3 path (when running on AWS), a path to a file resource on Google Colab (see \ud83d\udcd6 this tutorial for an overview), or a path on the \ud83d\udcd6 Databricks File System (when running on Azure). If pre_existing_model is True , the pre-saved will be loaded from this location. If pre_existing_model is False , a file with lower/upper bounds will be saved at this location. By default, it is set to NA , indicating that there is neither a pre-saved file nor should such a file be generated. output_mode : Can be either \"replace\" or \"append\" . If set to \"replace\" , the original columns will be replaced with the treated columns. If set to \"append\" , the original columns will be kept and the treated columns will be appended to the dataset. The appended columns will be named as the original column with a suffix \"_outliered\" (e.g., the column \"cost_of_living_outliered\" corresponds to the original column \"cost_of_living\" ). \ud83e\udd13 Example: outlier_detection : list_of_cols : all drop_cols : [ 'id_column' , 'label_col' ] detection_side : upper detection_configs : pctile_lower : 0.05 pctile_upper : 0.90 stdev_lower : 3.0 stdev_upper : 3.0 IQR_lower : 1.5 IQR_upper : 1.5 min_validation : 2 treatment : True treatment_method : value_replacement pre_existing_model : False model_path : NA output_mode : replace nullColumns_detection \ud83d\udd0e Corresponds to quality_checker.nullColumns_detection list_of_cols : List of column names (list of strings or string of column names separated by | ) to be considered for null columns detection. Alternatively, if set to \"all\" , all columns are included. If set to \"missing\" (the default) only columns with missing values are included. One of the use cases where \"all\" may be preferable over \"missing\" is when the user wants to save the imputation model for future use. This can be useful, for example, if a column may not have missing values in the training dataset but missing values are acceptable in the test dataset. drop_cols : List of column names (list of strings or string of column names separated by | ) to be excluded from null columns detection. treatment : If False , null columns are detected and reported. If True , missing values are treated with the specified treatment_method . treatment_method : Specifies how null columns are treated. Possible values are \"MMM\" , \" row_removal\" , or \"column_removal\" . treatment_configs : Additional parameters for the treatment_method . If treatment_method is \"column_removal\" , the key treatment_threshold can be used to define the fraction of missing values above which a column is flagged as a null column and remove. If treatment_method is \"MMM\" , possible keys are the parameters of the imputation_MMM function. \ud83e\udd13 Example: nullColumns_detection : list_of_cols : all drop_cols : [ 'id_column' , 'label_col' ] treatment : True treatment_method : MMM treatment_configs : method_type : median pre_existing_model : False model_path : NA output_mode : replace \ud83d\udcd1 association_evaluator \ud83d\udd0e Corresponds to data_analyzer.association_evaluator This block configures the association evaluator that focuses on understanding the interaction between different attributes or the relationship between an attribute and a binary target variable. correlation_matrix \ud83d\udd0e Corresponds to association_evaluator.correlation_matrix list_of_cols : List of column names (list of strings or string of column names separated by | ) to include in the correlation matrix. Alternatively, when set to all , all columns are included. drop_cols : List of column names (list of strings or string of column names separated by | ) to be excluded from the correlation matrix. This is especially useful when almost all columns should be included in the correlation matrix: Set list_of_cols to all and drop the few excluded columns. \ud83e\udd13 Example: correlation_matrix : list_of_cols : all drop_cols : [ 'id_column' ] IV_calculation \ud83d\udd0e Corresponds to association_evaluator.IV_calculation list_of_cols : List of column names (list of strings or string of column names separated by | ) to include in the IV calculation. drop_cols : List of column names (list of strings or string of column names separated by | ) to exclude from IV calculation. label_col : Name of label or target column in the input dataset. event_label : Value of event (label 1 / true ) in the label column. encoding_configs : Detailed configuration of the binning step. bin_method : The binning method. Defaults to equal_frequency . bin_size : The bin size. Defaults to 10 . monotonicity_check : If set to 1 , dynamically computes the bin_size such that monotonicity is ensured. Can be a computationally expensive calculation. Defaults to 0 . \ud83e\udd13 Example: IV_calculation : list_of_cols : all drop_cols : id_column label_col : label_col event_label : 'class1' encoding_configs : bin_method : equal_frequency bin_size : 10 monotonicity_check : 0 IG_calculation \ud83d\udd0e Corresponds to association_evaluator.IG_calculation list_of_cols : List of column names (list of strings or string of column names separated by | ) to consider for IG calculation. drop_cols : List of column names (list of strings or string of column names separated by | ) to exclude from IG calculation. label_col : Name of label or target column in the input dataset. event_label : Value of event (label 1 / true ) in the label column. encoding_configs : Detailed configuration of the binning step. bin_method : The binning method. Defaults to equal_frequency . bin_size : The bin size. Defaults to 10 . monotonicity_check : If set to 1 , dynamically computes the bin_size such that monotonicity is ensured. Can be a computationally expensive calculation. Defaults to 0 . \ud83e\udd13 Example: IG_calculation : list_of_cols : all drop_cols : id_column label_col : label_col event_label : 'class1' encoding_configs : bin_method : equal_frequency bin_size : 10 monotonicity_check : 0 variable_clustering \ud83d\udd0e Corresponds to association_evaluator.variable_clustering list_of_cols : List of column names (list of strings or string of column names separated by | ) to include for variable clustering drop_cols : List of column names (list of strings or string of column names separated by | ) to exclude from variable clustering. \ud83e\udd13 Example: variable_clustering : list_of_cols : all drop_cols : id_column|label_col \ud83d\udcd1 drift_detector \ud83d\udd0e Corresponds to drift.detector This block configures the drift detector module that provides a range of methods to detect drift within and between datasets. drift_statistics \ud83d\udd0e Corresponds to drift.detector.statistics configs list_of_cols : List of columns to check drift (list or string of col names separated by | ) to include in the drift statistics. Can be set to all to include all non-array columns (except those given in drop_cols ). drop_cols : List of columns to be dropped (list or string of col names separated by | ) to exclude from the drift statistics. method_type : Method(s) to apply to detect drift (list or string of methods separated by | ). Possible values are PSI , JSD , HD , and KS . If set to all , all available metrics are calculated. threshold : Threshold above which attributes are flagged as exhibiting drift. bin_method : The binning method. Possible values are equal_frequency and equal_range . bin_size : The bin size. We recommend setting it to 10 to 20 for PSI and above 100 for all other metrics. pre_existing_source : Set to true if a pre-computed binning model as well as frequency counts and attributes are available. false otherwise. source_path : If pre_existing_source is true , this described from where the pre-computed data is loaded. drift_statistics_folder . drift_statistics folder must contain the output from attribute_binning & frequency_counts . If pre_existing_source is False, this can be used for saving the details. Default folder \"NA\" is used for saving the intermediate output \ud83e\udd13 Example: configs : list_of_cols : all drop_cols : [ 'id_column' , 'label_col' ] method_type : all threshold : 0.1 bin_method : equal_range bin_size : 10 pre_existing_source : False source_path : NA source_dataset The reference/baseline dataset. read_dataset file_path : The file (or directory) path to read the source dataset from. It can be a local path, an \ud83d\udcd6 S3 path (when running on AWS), a path to a file resource on Google Colab (see \ud83d\udcd6 this tutorial for an overview), or a path on the \ud83d\udcd6 Databricks File System (when running on Azure). file_type : The file format of the source data. Currently, Anovos supports CSV ( csv ), Parquet ( parquet ), and Avro ( avro ). (Please note that if you're using Avro data sources, you need to add the external package org.apache.spark:spark-avro when submitting the Spark job.) file_configs (optional): Options to pass to the respective Spark file reader, e.g., delimiters, schemas, headers. delete_column List of column names (list of strings or string of column names separated by | ) to be deleted from the loaded input data. select_column List of column names (list of strings or string of column names separated by | ) to be selected for further processing. rename_column list_of_cols : List of the names of columns (list of strings or string of column names separated by | ) to be renamed. list_of_newcols : The new column names. The first element in list_of_cols will be renamed to the first name in list_of_newcols and so on. recast_column list_of_cols : List of the names of columns (list of strings or string of column names separated by | ) to be cast to a different type. list_of_dtypes : The new datatypes. The first element in list_of_cols will be recast to the first type in list_of_dtypes and so on. See the \ud83d\udcd6 Spark documentation for a list of valid datatypes. Note that this field is case-insensitive. stability_index \ud83d\udd0e Corresponds to detector.stability_index_computation configs metric_weightages : A dictionary where the keys are the metric names ( mean , stdev , kurtosis ) and the values are the weight of the metric (between 0 and 1 ). All weights must sum to 1 . existing_metric_path : Location of previously computed metrics of historical datasets ( idx , attribute , mean , stdev , kurtosis where idx is index number of the historical datasets in chronological order). appended_metric_path : The path where the input dataframe metrics are saved after they have been appended to the historical metrics. threshold : The threshold above which attributes are flagged as unstable. \ud83e\udd13 Example: configs : metric_weightages : mean : 0.5 stddev : 0.3 kurtosis : 0.2 existing_metric_path : '' appended_metric_path : 'si_metrics' threshold : 2 dataset1 read_dataset Corresponds to data_ingest.read_dataset file_path : The file (or directory) path to read the other joining input dataset from. It can be a local path, an \ud83d\udcd6 S3 path (when running on AWS), a path to a file resource on Google Colab (see \ud83d\udcd6 this tutorial for an overview), or a path on the \ud83d\udcd6 Databricks File System (when running on Azure). file_type : The file format of the other joining input data. Currently, Anovos supports CSV ( csv ), Parquet ( parquet ), and Avro ( avro ). (Please note that if you're using Avro data sources, you need to add the external package org.apache.spark:spark-avro when submitting the Spark job.) file_configs (optional): Options to pass to the respective Spark file reader, e.g., delimiters, schemas, headers. dataset2 , dataset3 , ... Additional datasets are configured in the same manner as dataset1 . \ud83d\udcd1 report_preprocessing \ud83d\udd0e Corresponds to data_report.report_preprocessing This configuration block describes the data pre\u2013processing necessary for report generation. master_path The path where all outputs are saved. \ud83e\udd13 Example: master_path : 'report_stats' charts_to_objects \ud83d\udd0e Corresponds to report_preprocessing.charts_to_objects This is the core function of the report preprocessing stage. It saves the chart data in the form of objects that are used by the subsequent report generation scripts. See the intermediate report documentation for more details. list_of_cols : List of column names (list of strings or string of column names separated by | ) to include in preprocessing. drop_cols : List of column names (list of strings or string of column names separated by | ) to exclude from preprocessing. label_col : Name of the label or target column in the input dataset. event_label : Value of the event (label 1 / true ) in the label column. bin_method : The binning method. Possible values are equal_frequency and equal_range . bin_size : The bin size. We recommend setting it to 10 to 20 for PSI and above 100 for all other metrics. drift_detector : Indicates whether data drift has already analyzed. Defaults to False . outlier_charts : Indicates whether outlier charts should be included. Defaults to False . source_path : The source data path for drift analysis. If it has not been computed or is not required, set it to the default value \"NA\" . \ud83e\udd13 Example: charts_to_objects : list_of_cols : all drop_cols : id_column label_col : label_col event_label : 'class1' bin_method : equal_frequency bin_size : 10 drift_detector : True outlier_charts : False source_path : \"NA\" \ud83d\udcd1 report_generation \ud83d\udd0e Corresponds to data_report.report_generation This configuration block controls the generation of the actual report, i.e., the data that is included and the layout. See the report generation documentation for more details. master_path : The path to the preprocessed data generated during the report_preprocessing step. id_col : The ID column present in the input dataset label_col : Name of label or target column in the input dataset. corr_threshold : The threshold above which attributes are considered to be correlated and thus, redundant. Its value is between 0 and 1 . iv_threshold : The threshold above which attributes are considered ot be significant. Its value is between 0 and 1 . Information Value Variable's Predictiveness <0.02 Not useful for prediction 0.02 to 0.1 Weak predictive power 0.1 to 0.3 Medium predictive power 0.3 to 0.5 Strong predictive power >0.5 Suspicious predictive power drift_threshold_model : The threshold above which an attribute is flagged as exhibiting drift. Its value is between 0 and 1 . dataDict_path : The path to the data dictionary containing the exact names and definitions of the attributes. This information is used in the report to aid comprehensibility. metricDict_path : Path to the metric dictionary. final_report_path : The path where final report will be saved. It can be a local path, an \ud83d\udcd6 S3 path (when running on AWS), a path to a file resource on Google Colab (see \ud83d\udcd6 this tutorial for an overview), or a path on the \ud83d\udcd6 Databricks File System (when running on Azure). \ud83e\udd13 Example: report_generation : master_path : 'report_stats' id_col : 'id_column' label_col : 'label_col' corr_threshold : 0.4 iv_threshold : 0.02 drift_threshold_model : 0.1 dataDict_path : 'data/income_dataset/data_dictionary.csv' metricDict_path : 'data/metric_dictionary.csv' final_report_path : 'report_stats' \ud83d\udcd1 transformers \ud83d\udd0e Corresponds to data_transformer.transformers This block configures the data_transformer module that supports numerous pre-processing and transformation functions, such as binning, encoding, scaling, and imputation. numerical_mathops This group of functions is used to perform mathematical transformations of numerical attributes. feature_transformation \ud83d\udd0e Corresponds to transformers.feature_transformation list_of_cols : The numerical columns (list of strings or string of column names separated by | ) to transform. Can be set to \"all\" to include all numerical columns. drop_cols : The numerical columns (list of strings or string of column names separated by | ) to exclude from feature transformation. method_type : The method to apply to use for transformation. The default method is sqrt ( \\sqrt{x} \\sqrt{x} ). Possible values are: ln log10 log2 exp powOf2 ( 2^x 2^x ) powOf10 ( 10^x 10^x ) powOfN ( N^x N^x ) sqrt ( \\sqrt{x} \\sqrt{x} ) cbrt ( \\sqrt[3]{x} \\sqrt[3]{x} ) sq ( x^2 x^2 ) cb ( x^3 x^3 ) toPowerN ( x^N x^N ) sin cos tan asin acos atan radians remainderDivByN ( x % N x % N ) factorial ( x! x! ) mul_inv ( 1/x 1/x ) floor ceil roundN (round to N decimal places) N : None by default. If method_type is powOfN , toPowerN , remainderDivByN , or roundN , N will be used as the required constant. \ud83e\udd13 Example 1: feature_transformation : list_of_cols : all drop_cols : [] method_type : sqrt \ud83e\udd13 Example 2: feature_transformation : list_of_cols : [ 'capital-gain' , 'capital-loss' ] drop_cols : [] method_type : sqrt feature_transformation : list_of_cols : [ 'age' , 'education_num' ] drop_cols : [] method_type : sq boxcox_transformation \ud83d\udd0e Corresponds to transformers.boxcox_transformation list_of_cols : The columns (list of strings or string of column names separated by | ) to transform. Can be set to \"all\" to include all columns. drop_cols : The columns (list of strings or string of column names separated by | ) to exclude from Box-Cox transformation. boxcox_lambda : The \\lambda \\lambda value for the Box-Cox transformation. It can be given as a list where each element represents the value of \\lambda \\lambda for a single attribute. The length of the list must be the same as the number of columns to transform. number that is used for all attributes. If no value is given (the default), a search for the best \\lambda \\lambda will be conducted among the following values: [1, -1, 0.5, -0.5, 2, -2, 0.25, -0.25, 3, -3, 4, -4, 5, -5] . The search is conducted independently for each column. \ud83e\udd13 Example 1: boxcox_transformation : list_of_cols : num_feature1|num_feature2 drop_cols : [] \ud83e\udd13 Example 2: boxcox_transformation : list_of_cols : num_feature3|num_feature4 drop_cols : [] boxcox_lambda : [ -2 , -1 ] numerical_binning This group of functions is used to transform numerical attributes into discrete (integer or categorical) attribute. attribute_binning \ud83d\udd0e Corresponds to transformers.attribute_binning list_of_cols : The numerical columns (list of strings or string of column names separated by | ) to transform. Can be set to \"all\" to include all numerical columns. drop_cols : The columns (list of strings or string of column names separated by | ) to exclude from attribute binning. method_type : The binning method. Possible values are equal_frequency and equal_range . With equal_range , each bin is of equal size/width and with equal_frequency , each bin contains an equal number of rows. Defaults to equal_range . bin_size : The number of bins. Defaults to 10 . bin_dtype : The dtype of the transformed column. Possible values are numerical and categorical . With numerical , the values reflect the bin number ( 1 , 2 , \u2026). With categorical option, the values are a string that describes the minimal and maximal value of the bin. Defaults to numerical . \ud83e\udd13 Example: attribute_binning : list_of_cols : num_feature1|num_feature2 drop_cols : [] method_type : equal_frequency bin_size : 10 bin_dtype : numerical monotonic_binning \ud83d\udd0e Corresponds to transformers.monotonic_binning list_of_cols : The numerical columns (list of strings or string of column names separated by | ) to transform. Can be set to \"all\" to include all numerical columns. drop_cols : The columns (list of strings or string of column names separated by | ) to exclude from monotonic binning. method_type : The binning method. Possible values are equal_frequency and equal_range . With equal_range , each bin is of equal size/width and with equal_frequency , each bin contains an equal number of rows. Defaults to equal_range . bin_size : The number of bins. Defaults to 10 . bin_dtype : The dtype of the transformed column. Possible values are numerical and categorical . With numerical , the values reflect the bin number ( 1 , 2 , \u2026). With categorical option, the values are a string that describes the minimal and maximal value of the bin. Defaults to numerical . \ud83e\udd13 Example: attribute_binning : list_of_cols : num_feature1|num_feature2 drop_cols : [] label_col : [ \"label_col\" ] event_label : [ \"class1\" ] method_type : equal_frequency bin_size : 10 bin_dtype : numerical numerical_expression expression_parser \ud83d\udd0e Corresponds to transformers.expression_parser This function can be used to evaluate a list of SQL expressions and output the result as new features. Columns used in the SQL expression must be available in the dataset. list_of_expr : List of expressions to evaluate as new features e.g., [\"expr1\", \"expr2\"]. Alternatively, expressions can be specified in a string format, where different expressions are separated by pipe delimiter \u201c|\u201d e.g., \"expr1|expr2\". postfix : postfix for new feature name.Naming convention \"f\" + expression_index + postfix e.g. with postfix of \"new\", new added features are named as f0new, f1new etc. (Default value = \"\"). \ud83e\udd13 Example 1: expression_parser : list_of_expr : 'log(age) + 1.5|sin(capital-gain)+cos(capital-loss)' \ud83e\udd13 Example 2: expression_parser : list_of_expr : [ 'log(age) + 1.5' , 'sin(capital-gain)+cos(capital-loss)' ] Both Example 1 and Example 2 generate 2 new features: log(age) + 1.5 and sin(capital-gain)+cos(capital-loss) . The newly generated features will be appended to the dataframe as new columns: f0 and f1. categorical_outliers This function assigns less frequently seen values in a categorical column to a new category others . outlier_categories \ud83d\udd0e Corresponds to transformers.outlier_categories list_of_cols : The categorical columns (list of strings or string of column names separated by | ) to transform. Can be set to \"all\" to include all categorical columns. drop_cols : The columns (list of strings or string of column names separated by | ) to exclude from outlier transformation. coverage : The minimum fraction of rows that remain in their original category, given as a value between 0 and 1 . For example, with a coverage of 0.8 , the categories that 80% of the rows belong to remain and the more seldom occurring categories are mapped to others . The default value is 1.0 , which means that no rows are changed to others . max_category : Even if coverage is less, only (max_category - 1) categories will be mapped to actual name and rest to others. Caveat is when multiple categories have same rank, then #categories can be more than max_category. Defaults to 50 . \ud83e\udd13 Example 1: outlier_categories : list_of_cols : all drop_cols : [ 'id_column' , 'label_col' ] coverage : 0.9 max_category : 20 \ud83e\udd13 Example 2: outlier_categories : list_of_cols : [ \"cat_feature1\" , \"cat_feature2\" ] drop_cols : [] coverage : 0.8 max_category : 10 outlier_categories : list_of_cols : [ \"cat_feature3\" , \"cat_feature4\" ] drop_cols : [] coverage : 0.9 max_category : 15 categorical_encoding This group of transformers functions used to converting a categorical attribute into numerical attribute(s). cat_to_num_unsupervised \ud83d\udd0e Corresponds to transformers.cat_to_num_unsupervised list_of_cols : The categorical columns (list of strings or string of column names separated by | ) to encode. Can be set to \"all\" to include all categorical columns. drop_cols : The columns (list of strings or string of column names separated by | ) to exclude from categorical encoding. method_type : The encoding method. Set to 1 for label encoding and to 0 for one-hot encoding. With label encoding, each categorical value is assigned a unique integer based on the ordering specified through index_order . With one-hot encoding, each categorical value will be represented by a binary column. Defaults to 1 (label encoding). index_order : The order assigned to the categorical values when method_type is set to 1 (label encoding). Possible values are: frequencyDesc (default): Order by descending frequency. frequencyAsc : Order by ascending frequency. alphabetDesc : Order alphabetically (descending). alphabetAsc : Order alphabetically (ascending). cardinality_threshold : Columns with a cardinality above this threshold are excluded from enconding. Defaults to 100 . \ud83e\udd13 Example 1: cat_to_num_unsupervised : list_of_cols : all drop_cols : [ 'id_column' ] method_type : 0 cardinality_threshold : 10 \ud83e\udd13 Example 2: cat_to_num_unsupervised : list_of_cols : [ \"cat_feature1\" , \"cat_feature2\" ] drop_cols : [] method_type : 0 cardinality_threshold : 10 cat_to_num_unsupervised : list_of_cols : [ \"cat_feature3\" , \"cat_feature4\" ] drop_cols : [] method_type : 1 cat_to_num_supervised \ud83d\udd0e Corresponds to transformers.cat_to_num_supervised list_of_cols : The categorical columns (list of strings or string of column names separated by | ) to encode. Can be set to \"all\" to include all categorical columns. drop_cols : The columns (list of strings or string of column names separated by | ) to exclude from categorical encoding. label_col : The label/target column. Defaults to label . event_label : Value of the (positive) event (i.e, label 1 / true ). Defaults to 1 . \ud83e\udd13 Example: cat_to_num_supervised : list_of_cols : cat_feature1 | cat_feature2 drop_cols : [ 'id_column' ] label_col : income event_label : '>50K' numerical_rescaling Group of functions to rescale numerical attributes. normalization \ud83d\udd0e Corresponds to transformers.normalization list_of_cols : The numerical columns (list of strings or string of column names separated by | ) to normalize. Can be set to \"all\" to include all numerical columns. drop_cols : The columns (list of strings or string of column names separated by | ) to exclude from normalization. \ud83e\udd13 Example: normalization : list_of_cols : [ \"num_feature1\" , \"num_feature2\" ] drop_cols : [] z_standardization \ud83d\udd0e Corresponds to transformers.z_standardization list_of_cols : The numerical columns (list of strings or string of column names separated by | ) to standardize. Can be set to \"all\" to include all numerical columns. drop_cols : The columns (list of strings or string of column names separated by | ) to exclude from standardization. \ud83e\udd13 Example: z_standardization : list_of_cols : [ \"num_feature1\" , \"num_feature2\" ] drop_cols : [] IQR_standardization \ud83d\udd0e Corresponds to transformers.IQR_standardization list_of_cols : The numerical columns (list of strings or string of column names separated by | ) to standardize. Can be set to \"all\" to include all numerical columns. drop_cols : The columns (list of strings or string of column names separated by | ) to exclude from standardization. \ud83e\udd13 Example: IQR_standardization : list_of_cols : [ \"num_feature1\" , \"num_feature2\" , \"num_feature3\" ] drop_cols : [] numerical_latentFeatures Group of functions to generate latent features to reduce the dimensionality of the input dataset. PCA_latentFeatures \ud83d\udd0e Corresponds to transformers.PCA_latentFeatures list_of_cols : The numerical columns (list of strings or string of column names separated by | ) to standardize. Can be set to \"all\" to include all numerical columns. drop_cols : The columns (list of strings or string of column names separated by | ) to exclude from latent features computation. explained_variance_cutoff : The required explained variance cutoff. Determines the number of encoded columns in the output. If N is the smallest integer such that the top N encoded columns explain more than the given variance threshold, these N columns be selected. Defaults to 0.95 . standardization : If True (the default), standardization is applied. False otherwise. standardization_configs : The arguments for the z_standardization function in dictionary format. Defaults to {\"pre_existing_model\": False} imputation : If True , imputation is applied. False (the default) otherwise. imputation_configs : Configuration for imputation in dictionary format. The name of the imputation function is specified with the key imputation_name (defaults to imputation_MMM ). Arguments for the imputation function can be passed using additional keys. \ud83e\udd13 Example 1: PCA_latentFeatures : list_of_cols : [ \"num_feature1\" , \"num_feature2\" , \"num_feature3\" ] explained_variance_cutoff : 0.95 standardization : False imputation : True \ud83e\udd13 Example 2: PCA_latentFeatures : list_of_cols : [ \"num_feature1\" , \"num_feature2\" , \"num_feature3\" ] explained_variance_cutoff : 0.8 standardization : False imputation : True PCA_latentFeatures : list_of_cols : [ \"num_feature4\" , \"num_feature5\" , \"num_feature6\" ] explained_variance_cutoff : 0.6 standardization : True imputation : True autoencoder_latentFeatures \ud83d\udd0e Corresponds to transformers.autoencoder_latentFeatures list_of_cols : The numerical columns (list of strings or string of column names separated by | ) to standardize. Can be set to \"all\" to include all numerical columns. drop_cols : The columns (list of strings or string of column names separated by | ) to exclude from latent features computation. reduction_params : Determines the number of resulting encoded features. If reduction_params is below 1 , reduction_params * \"number of columns\" columns will be generated. Else, reduction_params columns will be generated. Defaults to 0.5 , i.e., the number of columns in the result is half the of number of columns in the input. sample_size : Maximum number of rows used for training the autoencoder model. Defaults to 500000 ( 5e5 ). epochs : The number of epochs to train the autoencoder model. Defaults to 100 . batch_size : The batch size for autoencoder model training. Defaults to 256 . standardization : If True (the default), standardization is applied. False otherwise. standardization_configs : The arguments for the z_standardization function in dictionary format. Defaults to {\"pre_existing_model\": False} imputation : If True , imputation is applied. False (the default) otherwise. imputation_configs : Configuration for imputation in dictionary format. The name of the imputation function is specified with the key imputation_name (defaults to imputation_MMM ). Arguments for the imputation function can be passed using additional keys. \ud83e\udd13 Example 1: autoencoder_latentFeatures : list_of_cols : [ \"num_feature1\" , \"num_feature2\" , \"num_feature3\" ] reduction_params : 0.5 sample_size : 10000 epochs : 20 batch_size : 256 \ud83e\udd13 Example 2: autoencoder_latentFeatures : list_of_cols : [ \"num_feature1\" , \"num_feature2\" ] reduction_params : 0.5 sample_size : 10000 epochs : 20 batch_size : 256 autoencoder_latentFeatures : list_of_cols : [ \"num_feature3\" , \"num_feature4\" , \"num_feature5\" , \"num_feature6\" , \"num_feature7\" ] reduction_params : 0.8 sample_size : 10000 epochs : 100 batch_size : 256 \ud83d\udcd1 write_intermediate file_path : Path where intermediate datasets (after selecting, dropping, renaming, and recasting of columns) for quality checker operations, join dataset and concatenate dataset will be saved. file_type : (CSV, Parquet or Avro). file format of intermediate dataset file_configs (optional): Rest of the valid configuration can be passed through this options e.g., repartition, mode, compression, header, delimiter, inferSchema etc. This might look like: file_configs : mode : overwrite header : True delimiter : \", \" inferSchema : True For more information on available configuration options, see the following external documentation: \ud83d\udcd6 Writing CSV files \ud83d\udcd6 Writing Parquet files \ud83d\udcd6 Writing Avro files \ud83d\udcd1 write_main file_path : Path where final cleaned input dataset will be saved. file_type : (CSV, Parquet or Avro). file format of final dataset file_configs (optional): Rest of the valid configuration can be passed through this options e.g., repartition, mode, compression, header, delimiter, inferSchema etc. This might look like: file_configs : mode : overwrite header : True delimiter : \", \" inferSchema : True For more information on available configuration options, see the following external documentation: \ud83d\udcd6 Writing CSV files \ud83d\udcd6 Writing Parquet files \ud83d\udcd6 Writing Avro files \ud83d\udcd1 write_stats file_path : Path where all tables/stats of anovos modules (data drift & data analyzer) will be saved. file_type : (CSV, Parquet or Avro). file format of final dataset file_configs (optional): Rest of the valid configuration can be passed through this options e.g., repartition, mode, compression, header, delimiter, inferSchema etc. This might look like: file_configs : mode : overwrite header : True delimiter : \", \" inferSchema : True For more information on available configuration options, see the following external documentation: \ud83d\udcd6 Writing CSV files \ud83d\udcd6 Writing Parquet files \ud83d\udcd6 Writing Avro files","title":"Configuration"},{"location":"using-anovos/config_file.html#configuring-workloads","text":"Anovos workloads can be described by a YAML configuration file. Such a configuration file defines: the input dataset(s) the analyses and transformations to be performed on the data the output files and dataset(s) the reports to be generated Defining workloads this way allows users to make full use of Anovos capabilities while maintaining an easy-to-grasp overview. Since each configuration file fully describes one workload, these files can be shared, versioned, and run across different compute environments. In the following, we'll describe in detail each of the sections in an Anovos configuration file. If you'd rather see a full example right away, have a look at this example . Note that each section of the configuration file maps to a module of Anovos . You'll find links to the respective sections of the API Documentation that provide much more detailed information on each modules' capabilities than we can squeeze into this guide.","title":"Configuring Workloads"},{"location":"using-anovos/config_file.html#input_dataset","text":"This configuration block describes how the input dataset is loaded and prepared using the data_ingest.data_ingest module. Each Anovos configuration file must contain exactly one input_dataset block. Note that the subsequent operations are performed in the order given here: First, columns are deleted, then selected, then renamed, and then recast.","title":"\ud83d\udcd1 input_dataset"},{"location":"using-anovos/config_file.html#read_dataset","text":"\ud83d\udd0e Corresponds to data_ingest.read_dataset file_path : The file (or directory) path to read the input dataset from. It can be a local path, an \ud83d\udcd6 S3 path (when running on AWS), a path to a file resource on Google Colab (see \ud83d\udcd6 this tutorial for an overview), or a path on the \ud83d\udcd6 Databricks File System (when running on Azure). file_type : The file format of the input data. Currently, Anovos supports CSV ( csv ), Parquet ( parquet ), and Avro ( avro ). (Please note that if you're using Avro data sources, you need to add the external package org.apache.spark:spark-avro when submitting the Spark job.) file_configs (optional): Options to pass to the respective Spark file reader, e.g., delimiters, schemas, headers. In the case of a CSV file, this might look like: file_configs : delimiter : \", \" header : True inferSchema : True For more information on available configuration options, see the following external documentation: Read CSV files Read Parquet files Read Avro files","title":"read_dataset"},{"location":"using-anovos/config_file.html#delete_column","text":"\ud83d\udd0e Corresponds to data_ingest.delete_column List of column names (list of strings or string of column names separated by | ) to be deleted from the loaded input data. \ud83e\udd13 Example: delete_column : [ 'unnecessary' , 'obsolete' , 'outdated' ]","title":"delete_column"},{"location":"using-anovos/config_file.html#select_column","text":"\ud83d\udd0e Corresponds to data_ingest.select_column List of column names (list of strings or string of column names separated by | ) to be selected for further processing. \ud83e\udd13 Example: select_column : [ 'feature1' , 'feature2' , 'feature3' , 'label' ]","title":"select_column"},{"location":"using-anovos/config_file.html#rename_column","text":"\ud83d\udd0e Corresponds to data_ingest.rename_column list_of_cols : List of the names of columns (list of strings or string of column names separated by | ) to be renamed. list_of_newcols : The new column names. The first element in list_of_cols will be renamed to the first name in list_of_newcols and so on. \ud83e\udd13 Example: rename_column : list_of_cols : [ 'very_long_column_name' , 'price' ] list_of_newcols : [ 'short_name' , 'label' ] This will rename the column very_long_column_name to short_name and the column price to label .","title":"rename_column"},{"location":"using-anovos/config_file.html#recast_column","text":"\ud83d\udd0e Corresponds to data_ingest.recast_column list_of_cols : List of the names of columns (list of strings or string of column names separated by | ) to be cast to a different type. list_of_dtypes : The new datatypes. The first element in list_of_cols will be recast to the first type in list_of_dtypes and so on. See \ud83d\udcd6 the Spark documentation for a list of valid datatypes. Note that this field is case-insensitive. \ud83e\udd13 Example: recast_column : list_of_cols : [ 'price' , 'quantity' ] list_of_dtypes : [ 'double' , 'int' ]","title":"recast_column"},{"location":"using-anovos/config_file.html#concatenate_dataset","text":"\ud83d\udd0e Corresponds to data_ingest.concatenate_dataset This configuration block describes how to combine multiple loaded dataframes into a single one.","title":"\ud83d\udcd1 concatenate_dataset"},{"location":"using-anovos/config_file.html#method","text":"There are two different methods to concatenate dataframes: index : Concatenate by column index, i.e., the first column of the first dataframe is matched with the first column of the second dataframe and so forth. name : Concatenate by column name, i.e., columns of the same name are matched. Note that in both cases, the first dataframe will define both the names and the order of the columns in the final dataframe. If the subsequent dataframes have too few columns ( index ) or are missing named columns (`name\u00b4) for the concatenation to proceed, an error will be raised. \ud83e\udd13 Example: method : name","title":"method"},{"location":"using-anovos/config_file.html#dataset1","text":"","title":"dataset1"},{"location":"using-anovos/config_file.html#read_dataset_1","text":"\ud83d\udd0e Corresponds to data_ingest.read_dataset file_path : The file (or directory) path to read the other concatenating input dataset from. It can be a local path, an \ud83d\udcd6 S3 path (when running on AWS), a path to a file resource on Google Colab (see \ud83d\udcd6 this tutorial for an overview), or a path on the \ud83d\udcd6 Databricks File System (when running on Azure). file_type : The file format of the other concatenating input data. Currently, Anovos supports CSV ( csv ), Parquet ( parquet ), and Avro ( avro ). (Please note that if you're using Avro data sources, you need to add the external package org.apache.spark:spark-avro when submitting the Spark job.) file_configs (optional): Options to pass to the respective Spark file reader, e.g., delimiters, schemas, headers.","title":"read_dataset"},{"location":"using-anovos/config_file.html#delete_column_1","text":"\ud83d\udd0e Corresponds to data_ingest.delete_column List of column names (list of strings or string of column names separated by | ) to be deleted from the loaded input data.","title":"delete_column"},{"location":"using-anovos/config_file.html#select_column_1","text":"\ud83d\udd0e Corresponds to data_ingest.select_column List of column names (list of strings or string of column names separated by | ) to be selected for further processing.","title":"select_column"},{"location":"using-anovos/config_file.html#rename_column_1","text":"\ud83d\udd0e Corresponds to data_ingest.rename_column list_of_cols : List of the names of columns (list of strings or string of column names separated by | ) to be renamed. list_of_newcols : The new column names. The first element in list_of_cols will be renamed to the first name in list_of_newcols and so on.","title":"rename_column"},{"location":"using-anovos/config_file.html#recast_column_1","text":"\ud83d\udd0e Corresponds to data_ingest.recast_column list_of_cols : List of the names of columns (list of strings or string of column names separated by | ) to be cast to a different type. list_of_dtypes : The new datatypes. The first element in list_of_cols will be recast to the first type in list_of_dtypes and so on. See \ud83d\udcd6 the Spark documentation for a list of valid datatypes. Note that this field is case-insensitive.","title":"recast_column"},{"location":"using-anovos/config_file.html#dataset2-dataset3","text":"Additional datasets are configured in the same manner as dataset1 .","title":"dataset2,  dataset3,  ..."},{"location":"using-anovos/config_file.html#join_dataset","text":"\ud83d\udd0e Corresponds to data_ingest.join_dataset This configuration block describes how multiple dataframes are joined into a single one.","title":"\ud83d\udcd1 join_dataset"},{"location":"using-anovos/config_file.html#join_cols","text":"The key of the column(s) to join on. In the case that the key consists of multiple columns, they can be passed as a list of strings or a single string where the column names are separated by | . \ud83e\udd13 Example: join_cols : id_column","title":"join_cols"},{"location":"using-anovos/config_file.html#join_type","text":"The type of join to perform: inner , full , left , right , left_semi , or left_anti . For a general introduction to joins, see \ud83d\udcd6 this tutorial . \ud83e\udd13 Example: join_type : inner","title":"join_type"},{"location":"using-anovos/config_file.html#dataset1_1","text":"","title":"dataset1"},{"location":"using-anovos/config_file.html#read_dataset_2","text":"\ud83d\udd0e Corresponds to data_ingest.read_dataset file_path : The file (or directory) path to read the other joining input dataset from. It can be a local path, an \ud83d\udcd6 S3 path (when running on AWS), a path to a file resource on Google Colab (see \ud83d\udcd6 this tutorial for an overview), or a path on the \ud83d\udcd6 Databricks File System (when running on Azure). file_type : The file format of the other joining input data. Currently, Anovos supports CSV ( csv ), Parquet ( parquet ), and Avro ( avro ). (Please note that if you're using Avro data sources, you need to add the external package org.apache.spark:spark-avro when submitting the Spark job.) file_configs (optional): Options to pass to the respective Spark file reader, e.g., delimiters, schemas, headers.","title":"read_dataset"},{"location":"using-anovos/config_file.html#delete_column_2","text":"\ud83d\udd0e Corresponds to data_ingest.delete_column List of column names (list of strings or string of column names separated by | ) to be deleted from the loaded input data.","title":"delete_column"},{"location":"using-anovos/config_file.html#select_column_2","text":"\ud83d\udd0e Corresponds to data_ingest.select_column List of column names (list of strings or string of column names separated by | ) to be selected for further processing.","title":"select_column"},{"location":"using-anovos/config_file.html#rename_column_2","text":"\ud83d\udd0e Corresponds to data_ingest.rename_column list_of_cols : List of the names of columns (list of strings or string of column names separated by | ) to be renamed. list_of_newcols : The new column names. The first element in list_of_cols will be renamed to the first name in list_of_newcols and so on.","title":"rename_column"},{"location":"using-anovos/config_file.html#recast_column_2","text":"\ud83d\udd0e Corresponds to data_ingest.recast_column list_of_cols : List of the names of columns (list of strings or string of column names separated by | ) to be cast to a different type. list_of_dtypes : The new datatypes. The first element in list_of_cols will be recast to the first type in list_of_dtypes and so on. See \ud83d\udcd6 the Spark documentation for a list of valid datatypes. Note that this field is case-insensitive.","title":"recast_column"},{"location":"using-anovos/config_file.html#dataset2-dataset3_1","text":"Additional datasets are configured in the same manner as dataset1 .","title":"dataset2,  dataset3,  ..."},{"location":"using-anovos/config_file.html#timeseries_analyzer","text":"\ud83d\udd0e Corresponds to data_analyzer.ts_analyzer Configuration for the time series analyzer. auto_detection : Can be set to True or False . If True , it attempts to automatically infer the date/timestamp format in the input dataset. id_col : Name of the ID column in the input dataset. tz_offset : The timezone offset of the timestamps in the input dataset. Can be set to either local , gmt , or utc . The default setting is local . inspection : Can be set to True or False . If True , the time series elements undergo an inspection. analysis_level : Can be set to daily , weekly , or hourly . The default setting is daily . If set to daily , the daily view is populated. If set to hourly , the view is shown at a day part level. If set to weekly , the display it per individual weekdays (1-7) as captured. max_days : Maximum number of days up to which the data will be aggregated. If the dataset contains a timestamp/date field with very high number of unique dates (e.g., 20 years worth of daily data), this option can be used to reduce the timespan that is analyzed. \ud83e\udd13 Example: timeseries_analyzer : auto_detection : True id_col : 'id_column' tz_offset : 'local' inspection : True analysis_level : 'daily' max_days : 3600","title":"\ud83d\udcd1 timeseries_analyzer"},{"location":"using-anovos/config_file.html#anovos_basic_report","text":"\ud83d\udd0e Corresponds to data_report.basic_report_generation The basic report consists of a summary of the outputs of the stats_generator , quality_checker , and association evaluator See the \ud83d\udcd6 documentation for data reports for more details. The basic report can be customized using the following options:","title":"\ud83d\udcd1 anovos_basic_report"},{"location":"using-anovos/config_file.html#basic_report","text":"If True , a basic report is generated after completion of the data_analyzer modules. If False , no report is generated. Nevertheless, all the computed statistics and metrics will be available in the final report.","title":"basic_report"},{"location":"using-anovos/config_file.html#report_args","text":"id_col : The name of the ID column in the input dataset. label_col : The name of the label or target column in the input dataset. event_lable : The value of the event (label 1 / true ) in the label column. output_path : Path where the basic report is saved. It can be a local path, an \ud83d\udcd6 S3 path (when running on AWS), a path to a file resource on Google Colab (see \ud83d\udcd6 this tutorial for an overview), or a path on the \ud83d\udcd6 Databricks File System (when running on Azure). \ud83e\udd13 Example: report_args : id_col : id_column label_col : label_col event_label : 'class1' output_path : report_stats","title":"report_args"},{"location":"using-anovos/config_file.html#stats_generator","text":"\ud83d\udd0e Corresponds to data_analyzer.stats_generator This module generates descriptive statistics of the ingested data. Descriptive statistics are split into different metric types. Each function corresponds to one metric type.","title":"\ud83d\udcd1 stats_generator"},{"location":"using-anovos/config_file.html#metric","text":"List of metrics to calculate for the input dataset. Available options are: \ud83d\udcd6 global_summary \ud83d\udcd6 measures_of_count \ud83d\udcd6 measures_of_centralTendency \ud83d\udcd6 measures_of_cardinality \ud83d\udcd6 measures_of_dispersion \ud83d\udcd6 measures_of_percentiles \ud83d\udcd6 measures_of_shape \ud83e\udd13 Example: metric : [ 'global_summary' , 'measures_of_counts' , 'measures_of_cardinality' , 'measures_of_dispersion' ]","title":"metric"},{"location":"using-anovos/config_file.html#metric_args","text":"list_of_cols : List of column names (list of strings or string of column names separated by | ) to compute the metrics for. Alternatively, if set to \"all\" , all columns are included. drop_cols : List of column names (list of strings or string of column names separated by | ) to exclude from metrics computation. This option is especially useful if list_of_cols is set to \"all\" , as it allows computing metrics for all except a few columns without having to specify a potentially very long list of column names to include. \ud83e\udd13 Example: metric_args : list_of_cols : all drop_cols : [ 'id_column' ]","title":"metric_args"},{"location":"using-anovos/config_file.html#quality_checker","text":"\ud83d\udd0e Corresponds to data_analyzer.quality_checker This module assesses the data quality along different dimensions. Quality metrics are computed at both the row and column level. Further, the module includes appropriate treatment options to fix several common quality issues.","title":"\ud83d\udcd1 quality_checker"},{"location":"using-anovos/config_file.html#duplicate_detection","text":"\ud83d\udd0e Corresponds to quality_checker.duplicate_detection list_of_cols : List of column names (list of strings or string of column names separated by | ) to consider when searching for duplicates. Alternatively, if set to \"all\" , all columns are included. drop_cols : List of column names (list of strings or string of column names separated by | ) to be excluded from duplicate detection. treatment : If False , duplicates are detected and reported. If True , duplicate rows are removed from the input dataset. \ud83e\udd13 Example: duplicate_detection : list_of_cols : all drop_cols : [ 'id_column' ] treatment : True","title":"duplicate_detection"},{"location":"using-anovos/config_file.html#nullrows_detection","text":"\ud83d\udd0e Corresponds to quality_checker.nullRows_detection list_of_cols : List of column names (list of strings or string of column names separated by | ) to consider during null rows detection. Alternatively, if set to \"all\" , all columns are included. drop_cols : List of column names (list of strings or string of column names separated by | ) to exclude from null rows detection. treatment : If False , null rows are detected and reported. If True , rows where more than treatment_threshold columns are null are removed from the input dataset. treatment_threshold : It takes a value between 0 and 1 (default 0.8 ) that specifies which fraction of columns has to be null for a row to be considered a null row. If the threshold is 0 , rows with any missing value will be flagged as null . If the threshold is 1 , only rows where all values are missing will be flagged as null . \ud83e\udd13 Example: nullRows_detection : list_of_cols : all drop_cols : [] treatment : True treatment_threshold : 0.75","title":"nullRows_detection"},{"location":"using-anovos/config_file.html#invalidentries_detection","text":"\ud83d\udd0e Corresponds to quality_checker.invalidEntries_detection list_of_cols : List of column names (list of strings or string of column names separated by | ) to be considered during invalid entries' detection. Alternatively, if set to \"all\" , all columns are included. drop_cols : List of column names (list of strings or string of column names separated by | ) to exclude from invalid entries' detection. treatment : If False , invalid entries are detected and reported. If True , invalid entries are replaced with null . output_mode : Can be either \"replace\" or \"append\" . If set to \"replace\" , the original columns will be replaced with the treated columns. If set to \"append\" , the original columns will be kept and the treated columns will be appended to the dataset. The appended columns will be named as the original column with a suffix \"_cleaned\" (e.g., the column \"cost_of_living_cleaned\" corresponds to the original column \"cost_of_living\" ). \ud83e\udd13 Example: invalidEntries_detection : list_of_cols : all drop_cols : [ 'id_column' ] treatment : True output_mode : replace","title":"invalidEntries_detection"},{"location":"using-anovos/config_file.html#idness_detection","text":"\ud83d\udd0e Corresponds to quality_checker.IDness_detection list_of_cols : List of column names (list of strings or string of column names separated by | ) to be considered for IDness detection. Alternatively, if set to \"all\" , all columns are included. drop_cols : List of column names (list of strings or string of column names separated by | ) to exclude from IDness detection. treatment : If False , columns with high IDness are detected and reported. If True , columns with an IDness above treatment_threshold are removed. treatment_threshold : A value between 0 and 1 (default 1.0 ). \ud83e\udd13 Example: IDness_detection : list_of_cols : all drop_cols : [ 'id_column' ] treatment : True treatment_threshold : 0.9","title":"IDness_detection"},{"location":"using-anovos/config_file.html#biasedness_detection","text":"\ud83d\udd0e Corresponds to quality_checker.biasedness_detection list_of_cols : List of column names (list of strings or string of column names separated by | ) to be considered for biasedness detection. Alternatively, if set to \"all\" , all columns are included. drop_cols : List of column names (list of strings or string of column names separated by | ) to exclude from biasedness detection. treatment : If False , columns with high IDness are detected and reported. If True , columns with a bias above treatment_threshold are removed. treatment_threshold : A value between 0 and 1 (default 1.0 ). \ud83e\udd13 Example: biasedness_detection : list_of_cols : all drop_cols : [ 'label_col' ] treatment : True treatment_threshold : 0.98","title":"biasedness_detection"},{"location":"using-anovos/config_file.html#outlier_detection","text":"\ud83d\udd0e Corresponds to quality_checker.outlier_detection list_of_cols : List of column names (list of strings or string of column names separated by | ) to be considered for outlier detection. Alternatively, if set to \"all\" , all columns are included. \u26a0 Note that any column that contains just a single value or only null values is not subjected to outlier detection even if it is selected under this argument. drop_cols : List of column names (list of strings or string of column names separated by | ) to exclude from outlier detection. detection_side : Whether outliers should be detected on the \"upper\" , the \"lower\" , or \"both\" sides. detection_configs : A map that defines the input parameters for different outlier detection methods. Possible keys are: pctile_lower (default 0.05 ) pctile_upper (default 0.95 ) stdev_lower (default 3.0 ) stdev_upper (default 3.0 ) IQR_lower (default 1.5 ) IQR_upper (default 1.5 ) min_validation (default 2 ) For details, see \ud83d\udcd6 the outlier_detection API documentation treatment : If False , outliers are detected and reported. If True , outliers are treated with the specified treatment_method . treatment_method : Specifies how outliers are treated. Possible options are \"null_replacement\" , \"row_removal\" , \"value_replacement\" . pre_existing_model : If True , the file specified under model_path with lower/upper bounds is loaded. If no such file exists, set to False (the default). model_path : The path to the file with lower/upper bounds. It can be a local path, an \ud83d\udcd6 S3 path (when running on AWS), a path to a file resource on Google Colab (see \ud83d\udcd6 this tutorial for an overview), or a path on the \ud83d\udcd6 Databricks File System (when running on Azure). If pre_existing_model is True , the pre-saved will be loaded from this location. If pre_existing_model is False , a file with lower/upper bounds will be saved at this location. By default, it is set to NA , indicating that there is neither a pre-saved file nor should such a file be generated. output_mode : Can be either \"replace\" or \"append\" . If set to \"replace\" , the original columns will be replaced with the treated columns. If set to \"append\" , the original columns will be kept and the treated columns will be appended to the dataset. The appended columns will be named as the original column with a suffix \"_outliered\" (e.g., the column \"cost_of_living_outliered\" corresponds to the original column \"cost_of_living\" ). \ud83e\udd13 Example: outlier_detection : list_of_cols : all drop_cols : [ 'id_column' , 'label_col' ] detection_side : upper detection_configs : pctile_lower : 0.05 pctile_upper : 0.90 stdev_lower : 3.0 stdev_upper : 3.0 IQR_lower : 1.5 IQR_upper : 1.5 min_validation : 2 treatment : True treatment_method : value_replacement pre_existing_model : False model_path : NA output_mode : replace","title":"outlier_detection"},{"location":"using-anovos/config_file.html#nullcolumns_detection","text":"\ud83d\udd0e Corresponds to quality_checker.nullColumns_detection list_of_cols : List of column names (list of strings or string of column names separated by | ) to be considered for null columns detection. Alternatively, if set to \"all\" , all columns are included. If set to \"missing\" (the default) only columns with missing values are included. One of the use cases where \"all\" may be preferable over \"missing\" is when the user wants to save the imputation model for future use. This can be useful, for example, if a column may not have missing values in the training dataset but missing values are acceptable in the test dataset. drop_cols : List of column names (list of strings or string of column names separated by | ) to be excluded from null columns detection. treatment : If False , null columns are detected and reported. If True , missing values are treated with the specified treatment_method . treatment_method : Specifies how null columns are treated. Possible values are \"MMM\" , \" row_removal\" , or \"column_removal\" . treatment_configs : Additional parameters for the treatment_method . If treatment_method is \"column_removal\" , the key treatment_threshold can be used to define the fraction of missing values above which a column is flagged as a null column and remove. If treatment_method is \"MMM\" , possible keys are the parameters of the imputation_MMM function. \ud83e\udd13 Example: nullColumns_detection : list_of_cols : all drop_cols : [ 'id_column' , 'label_col' ] treatment : True treatment_method : MMM treatment_configs : method_type : median pre_existing_model : False model_path : NA output_mode : replace","title":"nullColumns_detection"},{"location":"using-anovos/config_file.html#association_evaluator","text":"\ud83d\udd0e Corresponds to data_analyzer.association_evaluator This block configures the association evaluator that focuses on understanding the interaction between different attributes or the relationship between an attribute and a binary target variable.","title":"\ud83d\udcd1 association_evaluator"},{"location":"using-anovos/config_file.html#correlation_matrix","text":"\ud83d\udd0e Corresponds to association_evaluator.correlation_matrix list_of_cols : List of column names (list of strings or string of column names separated by | ) to include in the correlation matrix. Alternatively, when set to all , all columns are included. drop_cols : List of column names (list of strings or string of column names separated by | ) to be excluded from the correlation matrix. This is especially useful when almost all columns should be included in the correlation matrix: Set list_of_cols to all and drop the few excluded columns. \ud83e\udd13 Example: correlation_matrix : list_of_cols : all drop_cols : [ 'id_column' ]","title":"correlation_matrix"},{"location":"using-anovos/config_file.html#iv_calculation","text":"\ud83d\udd0e Corresponds to association_evaluator.IV_calculation list_of_cols : List of column names (list of strings or string of column names separated by | ) to include in the IV calculation. drop_cols : List of column names (list of strings or string of column names separated by | ) to exclude from IV calculation. label_col : Name of label or target column in the input dataset. event_label : Value of event (label 1 / true ) in the label column. encoding_configs : Detailed configuration of the binning step. bin_method : The binning method. Defaults to equal_frequency . bin_size : The bin size. Defaults to 10 . monotonicity_check : If set to 1 , dynamically computes the bin_size such that monotonicity is ensured. Can be a computationally expensive calculation. Defaults to 0 . \ud83e\udd13 Example: IV_calculation : list_of_cols : all drop_cols : id_column label_col : label_col event_label : 'class1' encoding_configs : bin_method : equal_frequency bin_size : 10 monotonicity_check : 0","title":"IV_calculation"},{"location":"using-anovos/config_file.html#ig_calculation","text":"\ud83d\udd0e Corresponds to association_evaluator.IG_calculation list_of_cols : List of column names (list of strings or string of column names separated by | ) to consider for IG calculation. drop_cols : List of column names (list of strings or string of column names separated by | ) to exclude from IG calculation. label_col : Name of label or target column in the input dataset. event_label : Value of event (label 1 / true ) in the label column. encoding_configs : Detailed configuration of the binning step. bin_method : The binning method. Defaults to equal_frequency . bin_size : The bin size. Defaults to 10 . monotonicity_check : If set to 1 , dynamically computes the bin_size such that monotonicity is ensured. Can be a computationally expensive calculation. Defaults to 0 . \ud83e\udd13 Example: IG_calculation : list_of_cols : all drop_cols : id_column label_col : label_col event_label : 'class1' encoding_configs : bin_method : equal_frequency bin_size : 10 monotonicity_check : 0","title":"IG_calculation"},{"location":"using-anovos/config_file.html#variable_clustering","text":"\ud83d\udd0e Corresponds to association_evaluator.variable_clustering list_of_cols : List of column names (list of strings or string of column names separated by | ) to include for variable clustering drop_cols : List of column names (list of strings or string of column names separated by | ) to exclude from variable clustering. \ud83e\udd13 Example: variable_clustering : list_of_cols : all drop_cols : id_column|label_col","title":"variable_clustering"},{"location":"using-anovos/config_file.html#drift_detector","text":"\ud83d\udd0e Corresponds to drift.detector This block configures the drift detector module that provides a range of methods to detect drift within and between datasets.","title":"\ud83d\udcd1 drift_detector"},{"location":"using-anovos/config_file.html#drift_statistics","text":"\ud83d\udd0e Corresponds to drift.detector.statistics","title":"drift_statistics"},{"location":"using-anovos/config_file.html#configs","text":"list_of_cols : List of columns to check drift (list or string of col names separated by | ) to include in the drift statistics. Can be set to all to include all non-array columns (except those given in drop_cols ). drop_cols : List of columns to be dropped (list or string of col names separated by | ) to exclude from the drift statistics. method_type : Method(s) to apply to detect drift (list or string of methods separated by | ). Possible values are PSI , JSD , HD , and KS . If set to all , all available metrics are calculated. threshold : Threshold above which attributes are flagged as exhibiting drift. bin_method : The binning method. Possible values are equal_frequency and equal_range . bin_size : The bin size. We recommend setting it to 10 to 20 for PSI and above 100 for all other metrics. pre_existing_source : Set to true if a pre-computed binning model as well as frequency counts and attributes are available. false otherwise. source_path : If pre_existing_source is true , this described from where the pre-computed data is loaded. drift_statistics_folder . drift_statistics folder must contain the output from attribute_binning & frequency_counts . If pre_existing_source is False, this can be used for saving the details. Default folder \"NA\" is used for saving the intermediate output \ud83e\udd13 Example: configs : list_of_cols : all drop_cols : [ 'id_column' , 'label_col' ] method_type : all threshold : 0.1 bin_method : equal_range bin_size : 10 pre_existing_source : False source_path : NA","title":"configs"},{"location":"using-anovos/config_file.html#source_dataset","text":"The reference/baseline dataset.","title":"source_dataset"},{"location":"using-anovos/config_file.html#read_dataset_3","text":"file_path : The file (or directory) path to read the source dataset from. It can be a local path, an \ud83d\udcd6 S3 path (when running on AWS), a path to a file resource on Google Colab (see \ud83d\udcd6 this tutorial for an overview), or a path on the \ud83d\udcd6 Databricks File System (when running on Azure). file_type : The file format of the source data. Currently, Anovos supports CSV ( csv ), Parquet ( parquet ), and Avro ( avro ). (Please note that if you're using Avro data sources, you need to add the external package org.apache.spark:spark-avro when submitting the Spark job.) file_configs (optional): Options to pass to the respective Spark file reader, e.g., delimiters, schemas, headers.","title":"read_dataset"},{"location":"using-anovos/config_file.html#delete_column_3","text":"List of column names (list of strings or string of column names separated by | ) to be deleted from the loaded input data.","title":"delete_column"},{"location":"using-anovos/config_file.html#select_column_3","text":"List of column names (list of strings or string of column names separated by | ) to be selected for further processing.","title":"select_column"},{"location":"using-anovos/config_file.html#rename_column_3","text":"list_of_cols : List of the names of columns (list of strings or string of column names separated by | ) to be renamed. list_of_newcols : The new column names. The first element in list_of_cols will be renamed to the first name in list_of_newcols and so on.","title":"rename_column"},{"location":"using-anovos/config_file.html#recast_column_3","text":"list_of_cols : List of the names of columns (list of strings or string of column names separated by | ) to be cast to a different type. list_of_dtypes : The new datatypes. The first element in list_of_cols will be recast to the first type in list_of_dtypes and so on. See the \ud83d\udcd6 Spark documentation for a list of valid datatypes. Note that this field is case-insensitive.","title":"recast_column"},{"location":"using-anovos/config_file.html#stability_index","text":"\ud83d\udd0e Corresponds to detector.stability_index_computation","title":"stability_index"},{"location":"using-anovos/config_file.html#configs_1","text":"metric_weightages : A dictionary where the keys are the metric names ( mean , stdev , kurtosis ) and the values are the weight of the metric (between 0 and 1 ). All weights must sum to 1 . existing_metric_path : Location of previously computed metrics of historical datasets ( idx , attribute , mean , stdev , kurtosis where idx is index number of the historical datasets in chronological order). appended_metric_path : The path where the input dataframe metrics are saved after they have been appended to the historical metrics. threshold : The threshold above which attributes are flagged as unstable. \ud83e\udd13 Example: configs : metric_weightages : mean : 0.5 stddev : 0.3 kurtosis : 0.2 existing_metric_path : '' appended_metric_path : 'si_metrics' threshold : 2","title":"configs"},{"location":"using-anovos/config_file.html#dataset1_2","text":"","title":"dataset1"},{"location":"using-anovos/config_file.html#read_dataset_4","text":"Corresponds to data_ingest.read_dataset file_path : The file (or directory) path to read the other joining input dataset from. It can be a local path, an \ud83d\udcd6 S3 path (when running on AWS), a path to a file resource on Google Colab (see \ud83d\udcd6 this tutorial for an overview), or a path on the \ud83d\udcd6 Databricks File System (when running on Azure). file_type : The file format of the other joining input data. Currently, Anovos supports CSV ( csv ), Parquet ( parquet ), and Avro ( avro ). (Please note that if you're using Avro data sources, you need to add the external package org.apache.spark:spark-avro when submitting the Spark job.) file_configs (optional): Options to pass to the respective Spark file reader, e.g., delimiters, schemas, headers.","title":"read_dataset"},{"location":"using-anovos/config_file.html#dataset2-dataset3_2","text":"Additional datasets are configured in the same manner as dataset1 .","title":"dataset2,  dataset3,  ..."},{"location":"using-anovos/config_file.html#report_preprocessing","text":"\ud83d\udd0e Corresponds to data_report.report_preprocessing This configuration block describes the data pre\u2013processing necessary for report generation.","title":"\ud83d\udcd1 report_preprocessing"},{"location":"using-anovos/config_file.html#master_path","text":"The path where all outputs are saved. \ud83e\udd13 Example: master_path : 'report_stats'","title":"master_path"},{"location":"using-anovos/config_file.html#charts_to_objects","text":"\ud83d\udd0e Corresponds to report_preprocessing.charts_to_objects This is the core function of the report preprocessing stage. It saves the chart data in the form of objects that are used by the subsequent report generation scripts. See the intermediate report documentation for more details. list_of_cols : List of column names (list of strings or string of column names separated by | ) to include in preprocessing. drop_cols : List of column names (list of strings or string of column names separated by | ) to exclude from preprocessing. label_col : Name of the label or target column in the input dataset. event_label : Value of the event (label 1 / true ) in the label column. bin_method : The binning method. Possible values are equal_frequency and equal_range . bin_size : The bin size. We recommend setting it to 10 to 20 for PSI and above 100 for all other metrics. drift_detector : Indicates whether data drift has already analyzed. Defaults to False . outlier_charts : Indicates whether outlier charts should be included. Defaults to False . source_path : The source data path for drift analysis. If it has not been computed or is not required, set it to the default value \"NA\" . \ud83e\udd13 Example: charts_to_objects : list_of_cols : all drop_cols : id_column label_col : label_col event_label : 'class1' bin_method : equal_frequency bin_size : 10 drift_detector : True outlier_charts : False source_path : \"NA\"","title":"charts_to_objects"},{"location":"using-anovos/config_file.html#report_generation","text":"\ud83d\udd0e Corresponds to data_report.report_generation This configuration block controls the generation of the actual report, i.e., the data that is included and the layout. See the report generation documentation for more details. master_path : The path to the preprocessed data generated during the report_preprocessing step. id_col : The ID column present in the input dataset label_col : Name of label or target column in the input dataset. corr_threshold : The threshold above which attributes are considered to be correlated and thus, redundant. Its value is between 0 and 1 . iv_threshold : The threshold above which attributes are considered ot be significant. Its value is between 0 and 1 . Information Value Variable's Predictiveness <0.02 Not useful for prediction 0.02 to 0.1 Weak predictive power 0.1 to 0.3 Medium predictive power 0.3 to 0.5 Strong predictive power >0.5 Suspicious predictive power drift_threshold_model : The threshold above which an attribute is flagged as exhibiting drift. Its value is between 0 and 1 . dataDict_path : The path to the data dictionary containing the exact names and definitions of the attributes. This information is used in the report to aid comprehensibility. metricDict_path : Path to the metric dictionary. final_report_path : The path where final report will be saved. It can be a local path, an \ud83d\udcd6 S3 path (when running on AWS), a path to a file resource on Google Colab (see \ud83d\udcd6 this tutorial for an overview), or a path on the \ud83d\udcd6 Databricks File System (when running on Azure). \ud83e\udd13 Example: report_generation : master_path : 'report_stats' id_col : 'id_column' label_col : 'label_col' corr_threshold : 0.4 iv_threshold : 0.02 drift_threshold_model : 0.1 dataDict_path : 'data/income_dataset/data_dictionary.csv' metricDict_path : 'data/metric_dictionary.csv' final_report_path : 'report_stats'","title":"\ud83d\udcd1 report_generation"},{"location":"using-anovos/config_file.html#transformers","text":"\ud83d\udd0e Corresponds to data_transformer.transformers This block configures the data_transformer module that supports numerous pre-processing and transformation functions, such as binning, encoding, scaling, and imputation.","title":"\ud83d\udcd1 transformers"},{"location":"using-anovos/config_file.html#numerical_mathops","text":"This group of functions is used to perform mathematical transformations of numerical attributes.","title":"numerical_mathops"},{"location":"using-anovos/config_file.html#feature_transformation","text":"\ud83d\udd0e Corresponds to transformers.feature_transformation list_of_cols : The numerical columns (list of strings or string of column names separated by | ) to transform. Can be set to \"all\" to include all numerical columns. drop_cols : The numerical columns (list of strings or string of column names separated by | ) to exclude from feature transformation. method_type : The method to apply to use for transformation. The default method is sqrt ( \\sqrt{x} \\sqrt{x} ). Possible values are: ln log10 log2 exp powOf2 ( 2^x 2^x ) powOf10 ( 10^x 10^x ) powOfN ( N^x N^x ) sqrt ( \\sqrt{x} \\sqrt{x} ) cbrt ( \\sqrt[3]{x} \\sqrt[3]{x} ) sq ( x^2 x^2 ) cb ( x^3 x^3 ) toPowerN ( x^N x^N ) sin cos tan asin acos atan radians remainderDivByN ( x % N x % N ) factorial ( x! x! ) mul_inv ( 1/x 1/x ) floor ceil roundN (round to N decimal places) N : None by default. If method_type is powOfN , toPowerN , remainderDivByN , or roundN , N will be used as the required constant. \ud83e\udd13 Example 1: feature_transformation : list_of_cols : all drop_cols : [] method_type : sqrt \ud83e\udd13 Example 2: feature_transformation : list_of_cols : [ 'capital-gain' , 'capital-loss' ] drop_cols : [] method_type : sqrt feature_transformation : list_of_cols : [ 'age' , 'education_num' ] drop_cols : [] method_type : sq","title":"feature_transformation"},{"location":"using-anovos/config_file.html#boxcox_transformation","text":"\ud83d\udd0e Corresponds to transformers.boxcox_transformation list_of_cols : The columns (list of strings or string of column names separated by | ) to transform. Can be set to \"all\" to include all columns. drop_cols : The columns (list of strings or string of column names separated by | ) to exclude from Box-Cox transformation. boxcox_lambda : The \\lambda \\lambda value for the Box-Cox transformation. It can be given as a list where each element represents the value of \\lambda \\lambda for a single attribute. The length of the list must be the same as the number of columns to transform. number that is used for all attributes. If no value is given (the default), a search for the best \\lambda \\lambda will be conducted among the following values: [1, -1, 0.5, -0.5, 2, -2, 0.25, -0.25, 3, -3, 4, -4, 5, -5] . The search is conducted independently for each column. \ud83e\udd13 Example 1: boxcox_transformation : list_of_cols : num_feature1|num_feature2 drop_cols : [] \ud83e\udd13 Example 2: boxcox_transformation : list_of_cols : num_feature3|num_feature4 drop_cols : [] boxcox_lambda : [ -2 , -1 ]","title":"boxcox_transformation"},{"location":"using-anovos/config_file.html#numerical_binning","text":"This group of functions is used to transform numerical attributes into discrete (integer or categorical) attribute.","title":"numerical_binning"},{"location":"using-anovos/config_file.html#attribute_binning","text":"\ud83d\udd0e Corresponds to transformers.attribute_binning list_of_cols : The numerical columns (list of strings or string of column names separated by | ) to transform. Can be set to \"all\" to include all numerical columns. drop_cols : The columns (list of strings or string of column names separated by | ) to exclude from attribute binning. method_type : The binning method. Possible values are equal_frequency and equal_range . With equal_range , each bin is of equal size/width and with equal_frequency , each bin contains an equal number of rows. Defaults to equal_range . bin_size : The number of bins. Defaults to 10 . bin_dtype : The dtype of the transformed column. Possible values are numerical and categorical . With numerical , the values reflect the bin number ( 1 , 2 , \u2026). With categorical option, the values are a string that describes the minimal and maximal value of the bin. Defaults to numerical . \ud83e\udd13 Example: attribute_binning : list_of_cols : num_feature1|num_feature2 drop_cols : [] method_type : equal_frequency bin_size : 10 bin_dtype : numerical","title":"attribute_binning"},{"location":"using-anovos/config_file.html#monotonic_binning","text":"\ud83d\udd0e Corresponds to transformers.monotonic_binning list_of_cols : The numerical columns (list of strings or string of column names separated by | ) to transform. Can be set to \"all\" to include all numerical columns. drop_cols : The columns (list of strings or string of column names separated by | ) to exclude from monotonic binning. method_type : The binning method. Possible values are equal_frequency and equal_range . With equal_range , each bin is of equal size/width and with equal_frequency , each bin contains an equal number of rows. Defaults to equal_range . bin_size : The number of bins. Defaults to 10 . bin_dtype : The dtype of the transformed column. Possible values are numerical and categorical . With numerical , the values reflect the bin number ( 1 , 2 , \u2026). With categorical option, the values are a string that describes the minimal and maximal value of the bin. Defaults to numerical . \ud83e\udd13 Example: attribute_binning : list_of_cols : num_feature1|num_feature2 drop_cols : [] label_col : [ \"label_col\" ] event_label : [ \"class1\" ] method_type : equal_frequency bin_size : 10 bin_dtype : numerical","title":"monotonic_binning"},{"location":"using-anovos/config_file.html#numerical_expression","text":"","title":"numerical_expression"},{"location":"using-anovos/config_file.html#expression_parser","text":"\ud83d\udd0e Corresponds to transformers.expression_parser This function can be used to evaluate a list of SQL expressions and output the result as new features. Columns used in the SQL expression must be available in the dataset. list_of_expr : List of expressions to evaluate as new features e.g., [\"expr1\", \"expr2\"]. Alternatively, expressions can be specified in a string format, where different expressions are separated by pipe delimiter \u201c|\u201d e.g., \"expr1|expr2\". postfix : postfix for new feature name.Naming convention \"f\" + expression_index + postfix e.g. with postfix of \"new\", new added features are named as f0new, f1new etc. (Default value = \"\"). \ud83e\udd13 Example 1: expression_parser : list_of_expr : 'log(age) + 1.5|sin(capital-gain)+cos(capital-loss)' \ud83e\udd13 Example 2: expression_parser : list_of_expr : [ 'log(age) + 1.5' , 'sin(capital-gain)+cos(capital-loss)' ] Both Example 1 and Example 2 generate 2 new features: log(age) + 1.5 and sin(capital-gain)+cos(capital-loss) . The newly generated features will be appended to the dataframe as new columns: f0 and f1.","title":"expression_parser"},{"location":"using-anovos/config_file.html#categorical_outliers","text":"This function assigns less frequently seen values in a categorical column to a new category others .","title":"categorical_outliers"},{"location":"using-anovos/config_file.html#outlier_categories","text":"\ud83d\udd0e Corresponds to transformers.outlier_categories list_of_cols : The categorical columns (list of strings or string of column names separated by | ) to transform. Can be set to \"all\" to include all categorical columns. drop_cols : The columns (list of strings or string of column names separated by | ) to exclude from outlier transformation. coverage : The minimum fraction of rows that remain in their original category, given as a value between 0 and 1 . For example, with a coverage of 0.8 , the categories that 80% of the rows belong to remain and the more seldom occurring categories are mapped to others . The default value is 1.0 , which means that no rows are changed to others . max_category : Even if coverage is less, only (max_category - 1) categories will be mapped to actual name and rest to others. Caveat is when multiple categories have same rank, then #categories can be more than max_category. Defaults to 50 . \ud83e\udd13 Example 1: outlier_categories : list_of_cols : all drop_cols : [ 'id_column' , 'label_col' ] coverage : 0.9 max_category : 20 \ud83e\udd13 Example 2: outlier_categories : list_of_cols : [ \"cat_feature1\" , \"cat_feature2\" ] drop_cols : [] coverage : 0.8 max_category : 10 outlier_categories : list_of_cols : [ \"cat_feature3\" , \"cat_feature4\" ] drop_cols : [] coverage : 0.9 max_category : 15","title":"outlier_categories"},{"location":"using-anovos/config_file.html#categorical_encoding","text":"This group of transformers functions used to converting a categorical attribute into numerical attribute(s).","title":"categorical_encoding"},{"location":"using-anovos/config_file.html#cat_to_num_unsupervised","text":"\ud83d\udd0e Corresponds to transformers.cat_to_num_unsupervised list_of_cols : The categorical columns (list of strings or string of column names separated by | ) to encode. Can be set to \"all\" to include all categorical columns. drop_cols : The columns (list of strings or string of column names separated by | ) to exclude from categorical encoding. method_type : The encoding method. Set to 1 for label encoding and to 0 for one-hot encoding. With label encoding, each categorical value is assigned a unique integer based on the ordering specified through index_order . With one-hot encoding, each categorical value will be represented by a binary column. Defaults to 1 (label encoding). index_order : The order assigned to the categorical values when method_type is set to 1 (label encoding). Possible values are: frequencyDesc (default): Order by descending frequency. frequencyAsc : Order by ascending frequency. alphabetDesc : Order alphabetically (descending). alphabetAsc : Order alphabetically (ascending). cardinality_threshold : Columns with a cardinality above this threshold are excluded from enconding. Defaults to 100 . \ud83e\udd13 Example 1: cat_to_num_unsupervised : list_of_cols : all drop_cols : [ 'id_column' ] method_type : 0 cardinality_threshold : 10 \ud83e\udd13 Example 2: cat_to_num_unsupervised : list_of_cols : [ \"cat_feature1\" , \"cat_feature2\" ] drop_cols : [] method_type : 0 cardinality_threshold : 10 cat_to_num_unsupervised : list_of_cols : [ \"cat_feature3\" , \"cat_feature4\" ] drop_cols : [] method_type : 1","title":"cat_to_num_unsupervised"},{"location":"using-anovos/config_file.html#cat_to_num_supervised","text":"\ud83d\udd0e Corresponds to transformers.cat_to_num_supervised list_of_cols : The categorical columns (list of strings or string of column names separated by | ) to encode. Can be set to \"all\" to include all categorical columns. drop_cols : The columns (list of strings or string of column names separated by | ) to exclude from categorical encoding. label_col : The label/target column. Defaults to label . event_label : Value of the (positive) event (i.e, label 1 / true ). Defaults to 1 . \ud83e\udd13 Example: cat_to_num_supervised : list_of_cols : cat_feature1 | cat_feature2 drop_cols : [ 'id_column' ] label_col : income event_label : '>50K'","title":"cat_to_num_supervised"},{"location":"using-anovos/config_file.html#numerical_rescaling","text":"Group of functions to rescale numerical attributes.","title":"numerical_rescaling"},{"location":"using-anovos/config_file.html#normalization","text":"\ud83d\udd0e Corresponds to transformers.normalization list_of_cols : The numerical columns (list of strings or string of column names separated by | ) to normalize. Can be set to \"all\" to include all numerical columns. drop_cols : The columns (list of strings or string of column names separated by | ) to exclude from normalization. \ud83e\udd13 Example: normalization : list_of_cols : [ \"num_feature1\" , \"num_feature2\" ] drop_cols : []","title":"normalization"},{"location":"using-anovos/config_file.html#z_standardization","text":"\ud83d\udd0e Corresponds to transformers.z_standardization list_of_cols : The numerical columns (list of strings or string of column names separated by | ) to standardize. Can be set to \"all\" to include all numerical columns. drop_cols : The columns (list of strings or string of column names separated by | ) to exclude from standardization. \ud83e\udd13 Example: z_standardization : list_of_cols : [ \"num_feature1\" , \"num_feature2\" ] drop_cols : []","title":"z_standardization"},{"location":"using-anovos/config_file.html#iqr_standardization","text":"\ud83d\udd0e Corresponds to transformers.IQR_standardization list_of_cols : The numerical columns (list of strings or string of column names separated by | ) to standardize. Can be set to \"all\" to include all numerical columns. drop_cols : The columns (list of strings or string of column names separated by | ) to exclude from standardization. \ud83e\udd13 Example: IQR_standardization : list_of_cols : [ \"num_feature1\" , \"num_feature2\" , \"num_feature3\" ] drop_cols : []","title":"IQR_standardization"},{"location":"using-anovos/config_file.html#numerical_latentfeatures","text":"Group of functions to generate latent features to reduce the dimensionality of the input dataset.","title":"numerical_latentFeatures"},{"location":"using-anovos/config_file.html#pca_latentfeatures","text":"\ud83d\udd0e Corresponds to transformers.PCA_latentFeatures list_of_cols : The numerical columns (list of strings or string of column names separated by | ) to standardize. Can be set to \"all\" to include all numerical columns. drop_cols : The columns (list of strings or string of column names separated by | ) to exclude from latent features computation. explained_variance_cutoff : The required explained variance cutoff. Determines the number of encoded columns in the output. If N is the smallest integer such that the top N encoded columns explain more than the given variance threshold, these N columns be selected. Defaults to 0.95 . standardization : If True (the default), standardization is applied. False otherwise. standardization_configs : The arguments for the z_standardization function in dictionary format. Defaults to {\"pre_existing_model\": False} imputation : If True , imputation is applied. False (the default) otherwise. imputation_configs : Configuration for imputation in dictionary format. The name of the imputation function is specified with the key imputation_name (defaults to imputation_MMM ). Arguments for the imputation function can be passed using additional keys. \ud83e\udd13 Example 1: PCA_latentFeatures : list_of_cols : [ \"num_feature1\" , \"num_feature2\" , \"num_feature3\" ] explained_variance_cutoff : 0.95 standardization : False imputation : True \ud83e\udd13 Example 2: PCA_latentFeatures : list_of_cols : [ \"num_feature1\" , \"num_feature2\" , \"num_feature3\" ] explained_variance_cutoff : 0.8 standardization : False imputation : True PCA_latentFeatures : list_of_cols : [ \"num_feature4\" , \"num_feature5\" , \"num_feature6\" ] explained_variance_cutoff : 0.6 standardization : True imputation : True","title":"PCA_latentFeatures"},{"location":"using-anovos/config_file.html#autoencoder_latentfeatures","text":"\ud83d\udd0e Corresponds to transformers.autoencoder_latentFeatures list_of_cols : The numerical columns (list of strings or string of column names separated by | ) to standardize. Can be set to \"all\" to include all numerical columns. drop_cols : The columns (list of strings or string of column names separated by | ) to exclude from latent features computation. reduction_params : Determines the number of resulting encoded features. If reduction_params is below 1 , reduction_params * \"number of columns\" columns will be generated. Else, reduction_params columns will be generated. Defaults to 0.5 , i.e., the number of columns in the result is half the of number of columns in the input. sample_size : Maximum number of rows used for training the autoencoder model. Defaults to 500000 ( 5e5 ). epochs : The number of epochs to train the autoencoder model. Defaults to 100 . batch_size : The batch size for autoencoder model training. Defaults to 256 . standardization : If True (the default), standardization is applied. False otherwise. standardization_configs : The arguments for the z_standardization function in dictionary format. Defaults to {\"pre_existing_model\": False} imputation : If True , imputation is applied. False (the default) otherwise. imputation_configs : Configuration for imputation in dictionary format. The name of the imputation function is specified with the key imputation_name (defaults to imputation_MMM ). Arguments for the imputation function can be passed using additional keys. \ud83e\udd13 Example 1: autoencoder_latentFeatures : list_of_cols : [ \"num_feature1\" , \"num_feature2\" , \"num_feature3\" ] reduction_params : 0.5 sample_size : 10000 epochs : 20 batch_size : 256 \ud83e\udd13 Example 2: autoencoder_latentFeatures : list_of_cols : [ \"num_feature1\" , \"num_feature2\" ] reduction_params : 0.5 sample_size : 10000 epochs : 20 batch_size : 256 autoencoder_latentFeatures : list_of_cols : [ \"num_feature3\" , \"num_feature4\" , \"num_feature5\" , \"num_feature6\" , \"num_feature7\" ] reduction_params : 0.8 sample_size : 10000 epochs : 100 batch_size : 256","title":"autoencoder_latentFeatures"},{"location":"using-anovos/config_file.html#write_intermediate","text":"file_path : Path where intermediate datasets (after selecting, dropping, renaming, and recasting of columns) for quality checker operations, join dataset and concatenate dataset will be saved. file_type : (CSV, Parquet or Avro). file format of intermediate dataset file_configs (optional): Rest of the valid configuration can be passed through this options e.g., repartition, mode, compression, header, delimiter, inferSchema etc. This might look like: file_configs : mode : overwrite header : True delimiter : \", \" inferSchema : True For more information on available configuration options, see the following external documentation: \ud83d\udcd6 Writing CSV files \ud83d\udcd6 Writing Parquet files \ud83d\udcd6 Writing Avro files","title":"\ud83d\udcd1 write_intermediate"},{"location":"using-anovos/config_file.html#write_main","text":"file_path : Path where final cleaned input dataset will be saved. file_type : (CSV, Parquet or Avro). file format of final dataset file_configs (optional): Rest of the valid configuration can be passed through this options e.g., repartition, mode, compression, header, delimiter, inferSchema etc. This might look like: file_configs : mode : overwrite header : True delimiter : \", \" inferSchema : True For more information on available configuration options, see the following external documentation: \ud83d\udcd6 Writing CSV files \ud83d\udcd6 Writing Parquet files \ud83d\udcd6 Writing Avro files","title":"\ud83d\udcd1 write_main"},{"location":"using-anovos/config_file.html#write_stats","text":"file_path : Path where all tables/stats of anovos modules (data drift & data analyzer) will be saved. file_type : (CSV, Parquet or Avro). file format of final dataset file_configs (optional): Rest of the valid configuration can be passed through this options e.g., repartition, mode, compression, header, delimiter, inferSchema etc. This might look like: file_configs : mode : overwrite header : True delimiter : \", \" inferSchema : True For more information on available configuration options, see the following external documentation: \ud83d\udcd6 Writing CSV files \ud83d\udcd6 Writing Parquet files \ud83d\udcd6 Writing Avro files","title":"\ud83d\udcd1 write_stats"},{"location":"using-anovos/feature_recommender.html","text":"Feature Recommender Feature engineering has always played a crucial role in solving any Machine learning (ML) related problems. Features/Predictors decide whether the Machine Learning projects are successful or not. However, coming up with good & intuitive features is not an easy task. Identifying list of potential features to build is a very hard to come up and requires both expertise in domain knowledge and technical aspects of Machine learning. In fact, 80% of Data Scientists' time are being spent on data wrangling and Feature Engineering tasks, and only 20% is for fine-tuning the model and testing out. Building features from scratch is a cold-start problem for any Data Scientists to figure out what features would be used to help them in creating their models. There are many tools to help Data Scientist to narrow down the features, but they are either not scalable, or very comprehensive to understand and operate. Here, within ANOVOS V0.2, we launch an open-source tool, Feature Explorer and Recommender (FER) module, in order to help the Machine Learning community with these cold start Feature Engineering problems. With Feature Explorer and Recommender module, we mainly address two problems: Create a platform for Data Scientists to explore available/already used features based on their interest of Industries/Domain and Use cases Recommend better features for Data Scientists to address cold-start problems (based on the data they have in hand) Feature Explorer and Recommender utilizes Semantic similarity based Language Modeling in Natural Language Processing (NLP). Semantic matching techniques aims to determine the similarity between words, lines, and sentences through multiple metrics. In this module, we use all-mpnet-base-v2 for our semantic model. This model is built based upon Microsoft Mpnet base model, masked and permuted pre-training for language understanding. Its performance triumphs BERT, XLNet, RoBERTa for language modeling and text recognition. The importance features of this model are, Trained on more than 1 billion training pairs, including around 300 millions research paper citation pairs Fine-tuned using cosine similarity from sentence pairs, then apply cross entropy loss by comparing true pairs Our solution consists of 3 main steps: Using the pretrained model, convert textual data into tensors (Text Embedding Technique) Compute similarity scores of each input attribute name & description across both corpora (Anovos Feature Corpus & User Data Dictionary Corpus) Sort the results and get the matches for each input feature based on their scores See below for the solution workflow of FER for further understanding of our solution.","title":"Feature Recommender"},{"location":"using-anovos/feature_recommender.html#feature-recommender","text":"Feature engineering has always played a crucial role in solving any Machine learning (ML) related problems. Features/Predictors decide whether the Machine Learning projects are successful or not. However, coming up with good & intuitive features is not an easy task. Identifying list of potential features to build is a very hard to come up and requires both expertise in domain knowledge and technical aspects of Machine learning. In fact, 80% of Data Scientists' time are being spent on data wrangling and Feature Engineering tasks, and only 20% is for fine-tuning the model and testing out. Building features from scratch is a cold-start problem for any Data Scientists to figure out what features would be used to help them in creating their models. There are many tools to help Data Scientist to narrow down the features, but they are either not scalable, or very comprehensive to understand and operate. Here, within ANOVOS V0.2, we launch an open-source tool, Feature Explorer and Recommender (FER) module, in order to help the Machine Learning community with these cold start Feature Engineering problems. With Feature Explorer and Recommender module, we mainly address two problems: Create a platform for Data Scientists to explore available/already used features based on their interest of Industries/Domain and Use cases Recommend better features for Data Scientists to address cold-start problems (based on the data they have in hand) Feature Explorer and Recommender utilizes Semantic similarity based Language Modeling in Natural Language Processing (NLP). Semantic matching techniques aims to determine the similarity between words, lines, and sentences through multiple metrics. In this module, we use all-mpnet-base-v2 for our semantic model. This model is built based upon Microsoft Mpnet base model, masked and permuted pre-training for language understanding. Its performance triumphs BERT, XLNet, RoBERTa for language modeling and text recognition. The importance features of this model are, Trained on more than 1 billion training pairs, including around 300 millions research paper citation pairs Fine-tuned using cosine similarity from sentence pairs, then apply cross entropy loss by comparing true pairs Our solution consists of 3 main steps: Using the pretrained model, convert textual data into tensors (Text Embedding Technique) Compute similarity scores of each input attribute name & description across both corpora (Anovos Feature Corpus & User Data Dictionary Corpus) Sort the results and get the matches for each input feature based on their scores See below for the solution workflow of FER for further understanding of our solution.","title":"Feature Recommender"},{"location":"using-anovos/limitations.html","text":"Current Limitations of Anovos The current V0.2.0 release of Anovos still has some limitations, which we will address in the upcoming releases. To learn more about what's on the horizon, check out our roadmap . \ud83d\udd23 Data Anovos currently only supports numerical, categorical and datetime/timestamp related columns (cross sectional & transactional level). We plan to add support for additional data types such as (struct) arrays, geo spatical etc. in the future. Geospatial columns like geohash or lat/long can only be analysed as categorical (geohash) or numerical (lat/long). Functionalities specific to geospatial features will be supported in the later releases. Anovos currently relies on Apache Spark's automatic schema detection . In case some numerical columns were deliberately saved as string, they will show up as categorical columns when loaded into a DataFrame (except for CSV files). \ud83c\udfce Performance Computing the mode and/or distinct value counts are the most expensive operations in Anovos . We aim to further optimize them in the upcoming releases. Calculating a correlation matrix may result in memory issues if very high cardinality categorical features are involved \u2013 a limitation that was propagated from the underlying phik library. Therefore, we recommend dropping these columns if not necessary. The invalid entries detection may yield false positives. Hence, be cautious when using the inbuilt treatment option. The categorical encoding functions - cat_to_num_supervised & cat_to_num_unsupervised - may show some performance issues with very high cardinality columns. Therefore, it is recommended to reduce cardinality before subjecting them to encoding or put an appropriate threshold to drop them from the analysis while encoding. Sample size for constructing the imputation models in imputation_sklearn or creating latent features through autoencoder_latentFeatures should be selected with caution, taking into the consideration the dataset size and the number of columns. This sample dataset is converted into pandas dataframe which run operations on a single node (driver). If sample dataset is too large to fit into the driver, it may throw error or cause severe performance issue. \ud83d\udd29 Other The stability index can currently only be calculated for numerical columns. The exception and error handling is at times inconsistent. Please don't hesitate to file an issue on GitHub if you encounter any problems.","title":"Limitations"},{"location":"using-anovos/limitations.html#current-limitations-of-anovos","text":"The current V0.2.0 release of Anovos still has some limitations, which we will address in the upcoming releases. To learn more about what's on the horizon, check out our roadmap .","title":"Current Limitations of Anovos"},{"location":"using-anovos/limitations.html#data","text":"Anovos currently only supports numerical, categorical and datetime/timestamp related columns (cross sectional & transactional level). We plan to add support for additional data types such as (struct) arrays, geo spatical etc. in the future. Geospatial columns like geohash or lat/long can only be analysed as categorical (geohash) or numerical (lat/long). Functionalities specific to geospatial features will be supported in the later releases. Anovos currently relies on Apache Spark's automatic schema detection . In case some numerical columns were deliberately saved as string, they will show up as categorical columns when loaded into a DataFrame (except for CSV files).","title":"\ud83d\udd23 Data"},{"location":"using-anovos/limitations.html#performance","text":"Computing the mode and/or distinct value counts are the most expensive operations in Anovos . We aim to further optimize them in the upcoming releases. Calculating a correlation matrix may result in memory issues if very high cardinality categorical features are involved \u2013 a limitation that was propagated from the underlying phik library. Therefore, we recommend dropping these columns if not necessary. The invalid entries detection may yield false positives. Hence, be cautious when using the inbuilt treatment option. The categorical encoding functions - cat_to_num_supervised & cat_to_num_unsupervised - may show some performance issues with very high cardinality columns. Therefore, it is recommended to reduce cardinality before subjecting them to encoding or put an appropriate threshold to drop them from the analysis while encoding. Sample size for constructing the imputation models in imputation_sklearn or creating latent features through autoencoder_latentFeatures should be selected with caution, taking into the consideration the dataset size and the number of columns. This sample dataset is converted into pandas dataframe which run operations on a single node (driver). If sample dataset is too large to fit into the driver, it may throw error or cause severe performance issue.","title":"\ud83c\udfce Performance"},{"location":"using-anovos/limitations.html#other","text":"The stability index can currently only be calculated for numerical columns. The exception and error handling is at times inconsistent. Please don't hesitate to file an issue on GitHub if you encounter any problems.","title":"\ud83d\udd29 Other"},{"location":"using-anovos/mlflow.html","text":"MLflow Integration MLflow is a popular open source solution for managing all aspects of the machine learning lifecycle. The platform encompasses four components: MLflow Tracking to record code, data, configuration, and results of ML experiments MLflow Projects to package data science code in a format that allows it to run reproducibly in different environments MLflow Models to deploy ML models in different environments MLflow Model Registry to store and manage ML models in a central repository To learn more about MLflow and its capabilities, see the MLflow documentation . Reporting Anovos data to MLflow Tracking Anovos integrates with MLflow by reporting workflow metadata and results to MLflow Tracking . To track your workflows with MLflow , add an mlflow block to your workflow configuration file : mlflow : experiment : \"Anovos\" # The name of the MLflow experiment associated with your workflow tracking_uri : \"http://127.0.0.1:8889\" # The URL of the MLflow Tracking server track_output : True # Store the workflow output (i.e., resulting dataset(s)) track_reports : True # Store the generated reports track_intermediates : False # Store any intermediate data generated by your workflow Current Limitations It is currently not possible to select which intermediate outputs are stored. If track_intermediate is set to True , all intermediate outputs will be stored. Roadmap We're exploring integration of Anovos with MLflow Projects and MLFlow Pipelines . Let us know which capabilities you'd like to see in future versions of Anovos !","title":"MLflow Integration"},{"location":"using-anovos/mlflow.html#mlflow-integration","text":"MLflow is a popular open source solution for managing all aspects of the machine learning lifecycle. The platform encompasses four components: MLflow Tracking to record code, data, configuration, and results of ML experiments MLflow Projects to package data science code in a format that allows it to run reproducibly in different environments MLflow Models to deploy ML models in different environments MLflow Model Registry to store and manage ML models in a central repository To learn more about MLflow and its capabilities, see the MLflow documentation .","title":"MLflow Integration"},{"location":"using-anovos/mlflow.html#reporting-anovos-data-to-mlflow-tracking","text":"Anovos integrates with MLflow by reporting workflow metadata and results to MLflow Tracking . To track your workflows with MLflow , add an mlflow block to your workflow configuration file : mlflow : experiment : \"Anovos\" # The name of the MLflow experiment associated with your workflow tracking_uri : \"http://127.0.0.1:8889\" # The URL of the MLflow Tracking server track_output : True # Store the workflow output (i.e., resulting dataset(s)) track_reports : True # Store the generated reports track_intermediates : False # Store any intermediate data generated by your workflow","title":"Reporting Anovos data to MLflow Tracking"},{"location":"using-anovos/mlflow.html#current-limitations","text":"It is currently not possible to select which intermediate outputs are stored. If track_intermediate is set to True , all intermediate outputs will be stored.","title":"Current Limitations"},{"location":"using-anovos/mlflow.html#roadmap","text":"We're exploring integration of Anovos with MLflow Projects and MLFlow Pipelines . Let us know which capabilities you'd like to see in future versions of Anovos !","title":"Roadmap"},{"location":"using-anovos/roadmap.html","text":"Anovos Product Roadmap Anovos is built and released as an open source project based on our experience in handling massive data sets to produce predictive features. At Mobilewalla , we process terabytes of mobile engagement signals daily to mine consumer behavior and use features from that data to build distributed machine learning models to solve a wide range of business problems. On this journey, we faced lots of challenges due to the lack of a comprehensive and scalable library. After realizing the unavailability of such libraries, we designed and implemented Anovos as an open source library for every data scientists\u2019 use. \ud83d\udee3 The Roadmap We plan to bring fully functional Anovos over the course of three major releases: V0.1,V0.2 and version 1.0. V0.1 (November 2021) The V0.1 release of Anovos had all the essential data ingestion and comprehensive data analysis functionalities, as well as the data pre-processing and cleaning mechanisms. It also included some key differentiating functionalities, like data drift and stability computations, which are crucial in deciding the need for model refresh/tweak options. Another benefit of Anovos is a dynamic visualization component configured based on data ingestion pipelines. Every data metric computed from the Anovos ingestion process can be visualized and utilized for CXO level decision-making. Details Data Ingest AWS S3 Storage integration Read and write to/from local files Column selection and renaming Support for Parquet and CSV files Support for numerical and categorical data types Data Analyzer and Diagnostics Frequency analysis Attribute/feature vs. target Attribute/feature interaction/association Data Preprocessing and Cleaning Outlier detection (IQR/Standardization) Treatment of invalid values Missing attributes analysis Data Health and Monitoring Data drift identification (Hellinger Distance, KS, JSD, and PSI) Attribute stability analysis Overall data quality analysis Runtime Environment support Local Docker-based AWS EMR Report Visualization Comprehensive 360 degree view report of the ingested data (Numerical & Categorical) Executive summary Wiki Descriptive statistics Quality Checker Attribute association Data drift & stability V0.2 Release (March 2022) In this release of Anovos , the library will support ingesting from cloud service providers like MS Azure and will also have mechanisms to read/write different file formats such as Avro and nested Json. It will also enable ingesting various data types (see the above figure for the details). The key differentiating functionality of this release is that \u201cFeature Explorer & Feature Recommender\u201d for data scientists and end-users to resolve their cold-start problems, which will immensely reduce their literature review time. The V0.2 release will also have another important capability named as Feature Stability estimator based on the composition of a given feature using set of attribute/s. This will greatly benefit data scientists to understand the potential feature instabilities which could harm the resillency of a ML model. With the V0.2 release Anovos can be used in day to day practices of any Data Scientists or Data Analysts Details Data Ingest Microsoft Azure Blob Storage integration Support for Avro and nested JSON files Support for additional data types: Time stamps columns Support for Timeseries data ingestion Data Cleaning and Transformation Parsing Merging Converting/Coding Derivations Calculations Imputations Auto encoders Dimension reduction Date/Time related transformations Feature Explorer / Feature recommender (Semantic search enabled) To recommend potential features based on the industry, use case, and the ingested data dictionary Industry specific use cases and respective features Telco BFSI Retail Healthcare Transportation Supply chain Recommendations are enabled by Semantic search capability Supported by pre-compiled feature corpus Feature Stability This will be an extension of the attribute stability capabilities of the V0.1 release Extended Spark & Python support Compatibility with different Spark & Python versions Apache Spark 2.4.x on Java 8 with Python 3.7.x Apache Spark 3.1.x on Java 11 with Python 3.9.x Apache Spark 3.2.x on Java 11 with Python 3.9.x Runtime Environment support Microsoft Azure Databricks Version 1.0 (Summer of 2022) We'll release version 1.0 of Anovos in June 2022 with the functionalities to support an end-to-end machine learning workflow. It will be able to store the generated features in an open source feature store, like Feast. It will also support running open source based Auto ML models and ML workflow integration. This release will also include a mechanism to explain the model behavior by having the respective Shapley values. Details Feature Store Integration: APIs to integrate Anovos with existing OSS Feature Stores Explainable AI: SHAP value computations Auto ML Integration: APIs to integrate Anovos with existing OSS Auto ML solutions ML Flow workflow integration","title":"Roadmap"},{"location":"using-anovos/roadmap.html#anovos-product-roadmap","text":"Anovos is built and released as an open source project based on our experience in handling massive data sets to produce predictive features. At Mobilewalla , we process terabytes of mobile engagement signals daily to mine consumer behavior and use features from that data to build distributed machine learning models to solve a wide range of business problems. On this journey, we faced lots of challenges due to the lack of a comprehensive and scalable library. After realizing the unavailability of such libraries, we designed and implemented Anovos as an open source library for every data scientists\u2019 use.","title":"Anovos Product Roadmap"},{"location":"using-anovos/roadmap.html#the-roadmap","text":"We plan to bring fully functional Anovos over the course of three major releases: V0.1,V0.2 and version 1.0.","title":"\ud83d\udee3 The Roadmap"},{"location":"using-anovos/roadmap.html#v01-november-2021","text":"The V0.1 release of Anovos had all the essential data ingestion and comprehensive data analysis functionalities, as well as the data pre-processing and cleaning mechanisms. It also included some key differentiating functionalities, like data drift and stability computations, which are crucial in deciding the need for model refresh/tweak options. Another benefit of Anovos is a dynamic visualization component configured based on data ingestion pipelines. Every data metric computed from the Anovos ingestion process can be visualized and utilized for CXO level decision-making.","title":"V0.1 (November 2021)"},{"location":"using-anovos/roadmap.html#details","text":"Data Ingest AWS S3 Storage integration Read and write to/from local files Column selection and renaming Support for Parquet and CSV files Support for numerical and categorical data types Data Analyzer and Diagnostics Frequency analysis Attribute/feature vs. target Attribute/feature interaction/association Data Preprocessing and Cleaning Outlier detection (IQR/Standardization) Treatment of invalid values Missing attributes analysis Data Health and Monitoring Data drift identification (Hellinger Distance, KS, JSD, and PSI) Attribute stability analysis Overall data quality analysis Runtime Environment support Local Docker-based AWS EMR Report Visualization Comprehensive 360 degree view report of the ingested data (Numerical & Categorical) Executive summary Wiki Descriptive statistics Quality Checker Attribute association Data drift & stability","title":"Details"},{"location":"using-anovos/roadmap.html#v02-release-march-2022","text":"In this release of Anovos , the library will support ingesting from cloud service providers like MS Azure and will also have mechanisms to read/write different file formats such as Avro and nested Json. It will also enable ingesting various data types (see the above figure for the details). The key differentiating functionality of this release is that \u201cFeature Explorer & Feature Recommender\u201d for data scientists and end-users to resolve their cold-start problems, which will immensely reduce their literature review time. The V0.2 release will also have another important capability named as Feature Stability estimator based on the composition of a given feature using set of attribute/s. This will greatly benefit data scientists to understand the potential feature instabilities which could harm the resillency of a ML model. With the V0.2 release Anovos can be used in day to day practices of any Data Scientists or Data Analysts","title":"V0.2 Release (March 2022)"},{"location":"using-anovos/roadmap.html#details_1","text":"Data Ingest Microsoft Azure Blob Storage integration Support for Avro and nested JSON files Support for additional data types: Time stamps columns Support for Timeseries data ingestion Data Cleaning and Transformation Parsing Merging Converting/Coding Derivations Calculations Imputations Auto encoders Dimension reduction Date/Time related transformations Feature Explorer / Feature recommender (Semantic search enabled) To recommend potential features based on the industry, use case, and the ingested data dictionary Industry specific use cases and respective features Telco BFSI Retail Healthcare Transportation Supply chain Recommendations are enabled by Semantic search capability Supported by pre-compiled feature corpus Feature Stability This will be an extension of the attribute stability capabilities of the V0.1 release Extended Spark & Python support Compatibility with different Spark & Python versions Apache Spark 2.4.x on Java 8 with Python 3.7.x Apache Spark 3.1.x on Java 11 with Python 3.9.x Apache Spark 3.2.x on Java 11 with Python 3.9.x Runtime Environment support Microsoft Azure Databricks","title":"Details"},{"location":"using-anovos/roadmap.html#version-10-summer-of-2022","text":"We'll release version 1.0 of Anovos in June 2022 with the functionalities to support an end-to-end machine learning workflow. It will be able to store the generated features in an open source feature store, like Feast. It will also support running open source based Auto ML models and ML workflow integration. This release will also include a mechanism to explain the model behavior by having the respective Shapley values.","title":"Version 1.0 (Summer of 2022)"},{"location":"using-anovos/roadmap.html#details_2","text":"Feature Store Integration: APIs to integrate Anovos with existing OSS Feature Stores Explainable AI: SHAP value computations Auto ML Integration: APIs to integrate Anovos with existing OSS Auto ML solutions ML Flow workflow integration","title":"Details"},{"location":"using-anovos/scaling.html","text":"Using Anovos at Scale Anovos is built for feature engineering and data processing at scale. The library was built for and tested on Mobilewalla's mobile engagement data with the following attributes: Property Value Size 50 GB No. of Rows 384,694,946 No. of Columns 35 No. of Numerical Columns 4 No. of Categorical Columns 31 \u23f1 Benchmark To benchmark Anovos ' performance, we ran a pipeline on this dataset. The entire pipeline was optimized such that the computed statistics could be reused by other functions as much as possible. For example, the modes (the most frequently seen values) computed by the measures_of_centralTendency function were also used for imputation while treating null values in a column with nullColumns_detection or detecting a columns' biasedness using biasedness_detection . Hence, the time recorded for a function in the benchmark might (Pipeline Mode) differ significantly from the time taken by the same function when running in isolation (Standalone Mode). Further, Apache Spark does its own set of optimizations of transformations under the hood while running multiple functions together, which further adds to the time difference. Function Pipeline (mins) Standalone (minutes) global_summary 1 1 measures_of_counts 5 5 measures_of_centralTendency 30 30 measures_of_cardinality 49 32 measures_of_percentiles 1 1 measures_of_dispersion 1 1 measures_of_shape 3 3 duplicate_detection 5 5 nullRows_detection 4 5 invalidEntries_detection 15 41 IDness_detection 2 8 biasedness_detection 2 28 outlier_detection 4 8 nullColumns_detection 2 63 variable_clustering 2 3 IV_calculation * 8 9 IG_calculation * 6 8 * A binary categorical column was selected as a target variable to test this function. To see if the library works with large number of attributes, we horizontally scale tested on different dataset with the following attributes: Property Value Size 15 GB No. of Rows 40,507,005 No. of Columns 284 No. of Numerical Columns 252 No. of Categorical Columns 23 Function Time (mins) global_summary 0.2 measures_of_counts 3 measures_of_centralTendency 9 measures_of_cardinality 12 measures_of_percentiles 7 measures_of_dispersion 9 measures_of_shape 5 duplicate_detection 2 nullRows_detection 4 invalidEntries_detection 9 IDness_detection 2 biasedness_detection 2 outlier_detection 85 nullColumns_detection 3 cat_to_num_unsupervised 4 cat_to_num_supervised 2 z_standardization 6 IQR_standardization 3 normalization 6 PCA_latentFeatures 20 Limitations For current performance limitations, see the dedicated overview of Anovos ' limitations .","title":"Scaling Up"},{"location":"using-anovos/scaling.html#using-anovos-at-scale","text":"Anovos is built for feature engineering and data processing at scale. The library was built for and tested on Mobilewalla's mobile engagement data with the following attributes: Property Value Size 50 GB No. of Rows 384,694,946 No. of Columns 35 No. of Numerical Columns 4 No. of Categorical Columns 31","title":"Using Anovos at Scale"},{"location":"using-anovos/scaling.html#benchmark","text":"To benchmark Anovos ' performance, we ran a pipeline on this dataset. The entire pipeline was optimized such that the computed statistics could be reused by other functions as much as possible. For example, the modes (the most frequently seen values) computed by the measures_of_centralTendency function were also used for imputation while treating null values in a column with nullColumns_detection or detecting a columns' biasedness using biasedness_detection . Hence, the time recorded for a function in the benchmark might (Pipeline Mode) differ significantly from the time taken by the same function when running in isolation (Standalone Mode). Further, Apache Spark does its own set of optimizations of transformations under the hood while running multiple functions together, which further adds to the time difference. Function Pipeline (mins) Standalone (minutes) global_summary 1 1 measures_of_counts 5 5 measures_of_centralTendency 30 30 measures_of_cardinality 49 32 measures_of_percentiles 1 1 measures_of_dispersion 1 1 measures_of_shape 3 3 duplicate_detection 5 5 nullRows_detection 4 5 invalidEntries_detection 15 41 IDness_detection 2 8 biasedness_detection 2 28 outlier_detection 4 8 nullColumns_detection 2 63 variable_clustering 2 3 IV_calculation * 8 9 IG_calculation * 6 8 * A binary categorical column was selected as a target variable to test this function. To see if the library works with large number of attributes, we horizontally scale tested on different dataset with the following attributes: Property Value Size 15 GB No. of Rows 40,507,005 No. of Columns 284 No. of Numerical Columns 252 No. of Categorical Columns 23 Function Time (mins) global_summary 0.2 measures_of_counts 3 measures_of_centralTendency 9 measures_of_cardinality 12 measures_of_percentiles 7 measures_of_dispersion 9 measures_of_shape 5 duplicate_detection 2 nullRows_detection 4 invalidEntries_detection 9 IDness_detection 2 biasedness_detection 2 outlier_detection 85 nullColumns_detection 3 cat_to_num_unsupervised 4 cat_to_num_supervised 2 z_standardization 6 IQR_standardization 3 normalization 6 PCA_latentFeatures 20","title":"\u23f1 Benchmark"},{"location":"using-anovos/scaling.html#limitations","text":"For current performance limitations, see the dedicated overview of Anovos ' limitations .","title":"Limitations"},{"location":"using-anovos/workflow.html","text":"The Anovos Workflow We designed Anovos with an end-to-end machine learning workflow mind. Teams can use Anovos either as the foundation of their entire workflow or use as part of an existing pipeline. For example, an organization might have an end-to-end workflow that lacks components offered by Anovos . The organization can incorporate additional components by a simple API call for a function from Anovos . The following workflow diagram shows the potential ways to use Anovos in an end-to-end workflow settings:","title":"Workflow"},{"location":"using-anovos/workflow.html#the-anovos-workflow","text":"We designed Anovos with an end-to-end machine learning workflow mind. Teams can use Anovos either as the foundation of their entire workflow or use as part of an existing pipeline. For example, an organization might have an end-to-end workflow that lacks components offered by Anovos . The organization can incorporate additional components by a simple API call for a function from Anovos . The following workflow diagram shows the potential ways to use Anovos in an end-to-end workflow settings:","title":"The Anovos Workflow"},{"location":"using-anovos/data-reports/final_report.html","text":"Generating Finalized Reports with Anovos This section covers the final execution part where primarily the output generated by the previous step is being fetched upon and structured in the desirable UI layout. The primary function dealt here is anovos_report which caters to the: reading of available data produced from data analyzer module and chart objects as produced by the report preprocessing module computation of additional charts based on available data populating the reporting layer leveraging an open-sourced python package called datapane. capability of producing stand alone reports for individual sections (Descriptive Statistics, Quality Check, Attribute Associations, Data Drift & Stability & Time Series Analyzer) The following parameters are specified in the function anovos_report : master_path : The path which contains the data of intermediate output in terms of json chart objects, csv file (pandas df) id_col : The ID column is accepted to ensure & restrict unnecessary analysis to be performed on the same label_col : Name of label or target column in the input dataset. By default, the label_col is set as blank. corr_threshold : The threshold chosen beyond which the attributes are found to be redundant iv_threshold : The threshold beyond which the attributes are found to be significant in terms of model. drift_threshold_model: The threshold beyond which the attribute can be flagged as 1 or drifted as measured across different drift metrices specified by the user dataDict_path : The path containing the exact name, definition mapping of the attributes. This is eventually used to populate at the report for easy referencing metricDict_path : The path containing the metric dictionary run_type : Option to specify whether the execution happen locally or in EMR way final_report_path : Path where the final report needs to be saved output_type : Time category of analysis which can be between \"Daily\", \"Hourly\", \"Weekly\"","title":"Final Report"},{"location":"using-anovos/data-reports/final_report.html#generating-finalized-reports-with-anovos","text":"This section covers the final execution part where primarily the output generated by the previous step is being fetched upon and structured in the desirable UI layout. The primary function dealt here is anovos_report which caters to the: reading of available data produced from data analyzer module and chart objects as produced by the report preprocessing module computation of additional charts based on available data populating the reporting layer leveraging an open-sourced python package called datapane. capability of producing stand alone reports for individual sections (Descriptive Statistics, Quality Check, Attribute Associations, Data Drift & Stability & Time Series Analyzer) The following parameters are specified in the function anovos_report : master_path : The path which contains the data of intermediate output in terms of json chart objects, csv file (pandas df) id_col : The ID column is accepted to ensure & restrict unnecessary analysis to be performed on the same label_col : Name of label or target column in the input dataset. By default, the label_col is set as blank. corr_threshold : The threshold chosen beyond which the attributes are found to be redundant iv_threshold : The threshold beyond which the attributes are found to be significant in terms of model. drift_threshold_model: The threshold beyond which the attribute can be flagged as 1 or drifted as measured across different drift metrices specified by the user dataDict_path : The path containing the exact name, definition mapping of the attributes. This is eventually used to populate at the report for easy referencing metricDict_path : The path containing the metric dictionary run_type : Option to specify whether the execution happen locally or in EMR way final_report_path : Path where the final report needs to be saved output_type : Time category of analysis which can be between \"Daily\", \"Hourly\", \"Weekly\"","title":"Generating Finalized Reports with Anovos"},{"location":"using-anovos/data-reports/html_report.html","text":"Generating HTML Reports with Anovos The final output is generated in form of HTML report. This has 7 sections viz. Executive Summary , Wiki , Descriptive Statistics , Quality Check , Attribute Associations , Data Drift & Data Stability and Time Series Analyzer at most which can be seen basis user input. We\u2019ve tried to detail each section based on the analysis performed referring to a publicly available Income Dataset . Executive Summary The Executive Summary gives an overall summary of the key statistics obtained from the analyzed data such as : - dimensions of the analysis data - nature of use case whether target variable is involved or not along with the distribution - overall data diagnosis view as seen across some of the key metrices. Wiki The Wiki tab has two different sections consisting of: Data Dictionary : This section contains the details of the attributes present in the data frame. The user if specifies the attribute wise definition at a specific path, then the details of the same will be populated along with the data type else only the attribute wise datatype will be seen. Metric Dictionary : Mostly containing the details about the different sections under the report. This could be a quick reference for the user. Descriptive Statistics The Descriptive Statistics section summarizes the dataset with key statistical metrics and distribution plots through the following modules: Global Summary : Details about the data dimensions and the attribute wise information. Statistics By Metric Type includes the following modules: Measures of Counts : Details about the attribute wise count, fill rate, etc. Measures of Central Tendency : Details about the measurement of central tendency in terms of mean, median, mode. Measures of Cardinality : Details about the uniqueness in categories for each attribute. Measures of Percentiles : Indicates the different attribute value associated against the range of percentile cut offs. This helps to understand the spread of attributes. Measures of Dispersion : Explains how much the data is dispersed through metrics like Standard Deviation, Variance, Covariance, IQR and range for each attribute Measures of Shape : Describe the tail ness of distribution (or pattern) for different attributes through skewness and kurtosis. Attribute Visuations includes the following modules: Numeric : Visualize the distributions of Numerical attributes using Histograms Categorical : Visualize the distributions of Categorical attributes using Barplot Quality Check The Quality Check section consists of a qualitative inspection of the data at a row & columnar level. It consists of the following modules: Column Level Null Columns Detections \u2013 Detect the sparsity of the datasets, e.g., count and percentage of missing value of attributes Outlier Detection \u2013 Used to detect and visualize the outlier present in numerical attributes of the datasets Violin Plot - Displays the spread of numerical attributes IDness Detection - Measures the cardinality associated across attributes Biasedness Detection - Useful to identifying columns to see if they are biased or skewed towards one specific value Invalid Entries Detection - Checks for certain suspicious patterns in attributes\u2019 values Row Level Duplicate Detection \u2013 Measure number of rows in the datasets that have same value for each attribute NullRows Detection - Measure the count/percentage of rows which have missing/null attributes Attribute Associations Attribute Associations section details of the interaction between different attributes and/or the relationship between an attribute & the binary target variable Association Matrix & Plot is a Correlation Measure of the strength of relationship among each attribute by finding correlation coefficient having range -1.0 to 1.0. Visualization is shown through heat map to describe the strength of relationship among attributes. Information Value Computation is used to rank variables on the basis of their importance. Greater the value of IV higher rthe attribute importance. IV less than 0.02 is not useful for prediction. Bar plot is used to show the significance in descending order. Information Gain Computation measures the reduction in entropy by splitting a dataset according to given value of a attribute. Bar plot is used to show the significance in descending order. Variable Clustering divides the numerical attributes into disjoint or hierarchical clusters based on linear relationship of attributes. Attribute to Target Association determines the event rate trend across different attribute categories : Numeric Categorical Data Drift & Data Stability Data Drift Analysis This module mainly focus on covariate shift based drift detection. The table below describes about the statistical metrics measuring the data drift of an attribute from source to target distribution: Population Stability Index (PSI), Jensen-Shannon Divergence (JSD), Hellinger Distance (HD) and Kolmogorov-Smirnov Distance (KS). An attribute is flagged as drifted if any drift metric is found to be above the threshold set by the user or 0.1 (default) Overall Data Health The data stability is represented by a single metric to summarise the stability of an attribute over multiple time periods. For example, given 9 datasets collected in 9 consecutive time periods (D1, D2, \u2026, D9), data stability index of an attribute measures how stable the attribute is from D1 to D9. Data Stability Analysis The major difference between data drift and data stability is that data drift analysis is only based on 2 datasets: source and target. However data stability analysis can be performed on multiple datasets. In addition, the source dataset is not required indicating that the stability index can be directly computed among multiple target datasets by comparing the statistical properties among them. Attribute wise stability charts have been plotted and the results are being highlighted. Time Series Analyzer This section summarizes the information about timestamp features and how they are interactive with other attributes. An exhaustive diagnosis is done by looking at different time series components, how they could be useful in deriving insights for further downstream applications. The Basic Landscaping The initial analysis details we records where we understand whether a particular field qualifies for Time Series check or not. Time Stamp Data Diagnosis The landscaping & diagnosis work done on the fields which have been auto-detected as time series. Different statistics are taken out pertaining to the association of devices for id_date & date_id pair combination as specified. Additionally, vital stats are also produced. Visualization across the Shortlisted Timestamp Attributes The visualization below shows the typical time series plots generated based on the analysis attributes and the granularity of data preferred for analysis ( daily , weekly , hourly ). The decomposed view largely describes about some of the typical components of time series forecasting like Trend, Seasonal & Residual on top of the Observed series. Inspecting on the decomposed view of Time Series is supposedly one of the key steps from analysis point irrespective of the model used later. The stationarity & transformation view help us in determinining how much the data can be quantified (through KPSS & ADSS test) in terms of transformation needed to attain stationarity. Additionally, we're showing on how a post transformation view basis Box-Cox-Transformation can be further used in the downstream applications.","title":"HTML Report"},{"location":"using-anovos/data-reports/html_report.html#generating-html-reports-with-anovos","text":"The final output is generated in form of HTML report. This has 7 sections viz. Executive Summary , Wiki , Descriptive Statistics , Quality Check , Attribute Associations , Data Drift & Data Stability and Time Series Analyzer at most which can be seen basis user input. We\u2019ve tried to detail each section based on the analysis performed referring to a publicly available Income Dataset .","title":"Generating HTML Reports with Anovos"},{"location":"using-anovos/data-reports/html_report.html#executive-summary","text":"The Executive Summary gives an overall summary of the key statistics obtained from the analyzed data such as : - dimensions of the analysis data - nature of use case whether target variable is involved or not along with the distribution - overall data diagnosis view as seen across some of the key metrices.","title":"Executive Summary"},{"location":"using-anovos/data-reports/html_report.html#wiki","text":"The Wiki tab has two different sections consisting of: Data Dictionary : This section contains the details of the attributes present in the data frame. The user if specifies the attribute wise definition at a specific path, then the details of the same will be populated along with the data type else only the attribute wise datatype will be seen. Metric Dictionary : Mostly containing the details about the different sections under the report. This could be a quick reference for the user.","title":"Wiki"},{"location":"using-anovos/data-reports/html_report.html#descriptive-statistics","text":"The Descriptive Statistics section summarizes the dataset with key statistical metrics and distribution plots through the following modules: Global Summary : Details about the data dimensions and the attribute wise information. Statistics By Metric Type includes the following modules: Measures of Counts : Details about the attribute wise count, fill rate, etc. Measures of Central Tendency : Details about the measurement of central tendency in terms of mean, median, mode. Measures of Cardinality : Details about the uniqueness in categories for each attribute. Measures of Percentiles : Indicates the different attribute value associated against the range of percentile cut offs. This helps to understand the spread of attributes. Measures of Dispersion : Explains how much the data is dispersed through metrics like Standard Deviation, Variance, Covariance, IQR and range for each attribute Measures of Shape : Describe the tail ness of distribution (or pattern) for different attributes through skewness and kurtosis. Attribute Visuations includes the following modules: Numeric : Visualize the distributions of Numerical attributes using Histograms Categorical : Visualize the distributions of Categorical attributes using Barplot","title":"Descriptive Statistics"},{"location":"using-anovos/data-reports/html_report.html#quality-check","text":"The Quality Check section consists of a qualitative inspection of the data at a row & columnar level. It consists of the following modules: Column Level Null Columns Detections \u2013 Detect the sparsity of the datasets, e.g., count and percentage of missing value of attributes Outlier Detection \u2013 Used to detect and visualize the outlier present in numerical attributes of the datasets Violin Plot - Displays the spread of numerical attributes IDness Detection - Measures the cardinality associated across attributes Biasedness Detection - Useful to identifying columns to see if they are biased or skewed towards one specific value Invalid Entries Detection - Checks for certain suspicious patterns in attributes\u2019 values Row Level Duplicate Detection \u2013 Measure number of rows in the datasets that have same value for each attribute NullRows Detection - Measure the count/percentage of rows which have missing/null attributes","title":"Quality Check"},{"location":"using-anovos/data-reports/html_report.html#attribute-associations","text":"Attribute Associations section details of the interaction between different attributes and/or the relationship between an attribute & the binary target variable Association Matrix & Plot is a Correlation Measure of the strength of relationship among each attribute by finding correlation coefficient having range -1.0 to 1.0. Visualization is shown through heat map to describe the strength of relationship among attributes. Information Value Computation is used to rank variables on the basis of their importance. Greater the value of IV higher rthe attribute importance. IV less than 0.02 is not useful for prediction. Bar plot is used to show the significance in descending order. Information Gain Computation measures the reduction in entropy by splitting a dataset according to given value of a attribute. Bar plot is used to show the significance in descending order. Variable Clustering divides the numerical attributes into disjoint or hierarchical clusters based on linear relationship of attributes. Attribute to Target Association determines the event rate trend across different attribute categories : Numeric Categorical","title":"Attribute Associations"},{"location":"using-anovos/data-reports/html_report.html#data-drift-data-stability","text":"Data Drift Analysis This module mainly focus on covariate shift based drift detection. The table below describes about the statistical metrics measuring the data drift of an attribute from source to target distribution: Population Stability Index (PSI), Jensen-Shannon Divergence (JSD), Hellinger Distance (HD) and Kolmogorov-Smirnov Distance (KS). An attribute is flagged as drifted if any drift metric is found to be above the threshold set by the user or 0.1 (default) Overall Data Health The data stability is represented by a single metric to summarise the stability of an attribute over multiple time periods. For example, given 9 datasets collected in 9 consecutive time periods (D1, D2, \u2026, D9), data stability index of an attribute measures how stable the attribute is from D1 to D9. Data Stability Analysis The major difference between data drift and data stability is that data drift analysis is only based on 2 datasets: source and target. However data stability analysis can be performed on multiple datasets. In addition, the source dataset is not required indicating that the stability index can be directly computed among multiple target datasets by comparing the statistical properties among them. Attribute wise stability charts have been plotted and the results are being highlighted.","title":"Data Drift &amp; Data Stability"},{"location":"using-anovos/data-reports/html_report.html#time-series-analyzer","text":"This section summarizes the information about timestamp features and how they are interactive with other attributes. An exhaustive diagnosis is done by looking at different time series components, how they could be useful in deriving insights for further downstream applications. The Basic Landscaping The initial analysis details we records where we understand whether a particular field qualifies for Time Series check or not. Time Stamp Data Diagnosis The landscaping & diagnosis work done on the fields which have been auto-detected as time series. Different statistics are taken out pertaining to the association of devices for id_date & date_id pair combination as specified. Additionally, vital stats are also produced. Visualization across the Shortlisted Timestamp Attributes The visualization below shows the typical time series plots generated based on the analysis attributes and the granularity of data preferred for analysis ( daily , weekly , hourly ). The decomposed view largely describes about some of the typical components of time series forecasting like Trend, Seasonal & Residual on top of the Observed series. Inspecting on the decomposed view of Time Series is supposedly one of the key steps from analysis point irrespective of the model used later. The stationarity & transformation view help us in determinining how much the data can be quantified (through KPSS & ADSS test) in terms of transformation needed to attain stationarity. Additionally, we're showing on how a post transformation view basis Box-Cox-Transformation can be further used in the downstream applications.","title":"Time Series Analyzer"},{"location":"using-anovos/data-reports/intermediate_report.html","text":"Generating Intermediate Data for Reports This section largely covers the data pre\u2013processing. The primary function which is used to address all the subsequent modules is charts_to_objects . It precisely helps in saving the chart data in form of objects, which is eventually read by the final report generation script. The objects saved are specifically used at the modules shown at the Report based on the user input. Wide variations of chart are used for showcasing the data trends through Bar Plot, Histogram, Violin Plot, Heat Map, Gauge Chart, Line Chart, etc. Following arguments are specified in the primary function charts_to_objects : spark : Spark session idf : Input Dataframe list_of_cols : This argument, in a list format, is used to specify the columns which are subjected to the analysis in the input dataframe. Alternatively, instead of list, columns can be specified in a single text format where different column names are separated by pipe delimiter \u201c|\u201d. The user can also use \u201call\u201d as an input to this argument to consider all columns. This is super useful instead of specifying all column names manually. drop_cols : This argument, in a list format, is used to specify the columns which needs to be dropped from list_of_cols. Alternatively, instead of list, columns can be specified in a single text format where different column names are separated by pipe delimiter \u201c|\u201d. It is most useful when used coupled with \u201call\u201d value of list_of_cols, when we need to consider all columns except few handful of them. label_col : Name of label or target column in the input dataset. By default, the label_col is set as None to accommodate unsupervised use case. event_label : Value of event (label 1) in the label column. By default, the event_label is kept as 1 unless otherwise specified explicitly. bin_method : equal_frequency or equal_range. The bin method is set as \u201cequal_range\u201d and is being further referred to the attribute binning function where the necessary aggregation / binning is done. bin_size : The maximum number of categories which the user wants to retain is to be set here. By default the size is kept as 10 beyond which remaining records would be grouped under \u201cothers\u201d. coverage : Minimum % of rows mapped to actual category name and rest will be mapped to others. The default value kept is 1.0 which is the maximum at 100%. drift_detector : This argument takes Boolean type input \u2013 True or False. It indicates whether the drift component is already analyzed or not. By default it is kept as False. outlier_charts : This argument takes Boolean type input - True or False. It indicates whether the Outlier Chart needs to be displayed or not. By default it is kept as False. source_path : The source data path which is needed for drift analysis. If it\u2019s not computed / out of scope, the default value of \"NA\" is considered. master_path : The path which will contain the data of intermediate output in terms of json chart objects, csv file (pandas df) stats_unique : Takes arguments for read_dataset (data_ingest module) function in a dictionary format to read pre-saved statistics on unique value count i.e. if measures_of_cardinality or uniqueCount_computation (data_analyzer.stats_generator module) has been computed & saved before. (Default value = {}) run_type : local or EMR. Option to specify whether the execution happen locally or in EMR way as it requires reading & writing to s3. The two form of output generated from this are chart objects and data frame . There are some secondary functions used alongside as a part of charts_to_objects processing.","title":"Intermediate Report"},{"location":"using-anovos/data-reports/intermediate_report.html#generating-intermediate-data-for-reports","text":"This section largely covers the data pre\u2013processing. The primary function which is used to address all the subsequent modules is charts_to_objects . It precisely helps in saving the chart data in form of objects, which is eventually read by the final report generation script. The objects saved are specifically used at the modules shown at the Report based on the user input. Wide variations of chart are used for showcasing the data trends through Bar Plot, Histogram, Violin Plot, Heat Map, Gauge Chart, Line Chart, etc. Following arguments are specified in the primary function charts_to_objects : spark : Spark session idf : Input Dataframe list_of_cols : This argument, in a list format, is used to specify the columns which are subjected to the analysis in the input dataframe. Alternatively, instead of list, columns can be specified in a single text format where different column names are separated by pipe delimiter \u201c|\u201d. The user can also use \u201call\u201d as an input to this argument to consider all columns. This is super useful instead of specifying all column names manually. drop_cols : This argument, in a list format, is used to specify the columns which needs to be dropped from list_of_cols. Alternatively, instead of list, columns can be specified in a single text format where different column names are separated by pipe delimiter \u201c|\u201d. It is most useful when used coupled with \u201call\u201d value of list_of_cols, when we need to consider all columns except few handful of them. label_col : Name of label or target column in the input dataset. By default, the label_col is set as None to accommodate unsupervised use case. event_label : Value of event (label 1) in the label column. By default, the event_label is kept as 1 unless otherwise specified explicitly. bin_method : equal_frequency or equal_range. The bin method is set as \u201cequal_range\u201d and is being further referred to the attribute binning function where the necessary aggregation / binning is done. bin_size : The maximum number of categories which the user wants to retain is to be set here. By default the size is kept as 10 beyond which remaining records would be grouped under \u201cothers\u201d. coverage : Minimum % of rows mapped to actual category name and rest will be mapped to others. The default value kept is 1.0 which is the maximum at 100%. drift_detector : This argument takes Boolean type input \u2013 True or False. It indicates whether the drift component is already analyzed or not. By default it is kept as False. outlier_charts : This argument takes Boolean type input - True or False. It indicates whether the Outlier Chart needs to be displayed or not. By default it is kept as False. source_path : The source data path which is needed for drift analysis. If it\u2019s not computed / out of scope, the default value of \"NA\" is considered. master_path : The path which will contain the data of intermediate output in terms of json chart objects, csv file (pandas df) stats_unique : Takes arguments for read_dataset (data_ingest module) function in a dictionary format to read pre-saved statistics on unique value count i.e. if measures_of_cardinality or uniqueCount_computation (data_analyzer.stats_generator module) has been computed & saved before. (Default value = {}) run_type : local or EMR. Option to specify whether the execution happen locally or in EMR way as it requires reading & writing to s3. The two form of output generated from this are chart objects and data frame . There are some secondary functions used alongside as a part of charts_to_objects processing.","title":"Generating Intermediate Data for Reports"},{"location":"using-anovos/data-reports/overview.html","text":"Creating Data Reports with Anovos The data report module is composed of two sections details of which is described further. The primary utility of keeping the two modules is to decouple the two steps in a way that one happens at a distributed way involving the intermediate report data generation while the other is restricted to only the pre-processing and generation of the Anovos report .","title":"Overview"},{"location":"using-anovos/data-reports/overview.html#creating-data-reports-with-anovos","text":"The data report module is composed of two sections details of which is described further. The primary utility of keeping the two modules is to decouple the two steps in a way that one happens at a distributed way involving the intermediate report data generation while the other is restricted to only the pre-processing and generation of the Anovos report .","title":"Creating Data Reports with Anovos"},{"location":"using-anovos/setting-up/locally.html","text":"Setting up Anovos locally \ud83d\udcbf Software Prerequisites The current Beta release of Anovos requires Spark, Python, and Java to be set up. We test for and officially support the following combinations: Spark 2.4.8 , Python 3.7 , and Java 8 Spark 3.1.3 , Python 3.9 , and Java 11 Spark 3.2.1 , Python 3.9 , and Java 11 The following tutorials can be helpful in setting up Apache Spark: Installing Apache Spark on Mac OSX Installing Apache Spark and using PySpark on Windows \ud83d\udca1 For the foreseeable future, _Anovos will support Spark 2.4.x, 3.1.x, and 3.2.x. _To see which precise combinations we're currently testing, see this workflow configuration . Installing Anovos Anovos can be installed and used in one of two ways: Cloning the GitHub repository and running via spark-submit . Installing through pip and importing it into your own Python scripts. Clone the GitHub repository to use Anovos with spark-submit Clone the Anovos repository to your local environment using the command: git clone https://github.com/anovos/anovos.git For production use, you'll always want to clone a specific version, e.g., git clone -b v0.1.0 --depth 1 https://github.com/anovos/anovos to just get the code for version 0.1.0 . Afterwards, go to the newly created anovos directory and execute the following command to clean and build the latest modules: make clean build Next, install Anovos ' dependencies by running pip install -r requirements.txt and go to the dist/ folder. There, you should Update the input and output paths in configs.yaml and configure the data set. You might also want to adapt the threshold settings to your needs. Adapt the main.py sample script. It demonstrates how different functions from Anovos can be stitched together to create a workflow. If necessary, update spark-submit.sh . This is the shell script used to run the Spark application via spark-submit . Once everything is configured, you can start your workflow run using the aforementioned script: nohup ./spark-submit.sh > run.txt & While the job is running, you can check the logs written to stdout using tail -f run.txt Once the run completes, the script will attempt to automatically open the final report ( report_stats/ml_anovos_report.html ) in your web browser. \ud83d\udc0d Install through pip to use Anovos within your Python applications To install Anovos , simply run pip install anovos Then, you can import Anovos as a module into your Python applications using import anovos To trigger Spark workloads from Python, you have to ensure that the necessary external packages are included in the SparkSession . For this, you can either use the pre-configured SparkSession provided by Anovos : from anovos.shared.spark import spark If you need to use your own custom SparkSession , make sure to include the following dependencies: io.github.histogrammar:histogrammar_2.11:1.0.20 io.github.histogrammar:histogrammar-sparksql_2.11:1.0.20 org.apache.spark:spark-avro_2.11:2.4.0","title":"Locally"},{"location":"using-anovos/setting-up/locally.html#setting-up-anovos-locally","text":"","title":"Setting up Anovos locally"},{"location":"using-anovos/setting-up/locally.html#software-prerequisites","text":"The current Beta release of Anovos requires Spark, Python, and Java to be set up. We test for and officially support the following combinations: Spark 2.4.8 , Python 3.7 , and Java 8 Spark 3.1.3 , Python 3.9 , and Java 11 Spark 3.2.1 , Python 3.9 , and Java 11 The following tutorials can be helpful in setting up Apache Spark: Installing Apache Spark on Mac OSX Installing Apache Spark and using PySpark on Windows \ud83d\udca1 For the foreseeable future, _Anovos will support Spark 2.4.x, 3.1.x, and 3.2.x. _To see which precise combinations we're currently testing, see this workflow configuration .","title":"\ud83d\udcbf Software Prerequisites"},{"location":"using-anovos/setting-up/locally.html#installing-anovos","text":"Anovos can be installed and used in one of two ways: Cloning the GitHub repository and running via spark-submit . Installing through pip and importing it into your own Python scripts.","title":"Installing Anovos"},{"location":"using-anovos/setting-up/locally.html#clone-the-github-repository-to-use-anovos-with-spark-submit","text":"Clone the Anovos repository to your local environment using the command: git clone https://github.com/anovos/anovos.git For production use, you'll always want to clone a specific version, e.g., git clone -b v0.1.0 --depth 1 https://github.com/anovos/anovos to just get the code for version 0.1.0 . Afterwards, go to the newly created anovos directory and execute the following command to clean and build the latest modules: make clean build Next, install Anovos ' dependencies by running pip install -r requirements.txt and go to the dist/ folder. There, you should Update the input and output paths in configs.yaml and configure the data set. You might also want to adapt the threshold settings to your needs. Adapt the main.py sample script. It demonstrates how different functions from Anovos can be stitched together to create a workflow. If necessary, update spark-submit.sh . This is the shell script used to run the Spark application via spark-submit . Once everything is configured, you can start your workflow run using the aforementioned script: nohup ./spark-submit.sh > run.txt & While the job is running, you can check the logs written to stdout using tail -f run.txt Once the run completes, the script will attempt to automatically open the final report ( report_stats/ml_anovos_report.html ) in your web browser.","title":"Clone the GitHub repository to use Anovos with spark-submit"},{"location":"using-anovos/setting-up/locally.html#install-through-pip-to-use-anovos-within-your-python-applications","text":"To install Anovos , simply run pip install anovos Then, you can import Anovos as a module into your Python applications using import anovos To trigger Spark workloads from Python, you have to ensure that the necessary external packages are included in the SparkSession . For this, you can either use the pre-configured SparkSession provided by Anovos : from anovos.shared.spark import spark If you need to use your own custom SparkSession , make sure to include the following dependencies: io.github.histogrammar:histogrammar_2.11:1.0.20 io.github.histogrammar:histogrammar-sparksql_2.11:1.0.20 org.apache.spark:spark-avro_2.11:2.4.0","title":"\ud83d\udc0d Install through pip to use Anovos within your Python applications"},{"location":"using-anovos/setting-up/on_aws.html","text":"Setting up Anovos on AWS EMR For large workloads, you can set up Anovos on AWS EMR . Installing/ Downloading Anovos Clone the Anovos repository on you local environment using command: git clone https://github.com/anovos/anovos.git After cloning, go to the anovos directory and execute the following command to clean and build the latest modules in dist folder: make clean build Copy all required files into an S3 bucket Copy the following files to AWS S3 : dist/anovos.zip This file contains all Anovos modules Zipped version is mandatory for running importing the modules as \u2013py-files dist/income_dataset (optional) This folder contains our demo dataset dist/main.py This is sample script to show how different functions from Anovos module can be stitched together to create a workflow. The users can create their own workflow script by importing the necessary functions. This script takes input from a yaml configuration file dist/configs.yaml This is the sample yaml configuration file that sets the argument for all functions. Update configs.yaml for all input & output s3 paths. All other changes depend on the dataset used. bin/req_packages_anovos.sh This shell script is used to install all required packages to run Anovos on EMR AWS copy command: aws s3 cp --recursive <local file path> <s3 path> --profile <profile name> Create a cluster Software Configuration Emr-5.33.0 Hadoop-2.10.1 Spark-2.4.7 Hive-2.3.7 Spark Submit Details Deploy mode client Spark-submit options --num-executors 1000 --executor-cores 4 --executor-memory 20g --driver-memory 20G --driver-cores 4 --conf spark.driver.maxResultSize=15g --conf spark.yarn.am.memoryOverhead=1000m --conf spark.executor.memoryOverhead=2000m --conf spark.kryo.referenceTracking=false --conf spark.network.timeout=18000s --conf spark.executor.heartbeatInterval=12000s --conf spark.dynamicAllocation.executorIdleTimeout=12000s --conf spark.rpc.message.maxSize=1024 --conf spark.yarn.maxAppAttempts=1 --conf spark.speculation=false --conf spark.kryoserializer.buffer.max=1024 --conf spark.executor.extraJavaOptions=-XX:+UseG1GC --conf spark.driver.extraJavaOptions=-XX:+UseG1GC --packages org.apache.spark:spark-avro_2.11:2.4.0 --jars {s3_bucket}/jars/histogrammar-sparksql_2.11-1.0.20.jar,{s3_bucket}/jars/histogrammar_2.11-1.0.20.jar --py-files {s3_bucket}/{test_folder}/com.zip Application location*: s3 path of main.py file Arguments: , - Bootstrap Actions script location : (bootsrap_shell_scrip_path/{file_name.sh})","title":"On AWS EMR"},{"location":"using-anovos/setting-up/on_aws.html#setting-up-anovos-on-aws-emr","text":"For large workloads, you can set up Anovos on AWS EMR .","title":"Setting up Anovos on AWS EMR"},{"location":"using-anovos/setting-up/on_aws.html#installing-downloading-anovos","text":"Clone the Anovos repository on you local environment using command: git clone https://github.com/anovos/anovos.git After cloning, go to the anovos directory and execute the following command to clean and build the latest modules in dist folder: make clean build","title":"Installing/ Downloading Anovos"},{"location":"using-anovos/setting-up/on_aws.html#copy-all-required-files-into-an-s3-bucket","text":"Copy the following files to AWS S3 : dist/anovos.zip This file contains all Anovos modules Zipped version is mandatory for running importing the modules as \u2013py-files dist/income_dataset (optional) This folder contains our demo dataset dist/main.py This is sample script to show how different functions from Anovos module can be stitched together to create a workflow. The users can create their own workflow script by importing the necessary functions. This script takes input from a yaml configuration file dist/configs.yaml This is the sample yaml configuration file that sets the argument for all functions. Update configs.yaml for all input & output s3 paths. All other changes depend on the dataset used. bin/req_packages_anovos.sh This shell script is used to install all required packages to run Anovos on EMR AWS copy command: aws s3 cp --recursive <local file path> <s3 path> --profile <profile name>","title":"Copy all required files into an S3 bucket"},{"location":"using-anovos/setting-up/on_aws.html#create-a-cluster","text":"Software Configuration Emr-5.33.0 Hadoop-2.10.1 Spark-2.4.7 Hive-2.3.7 Spark Submit Details Deploy mode client Spark-submit options --num-executors 1000 --executor-cores 4 --executor-memory 20g --driver-memory 20G --driver-cores 4 --conf spark.driver.maxResultSize=15g --conf spark.yarn.am.memoryOverhead=1000m --conf spark.executor.memoryOverhead=2000m --conf spark.kryo.referenceTracking=false --conf spark.network.timeout=18000s --conf spark.executor.heartbeatInterval=12000s --conf spark.dynamicAllocation.executorIdleTimeout=12000s --conf spark.rpc.message.maxSize=1024 --conf spark.yarn.maxAppAttempts=1 --conf spark.speculation=false --conf spark.kryoserializer.buffer.max=1024 --conf spark.executor.extraJavaOptions=-XX:+UseG1GC --conf spark.driver.extraJavaOptions=-XX:+UseG1GC --packages org.apache.spark:spark-avro_2.11:2.4.0 --jars {s3_bucket}/jars/histogrammar-sparksql_2.11-1.0.20.jar,{s3_bucket}/jars/histogrammar_2.11-1.0.20.jar --py-files {s3_bucket}/{test_folder}/com.zip Application location*: s3 path of main.py file Arguments: , - Bootstrap Actions script location : (bootsrap_shell_scrip_path/{file_name.sh})","title":"Create a cluster"},{"location":"using-anovos/setting-up/on_azure_databricks.html","text":"Setting up Anovos on Azure Databricks Azure Databricks is a hosted version of Apache Spark on Microsoft Azure . It is a convenient way to handle big data workloads of Spark without having to set up and maintain your own cluster. To learn more about Azure Databricks, have a look at the official documentation or the following introductory tutorials: A beginner\u2019s guide to Azure Databricks Azure Databricks Hands-on Currently, Anovos supports two ways of running workflows on Azure Databricks: Processing datasets stored directly on DBFS Processing datasets stored on Azure Blob Storage Generally, we recommend the first option, as it requires slightly less configuration. However, if you're already storing your datasets on Azure Blob Storage, mounting the respective containers to DBFS allows you to directly process them with Anovos . Anovos on Azure Databricks using DBFS The following steps are required for running Anovos workloads on Azure Databricks that process datasets stored on DBFS. Step 1: Installing Anovos on Azure Databricks To make Anovos available on Azure Databricks, you need to provide access to the Anovos Python package. The easiest way is to point Azure Databricks to the current release of Anovos on the Python Package Index (PyPI) . (This is where pip install anovos goes to fetch Anovos when installing from the terminal.) This has the advantage that you will get a well-tested and stable version of Anovos . Unless you need to make custom modifications to Anovos or need access to new features or bugfixes that have not been released yet, we recommend to go with this option. In this case, you can directly go to Step 2. (We will configure Azure Databricks to retrieve the correct Anovos Python package as part of Step 4.) Alternative: Manually uploading a wheel file Instead of pointing Azure Databricks to the Python Package Index (PyPI), you can make Anovos available by downloading the respective wheel file from PyPI yourself and manually uploading it to Azure Databricks. You'll find the link to the latest wheel file on the \"Download files\" tab . If you'd like to use an older version, you can navigate to the respective version in the Release history and access the \"Download files\" tab from there. Download the Anovos wheel file to your local machine and move on to Step 2. Alternative: Use a development version of Anovos If you would like to try the latest version of Anovos on Azure Databricks (or would like to make custom modifications to the library), you can also create a wheel file yourself. First, clone the Anovos GitHub repository to your local machine: git clone --depth 1 <https://github.com/anovos/anovos.git> Note : Using the --branch flag allows you to select a specific release of Anovos. For example, adding --branch v0.2.2 will give you the state of the 0.2.2 release. If you omit the flag, you will get the latest development version of Anovos, which might not be fully functional or exhibit unexpected behavior. After cloning, go to the anovos directory that was automatically created in the process and execute the following command to clean and prepare the environment: make clean It is a good practice to always run this command prior to generating a wheel file or another kind of build artifact. Note : To be able to create a wheel file, wheel , build , and setuptools need to be installed in the current Python environment. You can do so by running pip install build wheel setuptools . Then, to create the wheel file, run the following command directly inside the anovos folder: python -m build --wheel --outdir dist/ . Once the process is finished, the folder dist will contain the wheel file. It will have the file extension *.whl and might carry the latest version in its name. Note: The version in the file name will be that of the latest version of Anovos, even if you cloned the repository yourself and used the latest state of the code. This is due to the fact that the version is only updated right before new release is published. To avoid confusion, it's a good practice to rename the wheel file to a custom name. Step 2: Prepare and copy the workflow configuration and data to DBFS To run an Anovos workflow, both the data to be processed and the workflow configuration need to be stored on DBFS. You can either use the UI or the CLI to copy files from your local machine to DBFS. For detailed instructions, see the respective subsections below. In this tutorial, we will use \"income dataset\" and an accompanying pre-defined workflow. You can obtain these files by cloning the Anovos GitHub repository: git clone https://github.com/anovos/anovos.git You'll find the dataset under examples/data/income_dataset and the configuration file under config/configs_income_azure.yaml . You'll also need the metric_dictionary.csv file found under data/ . The configs_income_azure.yaml file contains the definition of the Anovos workflow. (To learn more about this file, see \ud83d\udcd6 Configuring Workloads .) First, you should have a look at the configured input paths to make sure that Anovos can find the data to be processed. It is also important to check that the output paths are set to a location on DBFS that suits your needs. For example, in the input_dataset block, you can see that by default the file_path is set to dbfs:/FileStore/tables/income_dataset/csv/ . If you would like to store your data at a different location, you need to adapt this path accordingly. Output paths are defined in several blocks. The output path for the report data is specified as master_path in the blocks report_preprocessing and report_generation . The path for the report is specified as final_report_path in the report_generation block. In this tutorial, by default, all these paths are set to dbfs:/FileStore/tables/report_stats . The location where the processed data is stored is given by file_path in the blocks write_main , write_intermediate , and write_stats . In this tutorial, by default, these are set to sub-folders of dbfs:/FileStore/tables/result . Finally, you need to ensure that the path to the metric_dictionary.csv file as well as the data_dictionary.csv file, which is part of the \"income dataset\", are correctly specified in the report_generation block. You can also make other changes to the workflow. For example, you can define which columns from the input dataset are used in the workflow. To try it yourself, find the delete_column configuration in the input_dataset block and add the column workclass to the list of columns to be deleted: delete_column : [ 'logfnl' , 'workclass' ] To learn more about defining workflows through config files, see \ud83d\udcd6 Configuring Workloads . Once the configs_income_azure.yaml file is complete, you can copy this file and the dataset to DBFS. For this, you can choose to either upload the files through the UI or use the Azure Databricks CLI. We describe both options in the following sections. In any case, make sure that you place the data files in the location defined in the configuration file. You should also remember the location of the configs_income_azure.yaml , as you will need this information in the subsequent steps. (For this tutorial, we have decided to place all files under dbfs:FileStore/tables/ .) Copying files to DBFS using the UI Launch the Azure Databricks workspace. Enter the data menu. Upload files by dragging files onto the marked area or click on it to upload using the file browser. For more detailed instructions, see the Databricks documentation . Copying files to DBFS using the CLI Install databricks-cli into a local Python environment by running pip install databricks-cli . Generate a personal access token for your Databricks workspace by going to Settings > User Settings > Generate new token . For details, see the Databricks documentation . Configure the CLI to access your workspace by running databricks configure --token . Enter the URL of the databricks host (the domain of your workspace, usually of the pattern https://<UNIQUE ID OF YOUR WORKSPACE>.azuredatabricks.net/ ) and the token when prompted for it. To verify the configuration, run databricks fs ls and check whether you are able to see the files stored on DBFS. Then copy the files using the dbfs cp command: For example: dbfs cp anovos/config/configs_income_azure.yaml dbfs:/FileStore/tables/configs_income_azure.yaml dbfs cp anovos/data/data_dictionary.csv dbfs:/FileStore/tables/data_dictionary.csv dbfs cp -r anovos/examples/data/income_dataset dbfs:/FileStore/tables/income_dataset For more information on the Databricks CLI, see the Databricks documentation . Step 3: Create a workflow script To launch the workflow on Azure Databricks, we need a single Python script as the entry point. Hence, we'll create a main.py script that invokes the Anovos' workflow runner: import sys from anovos import workflow workflow . run ( config_path = sys . argv [ 1 ], run_type = \"databricks\" ) Upload this script to DBFS as well using either of the methods described above. Again, you can place this file at a location of your choosing. In this tutorial, we have placed it at dbfs:/FileStore/tables/scripts/main.py . Step 4: Configure and launch an Anovos workflow as a Databricks job There are several types of jobs available on the Azure Databricks platform. For Anovos , the following job types are suitable choices: \"Python:\" The job runs from a single Python script. Anovos and the required Scala dependencies are installed through the respective package repositories. \"Spark Submit:\" The job is invoked through a bare spark-submit call. The installation of Anovos is handled by a cluster initialization script and the required Scala dependencies have to be provided as JAR files through DBFS. Note that there are several limitations for \"Spark Submit\" tasks: You can only run them on new clusters and autoscaling is not available. For more information, see the Databricks documentation on jobs . Unless you require the fine-grained control that this option offers with regard to cluster initialization and spark-submit options, we recommend to select \"Python\" as the job type. Using the \"Python\" job type Once all files have been copied to DBFS, we can create an Azure Databricks job that starts a cluster and launches the Anovos workflow. Here's an example of a job configuration: You can see that we set the \"Type\" to \"Python\" and have provided the path of the main.py script. In the parameters section, we pass the DBFS path of the config file. The cluster configuration comprises settings for the Databricks Runtime, the number of workers, worker and driver types, as well as the cluster's scaling behavior. Here's an example of a cluster configuration for this tutorials: For more detailed information, refer to the Databricks documentation . To give the Databricks platform access to Anovos , click on \"Advanced options\" and select \"Add dependent libraries\". If you chose the default way of obtaining Anovos directly from the Python Package Index, select \"PyPI\" as the \"Library Source\" and enter anovos as the \"Package\": If you chose to provide your own wheel file, select \"Upload\" as the library source and follow the instructions. In addition to the Anovos wheel file, we need to provide the histogrammar package to Azure Databricks. Anovos uses this library internally to compute correlation matrices. Following the same procedure as for Anovos , you can add histogrammar as a dependent library. This time, we use \"Maven\" as the \"Library Source\". Then, select io.github.histogrammar:histogrammar-sparksql_2.12:1.0.20 and io.github.histogrammar:histogrammar_2.12:1.0.20 as the \"Coordinates\": (In case you're running Anovos on Spark 2.4.x, you need to add io.github.histogrammar:histogrammar-sparksql_2.11:1.0.20 and io.github.histogrammar:histogrammar_2.11:1.0.20 ) Once the job is configured, click \"Create\" to instantiate it. Then, you'll see the full task configuration: On the subsequent screen, click on \"Run now\" to launch the job: For more information on creating and maintaining jobs, see the Databricks documentation . Using the \"Spark Submit\" job type Anovos internally uses the histogrammar library to compute correlation matrices. Hence, we need to provide the package to Azure Databricks. As the \"Spark Submit\" job type requires any dependency to be available through DBFS, you first need to upload the histogrammar JAR files to DBFS. If you're using Spark 3.x, download the following files and upload them to DBFS: io.github.histogrammar.histogrammar_2.12-1.0.20.jar io.github.histogrammar.histogrammar-sparksql_2.12-1.0.20.jar If you're using Spark 2.x, download the following files and upload them to DBFS: io.github.histogrammar.histogrammar_2.11-1.0.20.jar io.github.histogrammar.histogrammar-sparksql_2.11-1.0.20.jar Once these files have been uploaded to DBFS, we can create an Azure Databricks job that starts a cluster and launches the Anovos workflow. Here's an example of a job configuration: You can see that we set the \"Type\" to \"Spark Submit\". In the parameters section, we pass the DBFS paths of the histogrammar JAR files, the sample class, the main.py script, and configuration file. For example: [ \"--jars\" , \"dbfs:/FileStore/tables/histogramm_jar/histogrammar_sparksql_2_12_1_0_20.jar,dbfs:/FileStore/tables/histogramm_jar/histogrammar_2_12_1_0_20.jar\" , \"--class\" , \"org.apache.spark.examples.SparkPi\" , \"/dbfs/FileStore/tables/scripts/main.py\" , \"/dbfs/FileStore/tables/configs_income_azure.yaml\" ] The cluster configuration comprises settings for the Databricks Runtime, the number of workers, worker and driver types. (Note that autoscaling is not available for \"Spark Submit\" jobs on Azure Databricks.) Here's an example of a cluster configuration for this tutorial: For more detailed information, refer to the Databricks documentation . To give the Databricks platform access to Anovos , you need to create a shell script that is executed upon cluster initialization and fetches the package from PyPI. The anovos_packages.sh script contains just one line: sudo pip3 install anovos Note that you should specify the version of Anovos if you're running production workloads to ensure reproducibility: sudo pip3 install anovos == 0 .3.0 Place this script on DBFS as well. During cluster configuration, click on \"Advanced options\" and specify the path to the script in the \"Init Script\" section: To enable logging, configure a DBFS path in the \"Log\" section: Once the job is configured, click \"Create\" to instantiate it. On the subsequent screen, click on \"Run now\" to launch the job: For more information on creating and maintaining jobs, see the Azure Databricks documentation . Step 5: Retrieve the output Once the job finishes successfully, it will show up under \"Completed runs\". The intermediate data and the report data are saved at the master_path and the final_report_path specified in the configs_income_azure.yaml file. In this tutorial, we have set these paths to dbfs:FileStore/tables/report_stats/ . To retrieve the HTML report and the report data, you can either go to this path in the UI and copy the files, or use the CLI to copy everything to your local machine: dbfs cp -r dbfs:/FileStore/tables/report_stats/ ./ For more details regarding accessing files on DBFS, see the instructions on uploading files to DBFS in Step 2. Anovos on Azure Databricks using an Azure Blob Storage container mounted to DBFS We're currently working on this section. If you're interested in learning more about this option, let us know!","title":"On Azure Databricks"},{"location":"using-anovos/setting-up/on_azure_databricks.html#setting-up-anovos-on-azure-databricks","text":"Azure Databricks is a hosted version of Apache Spark on Microsoft Azure . It is a convenient way to handle big data workloads of Spark without having to set up and maintain your own cluster. To learn more about Azure Databricks, have a look at the official documentation or the following introductory tutorials: A beginner\u2019s guide to Azure Databricks Azure Databricks Hands-on Currently, Anovos supports two ways of running workflows on Azure Databricks: Processing datasets stored directly on DBFS Processing datasets stored on Azure Blob Storage Generally, we recommend the first option, as it requires slightly less configuration. However, if you're already storing your datasets on Azure Blob Storage, mounting the respective containers to DBFS allows you to directly process them with Anovos .","title":"Setting up Anovos on Azure Databricks"},{"location":"using-anovos/setting-up/on_azure_databricks.html#anovos-on-azure-databricks-using-dbfs","text":"The following steps are required for running Anovos workloads on Azure Databricks that process datasets stored on DBFS.","title":"Anovos on Azure Databricks using DBFS"},{"location":"using-anovos/setting-up/on_azure_databricks.html#step-1-installing-anovos-on-azure-databricks","text":"To make Anovos available on Azure Databricks, you need to provide access to the Anovos Python package. The easiest way is to point Azure Databricks to the current release of Anovos on the Python Package Index (PyPI) . (This is where pip install anovos goes to fetch Anovos when installing from the terminal.) This has the advantage that you will get a well-tested and stable version of Anovos . Unless you need to make custom modifications to Anovos or need access to new features or bugfixes that have not been released yet, we recommend to go with this option. In this case, you can directly go to Step 2. (We will configure Azure Databricks to retrieve the correct Anovos Python package as part of Step 4.)","title":"Step 1: Installing Anovos on Azure Databricks"},{"location":"using-anovos/setting-up/on_azure_databricks.html#alternative-manually-uploading-a-wheel-file","text":"Instead of pointing Azure Databricks to the Python Package Index (PyPI), you can make Anovos available by downloading the respective wheel file from PyPI yourself and manually uploading it to Azure Databricks. You'll find the link to the latest wheel file on the \"Download files\" tab . If you'd like to use an older version, you can navigate to the respective version in the Release history and access the \"Download files\" tab from there. Download the Anovos wheel file to your local machine and move on to Step 2.","title":"Alternative: Manually uploading a wheel file"},{"location":"using-anovos/setting-up/on_azure_databricks.html#alternative-use-a-development-version-of-anovos","text":"If you would like to try the latest version of Anovos on Azure Databricks (or would like to make custom modifications to the library), you can also create a wheel file yourself. First, clone the Anovos GitHub repository to your local machine: git clone --depth 1 <https://github.com/anovos/anovos.git> Note : Using the --branch flag allows you to select a specific release of Anovos. For example, adding --branch v0.2.2 will give you the state of the 0.2.2 release. If you omit the flag, you will get the latest development version of Anovos, which might not be fully functional or exhibit unexpected behavior. After cloning, go to the anovos directory that was automatically created in the process and execute the following command to clean and prepare the environment: make clean It is a good practice to always run this command prior to generating a wheel file or another kind of build artifact. Note : To be able to create a wheel file, wheel , build , and setuptools need to be installed in the current Python environment. You can do so by running pip install build wheel setuptools . Then, to create the wheel file, run the following command directly inside the anovos folder: python -m build --wheel --outdir dist/ . Once the process is finished, the folder dist will contain the wheel file. It will have the file extension *.whl and might carry the latest version in its name. Note: The version in the file name will be that of the latest version of Anovos, even if you cloned the repository yourself and used the latest state of the code. This is due to the fact that the version is only updated right before new release is published. To avoid confusion, it's a good practice to rename the wheel file to a custom name.","title":"Alternative: Use a development version of Anovos"},{"location":"using-anovos/setting-up/on_azure_databricks.html#step-2-prepare-and-copy-the-workflow-configuration-and-data-to-dbfs","text":"To run an Anovos workflow, both the data to be processed and the workflow configuration need to be stored on DBFS. You can either use the UI or the CLI to copy files from your local machine to DBFS. For detailed instructions, see the respective subsections below. In this tutorial, we will use \"income dataset\" and an accompanying pre-defined workflow. You can obtain these files by cloning the Anovos GitHub repository: git clone https://github.com/anovos/anovos.git You'll find the dataset under examples/data/income_dataset and the configuration file under config/configs_income_azure.yaml . You'll also need the metric_dictionary.csv file found under data/ . The configs_income_azure.yaml file contains the definition of the Anovos workflow. (To learn more about this file, see \ud83d\udcd6 Configuring Workloads .) First, you should have a look at the configured input paths to make sure that Anovos can find the data to be processed. It is also important to check that the output paths are set to a location on DBFS that suits your needs. For example, in the input_dataset block, you can see that by default the file_path is set to dbfs:/FileStore/tables/income_dataset/csv/ . If you would like to store your data at a different location, you need to adapt this path accordingly. Output paths are defined in several blocks. The output path for the report data is specified as master_path in the blocks report_preprocessing and report_generation . The path for the report is specified as final_report_path in the report_generation block. In this tutorial, by default, all these paths are set to dbfs:/FileStore/tables/report_stats . The location where the processed data is stored is given by file_path in the blocks write_main , write_intermediate , and write_stats . In this tutorial, by default, these are set to sub-folders of dbfs:/FileStore/tables/result . Finally, you need to ensure that the path to the metric_dictionary.csv file as well as the data_dictionary.csv file, which is part of the \"income dataset\", are correctly specified in the report_generation block. You can also make other changes to the workflow. For example, you can define which columns from the input dataset are used in the workflow. To try it yourself, find the delete_column configuration in the input_dataset block and add the column workclass to the list of columns to be deleted: delete_column : [ 'logfnl' , 'workclass' ] To learn more about defining workflows through config files, see \ud83d\udcd6 Configuring Workloads . Once the configs_income_azure.yaml file is complete, you can copy this file and the dataset to DBFS. For this, you can choose to either upload the files through the UI or use the Azure Databricks CLI. We describe both options in the following sections. In any case, make sure that you place the data files in the location defined in the configuration file. You should also remember the location of the configs_income_azure.yaml , as you will need this information in the subsequent steps. (For this tutorial, we have decided to place all files under dbfs:FileStore/tables/ .)","title":"Step 2: Prepare and copy the workflow configuration and data to DBFS"},{"location":"using-anovos/setting-up/on_azure_databricks.html#copying-files-to-dbfs-using-the-ui","text":"Launch the Azure Databricks workspace. Enter the data menu. Upload files by dragging files onto the marked area or click on it to upload using the file browser. For more detailed instructions, see the Databricks documentation .","title":"Copying files to DBFS using the UI"},{"location":"using-anovos/setting-up/on_azure_databricks.html#copying-files-to-dbfs-using-the-cli","text":"Install databricks-cli into a local Python environment by running pip install databricks-cli . Generate a personal access token for your Databricks workspace by going to Settings > User Settings > Generate new token . For details, see the Databricks documentation . Configure the CLI to access your workspace by running databricks configure --token . Enter the URL of the databricks host (the domain of your workspace, usually of the pattern https://<UNIQUE ID OF YOUR WORKSPACE>.azuredatabricks.net/ ) and the token when prompted for it. To verify the configuration, run databricks fs ls and check whether you are able to see the files stored on DBFS. Then copy the files using the dbfs cp command: For example: dbfs cp anovos/config/configs_income_azure.yaml dbfs:/FileStore/tables/configs_income_azure.yaml dbfs cp anovos/data/data_dictionary.csv dbfs:/FileStore/tables/data_dictionary.csv dbfs cp -r anovos/examples/data/income_dataset dbfs:/FileStore/tables/income_dataset For more information on the Databricks CLI, see the Databricks documentation .","title":"Copying files to DBFS using the CLI"},{"location":"using-anovos/setting-up/on_azure_databricks.html#step-3-create-a-workflow-script","text":"To launch the workflow on Azure Databricks, we need a single Python script as the entry point. Hence, we'll create a main.py script that invokes the Anovos' workflow runner: import sys from anovos import workflow workflow . run ( config_path = sys . argv [ 1 ], run_type = \"databricks\" ) Upload this script to DBFS as well using either of the methods described above. Again, you can place this file at a location of your choosing. In this tutorial, we have placed it at dbfs:/FileStore/tables/scripts/main.py .","title":"Step 3: Create a workflow script"},{"location":"using-anovos/setting-up/on_azure_databricks.html#step-4-configure-and-launch-an-anovos-workflow-as-a-databricks-job","text":"There are several types of jobs available on the Azure Databricks platform. For Anovos , the following job types are suitable choices: \"Python:\" The job runs from a single Python script. Anovos and the required Scala dependencies are installed through the respective package repositories. \"Spark Submit:\" The job is invoked through a bare spark-submit call. The installation of Anovos is handled by a cluster initialization script and the required Scala dependencies have to be provided as JAR files through DBFS. Note that there are several limitations for \"Spark Submit\" tasks: You can only run them on new clusters and autoscaling is not available. For more information, see the Databricks documentation on jobs . Unless you require the fine-grained control that this option offers with regard to cluster initialization and spark-submit options, we recommend to select \"Python\" as the job type.","title":"Step 4: Configure and launch an Anovos workflow as a Databricks job"},{"location":"using-anovos/setting-up/on_azure_databricks.html#using-the-python-job-type","text":"Once all files have been copied to DBFS, we can create an Azure Databricks job that starts a cluster and launches the Anovos workflow. Here's an example of a job configuration: You can see that we set the \"Type\" to \"Python\" and have provided the path of the main.py script. In the parameters section, we pass the DBFS path of the config file. The cluster configuration comprises settings for the Databricks Runtime, the number of workers, worker and driver types, as well as the cluster's scaling behavior. Here's an example of a cluster configuration for this tutorials: For more detailed information, refer to the Databricks documentation . To give the Databricks platform access to Anovos , click on \"Advanced options\" and select \"Add dependent libraries\". If you chose the default way of obtaining Anovos directly from the Python Package Index, select \"PyPI\" as the \"Library Source\" and enter anovos as the \"Package\": If you chose to provide your own wheel file, select \"Upload\" as the library source and follow the instructions. In addition to the Anovos wheel file, we need to provide the histogrammar package to Azure Databricks. Anovos uses this library internally to compute correlation matrices. Following the same procedure as for Anovos , you can add histogrammar as a dependent library. This time, we use \"Maven\" as the \"Library Source\". Then, select io.github.histogrammar:histogrammar-sparksql_2.12:1.0.20 and io.github.histogrammar:histogrammar_2.12:1.0.20 as the \"Coordinates\": (In case you're running Anovos on Spark 2.4.x, you need to add io.github.histogrammar:histogrammar-sparksql_2.11:1.0.20 and io.github.histogrammar:histogrammar_2.11:1.0.20 ) Once the job is configured, click \"Create\" to instantiate it. Then, you'll see the full task configuration: On the subsequent screen, click on \"Run now\" to launch the job: For more information on creating and maintaining jobs, see the Databricks documentation .","title":"Using the \"Python\" job type"},{"location":"using-anovos/setting-up/on_azure_databricks.html#using-the-spark-submit-job-type","text":"Anovos internally uses the histogrammar library to compute correlation matrices. Hence, we need to provide the package to Azure Databricks. As the \"Spark Submit\" job type requires any dependency to be available through DBFS, you first need to upload the histogrammar JAR files to DBFS. If you're using Spark 3.x, download the following files and upload them to DBFS: io.github.histogrammar.histogrammar_2.12-1.0.20.jar io.github.histogrammar.histogrammar-sparksql_2.12-1.0.20.jar If you're using Spark 2.x, download the following files and upload them to DBFS: io.github.histogrammar.histogrammar_2.11-1.0.20.jar io.github.histogrammar.histogrammar-sparksql_2.11-1.0.20.jar Once these files have been uploaded to DBFS, we can create an Azure Databricks job that starts a cluster and launches the Anovos workflow. Here's an example of a job configuration: You can see that we set the \"Type\" to \"Spark Submit\". In the parameters section, we pass the DBFS paths of the histogrammar JAR files, the sample class, the main.py script, and configuration file. For example: [ \"--jars\" , \"dbfs:/FileStore/tables/histogramm_jar/histogrammar_sparksql_2_12_1_0_20.jar,dbfs:/FileStore/tables/histogramm_jar/histogrammar_2_12_1_0_20.jar\" , \"--class\" , \"org.apache.spark.examples.SparkPi\" , \"/dbfs/FileStore/tables/scripts/main.py\" , \"/dbfs/FileStore/tables/configs_income_azure.yaml\" ] The cluster configuration comprises settings for the Databricks Runtime, the number of workers, worker and driver types. (Note that autoscaling is not available for \"Spark Submit\" jobs on Azure Databricks.) Here's an example of a cluster configuration for this tutorial: For more detailed information, refer to the Databricks documentation . To give the Databricks platform access to Anovos , you need to create a shell script that is executed upon cluster initialization and fetches the package from PyPI. The anovos_packages.sh script contains just one line: sudo pip3 install anovos Note that you should specify the version of Anovos if you're running production workloads to ensure reproducibility: sudo pip3 install anovos == 0 .3.0 Place this script on DBFS as well. During cluster configuration, click on \"Advanced options\" and specify the path to the script in the \"Init Script\" section: To enable logging, configure a DBFS path in the \"Log\" section: Once the job is configured, click \"Create\" to instantiate it. On the subsequent screen, click on \"Run now\" to launch the job: For more information on creating and maintaining jobs, see the Azure Databricks documentation .","title":"Using the \"Spark Submit\" job type"},{"location":"using-anovos/setting-up/on_azure_databricks.html#step-5-retrieve-the-output","text":"Once the job finishes successfully, it will show up under \"Completed runs\". The intermediate data and the report data are saved at the master_path and the final_report_path specified in the configs_income_azure.yaml file. In this tutorial, we have set these paths to dbfs:FileStore/tables/report_stats/ . To retrieve the HTML report and the report data, you can either go to this path in the UI and copy the files, or use the CLI to copy everything to your local machine: dbfs cp -r dbfs:/FileStore/tables/report_stats/ ./ For more details regarding accessing files on DBFS, see the instructions on uploading files to DBFS in Step 2.","title":"Step 5: Retrieve the output"},{"location":"using-anovos/setting-up/on_azure_databricks.html#anovos-on-azure-databricks-using-an-azure-blob-storage-container-mounted-to-dbfs","text":"We're currently working on this section. If you're interested in learning more about this option, let us know!","title":"Anovos on Azure Databricks using an Azure Blob Storage container mounted to DBFS"},{"location":"using-anovos/setting-up/on_google_colab.html","text":"Setting Up Anovos on Google Colab Colab is an offer by Google Research that provides access to cloud-hosted Jupyter notebooks for collaborating on and sharing data science work. Colab offers substantial compute resources even in its free tier and is integrated with Google Drive , making it an excellent place to explore libraries like Anovos without setting up anything on your local machine. If you're not yet familiar with Google Colab, the following selection of introductory tutorials are an excellent starting point to familiarize yourself with this platform: Google Colab \u2014 The Beginner\u2019s Guide (Lean In Women In Tech India) How to use Google Colab (GeeksforGeeks) Google Colab Tutorial for Data Scientists (Datacamp) Step-by-step Instructions for Using Anovos on Google Colab The following four steps will guide you through the entire setup of Anovos on Google Colab. The instructions assume that you're starting out with a fresh, empty notebook environment. Step 1: Installing Spark dependencies Anovos builds on Apache Spark , which is not available by default in Google Colab. Hence, before we can start working Anovos , we need to install Spark and set up a Spark environment. Since Spark is a Java application, we start out by installing the Java Development Kit: !apt-get install openjdk-8-jdk-headless -qq > /dev/null Then, we can download Spark: !wget https://archive.apache.org/dist/spark/spark-2.4.8/spark-2.4.8-bin-hadoop2.7.tgz Note : In this tutorial, we use Java 8 and Spark 2.4.8. You can use more recent versions as well. See the list of currently supported versions to learn about available options. Next, unzip the downloaded Spark archive to the current folder: !tar xf spark-2.4.8-bin-hadoop2.7.tgz Now we'll let the Colab notebook know where Java and Spark can be found by setting the corresponding environment variables: import os os . environ [ \"JAVA_HOME\" ] = \"/usr/lib/jvm/java-8-openjdk-amd64\" os . environ [ \"SPARK_HOME\" ] = \"/content/spark-2.4.8-bin-hadoop2.7\" To access Spark through Python, we need the pyspark library as well as the findspark utility: !pip install findspark pyspark == 2 .4.8 Note : Make sure that the version of pyspark matches the Spark versions you downloaded. Step 2: Installing Anovos and its dependencies Clone the Anovos GitHub repository to Google Colab: !git clone --branch v0.2.2 https://github.com/anovos/anovos.git Note : Using the --branch flag allows you to select the desired release of Anovos. If you omit the flag, you will get the latest development version of Anovos, which might not be fully functional or exhibit unexpected behavior. After cloning, let's enter the newly created Anovos directory: cd anovos As indicated by the output shown, Anovos was placed in the folder /content/anovos , which you can also access through the sidebar: The next step is to build Anovos : !make clean build As the final step before we can start working with Anovos , we need to install the required Python dependencies: !pip install -r requirements.txt Step 3: Configuring an Anovos workflow Anovos workflows are configured through a YAML configuration file. To learn more, have a look at the exhaustive Configuring Workflows documentation. But don't worry: We'll guide you through the necessary steps! First, open the file viewer in the sidebar and download the configs.yaml file from the dist folder by right-clicking on the file and selecting Download : After downloading the configs.yaml file, you can now adapt the workflow it describes to your needs. For example, you can define which columns from the input dataset are used in the workflow. To try it yourself, find the delete_column configuration in the input_dataset block and add the column workclass to the list of columns to be deleted: input_dataset : ... delete_column : [ 'logfnl' , 'workclass' ] ... You can learn more about this and all other configuration options in the Configuring Workflows documentation. Each configuration block is associated with one of the various Anovos modules and functions . Once you adapted the configs.yaml file, you can upload it again by right-clicking on the dist folder and selecting Upload : Step 4: Trigger a workflow run Once the workflow configuration has been uploaded, you can run your workflow. Anovos workflows are triggered by executing the spark-submit.sh file that you'll find in the dist folder. This script contains the configuration for the Spark executor. To change the number of executors, the executor's memory, driver memory, and other parameters, you can edit this file. For example, in case of a very large dataset of several GB in size, you might want to allocate more memory to the Anovos workflow. Let's go ahead and change the executor memory from the pre-defined 20g to 32g : spark-submit \\ ... --executor-memory 32g \\ ... To make this or any other change, you need to download and upload the spark-submit.sh file similarly to the configs.yaml file as described in the previous section. Once the adapted spark-submit.sh has been uploaded, we can trigger the Anovos workflow run by entering the dist directory and running spark-submit.sh : cd dist !nohup ./spark-submit.sh > run.txt & The nohup command together with the & at the end of line ensures that the workflow is executed in the background, allowing us to continue working in the Colab notebook. To see what your workflow is doing, have a look at run.txt , where all logs are collected: !tail -f run.txt Once the run completes, the reports generated by Anovos and all intermediate outputs are stored at the specified path. The intermediate data and the report data are saved at the master_path and the final_report_path as specified by the user inside the configs.yaml file. By default, these are set to report_stats and you should find all output files in this folder: !cd report_stats !ls -l To view the HTML report, you'll have to download the basic_report.html file to your local machine, using the same steps you took to download the configs.yaml and spark-submit.sh files. What's next? In this tutorial, you've learned the basics of running Anovos workflows on Google Colab. To learn all about the different modules and functions of Anovos , have a look at the API documentation . The Configuring Workflows documentation contains a complete list of all possible configuration options.","title":"On Google Colab"},{"location":"using-anovos/setting-up/on_google_colab.html#setting-up-anovos-on-google-colab","text":"Colab is an offer by Google Research that provides access to cloud-hosted Jupyter notebooks for collaborating on and sharing data science work. Colab offers substantial compute resources even in its free tier and is integrated with Google Drive , making it an excellent place to explore libraries like Anovos without setting up anything on your local machine. If you're not yet familiar with Google Colab, the following selection of introductory tutorials are an excellent starting point to familiarize yourself with this platform: Google Colab \u2014 The Beginner\u2019s Guide (Lean In Women In Tech India) How to use Google Colab (GeeksforGeeks) Google Colab Tutorial for Data Scientists (Datacamp)","title":"Setting Up Anovos on Google Colab"},{"location":"using-anovos/setting-up/on_google_colab.html#step-by-step-instructions-for-using-anovos-on-google-colab","text":"The following four steps will guide you through the entire setup of Anovos on Google Colab. The instructions assume that you're starting out with a fresh, empty notebook environment.","title":"Step-by-step Instructions for Using Anovos on Google Colab"},{"location":"using-anovos/setting-up/on_google_colab.html#step-1-installing-spark-dependencies","text":"Anovos builds on Apache Spark , which is not available by default in Google Colab. Hence, before we can start working Anovos , we need to install Spark and set up a Spark environment. Since Spark is a Java application, we start out by installing the Java Development Kit: !apt-get install openjdk-8-jdk-headless -qq > /dev/null Then, we can download Spark: !wget https://archive.apache.org/dist/spark/spark-2.4.8/spark-2.4.8-bin-hadoop2.7.tgz Note : In this tutorial, we use Java 8 and Spark 2.4.8. You can use more recent versions as well. See the list of currently supported versions to learn about available options. Next, unzip the downloaded Spark archive to the current folder: !tar xf spark-2.4.8-bin-hadoop2.7.tgz Now we'll let the Colab notebook know where Java and Spark can be found by setting the corresponding environment variables: import os os . environ [ \"JAVA_HOME\" ] = \"/usr/lib/jvm/java-8-openjdk-amd64\" os . environ [ \"SPARK_HOME\" ] = \"/content/spark-2.4.8-bin-hadoop2.7\" To access Spark through Python, we need the pyspark library as well as the findspark utility: !pip install findspark pyspark == 2 .4.8 Note : Make sure that the version of pyspark matches the Spark versions you downloaded.","title":"Step 1: Installing Spark dependencies"},{"location":"using-anovos/setting-up/on_google_colab.html#step-2-installing-anovos-and-its-dependencies","text":"Clone the Anovos GitHub repository to Google Colab: !git clone --branch v0.2.2 https://github.com/anovos/anovos.git Note : Using the --branch flag allows you to select the desired release of Anovos. If you omit the flag, you will get the latest development version of Anovos, which might not be fully functional or exhibit unexpected behavior. After cloning, let's enter the newly created Anovos directory: cd anovos As indicated by the output shown, Anovos was placed in the folder /content/anovos , which you can also access through the sidebar: The next step is to build Anovos : !make clean build As the final step before we can start working with Anovos , we need to install the required Python dependencies: !pip install -r requirements.txt","title":"Step 2: Installing Anovos and its dependencies"},{"location":"using-anovos/setting-up/on_google_colab.html#step-3-configuring-an-anovos-workflow","text":"Anovos workflows are configured through a YAML configuration file. To learn more, have a look at the exhaustive Configuring Workflows documentation. But don't worry: We'll guide you through the necessary steps! First, open the file viewer in the sidebar and download the configs.yaml file from the dist folder by right-clicking on the file and selecting Download : After downloading the configs.yaml file, you can now adapt the workflow it describes to your needs. For example, you can define which columns from the input dataset are used in the workflow. To try it yourself, find the delete_column configuration in the input_dataset block and add the column workclass to the list of columns to be deleted: input_dataset : ... delete_column : [ 'logfnl' , 'workclass' ] ... You can learn more about this and all other configuration options in the Configuring Workflows documentation. Each configuration block is associated with one of the various Anovos modules and functions . Once you adapted the configs.yaml file, you can upload it again by right-clicking on the dist folder and selecting Upload :","title":"Step 3: Configuring an Anovos workflow"},{"location":"using-anovos/setting-up/on_google_colab.html#step-4-trigger-a-workflow-run","text":"Once the workflow configuration has been uploaded, you can run your workflow. Anovos workflows are triggered by executing the spark-submit.sh file that you'll find in the dist folder. This script contains the configuration for the Spark executor. To change the number of executors, the executor's memory, driver memory, and other parameters, you can edit this file. For example, in case of a very large dataset of several GB in size, you might want to allocate more memory to the Anovos workflow. Let's go ahead and change the executor memory from the pre-defined 20g to 32g : spark-submit \\ ... --executor-memory 32g \\ ... To make this or any other change, you need to download and upload the spark-submit.sh file similarly to the configs.yaml file as described in the previous section. Once the adapted spark-submit.sh has been uploaded, we can trigger the Anovos workflow run by entering the dist directory and running spark-submit.sh : cd dist !nohup ./spark-submit.sh > run.txt & The nohup command together with the & at the end of line ensures that the workflow is executed in the background, allowing us to continue working in the Colab notebook. To see what your workflow is doing, have a look at run.txt , where all logs are collected: !tail -f run.txt Once the run completes, the reports generated by Anovos and all intermediate outputs are stored at the specified path. The intermediate data and the report data are saved at the master_path and the final_report_path as specified by the user inside the configs.yaml file. By default, these are set to report_stats and you should find all output files in this folder: !cd report_stats !ls -l To view the HTML report, you'll have to download the basic_report.html file to your local machine, using the same steps you took to download the configs.yaml and spark-submit.sh files.","title":"Step 4: Trigger a workflow run"},{"location":"using-anovos/setting-up/on_google_colab.html#whats-next","text":"In this tutorial, you've learned the basics of running Anovos workflows on Google Colab. To learn all about the different modules and functions of Anovos , have a look at the API documentation . The Configuring Workflows documentation contains a complete list of all possible configuration options.","title":"What's next?"},{"location":"using-anovos/setting-up/spark_cluster.html","text":"Setting up to run Anovos workloads on your Spark cluster COMING SOON Let us know if you're interested in learning more!","title":"On a Spark cluster"},{"location":"using-anovos/setting-up/spark_cluster.html#setting-up-to-run-anovos-workloads-on-your-spark-cluster","text":"COMING SOON Let us know if you're interested in learning more!","title":"Setting up to run Anovos workloads on your Spark cluster"}]}